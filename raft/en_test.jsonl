{"id":"b7edcd1f-a0a6-4871-8e07-2ffc91d24060","type":"general","question":"1. What is the goal of statistical language modeling?  ","context":{"sentences":[["The direct architecture was found about 2% better \nthan the cycling architecture. Conditional mixture helps but even without it the neural net is better. On Brown, the \nbest neural net without the mixture yields a test perplexity of 265, the smoothed trigram \nyields 348, and their conditional mixture yields 258 (i.e., better than both). On Hansard \nthe improvement is less: a neural network yielding 46.7 perplexity, mixed with the trigram \n(54.1), yields a mixture with perplexity 45.1. 5 Conclusions and Proposed Extensions \nThe experiments on two corpora, a medium one 0.2 million words), and a large one (34 \nmillion words) have shown that the proposed approach yields much better perplexity than \na state-of-the-art method, the smoothed trigram, with differences on the order of 20% to \n35%. We believe that the main reason for these improvements is that the proposed approach \nallows to take advantage of the learned distributed representation to fight the curse of di\u00ad\nmensionality with its own weapons: each training sentence informs the model about a \ncombinatorial number of other sentences. Note that if we had a separate feature vector \nfor each \"context\" (short sequence of words), the model would have much more capacity \n(which could grow like that of n-grams) but it would not naturally generalize between the \nmany different ways a word can be used. A more reasonable alternative would be to ex\u00ad\nplore language units other than words (e.g. some short word sequences , or alternatively \nsome sub-word morphemic units). There is probably much more to be done to improve the model, at the level of architecture, \ncomputational efficiency, and taking advantage of prior knowledge . An important priority \nof future research should be to evaluate and improve the speeding-up tricks proposed here, \nand find ways to increase capacity without increasing training time too much (to deal with corpora with hundreds of millions of words). A simple idea to take advantage of temporal \nstructure and extend the size of the input window to include possibly a whole paragraph, \nwithout increasing too much the number of parameters, is to use a time-delay and possibly \nrecurrent neural network. In such a multi-layered network the computation that has been \nperformed for small groups of consecutive words does not need to be redone when the \nnetwork input window is shifted. Similarly , one could use a recurrent network to capture \npotentially even longer term information about the subject of the text. A very important area in which the proposed model could be improved is in the use of prior \nlinguistic knowledge : semantic (e.g. Word Net), syntactic (e.g.","A Neural Probabilistic Language Model \nYoshua Bengio; Rejean Ducharme and Pascal Vincent \nDepartement d'Informatique et Recherche Operationnelle \nCentre de Recherche Mathematiques \nUniversite de Montreal \nMontreal, Quebec, Canada, H3C 317 \n{bengioy,ducharme, vincentp }@iro.umontreal.ca \nAbstract \nA goal of statistical language modeling is to learn the joint probability \nfunction of sequences of words. This is intrinsically difficult because of \nthe curse of dimensionality: we propose to fight it with its own weapons.","a tagger), and morpho\u00ad\nlogical (radix and morphemes). Looking at the word features learned by the model should \nhelp understand it and improve it. Finally, future research should establish how useful the \nproposed approach will be in applications to speech recognition, language translation, and \ninformation retrieval. Acknowledgments \nThe authors would like to thank Leon Bottou and Yann Le Cun for useful discussions.","The benchmark against which the neural network was compared is an interpolated or \nsmoothed trigram model [6]. Let qt = l(Jreq(Wt-l,Wt-2)) represent the discretized fre\u00ad\nquency of occurrence of the context (Wt-l, Wt-2) (we used l(x) = r -log((l + x)\/T)l \nwhere x is the frequency of occurrence of the context and T is the size of the training cor\u00ad\npus). A conditional mixture of the trigram, bigram, unigram and zero-gram was learned on \nthe validation set, with mixture weights conditional on discretized frequency. Below are measures of test set perplexity (geometric average of 1\/ p( Wt Iwi-1) for dif\u00ad\nferent models P. Apparent convergence of the stochastic gradient descent procedure was \nobtained after around 10 epochs for Hansard and after about 50 epochs for Brown, with \na learning rate gradually decreased from approximately 10-3 to 10-5. Weight decay of \n10-4 or 10-5 was used in all the experiments (based on a few experiments compared on \nthe validation set). The main result is that the neural network performs much better than the smoothed trigram. On Brown the best neural network system, according to validation perplexity \n(among different architectures tried, see below) yielded a perplexity of 258, while the \nsmoothed trigram yields a perplexity of 348, which is about 35% worse. This is obtained \nusing a network with the direct architecture mixed with the trigram (conditional mixture), \nwith 30 word features initialized with the SVD method, 40 hidden units, and n = 5 words \nof context. On Hansard, the corresponding figures are 44.8 for the neural network and 54.1 \nfor the smoothed trigram, which is 20.7% worse. This is obtained with a network with the \ndirect architecture, 100 randomly initialized words features, 120 hidden units, and n = 8 \nwords of context."]],"title":[["placeholder_title","placeholder_title","placeholder_title","placeholder_title"]]},"oracle_context":"A Neural Probabilistic Language Model \nYoshua Bengio; Rejean Ducharme and Pascal Vincent \nDepartement d'Informatique et Recherche Operationnelle \nCentre de Recherche Mathematiques \nUniversite de Montreal \nMontreal, Quebec, Canada, H3C 317 \n{bengioy,ducharme, vincentp }@iro.umontreal.ca \nAbstract \nA goal of statistical language modeling is to learn the joint probability \nfunction of sequences of words. This is intrinsically difficult because of \nthe curse of dimensionality: we propose to fight it with its own weapons.","cot_answer":"To answer the question about the goal of statistical language modeling, I will follow these steps:\n\n1. Identify the main objective of statistical language modeling as stated in the context.\n2. Look for any specific challenges mentioned that relate to achieving this goal.\n3. Summarize the findings succinctly.\n\nFrom the context, I can see that the goal of statistical language modeling is to learn the joint probability function of sequences of words. This is a complex task due to the \"curse of dimensionality,\" which suggests that as the number of dimensions (or words) increases, the amount of data needed to make accurate predictions grows exponentially.\n\nNow, I will extract the relevant sentence from the context to support my answer.\n\n##begin_quote##\n\"A goal of statistical language modeling is to learn the joint probability function of sequences of words.\"\n##end_quote##\n\nBased on this information, I can conclude that the primary goal of statistical language modeling is to understand and predict the likelihood of word sequences.\n\n<ANSWER>: The goal of statistical language modeling is to learn the joint probability function of sequences of words.","instruction":"<DOCUMENT>The direct architecture was found about 2% better \nthan the cycling architecture. Conditional mixture helps but even without it the neural net is better. On Brown, the \nbest neural net without the mixture yields a test perplexity of 265, the smoothed trigram \nyields 348, and their conditional mixture yields 258 (i.e., better than both). On Hansard \nthe improvement is less: a neural network yielding 46.7 perplexity, mixed with the trigram \n(54.1), yields a mixture with perplexity 45.1. 5 Conclusions and Proposed Extensions \nThe experiments on two corpora, a medium one 0.2 million words), and a large one (34 \nmillion words) have shown that the proposed approach yields much better perplexity than \na state-of-the-art method, the smoothed trigram, with differences on the order of 20% to \n35%. We believe that the main reason for these improvements is that the proposed approach \nallows to take advantage of the learned distributed representation to fight the curse of di\u00ad\nmensionality with its own weapons: each training sentence informs the model about a \ncombinatorial number of other sentences. Note that if we had a separate feature vector \nfor each \"context\" (short sequence of words), the model would have much more capacity \n(which could grow like that of n-grams) but it would not naturally generalize between the \nmany different ways a word can be used. A more reasonable alternative would be to ex\u00ad\nplore language units other than words (e.g. some short word sequences , or alternatively \nsome sub-word morphemic units). There is probably much more to be done to improve the model, at the level of architecture, \ncomputational efficiency, and taking advantage of prior knowledge . An important priority \nof future research should be to evaluate and improve the speeding-up tricks proposed here, \nand find ways to increase capacity without increasing training time too much (to deal with corpora with hundreds of millions of words). A simple idea to take advantage of temporal \nstructure and extend the size of the input window to include possibly a whole paragraph, \nwithout increasing too much the number of parameters, is to use a time-delay and possibly \nrecurrent neural network. In such a multi-layered network the computation that has been \nperformed for small groups of consecutive words does not need to be redone when the \nnetwork input window is shifted. Similarly , one could use a recurrent network to capture \npotentially even longer term information about the subject of the text. A very important area in which the proposed model could be improved is in the use of prior \nlinguistic knowledge : semantic (e.g. Word Net), syntactic (e.g.<\/DOCUMENT>\n<DOCUMENT>A Neural Probabilistic Language Model \nYoshua Bengio; Rejean Ducharme and Pascal Vincent \nDepartement d'Informatique et Recherche Operationnelle \nCentre de Recherche Mathematiques \nUniversite de Montreal \nMontreal, Quebec, Canada, H3C 317 \n{bengioy,ducharme, vincentp }@iro.umontreal.ca \nAbstract \nA goal of statistical language modeling is to learn the joint probability \nfunction of sequences of words. This is intrinsically difficult because of \nthe curse of dimensionality: we propose to fight it with its own weapons.<\/DOCUMENT>\n<DOCUMENT>a tagger), and morpho\u00ad\nlogical (radix and morphemes). Looking at the word features learned by the model should \nhelp understand it and improve it. Finally, future research should establish how useful the \nproposed approach will be in applications to speech recognition, language translation, and \ninformation retrieval. Acknowledgments \nThe authors would like to thank Leon Bottou and Yann Le Cun for useful discussions.<\/DOCUMENT>\n<DOCUMENT>The benchmark against which the neural network was compared is an interpolated or \nsmoothed trigram model [6]. Let qt = l(Jreq(Wt-l,Wt-2)) represent the discretized fre\u00ad\nquency of occurrence of the context (Wt-l, Wt-2) (we used l(x) = r -log((l + x)\/T)l \nwhere x is the frequency of occurrence of the context and T is the size of the training cor\u00ad\npus). A conditional mixture of the trigram, bigram, unigram and zero-gram was learned on \nthe validation set, with mixture weights conditional on discretized frequency. Below are measures of test set perplexity (geometric average of 1\/ p( Wt Iwi-1) for dif\u00ad\nferent models P. Apparent convergence of the stochastic gradient descent procedure was \nobtained after around 10 epochs for Hansard and after about 50 epochs for Brown, with \na learning rate gradually decreased from approximately 10-3 to 10-5. Weight decay of \n10-4 or 10-5 was used in all the experiments (based on a few experiments compared on \nthe validation set). The main result is that the neural network performs much better than the smoothed trigram. On Brown the best neural network system, according to validation perplexity \n(among different architectures tried, see below) yielded a perplexity of 258, while the \nsmoothed trigram yields a perplexity of 348, which is about 35% worse. This is obtained \nusing a network with the direct architecture mixed with the trigram (conditional mixture), \nwith 30 word features initialized with the SVD method, 40 hidden units, and n = 5 words \nof context. On Hansard, the corresponding figures are 44.8 for the neural network and 54.1 \nfor the smoothed trigram, which is 20.7% worse. This is obtained with a network with the \ndirect architecture, 100 randomly initialized words features, 120 hidden units, and n = 8 \nwords of context.<\/DOCUMENT>\n1. What is the goal of statistical language modeling?  "}
{"id":"d7b04a6d-27e0-475a-b5db-925f0cd043dc","type":"general","question":"2. Who are the authors of the paper?  ","context":{"sentences":[["Refinements to speed it up and extend it \nwill be described in the following sections. We decompose the function f (Wt, ... , Wt-n) = \nP( Wt Iwi-i) in two parts: \n1. A mapping C from any element of V to a real vector C(i) E Rm. It represents \nthe \"distributed feature vector\" associated with each word in the vocabulary . In \npractice, C is represented by a I V I x m matrix (of free parameters). 2. The probability function over words, expressed with C. We have considered two \nalternative formulations : \n(a) The direct architecture : a function 9 maps a sequence of feature \nvectors for words in context (C(Wt-n),\u00b7\u00b7 \u00b7, C(wt-d) to a probabil\u00ad\nity distribution over words in V. It is a vector function whose i-th \nelement estimates the probability P(Wt = ilwi-i) as in figure 1.","The direct architecture was found about 2% better \nthan the cycling architecture. Conditional mixture helps but even without it the neural net is better. On Brown, the \nbest neural net without the mixture yields a test perplexity of 265, the smoothed trigram \nyields 348, and their conditional mixture yields 258 (i.e., better than both). On Hansard \nthe improvement is less: a neural network yielding 46.7 perplexity, mixed with the trigram \n(54.1), yields a mixture with perplexity 45.1. 5 Conclusions and Proposed Extensions \nThe experiments on two corpora, a medium one 0.2 million words), and a large one (34 \nmillion words) have shown that the proposed approach yields much better perplexity than \na state-of-the-art method, the smoothed trigram, with differences on the order of 20% to \n35%. We believe that the main reason for these improvements is that the proposed approach \nallows to take advantage of the learned distributed representation to fight the curse of di\u00ad\nmensionality with its own weapons: each training sentence informs the model about a \ncombinatorial number of other sentences. Note that if we had a separate feature vector \nfor each \"context\" (short sequence of words), the model would have much more capacity \n(which could grow like that of n-grams) but it would not naturally generalize between the \nmany different ways a word can be used. A more reasonable alternative would be to ex\u00ad\nplore language units other than words (e.g. some short word sequences , or alternatively \nsome sub-word morphemic units). There is probably much more to be done to improve the model, at the level of architecture, \ncomputational efficiency, and taking advantage of prior knowledge . An important priority \nof future research should be to evaluate and improve the speeding-up tricks proposed here, \nand find ways to increase capacity without increasing training time too much (to deal with corpora with hundreds of millions of words). A simple idea to take advantage of temporal \nstructure and extend the size of the input window to include possibly a whole paragraph, \nwithout increasing too much the number of parameters, is to use a time-delay and possibly \nrecurrent neural network. In such a multi-layered network the computation that has been \nperformed for small groups of consecutive words does not need to be redone when the \nnetwork input window is shifted. Similarly , one could use a recurrent network to capture \npotentially even longer term information about the subject of the text. A very important area in which the proposed model could be improved is in the use of prior \nlinguistic knowledge : semantic (e.g. Word Net), syntactic (e.g.","A Neural Probabilistic Language Model \nYoshua Bengio; Rejean Ducharme and Pascal Vincent \nDepartement d'Informatique et Recherche Operationnelle \nCentre de Recherche Mathematiques \nUniversite de Montreal \nMontreal, Quebec, Canada, H3C 317 \n{bengioy,ducharme, vincentp }@iro.umontreal.ca \nAbstract \nA goal of statistical language modeling is to learn the joint probability \nfunction of sequences of words. This is intrinsically difficult because of \nthe curse of dimensionality: we propose to fight it with its own weapons.","Hinton. Learning distributed representations of concepts. In Proceedings of the Eighth An\u00ad\nnual Conference of the Cognitive Science Society, pages 1-12, Amherst 1986, 1986. Lawrence \nErlbaum, Hillsdale."]],"title":[["placeholder_title","placeholder_title","placeholder_title","placeholder_title"]]},"oracle_context":"A Neural Probabilistic Language Model \nYoshua Bengio; Rejean Ducharme and Pascal Vincent \nDepartement d'Informatique et Recherche Operationnelle \nCentre de Recherche Mathematiques \nUniversite de Montreal \nMontreal, Quebec, Canada, H3C 317 \n{bengioy,ducharme, vincentp }@iro.umontreal.ca \nAbstract \nA goal of statistical language modeling is to learn the joint probability \nfunction of sequences of words. This is intrinsically difficult because of \nthe curse of dimensionality: we propose to fight it with its own weapons.","cot_answer":"To answer the question about the authors of the paper, I will follow these steps:\n\n1. Identify the relevant section in the context that lists the authors.\n2. Extract the names of the authors from that section.\n3. Present the names in a clear and concise manner.\n\nFrom the context provided, I see the following information regarding the authors:\n\n##begin_quote##\n\"Yoshua Bengio; Rejean Ducharme and Pascal Vincent\"\n##end_quote##\n\nNow, I will compile the names into a succinct format.\n\n<ANSWER>: Yoshua Bengio, Rejean Ducharme, Pascal Vincent","instruction":"<DOCUMENT>Refinements to speed it up and extend it \nwill be described in the following sections. We decompose the function f (Wt, ... , Wt-n) = \nP( Wt Iwi-i) in two parts: \n1. A mapping C from any element of V to a real vector C(i) E Rm. It represents \nthe \"distributed feature vector\" associated with each word in the vocabulary . In \npractice, C is represented by a I V I x m matrix (of free parameters). 2. The probability function over words, expressed with C. We have considered two \nalternative formulations : \n(a) The direct architecture : a function 9 maps a sequence of feature \nvectors for words in context (C(Wt-n),\u00b7\u00b7 \u00b7, C(wt-d) to a probabil\u00ad\nity distribution over words in V. It is a vector function whose i-th \nelement estimates the probability P(Wt = ilwi-i) as in figure 1.<\/DOCUMENT>\n<DOCUMENT>The direct architecture was found about 2% better \nthan the cycling architecture. Conditional mixture helps but even without it the neural net is better. On Brown, the \nbest neural net without the mixture yields a test perplexity of 265, the smoothed trigram \nyields 348, and their conditional mixture yields 258 (i.e., better than both). On Hansard \nthe improvement is less: a neural network yielding 46.7 perplexity, mixed with the trigram \n(54.1), yields a mixture with perplexity 45.1. 5 Conclusions and Proposed Extensions \nThe experiments on two corpora, a medium one 0.2 million words), and a large one (34 \nmillion words) have shown that the proposed approach yields much better perplexity than \na state-of-the-art method, the smoothed trigram, with differences on the order of 20% to \n35%. We believe that the main reason for these improvements is that the proposed approach \nallows to take advantage of the learned distributed representation to fight the curse of di\u00ad\nmensionality with its own weapons: each training sentence informs the model about a \ncombinatorial number of other sentences. Note that if we had a separate feature vector \nfor each \"context\" (short sequence of words), the model would have much more capacity \n(which could grow like that of n-grams) but it would not naturally generalize between the \nmany different ways a word can be used. A more reasonable alternative would be to ex\u00ad\nplore language units other than words (e.g. some short word sequences , or alternatively \nsome sub-word morphemic units). There is probably much more to be done to improve the model, at the level of architecture, \ncomputational efficiency, and taking advantage of prior knowledge . An important priority \nof future research should be to evaluate and improve the speeding-up tricks proposed here, \nand find ways to increase capacity without increasing training time too much (to deal with corpora with hundreds of millions of words). A simple idea to take advantage of temporal \nstructure and extend the size of the input window to include possibly a whole paragraph, \nwithout increasing too much the number of parameters, is to use a time-delay and possibly \nrecurrent neural network. In such a multi-layered network the computation that has been \nperformed for small groups of consecutive words does not need to be redone when the \nnetwork input window is shifted. Similarly , one could use a recurrent network to capture \npotentially even longer term information about the subject of the text. A very important area in which the proposed model could be improved is in the use of prior \nlinguistic knowledge : semantic (e.g. Word Net), syntactic (e.g.<\/DOCUMENT>\n<DOCUMENT>A Neural Probabilistic Language Model \nYoshua Bengio; Rejean Ducharme and Pascal Vincent \nDepartement d'Informatique et Recherche Operationnelle \nCentre de Recherche Mathematiques \nUniversite de Montreal \nMontreal, Quebec, Canada, H3C 317 \n{bengioy,ducharme, vincentp }@iro.umontreal.ca \nAbstract \nA goal of statistical language modeling is to learn the joint probability \nfunction of sequences of words. This is intrinsically difficult because of \nthe curse of dimensionality: we propose to fight it with its own weapons.<\/DOCUMENT>\n<DOCUMENT>Hinton. Learning distributed representations of concepts. In Proceedings of the Eighth An\u00ad\nnual Conference of the Cognitive Science Society, pages 1-12, Amherst 1986, 1986. Lawrence \nErlbaum, Hillsdale.<\/DOCUMENT>\n2. Who are the authors of the paper?  "}
{"id":"c369c4e2-edda-4bfc-8b47-929f1c4ef6e0","type":"general","question":"3. Where is the research conducted?  ","context":{"sentences":[["Hinton. Extracting distributed representations of concepts and relations \nfrom positive and negative propositions. In Proceedings of the International Joint Conference \non Neural Network, lJCNN'2000, Como, Italy, 2000.","Katz. Estimation of probabilities from sparse data for the language model component \nof a speech recognizer . IEEE Transactions on Acoustics , Speech, and Signal Processing, ASSP-\n35(3):400-401 , March 1987.","A Neural Probabilistic Language Model \nYoshua Bengio; Rejean Ducharme and Pascal Vincent \nDepartement d'Informatique et Recherche Operationnelle \nCentre de Recherche Mathematiques \nUniversite de Montreal \nMontreal, Quebec, Canada, H3C 317 \n{bengioy,ducharme, vincentp }@iro.umontreal.ca \nAbstract \nA goal of statistical language modeling is to learn the joint probability \nfunction of sequences of words. This is intrinsically difficult because of \nthe curse of dimensionality: we propose to fight it with its own weapons.","f(i, Wt-i,\u00b7\u00b7\u00b7, Wt-n) = g(i, C(Wt-i),\u00b7 \u00b7\u00b7, C(Wt-n)). We used the \"soft\u00ad\nmax\" in the output layer of a neural net: P( Wt = ilwi-i) = ehi \/ Ej eh;, \nwhere hi is the neural network output score for word i. (b) The cycling architecture: a function h maps a sequence of feature \nvectors (C(Wt-n),\u00b7\u00b7\u00b7, C(Wt-i), C(i)) (i.e. including the context words \nand a candidate next word i) to a scalar hi, and again using a soft-\nmax, P(Wt = ilwi-i) = ehi\/Ejeh;. f(Wt,Wt-i,\u00b7 \u00b7\u00b7,Wt-n) = \ng(C(Wt), C(wt-d,\u00b7\u00b7\u00b7 ,C(Wt-n)). We call this architecture \"cycling\" be\u00ad\ncause one repeatedly runs h (e.g. a neural net), each time putting in input the \nfeature vector C(i) for a candidate next word i. The function f is a composition of these two mappings (C and g), with C being shared \nacross all the words in the context. To each of these two parts are associated some pa\u00ad\nrameters. The parameters of the mapping C are simply the feature vectors themselves \n(represented by a IVI x m matrix C whose row i is the feature vector C(i) for word i). The \nfunction 9 may be implemented by a feed-forward or recurrent neural network or another \nparameterized function, with parameters (). Table \nlook-up \ninC \nindex fOl wi-n i-ill output = P(Wt = i I eMtext) \nindex fot W'-2 index fot Wt_l : computed only \nfor WOlds in \nshOlt list \nFigure 1: \"Direct Architecture\": f(i, Wt-l,\u00b7\", Wt-n) = g(i, C(Wt-l),\u00b7\u00b7\u00b7, C(Wt-n)) \nwhere 9 is the neural network and C(i) is the i-th word feature vector. Training is achieved by looking for ((), C) that maximize the training corpus penalized log\u00ad\nlikelihood : L = ~ ~t logpw. (C(Wt-n),\u00b7\u00b7\u00b7, C(Wt-l)j ()) + R((), C), where R((), C) is a \nregularization term (e.g. a weight decay ).11()112, that penalizes slightly the norm of ()."]],"title":[["placeholder_title","placeholder_title","placeholder_title","placeholder_title"]]},"oracle_context":"A Neural Probabilistic Language Model \nYoshua Bengio; Rejean Ducharme and Pascal Vincent \nDepartement d'Informatique et Recherche Operationnelle \nCentre de Recherche Mathematiques \nUniversite de Montreal \nMontreal, Quebec, Canada, H3C 317 \n{bengioy,ducharme, vincentp }@iro.umontreal.ca \nAbstract \nA goal of statistical language modeling is to learn the joint probability \nfunction of sequences of words. This is intrinsically difficult because of \nthe curse of dimensionality: we propose to fight it with its own weapons.","cot_answer":"To answer the question \"Where is the research conducted?\", I will follow these steps:\n\n1. Identify the relevant information in the context that indicates the location of the research.\n2. Look for any specific mention of institutions, cities, or countries that could provide the answer.\n3. Extract the relevant sentences that contain this information.\n\nUpon reviewing the context, I find the following relevant information:\n\n##begin_quote##\n\"Departement d'Informatique et Recherche Operationnelle \nCentre de Recherche Mathematiques \nUniversite de Montreal \nMontreal, Quebec, Canada, H3C 317\"\n##end_quote##\n\nThis clearly indicates that the research is conducted at the Universite de Montreal, located in Montreal, Quebec, Canada.\n\nFinal answer: <ANSWER>: Montreal, Quebec, Canada.","instruction":"<DOCUMENT>Hinton. Extracting distributed representations of concepts and relations \nfrom positive and negative propositions. In Proceedings of the International Joint Conference \non Neural Network, lJCNN'2000, Como, Italy, 2000.<\/DOCUMENT>\n<DOCUMENT>Katz. Estimation of probabilities from sparse data for the language model component \nof a speech recognizer . IEEE Transactions on Acoustics , Speech, and Signal Processing, ASSP-\n35(3):400-401 , March 1987.<\/DOCUMENT>\n<DOCUMENT>A Neural Probabilistic Language Model \nYoshua Bengio; Rejean Ducharme and Pascal Vincent \nDepartement d'Informatique et Recherche Operationnelle \nCentre de Recherche Mathematiques \nUniversite de Montreal \nMontreal, Quebec, Canada, H3C 317 \n{bengioy,ducharme, vincentp }@iro.umontreal.ca \nAbstract \nA goal of statistical language modeling is to learn the joint probability \nfunction of sequences of words. This is intrinsically difficult because of \nthe curse of dimensionality: we propose to fight it with its own weapons.<\/DOCUMENT>\n<DOCUMENT>f(i, Wt-i,\u00b7\u00b7\u00b7, Wt-n) = g(i, C(Wt-i),\u00b7 \u00b7\u00b7, C(Wt-n)). We used the \"soft\u00ad\nmax\" in the output layer of a neural net: P( Wt = ilwi-i) = ehi \/ Ej eh;, \nwhere hi is the neural network output score for word i. (b) The cycling architecture: a function h maps a sequence of feature \nvectors (C(Wt-n),\u00b7\u00b7\u00b7, C(Wt-i), C(i)) (i.e. including the context words \nand a candidate next word i) to a scalar hi, and again using a soft-\nmax, P(Wt = ilwi-i) = ehi\/Ejeh;. f(Wt,Wt-i,\u00b7 \u00b7\u00b7,Wt-n) = \ng(C(Wt), C(wt-d,\u00b7\u00b7\u00b7 ,C(Wt-n)). We call this architecture \"cycling\" be\u00ad\ncause one repeatedly runs h (e.g. a neural net), each time putting in input the \nfeature vector C(i) for a candidate next word i. The function f is a composition of these two mappings (C and g), with C being shared \nacross all the words in the context. To each of these two parts are associated some pa\u00ad\nrameters. The parameters of the mapping C are simply the feature vectors themselves \n(represented by a IVI x m matrix C whose row i is the feature vector C(i) for word i). The \nfunction 9 may be implemented by a feed-forward or recurrent neural network or another \nparameterized function, with parameters (). Table \nlook-up \ninC \nindex fOl wi-n i-ill output = P(Wt = i I eMtext) \nindex fot W'-2 index fot Wt_l : computed only \nfor WOlds in \nshOlt list \nFigure 1: \"Direct Architecture\": f(i, Wt-l,\u00b7\", Wt-n) = g(i, C(Wt-l),\u00b7\u00b7\u00b7, C(Wt-n)) \nwhere 9 is the neural network and C(i) is the i-th word feature vector. Training is achieved by looking for ((), C) that maximize the training corpus penalized log\u00ad\nlikelihood : L = ~ ~t logpw. (C(Wt-n),\u00b7\u00b7\u00b7, C(Wt-l)j ()) + R((), C), where R((), C) is a \nregularization term (e.g. a weight decay ).11()112, that penalizes slightly the norm of ().<\/DOCUMENT>\n3. Where is the research conducted?  "}
{"id":"4645277a-f137-4244-aec0-c30686b63f64","type":"general","question":"4. What challenge does statistical language modeling face?  ","context":{"sentences":[["More context is useful. Experiments with the cycling architecture on Brown, with 30 \nword features, and 30 hidden units, varying the number of context words: n = 1 (like the \nbigram) yields a test perplexity of 302, n = 3 yields 291, n = 5 yields 281, n = 8 yields \n279 (N.B. the smoothed trigram yields 348). Hidden units help. Experiments with the direct architecture on Brown (with direct input \nto output connections), with 30 word features, 5 words of context, varying the number of \nhidden units: 0 yields a test perplexity of 275, 10 yields 267, 20 yields 266, 40 yields 265, \n80 yields 265. Learning the word features jointly is important. Experiments with the direct architec\u00ad\nture on Brown (40 hidden units, 5 words of context), in which the word features initialized \nwith the SVD method are kept fixed during training yield a test perplexity of 345.8 whereas \nif the word features are trained jointly with the rest of the parameters, the perplexity is 265. Initialization not so useful. Experiments on Brown with both architectures reveal that the \nSVD initialization of the word features does not bring much improvement with respect to \nrandom initialization : it speeds up initial convergence (saving about 2 epochs), and yields \na perplexity improvement of less than 0.3%. Direct architecture works a bit better.","A Neural Probabilistic Language Model \nYoshua Bengio; Rejean Ducharme and Pascal Vincent \nDepartement d'Informatique et Recherche Operationnelle \nCentre de Recherche Mathematiques \nUniversite de Montreal \nMontreal, Quebec, Canada, H3C 317 \n{bengioy,ducharme, vincentp }@iro.umontreal.ca \nAbstract \nA goal of statistical language modeling is to learn the joint probability \nfunction of sequences of words. This is intrinsically difficult because of \nthe curse of dimensionality: we propose to fight it with its own weapons.","Since we have millions of examples, it is important to con\u00ad\nverge within only a few passes through the data. For very large data sets, stochastic gradient \ndescent convergence time seems to increase sub-linearly with the size of the data set (see \nexperiments on Brown vs Hansard below). To speed up training using stochastic gradient descent, we have found it useful to break the corpus in paragraphs and to randomly permute \nthem. In this way, some of the non-stationarity in the word stream is eliminated, yielding \nfaster convergence. Capacity control. For the \"smaller corpora\" like Brown (1.2 million examples), we have \nfound early stopping and weight decay useful to avoid over-fitting. For the larger corpora, \nour networks still under-fit. For the larger corpora, we have found double-precision com\u00ad\nputation to be very important to obtain good results. Mixture of models. We have found improved performance by combining the probability \npredictions of the neural network with those of the smoothed trigram, with weights that \nwere conditional on the frequency of the context (same procedure used to combine trigram, \nbigram, and unigram in the smoothed trigram). Initialization of word feature vectors. We have tried both random initialization (uniform \nbetween -.01 and .01) and a \"smarter\" method based on a Singular Value Decomposition \n(SVD) of a very large matrix of \"context features\". These context features are formed \nby counting the frequency of occurrence of each word in each one of the most frequent \ncontexts (word sequences) in the corpus. The idea is that \"similar\" words should occur \nwith similar frequency in the same contexts. We used about 9000 most frequent contexts, \nand compressed these to 30 features with the SVD. Out-of-vocabulary words. For an out-of-vocabulary word Wt we need to come up with \na feature vector in order to predict the words that follow, or predict its probability (that \nis only possible with the cycling architecture). We used as feature vector the weighted \naverage feature vector of all the words in the short list, with the weights being the relative \nprobabilities ofthose words: E[C(wt)lhtl = Ei C(i)P(wt = ilht). 4 Experimental Results \nComparative experiments were performed on the Brown and Hansard corpora. The Brown \ncorpus is a stream of 1,181,041 words (from a large variety of English texts and books). The first 800,000 words were used for training, the following 200,000 for validation (model \nselection, weight decay, early stopping) and the remaining 181,041 for testing. The number \nof different words is 47, 578 (including punctuation, distinguishing between upper and \nlower case, and including the syntactical marks used to separate texts and paragraphs). Rare words with frequency:::; 3 were merged into a single token, reducing the vocabulary \nsize to IVI = 16,383. The Hansard corpus (Canadian parliament proceedings, French version) is a stream of \nabout 34 million words, of which 32 millions (set A) was used for training, 1.1 million \n(set B) was used for validation, and 1.2 million (set C) was used for out-of-sample tests. The original data has 106, 936 different words, and those with frequency:::; 10 were merged \ninto a single token, yielding IVI = 30,959 different words.","It is particularly obvious in the case when one wants to \nmodel the joint distribution between many discrete random variables (such as words in a \nsentence, or discrete attributes in a data-mining task). For example, if one wants to model \nthe joint distribution of 10 consecutive words in a natural language with a vocabulary V of \nsize 100,000, there are potentially 100000 10 -1 = 1050 -1 free parameters. A statistical model of language can be represented by the conditional probability of the \nnext word given all the previous ones in the sequence, since P( W'[) = rri=l P( Wt Iwf-1), \nwhere Wt is the t-th word, and writing subsequence w[ = (Wi, Wi+1, ... , Wj-1, Wj). When building statistical models of natural language, one reduces the difficulty by taking \nadvantage of word order, and the fact that temporally closer words in the word sequence are \nstatistically more dependent. Thus, n-gram models construct tables of conditional proba\u00ad\nbilities for the next word, for each one of a large number of contexts, i.e. combinations of \nthe last n -1 words: p(wtlwf-1) ~ P(WtIW!=~+l)' Only those combinations of succes\u00ad\nsive words that actually occur in the training corpus (or that occur frequently enough) are \nconsidered. What happens when a new combination of n words appears that was not seen \nin the training corpus? A simple answer is to look at the probability predicted using smaller \ncontext size, as done in back -off trigram models [7] or in smoothed (or interpolated) trigram \nmodels [6]. So, in such models, how is generalization basically obtained from sequences of \n\"Y.B. was also with AT&T Research while doing this research. words seen in the training corpus to new sequences of words? simply by looking at a short \nenough context, i.e., the probability for a long sequence of words is obtained by \"gluing\" \nvery short pieces of length 1, 2 or 3 words that have been seen frequently enough in the \ntraining data. Obviously there is much more information in the sequence that precedes the \nword to predict than just the identity of the previous couple of words. There are at least two \nobvious flaws in this approach (which however has turned out to be very difficult to beat): \nfirst it is not taking into account contexts farther than 1 or 2 words, second it is not taking \naccount of the \"similarity\" between words. For example, having seen the sentence The \ncat is walking in the bedroom in the training corpus should help us general\u00ad\nize to make the sentence A dog was running in a room almost as likely, simply \nbecause \"dog\" and \"cat\" (resp. \"the\" and \"a\", \"room\" and \"bedroom\", etc ..."]],"title":[["placeholder_title","placeholder_title","placeholder_title","placeholder_title"]]},"oracle_context":"A Neural Probabilistic Language Model \nYoshua Bengio; Rejean Ducharme and Pascal Vincent \nDepartement d'Informatique et Recherche Operationnelle \nCentre de Recherche Mathematiques \nUniversite de Montreal \nMontreal, Quebec, Canada, H3C 317 \n{bengioy,ducharme, vincentp }@iro.umontreal.ca \nAbstract \nA goal of statistical language modeling is to learn the joint probability \nfunction of sequences of words. This is intrinsically difficult because of \nthe curse of dimensionality: we propose to fight it with its own weapons.","cot_answer":"To answer the question about the challenge faced by statistical language modeling, I will follow these steps:\n\n1. Identify the main topic of the context provided, which is about statistical language modeling.\n2. Look for specific challenges mentioned in the context.\n3. Extract relevant sentences that describe the challenge and present them clearly.\n\nFrom the context, it is stated that \"A goal of statistical language modeling is to learn the joint probability function of sequences of words.\" This indicates that the task involves understanding complex relationships between words in sequences.\n\nThe challenge is further elaborated with the phrase \"This is intrinsically difficult because of the curse of dimensionality.\" This suggests that the complexity of the problem increases significantly as the number of dimensions (or possible word sequences) grows, making it hard to model effectively.\n\nNow, I will compile this information into a succinct answer.\n\n<ANSWER>: The challenge faced by statistical language modeling is the \"curse of dimensionality,\" which makes learning the joint probability function of sequences of words intrinsically difficult.","instruction":"<DOCUMENT>More context is useful. Experiments with the cycling architecture on Brown, with 30 \nword features, and 30 hidden units, varying the number of context words: n = 1 (like the \nbigram) yields a test perplexity of 302, n = 3 yields 291, n = 5 yields 281, n = 8 yields \n279 (N.B. the smoothed trigram yields 348). Hidden units help. Experiments with the direct architecture on Brown (with direct input \nto output connections), with 30 word features, 5 words of context, varying the number of \nhidden units: 0 yields a test perplexity of 275, 10 yields 267, 20 yields 266, 40 yields 265, \n80 yields 265. Learning the word features jointly is important. Experiments with the direct architec\u00ad\nture on Brown (40 hidden units, 5 words of context), in which the word features initialized \nwith the SVD method are kept fixed during training yield a test perplexity of 345.8 whereas \nif the word features are trained jointly with the rest of the parameters, the perplexity is 265. Initialization not so useful. Experiments on Brown with both architectures reveal that the \nSVD initialization of the word features does not bring much improvement with respect to \nrandom initialization : it speeds up initial convergence (saving about 2 epochs), and yields \na perplexity improvement of less than 0.3%. Direct architecture works a bit better.<\/DOCUMENT>\n<DOCUMENT>A Neural Probabilistic Language Model \nYoshua Bengio; Rejean Ducharme and Pascal Vincent \nDepartement d'Informatique et Recherche Operationnelle \nCentre de Recherche Mathematiques \nUniversite de Montreal \nMontreal, Quebec, Canada, H3C 317 \n{bengioy,ducharme, vincentp }@iro.umontreal.ca \nAbstract \nA goal of statistical language modeling is to learn the joint probability \nfunction of sequences of words. This is intrinsically difficult because of \nthe curse of dimensionality: we propose to fight it with its own weapons.<\/DOCUMENT>\n<DOCUMENT>Since we have millions of examples, it is important to con\u00ad\nverge within only a few passes through the data. For very large data sets, stochastic gradient \ndescent convergence time seems to increase sub-linearly with the size of the data set (see \nexperiments on Brown vs Hansard below). To speed up training using stochastic gradient descent, we have found it useful to break the corpus in paragraphs and to randomly permute \nthem. In this way, some of the non-stationarity in the word stream is eliminated, yielding \nfaster convergence. Capacity control. For the \"smaller corpora\" like Brown (1.2 million examples), we have \nfound early stopping and weight decay useful to avoid over-fitting. For the larger corpora, \nour networks still under-fit. For the larger corpora, we have found double-precision com\u00ad\nputation to be very important to obtain good results. Mixture of models. We have found improved performance by combining the probability \npredictions of the neural network with those of the smoothed trigram, with weights that \nwere conditional on the frequency of the context (same procedure used to combine trigram, \nbigram, and unigram in the smoothed trigram). Initialization of word feature vectors. We have tried both random initialization (uniform \nbetween -.01 and .01) and a \"smarter\" method based on a Singular Value Decomposition \n(SVD) of a very large matrix of \"context features\". These context features are formed \nby counting the frequency of occurrence of each word in each one of the most frequent \ncontexts (word sequences) in the corpus. The idea is that \"similar\" words should occur \nwith similar frequency in the same contexts. We used about 9000 most frequent contexts, \nand compressed these to 30 features with the SVD. Out-of-vocabulary words. For an out-of-vocabulary word Wt we need to come up with \na feature vector in order to predict the words that follow, or predict its probability (that \nis only possible with the cycling architecture). We used as feature vector the weighted \naverage feature vector of all the words in the short list, with the weights being the relative \nprobabilities ofthose words: E[C(wt)lhtl = Ei C(i)P(wt = ilht). 4 Experimental Results \nComparative experiments were performed on the Brown and Hansard corpora. The Brown \ncorpus is a stream of 1,181,041 words (from a large variety of English texts and books). The first 800,000 words were used for training, the following 200,000 for validation (model \nselection, weight decay, early stopping) and the remaining 181,041 for testing. The number \nof different words is 47, 578 (including punctuation, distinguishing between upper and \nlower case, and including the syntactical marks used to separate texts and paragraphs). Rare words with frequency:::; 3 were merged into a single token, reducing the vocabulary \nsize to IVI = 16,383. The Hansard corpus (Canadian parliament proceedings, French version) is a stream of \nabout 34 million words, of which 32 millions (set A) was used for training, 1.1 million \n(set B) was used for validation, and 1.2 million (set C) was used for out-of-sample tests. The original data has 106, 936 different words, and those with frequency:::; 10 were merged \ninto a single token, yielding IVI = 30,959 different words.<\/DOCUMENT>\n<DOCUMENT>It is particularly obvious in the case when one wants to \nmodel the joint distribution between many discrete random variables (such as words in a \nsentence, or discrete attributes in a data-mining task). For example, if one wants to model \nthe joint distribution of 10 consecutive words in a natural language with a vocabulary V of \nsize 100,000, there are potentially 100000 10 -1 = 1050 -1 free parameters. A statistical model of language can be represented by the conditional probability of the \nnext word given all the previous ones in the sequence, since P( W'[) = rri=l P( Wt Iwf-1), \nwhere Wt is the t-th word, and writing subsequence w[ = (Wi, Wi+1, ... , Wj-1, Wj). When building statistical models of natural language, one reduces the difficulty by taking \nadvantage of word order, and the fact that temporally closer words in the word sequence are \nstatistically more dependent. Thus, n-gram models construct tables of conditional proba\u00ad\nbilities for the next word, for each one of a large number of contexts, i.e. combinations of \nthe last n -1 words: p(wtlwf-1) ~ P(WtIW!=~+l)' Only those combinations of succes\u00ad\nsive words that actually occur in the training corpus (or that occur frequently enough) are \nconsidered. What happens when a new combination of n words appears that was not seen \nin the training corpus? A simple answer is to look at the probability predicted using smaller \ncontext size, as done in back -off trigram models [7] or in smoothed (or interpolated) trigram \nmodels [6]. So, in such models, how is generalization basically obtained from sequences of \n\"Y.B. was also with AT&T Research while doing this research. words seen in the training corpus to new sequences of words? simply by looking at a short \nenough context, i.e., the probability for a long sequence of words is obtained by \"gluing\" \nvery short pieces of length 1, 2 or 3 words that have been seen frequently enough in the \ntraining data. Obviously there is much more information in the sequence that precedes the \nword to predict than just the identity of the previous couple of words. There are at least two \nobvious flaws in this approach (which however has turned out to be very difficult to beat): \nfirst it is not taking into account contexts farther than 1 or 2 words, second it is not taking \naccount of the \"similarity\" between words. For example, having seen the sentence The \ncat is walking in the bedroom in the training corpus should help us general\u00ad\nize to make the sentence A dog was running in a room almost as likely, simply \nbecause \"dog\" and \"cat\" (resp. \"the\" and \"a\", \"room\" and \"bedroom\", etc ...<\/DOCUMENT>\n4. What challenge does statistical language modeling face?  "}
{"id":"a4ebbb72-0ccb-4edb-adc7-a7bdd1bffd99","type":"general","question":"5. How do the authors propose to address the curse of dimensionality?  ","context":{"sentences":[["f(i, Wt-i,\u00b7\u00b7\u00b7, Wt-n) = g(i, C(Wt-i),\u00b7 \u00b7\u00b7, C(Wt-n)). We used the \"soft\u00ad\nmax\" in the output layer of a neural net: P( Wt = ilwi-i) = ehi \/ Ej eh;, \nwhere hi is the neural network output score for word i. (b) The cycling architecture: a function h maps a sequence of feature \nvectors (C(Wt-n),\u00b7\u00b7\u00b7, C(Wt-i), C(i)) (i.e. including the context words \nand a candidate next word i) to a scalar hi, and again using a soft-\nmax, P(Wt = ilwi-i) = ehi\/Ejeh;. f(Wt,Wt-i,\u00b7 \u00b7\u00b7,Wt-n) = \ng(C(Wt), C(wt-d,\u00b7\u00b7\u00b7 ,C(Wt-n)). We call this architecture \"cycling\" be\u00ad\ncause one repeatedly runs h (e.g. a neural net), each time putting in input the \nfeature vector C(i) for a candidate next word i. The function f is a composition of these two mappings (C and g), with C being shared \nacross all the words in the context. To each of these two parts are associated some pa\u00ad\nrameters. The parameters of the mapping C are simply the feature vectors themselves \n(represented by a IVI x m matrix C whose row i is the feature vector C(i) for word i). The \nfunction 9 may be implemented by a feed-forward or recurrent neural network or another \nparameterized function, with parameters (). Table \nlook-up \ninC \nindex fOl wi-n i-ill output = P(Wt = i I eMtext) \nindex fot W'-2 index fot Wt_l : computed only \nfor WOlds in \nshOlt list \nFigure 1: \"Direct Architecture\": f(i, Wt-l,\u00b7\", Wt-n) = g(i, C(Wt-l),\u00b7\u00b7\u00b7, C(Wt-n)) \nwhere 9 is the neural network and C(i) is the i-th word feature vector. Training is achieved by looking for ((), C) that maximize the training corpus penalized log\u00ad\nlikelihood : L = ~ ~t logpw. (C(Wt-n),\u00b7\u00b7\u00b7, C(Wt-l)j ()) + R((), C), where R((), C) is a \nregularization term (e.g. a weight decay ).11()112, that penalizes slightly the norm of ().","More context is useful. Experiments with the cycling architecture on Brown, with 30 \nword features, and 30 hidden units, varying the number of context words: n = 1 (like the \nbigram) yields a test perplexity of 302, n = 3 yields 291, n = 5 yields 281, n = 8 yields \n279 (N.B. the smoothed trigram yields 348). Hidden units help. Experiments with the direct architecture on Brown (with direct input \nto output connections), with 30 word features, 5 words of context, varying the number of \nhidden units: 0 yields a test perplexity of 275, 10 yields 267, 20 yields 266, 40 yields 265, \n80 yields 265. Learning the word features jointly is important. Experiments with the direct architec\u00ad\nture on Brown (40 hidden units, 5 words of context), in which the word features initialized \nwith the SVD method are kept fixed during training yield a test perplexity of 345.8 whereas \nif the word features are trained jointly with the rest of the parameters, the perplexity is 265. Initialization not so useful. Experiments on Brown with both architectures reveal that the \nSVD initialization of the word features does not bring much improvement with respect to \nrandom initialization : it speeds up initial convergence (saving about 2 epochs), and yields \na perplexity improvement of less than 0.3%. Direct architecture works a bit better.",") have similar \nsemantics and grammatical roles. 1.1 Fighting the Curse of Dimensionality with its Own Weapons \nIn a nutshell, the idea of the proposed approach can be summarized as follows: \n1. associate with each word in the vocabulary a distributed \"feature vector\" (a real\u00ad\nvalued vector in ~m), thereby creating a notion of similarity between words, \n2. express the joint probability function of word sequences in terms of the feature \nvectors of these words in the sequence, and \n3. learn simultaneously the word feature vectors and the parameters of thatfitnction. The feature vector represents different aspects of a word: each word is associated with a \npoint in a vector space. The number of features (e.g.","A Neural Probabilistic Language Model \nYoshua Bengio; Rejean Ducharme and Pascal Vincent \nDepartement d'Informatique et Recherche Operationnelle \nCentre de Recherche Mathematiques \nUniversite de Montreal \nMontreal, Quebec, Canada, H3C 317 \n{bengioy,ducharme, vincentp }@iro.umontreal.ca \nAbstract \nA goal of statistical language modeling is to learn the joint probability \nfunction of sequences of words. This is intrinsically difficult because of \nthe curse of dimensionality: we propose to fight it with its own weapons."]],"title":[["placeholder_title","placeholder_title","placeholder_title","placeholder_title"]]},"oracle_context":"A Neural Probabilistic Language Model \nYoshua Bengio; Rejean Ducharme and Pascal Vincent \nDepartement d'Informatique et Recherche Operationnelle \nCentre de Recherche Mathematiques \nUniversite de Montreal \nMontreal, Quebec, Canada, H3C 317 \n{bengioy,ducharme, vincentp }@iro.umontreal.ca \nAbstract \nA goal of statistical language modeling is to learn the joint probability \nfunction of sequences of words. This is intrinsically difficult because of \nthe curse of dimensionality: we propose to fight it with its own weapons.","cot_answer":"To answer the question about how the authors propose to address the curse of dimensionality, I will follow these steps:\n\n1. Identify the main issue presented in the context, which is the curse of dimensionality in statistical language modeling.\n2. Look for any specific strategies or methods mentioned by the authors to combat this issue.\n3. Extract relevant quotes from the context that directly address the authors' approach to the curse of dimensionality.\n\nFrom the context, the authors mention that they aim to \"fight it with its own weapons.\" This suggests that they have a specific strategy in mind, but the details of that strategy are not provided in the excerpt.\n\nSince the context does not elaborate further on the methods they propose, I will summarize the key point regarding their approach.\n\nFinal answer: <ANSWER>: The authors propose to address the curse of dimensionality by \"fighting it with its own weapons,\" although specific methods are not detailed in the provided context.","instruction":"<DOCUMENT>f(i, Wt-i,\u00b7\u00b7\u00b7, Wt-n) = g(i, C(Wt-i),\u00b7 \u00b7\u00b7, C(Wt-n)). We used the \"soft\u00ad\nmax\" in the output layer of a neural net: P( Wt = ilwi-i) = ehi \/ Ej eh;, \nwhere hi is the neural network output score for word i. (b) The cycling architecture: a function h maps a sequence of feature \nvectors (C(Wt-n),\u00b7\u00b7\u00b7, C(Wt-i), C(i)) (i.e. including the context words \nand a candidate next word i) to a scalar hi, and again using a soft-\nmax, P(Wt = ilwi-i) = ehi\/Ejeh;. f(Wt,Wt-i,\u00b7 \u00b7\u00b7,Wt-n) = \ng(C(Wt), C(wt-d,\u00b7\u00b7\u00b7 ,C(Wt-n)). We call this architecture \"cycling\" be\u00ad\ncause one repeatedly runs h (e.g. a neural net), each time putting in input the \nfeature vector C(i) for a candidate next word i. The function f is a composition of these two mappings (C and g), with C being shared \nacross all the words in the context. To each of these two parts are associated some pa\u00ad\nrameters. The parameters of the mapping C are simply the feature vectors themselves \n(represented by a IVI x m matrix C whose row i is the feature vector C(i) for word i). The \nfunction 9 may be implemented by a feed-forward or recurrent neural network or another \nparameterized function, with parameters (). Table \nlook-up \ninC \nindex fOl wi-n i-ill output = P(Wt = i I eMtext) \nindex fot W'-2 index fot Wt_l : computed only \nfor WOlds in \nshOlt list \nFigure 1: \"Direct Architecture\": f(i, Wt-l,\u00b7\", Wt-n) = g(i, C(Wt-l),\u00b7\u00b7\u00b7, C(Wt-n)) \nwhere 9 is the neural network and C(i) is the i-th word feature vector. Training is achieved by looking for ((), C) that maximize the training corpus penalized log\u00ad\nlikelihood : L = ~ ~t logpw. (C(Wt-n),\u00b7\u00b7\u00b7, C(Wt-l)j ()) + R((), C), where R((), C) is a \nregularization term (e.g. a weight decay ).11()112, that penalizes slightly the norm of ().<\/DOCUMENT>\n<DOCUMENT>More context is useful. Experiments with the cycling architecture on Brown, with 30 \nword features, and 30 hidden units, varying the number of context words: n = 1 (like the \nbigram) yields a test perplexity of 302, n = 3 yields 291, n = 5 yields 281, n = 8 yields \n279 (N.B. the smoothed trigram yields 348). Hidden units help. Experiments with the direct architecture on Brown (with direct input \nto output connections), with 30 word features, 5 words of context, varying the number of \nhidden units: 0 yields a test perplexity of 275, 10 yields 267, 20 yields 266, 40 yields 265, \n80 yields 265. Learning the word features jointly is important. Experiments with the direct architec\u00ad\nture on Brown (40 hidden units, 5 words of context), in which the word features initialized \nwith the SVD method are kept fixed during training yield a test perplexity of 345.8 whereas \nif the word features are trained jointly with the rest of the parameters, the perplexity is 265. Initialization not so useful. Experiments on Brown with both architectures reveal that the \nSVD initialization of the word features does not bring much improvement with respect to \nrandom initialization : it speeds up initial convergence (saving about 2 epochs), and yields \na perplexity improvement of less than 0.3%. Direct architecture works a bit better.<\/DOCUMENT>\n<DOCUMENT>) have similar \nsemantics and grammatical roles. 1.1 Fighting the Curse of Dimensionality with its Own Weapons \nIn a nutshell, the idea of the proposed approach can be summarized as follows: \n1. associate with each word in the vocabulary a distributed \"feature vector\" (a real\u00ad\nvalued vector in ~m), thereby creating a notion of similarity between words, \n2. express the joint probability function of word sequences in terms of the feature \nvectors of these words in the sequence, and \n3. learn simultaneously the word feature vectors and the parameters of thatfitnction. The feature vector represents different aspects of a word: each word is associated with a \npoint in a vector space. The number of features (e.g.<\/DOCUMENT>\n<DOCUMENT>A Neural Probabilistic Language Model \nYoshua Bengio; Rejean Ducharme and Pascal Vincent \nDepartement d'Informatique et Recherche Operationnelle \nCentre de Recherche Mathematiques \nUniversite de Montreal \nMontreal, Quebec, Canada, H3C 317 \n{bengioy,ducharme, vincentp }@iro.umontreal.ca \nAbstract \nA goal of statistical language modeling is to learn the joint probability \nfunction of sequences of words. This is intrinsically difficult because of \nthe curse of dimensionality: we propose to fight it with its own weapons.<\/DOCUMENT>\n5. How do the authors propose to address the curse of dimensionality?  "}
{"id":"ed86836e-b282-4144-9b72-6e76dc1612d0","type":"general","question":"1. What does the proposed approach learn simultaneously?  ","context":{"sentences":[["The benchmark against which the neural network was compared is an interpolated or \nsmoothed trigram model [6]. Let qt = l(Jreq(Wt-l,Wt-2)) represent the discretized fre\u00ad\nquency of occurrence of the context (Wt-l, Wt-2) (we used l(x) = r -log((l + x)\/T)l \nwhere x is the frequency of occurrence of the context and T is the size of the training cor\u00ad\npus). A conditional mixture of the trigram, bigram, unigram and zero-gram was learned on \nthe validation set, with mixture weights conditional on discretized frequency. Below are measures of test set perplexity (geometric average of 1\/ p( Wt Iwi-1) for dif\u00ad\nferent models P. Apparent convergence of the stochastic gradient descent procedure was \nobtained after around 10 epochs for Hansard and after about 50 epochs for Brown, with \na learning rate gradually decreased from approximately 10-3 to 10-5. Weight decay of \n10-4 or 10-5 was used in all the experiments (based on a few experiments compared on \nthe validation set). The main result is that the neural network performs much better than the smoothed trigram. On Brown the best neural network system, according to validation perplexity \n(among different architectures tried, see below) yielded a perplexity of 258, while the \nsmoothed trigram yields a perplexity of 348, which is about 35% worse. This is obtained \nusing a network with the direct architecture mixed with the trigram (conditional mixture), \nwith 30 word features initialized with the SVD method, 40 hidden units, and n = 5 words \nof context. On Hansard, the corresponding figures are 44.8 for the neural network and 54.1 \nfor the smoothed trigram, which is 20.7% worse. This is obtained with a network with the \ndirect architecture, 100 randomly initialized words features, 120 hidden units, and n = 8 \nwords of context.","A Neural Probabilistic Language Model \nYoshua Bengio; Rejean Ducharme and Pascal Vincent \nDepartement d'Informatique et Recherche Operationnelle \nCentre de Recherche Mathematiques \nUniversite de Montreal \nMontreal, Quebec, Canada, H3C 317 \n{bengioy,ducharme, vincentp }@iro.umontreal.ca \nAbstract \nA goal of statistical language modeling is to learn the joint probability \nfunction of sequences of words. This is intrinsically difficult because of \nthe curse of dimensionality: we propose to fight it with its own weapons.","In the proposed approach one learns simultaneously (1) a distributed rep\u00ad\nresentation for each word (i.e. a similarity between words) along with (2) \nthe probability function for word sequences, expressed with these repre\u00ad\nsentations. Generalization is obtained because a sequence of words that \nhas never been seen before gets high probability if it is made of words \nthat are similar to words forming an already seen sentence. We report on \nexperiments using neural networks for the probability function, showing \non two text corpora that the proposed approach very significantly im\u00ad\nproves on a state-of-the-art trigram model. 1 Introduction \nA fundamental problem that makes language modeling and other learning problems diffi\u00ad\ncult is the curse of dimensionality.","Katz. Estimation of probabilities from sparse data for the language model component \nof a speech recognizer . IEEE Transactions on Acoustics , Speech, and Signal Processing, ASSP-\n35(3):400-401 , March 1987."]],"title":[["placeholder_title","placeholder_title","placeholder_title","placeholder_title"]]},"oracle_context":"In the proposed approach one learns simultaneously (1) a distributed rep\u00ad\nresentation for each word (i.e. a similarity between words) along with (2) \nthe probability function for word sequences, expressed with these repre\u00ad\nsentations. Generalization is obtained because a sequence of words that \nhas never been seen before gets high probability if it is made of words \nthat are similar to words forming an already seen sentence. We report on \nexperiments using neural networks for the probability function, showing \non two text corpora that the proposed approach very significantly im\u00ad\nproves on a state-of-the-art trigram model. 1 Introduction \nA fundamental problem that makes language modeling and other learning problems diffi\u00ad\ncult is the curse of dimensionality.","cot_answer":"To answer the question, we need to identify what the proposed approach learns simultaneously according to the context provided. \n\n1. First, we look for key phrases in the context that indicate what is being learned.\n2. The context mentions two specific aspects of learning: a distributed representation for words and a probability function for word sequences.\n3. We can find the relevant sentences in the context that explicitly state these two learning aspects.\n\nNow, let's extract the relevant information from the context:\n\n##begin_quote##\n\"In the proposed approach one learns simultaneously (1) a distributed representation for each word (i.e. a similarity between words) along with (2) the probability function for word sequences, expressed with these representations.\" ##end_quote##\n\nThis sentence clearly outlines the two components that are learned simultaneously: the distributed representation of words and the probability function for word sequences.\n\nFinally, we can summarize the answer succinctly.\n\n<ANSWER>: The proposed approach learns simultaneously a distributed representation for each word and the probability function for word sequences.","instruction":"<DOCUMENT>The benchmark against which the neural network was compared is an interpolated or \nsmoothed trigram model [6]. Let qt = l(Jreq(Wt-l,Wt-2)) represent the discretized fre\u00ad\nquency of occurrence of the context (Wt-l, Wt-2) (we used l(x) = r -log((l + x)\/T)l \nwhere x is the frequency of occurrence of the context and T is the size of the training cor\u00ad\npus). A conditional mixture of the trigram, bigram, unigram and zero-gram was learned on \nthe validation set, with mixture weights conditional on discretized frequency. Below are measures of test set perplexity (geometric average of 1\/ p( Wt Iwi-1) for dif\u00ad\nferent models P. Apparent convergence of the stochastic gradient descent procedure was \nobtained after around 10 epochs for Hansard and after about 50 epochs for Brown, with \na learning rate gradually decreased from approximately 10-3 to 10-5. Weight decay of \n10-4 or 10-5 was used in all the experiments (based on a few experiments compared on \nthe validation set). The main result is that the neural network performs much better than the smoothed trigram. On Brown the best neural network system, according to validation perplexity \n(among different architectures tried, see below) yielded a perplexity of 258, while the \nsmoothed trigram yields a perplexity of 348, which is about 35% worse. This is obtained \nusing a network with the direct architecture mixed with the trigram (conditional mixture), \nwith 30 word features initialized with the SVD method, 40 hidden units, and n = 5 words \nof context. On Hansard, the corresponding figures are 44.8 for the neural network and 54.1 \nfor the smoothed trigram, which is 20.7% worse. This is obtained with a network with the \ndirect architecture, 100 randomly initialized words features, 120 hidden units, and n = 8 \nwords of context.<\/DOCUMENT>\n<DOCUMENT>A Neural Probabilistic Language Model \nYoshua Bengio; Rejean Ducharme and Pascal Vincent \nDepartement d'Informatique et Recherche Operationnelle \nCentre de Recherche Mathematiques \nUniversite de Montreal \nMontreal, Quebec, Canada, H3C 317 \n{bengioy,ducharme, vincentp }@iro.umontreal.ca \nAbstract \nA goal of statistical language modeling is to learn the joint probability \nfunction of sequences of words. This is intrinsically difficult because of \nthe curse of dimensionality: we propose to fight it with its own weapons.<\/DOCUMENT>\n<DOCUMENT>In the proposed approach one learns simultaneously (1) a distributed rep\u00ad\nresentation for each word (i.e. a similarity between words) along with (2) \nthe probability function for word sequences, expressed with these repre\u00ad\nsentations. Generalization is obtained because a sequence of words that \nhas never been seen before gets high probability if it is made of words \nthat are similar to words forming an already seen sentence. We report on \nexperiments using neural networks for the probability function, showing \non two text corpora that the proposed approach very significantly im\u00ad\nproves on a state-of-the-art trigram model. 1 Introduction \nA fundamental problem that makes language modeling and other learning problems diffi\u00ad\ncult is the curse of dimensionality.<\/DOCUMENT>\n<DOCUMENT>Katz. Estimation of probabilities from sparse data for the language model component \nof a speech recognizer . IEEE Transactions on Acoustics , Speech, and Signal Processing, ASSP-\n35(3):400-401 , March 1987.<\/DOCUMENT>\n1. What does the proposed approach learn simultaneously?  "}
{"id":"0767d564-9a23-4d8b-bc14-0d57be8a416f","type":"general","question":"2. How is generalization achieved in the proposed method?  ","context":{"sentences":[["f(i, Wt-i,\u00b7\u00b7\u00b7, Wt-n) = g(i, C(Wt-i),\u00b7 \u00b7\u00b7, C(Wt-n)). We used the \"soft\u00ad\nmax\" in the output layer of a neural net: P( Wt = ilwi-i) = ehi \/ Ej eh;, \nwhere hi is the neural network output score for word i. (b) The cycling architecture: a function h maps a sequence of feature \nvectors (C(Wt-n),\u00b7\u00b7\u00b7, C(Wt-i), C(i)) (i.e. including the context words \nand a candidate next word i) to a scalar hi, and again using a soft-\nmax, P(Wt = ilwi-i) = ehi\/Ejeh;. f(Wt,Wt-i,\u00b7 \u00b7\u00b7,Wt-n) = \ng(C(Wt), C(wt-d,\u00b7\u00b7\u00b7 ,C(Wt-n)). We call this architecture \"cycling\" be\u00ad\ncause one repeatedly runs h (e.g. a neural net), each time putting in input the \nfeature vector C(i) for a candidate next word i. The function f is a composition of these two mappings (C and g), with C being shared \nacross all the words in the context. To each of these two parts are associated some pa\u00ad\nrameters. The parameters of the mapping C are simply the feature vectors themselves \n(represented by a IVI x m matrix C whose row i is the feature vector C(i) for word i). The \nfunction 9 may be implemented by a feed-forward or recurrent neural network or another \nparameterized function, with parameters (). Table \nlook-up \ninC \nindex fOl wi-n i-ill output = P(Wt = i I eMtext) \nindex fot W'-2 index fot Wt_l : computed only \nfor WOlds in \nshOlt list \nFigure 1: \"Direct Architecture\": f(i, Wt-l,\u00b7\", Wt-n) = g(i, C(Wt-l),\u00b7\u00b7\u00b7, C(Wt-n)) \nwhere 9 is the neural network and C(i) is the i-th word feature vector. Training is achieved by looking for ((), C) that maximize the training corpus penalized log\u00ad\nlikelihood : L = ~ ~t logpw. (C(Wt-n),\u00b7\u00b7\u00b7, C(Wt-l)j ()) + R((), C), where R((), C) is a \nregularization term (e.g. a weight decay ).11()112, that penalizes slightly the norm of ().","In the proposed approach one learns simultaneously (1) a distributed rep\u00ad\nresentation for each word (i.e. a similarity between words) along with (2) \nthe probability function for word sequences, expressed with these repre\u00ad\nsentations. Generalization is obtained because a sequence of words that \nhas never been seen before gets high probability if it is made of words \nthat are similar to words forming an already seen sentence. We report on \nexperiments using neural networks for the probability function, showing \non two text corpora that the proposed approach very significantly im\u00ad\nproves on a state-of-the-art trigram model. 1 Introduction \nA fundamental problem that makes language modeling and other learning problems diffi\u00ad\ncult is the curse of dimensionality.","Hinton. Extracting distributed representations of concepts and relations \nfrom positive and negative propositions. In Proceedings of the International Joint Conference \non Neural Network, lJCNN'2000, Como, Italy, 2000.","a tagger), and morpho\u00ad\nlogical (radix and morphemes). Looking at the word features learned by the model should \nhelp understand it and improve it. Finally, future research should establish how useful the \nproposed approach will be in applications to speech recognition, language translation, and \ninformation retrieval. Acknowledgments \nThe authors would like to thank Leon Bottou and Yann Le Cun for useful discussions."]],"title":[["placeholder_title","placeholder_title","placeholder_title","placeholder_title"]]},"oracle_context":"In the proposed approach one learns simultaneously (1) a distributed rep\u00ad\nresentation for each word (i.e. a similarity between words) along with (2) \nthe probability function for word sequences, expressed with these repre\u00ad\nsentations. Generalization is obtained because a sequence of words that \nhas never been seen before gets high probability if it is made of words \nthat are similar to words forming an already seen sentence. We report on \nexperiments using neural networks for the probability function, showing \non two text corpora that the proposed approach very significantly im\u00ad\nproves on a state-of-the-art trigram model. 1 Introduction \nA fundamental problem that makes language modeling and other learning problems diffi\u00ad\ncult is the curse of dimensionality.","cot_answer":"To answer the question about how generalization is achieved in the proposed method, I will follow these steps:\n\n1. Identify the key components of the proposed approach as described in the context.\n2. Understand how these components contribute to generalization.\n3. Summarize the findings related to generalization from the context.\n\nFrom the context, the proposed method involves learning two main aspects: (1) a distributed representation for each word, which captures the similarity between words, and (2) a probability function for word sequences based on these representations. \n\nThe context states that generalization is achieved because \"a sequence of words that has never been seen before gets high probability if it is made of words that are similar to words forming an already seen sentence.\" This indicates that the method leverages the learned word similarities to assign probabilities to new sequences based on their resemblance to previously encountered sequences.\n\nAdditionally, the context mentions that experiments using neural networks for the probability function show significant improvements over a state-of-the-art trigram model, which suggests that the proposed method's ability to generalize is effective in practice.\n\nNow, I will compile this information into a succinct answer.\n\n<ANSWER>: Generalization is achieved by learning word similarities and using these to assign high probabilities to unseen word sequences that resemble previously seen sentences.","instruction":"<DOCUMENT>f(i, Wt-i,\u00b7\u00b7\u00b7, Wt-n) = g(i, C(Wt-i),\u00b7 \u00b7\u00b7, C(Wt-n)). We used the \"soft\u00ad\nmax\" in the output layer of a neural net: P( Wt = ilwi-i) = ehi \/ Ej eh;, \nwhere hi is the neural network output score for word i. (b) The cycling architecture: a function h maps a sequence of feature \nvectors (C(Wt-n),\u00b7\u00b7\u00b7, C(Wt-i), C(i)) (i.e. including the context words \nand a candidate next word i) to a scalar hi, and again using a soft-\nmax, P(Wt = ilwi-i) = ehi\/Ejeh;. f(Wt,Wt-i,\u00b7 \u00b7\u00b7,Wt-n) = \ng(C(Wt), C(wt-d,\u00b7\u00b7\u00b7 ,C(Wt-n)). We call this architecture \"cycling\" be\u00ad\ncause one repeatedly runs h (e.g. a neural net), each time putting in input the \nfeature vector C(i) for a candidate next word i. The function f is a composition of these two mappings (C and g), with C being shared \nacross all the words in the context. To each of these two parts are associated some pa\u00ad\nrameters. The parameters of the mapping C are simply the feature vectors themselves \n(represented by a IVI x m matrix C whose row i is the feature vector C(i) for word i). The \nfunction 9 may be implemented by a feed-forward or recurrent neural network or another \nparameterized function, with parameters (). Table \nlook-up \ninC \nindex fOl wi-n i-ill output = P(Wt = i I eMtext) \nindex fot W'-2 index fot Wt_l : computed only \nfor WOlds in \nshOlt list \nFigure 1: \"Direct Architecture\": f(i, Wt-l,\u00b7\", Wt-n) = g(i, C(Wt-l),\u00b7\u00b7\u00b7, C(Wt-n)) \nwhere 9 is the neural network and C(i) is the i-th word feature vector. Training is achieved by looking for ((), C) that maximize the training corpus penalized log\u00ad\nlikelihood : L = ~ ~t logpw. (C(Wt-n),\u00b7\u00b7\u00b7, C(Wt-l)j ()) + R((), C), where R((), C) is a \nregularization term (e.g. a weight decay ).11()112, that penalizes slightly the norm of ().<\/DOCUMENT>\n<DOCUMENT>In the proposed approach one learns simultaneously (1) a distributed rep\u00ad\nresentation for each word (i.e. a similarity between words) along with (2) \nthe probability function for word sequences, expressed with these repre\u00ad\nsentations. Generalization is obtained because a sequence of words that \nhas never been seen before gets high probability if it is made of words \nthat are similar to words forming an already seen sentence. We report on \nexperiments using neural networks for the probability function, showing \non two text corpora that the proposed approach very significantly im\u00ad\nproves on a state-of-the-art trigram model. 1 Introduction \nA fundamental problem that makes language modeling and other learning problems diffi\u00ad\ncult is the curse of dimensionality.<\/DOCUMENT>\n<DOCUMENT>Hinton. Extracting distributed representations of concepts and relations \nfrom positive and negative propositions. In Proceedings of the International Joint Conference \non Neural Network, lJCNN'2000, Como, Italy, 2000.<\/DOCUMENT>\n<DOCUMENT>a tagger), and morpho\u00ad\nlogical (radix and morphemes). Looking at the word features learned by the model should \nhelp understand it and improve it. Finally, future research should establish how useful the \nproposed approach will be in applications to speech recognition, language translation, and \ninformation retrieval. Acknowledgments \nThe authors would like to thank Leon Bottou and Yann Le Cun for useful discussions.<\/DOCUMENT>\n2. How is generalization achieved in the proposed method?  "}
{"id":"e2a118b9-0b26-4acc-bb14-4c87c33372d4","type":"general","question":"3. What type of model does the proposed approach improve upon?  ","context":{"sentences":[["Hinton. Extracting distributed representations of concepts and relations \nfrom positive and negative propositions. In Proceedings of the International Joint Conference \non Neural Network, lJCNN'2000, Como, Italy, 2000.","The benchmark against which the neural network was compared is an interpolated or \nsmoothed trigram model [6]. Let qt = l(Jreq(Wt-l,Wt-2)) represent the discretized fre\u00ad\nquency of occurrence of the context (Wt-l, Wt-2) (we used l(x) = r -log((l + x)\/T)l \nwhere x is the frequency of occurrence of the context and T is the size of the training cor\u00ad\npus). A conditional mixture of the trigram, bigram, unigram and zero-gram was learned on \nthe validation set, with mixture weights conditional on discretized frequency. Below are measures of test set perplexity (geometric average of 1\/ p( Wt Iwi-1) for dif\u00ad\nferent models P. Apparent convergence of the stochastic gradient descent procedure was \nobtained after around 10 epochs for Hansard and after about 50 epochs for Brown, with \na learning rate gradually decreased from approximately 10-3 to 10-5. Weight decay of \n10-4 or 10-5 was used in all the experiments (based on a few experiments compared on \nthe validation set). The main result is that the neural network performs much better than the smoothed trigram. On Brown the best neural network system, according to validation perplexity \n(among different architectures tried, see below) yielded a perplexity of 258, while the \nsmoothed trigram yields a perplexity of 348, which is about 35% worse. This is obtained \nusing a network with the direct architecture mixed with the trigram (conditional mixture), \nwith 30 word features initialized with the SVD method, 40 hidden units, and n = 5 words \nof context. On Hansard, the corresponding figures are 44.8 for the neural network and 54.1 \nfor the smoothed trigram, which is 20.7% worse. This is obtained with a network with the \ndirect architecture, 100 randomly initialized words features, 120 hidden units, and n = 8 \nwords of context.","Katz. Estimation of probabilities from sparse data for the language model component \nof a speech recognizer . IEEE Transactions on Acoustics , Speech, and Signal Processing, ASSP-\n35(3):400-401 , March 1987.","In the proposed approach one learns simultaneously (1) a distributed rep\u00ad\nresentation for each word (i.e. a similarity between words) along with (2) \nthe probability function for word sequences, expressed with these repre\u00ad\nsentations. Generalization is obtained because a sequence of words that \nhas never been seen before gets high probability if it is made of words \nthat are similar to words forming an already seen sentence. We report on \nexperiments using neural networks for the probability function, showing \non two text corpora that the proposed approach very significantly im\u00ad\nproves on a state-of-the-art trigram model. 1 Introduction \nA fundamental problem that makes language modeling and other learning problems diffi\u00ad\ncult is the curse of dimensionality."]],"title":[["placeholder_title","placeholder_title","placeholder_title","placeholder_title"]]},"oracle_context":"In the proposed approach one learns simultaneously (1) a distributed rep\u00ad\nresentation for each word (i.e. a similarity between words) along with (2) \nthe probability function for word sequences, expressed with these repre\u00ad\nsentations. Generalization is obtained because a sequence of words that \nhas never been seen before gets high probability if it is made of words \nthat are similar to words forming an already seen sentence. We report on \nexperiments using neural networks for the probability function, showing \non two text corpora that the proposed approach very significantly im\u00ad\nproves on a state-of-the-art trigram model. 1 Introduction \nA fundamental problem that makes language modeling and other learning problems diffi\u00ad\ncult is the curse of dimensionality.","cot_answer":"To answer the question about what type of model the proposed approach improves upon, we can follow these steps:\n\n1. Identify the main focus of the proposed approach as described in the context.\n2. Look for any mention of existing models that the proposed approach is compared to or improves upon.\n3. Extract the relevant information that specifies the type of model being improved.\n\nFrom the context, we see that the proposed approach involves learning a distributed representation for words and a probability function for word sequences. It mentions that the approach \"very significantly improves on a state-of-the-art trigram model.\" \n\nNow, we can quote the relevant part of the context to support our answer:\n\n##begin_quote##\n\"We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach very significantly improves on a state-of-the-art trigram model.\"\n##end_quote##\n\nBased on this information, we can conclude that the proposed approach improves upon a trigram model.\n\nFinal answer: <ANSWER>: trigram model.","instruction":"<DOCUMENT>Hinton. Extracting distributed representations of concepts and relations \nfrom positive and negative propositions. In Proceedings of the International Joint Conference \non Neural Network, lJCNN'2000, Como, Italy, 2000.<\/DOCUMENT>\n<DOCUMENT>The benchmark against which the neural network was compared is an interpolated or \nsmoothed trigram model [6]. Let qt = l(Jreq(Wt-l,Wt-2)) represent the discretized fre\u00ad\nquency of occurrence of the context (Wt-l, Wt-2) (we used l(x) = r -log((l + x)\/T)l \nwhere x is the frequency of occurrence of the context and T is the size of the training cor\u00ad\npus). A conditional mixture of the trigram, bigram, unigram and zero-gram was learned on \nthe validation set, with mixture weights conditional on discretized frequency. Below are measures of test set perplexity (geometric average of 1\/ p( Wt Iwi-1) for dif\u00ad\nferent models P. Apparent convergence of the stochastic gradient descent procedure was \nobtained after around 10 epochs for Hansard and after about 50 epochs for Brown, with \na learning rate gradually decreased from approximately 10-3 to 10-5. Weight decay of \n10-4 or 10-5 was used in all the experiments (based on a few experiments compared on \nthe validation set). The main result is that the neural network performs much better than the smoothed trigram. On Brown the best neural network system, according to validation perplexity \n(among different architectures tried, see below) yielded a perplexity of 258, while the \nsmoothed trigram yields a perplexity of 348, which is about 35% worse. This is obtained \nusing a network with the direct architecture mixed with the trigram (conditional mixture), \nwith 30 word features initialized with the SVD method, 40 hidden units, and n = 5 words \nof context. On Hansard, the corresponding figures are 44.8 for the neural network and 54.1 \nfor the smoothed trigram, which is 20.7% worse. This is obtained with a network with the \ndirect architecture, 100 randomly initialized words features, 120 hidden units, and n = 8 \nwords of context.<\/DOCUMENT>\n<DOCUMENT>Katz. Estimation of probabilities from sparse data for the language model component \nof a speech recognizer . IEEE Transactions on Acoustics , Speech, and Signal Processing, ASSP-\n35(3):400-401 , March 1987.<\/DOCUMENT>\n<DOCUMENT>In the proposed approach one learns simultaneously (1) a distributed rep\u00ad\nresentation for each word (i.e. a similarity between words) along with (2) \nthe probability function for word sequences, expressed with these repre\u00ad\nsentations. Generalization is obtained because a sequence of words that \nhas never been seen before gets high probability if it is made of words \nthat are similar to words forming an already seen sentence. We report on \nexperiments using neural networks for the probability function, showing \non two text corpora that the proposed approach very significantly im\u00ad\nproves on a state-of-the-art trigram model. 1 Introduction \nA fundamental problem that makes language modeling and other learning problems diffi\u00ad\ncult is the curse of dimensionality.<\/DOCUMENT>\n3. What type of model does the proposed approach improve upon?  "}
{"id":"a7a5279a-052f-45cc-bea6-dc3d01072ee7","type":"general","question":"4. Which type of neural networks were used in the experiments?  ","context":{"sentences":[["In the proposed approach one learns simultaneously (1) a distributed rep\u00ad\nresentation for each word (i.e. a similarity between words) along with (2) \nthe probability function for word sequences, expressed with these repre\u00ad\nsentations. Generalization is obtained because a sequence of words that \nhas never been seen before gets high probability if it is made of words \nthat are similar to words forming an already seen sentence. We report on \nexperiments using neural networks for the probability function, showing \non two text corpora that the proposed approach very significantly im\u00ad\nproves on a state-of-the-art trigram model. 1 Introduction \nA fundamental problem that makes language modeling and other learning problems diffi\u00ad\ncult is the curse of dimensionality.","Katz. Estimation of probabilities from sparse data for the language model component \nof a speech recognizer . IEEE Transactions on Acoustics , Speech, and Signal Processing, ASSP-\n35(3):400-401 , March 1987.","Since we have millions of examples, it is important to con\u00ad\nverge within only a few passes through the data. For very large data sets, stochastic gradient \ndescent convergence time seems to increase sub-linearly with the size of the data set (see \nexperiments on Brown vs Hansard below). To speed up training using stochastic gradient descent, we have found it useful to break the corpus in paragraphs and to randomly permute \nthem. In this way, some of the non-stationarity in the word stream is eliminated, yielding \nfaster convergence. Capacity control. For the \"smaller corpora\" like Brown (1.2 million examples), we have \nfound early stopping and weight decay useful to avoid over-fitting. For the larger corpora, \nour networks still under-fit. For the larger corpora, we have found double-precision com\u00ad\nputation to be very important to obtain good results. Mixture of models. We have found improved performance by combining the probability \npredictions of the neural network with those of the smoothed trigram, with weights that \nwere conditional on the frequency of the context (same procedure used to combine trigram, \nbigram, and unigram in the smoothed trigram). Initialization of word feature vectors. We have tried both random initialization (uniform \nbetween -.01 and .01) and a \"smarter\" method based on a Singular Value Decomposition \n(SVD) of a very large matrix of \"context features\". These context features are formed \nby counting the frequency of occurrence of each word in each one of the most frequent \ncontexts (word sequences) in the corpus. The idea is that \"similar\" words should occur \nwith similar frequency in the same contexts. We used about 9000 most frequent contexts, \nand compressed these to 30 features with the SVD. Out-of-vocabulary words. For an out-of-vocabulary word Wt we need to come up with \na feature vector in order to predict the words that follow, or predict its probability (that \nis only possible with the cycling architecture). We used as feature vector the weighted \naverage feature vector of all the words in the short list, with the weights being the relative \nprobabilities ofthose words: E[C(wt)lhtl = Ei C(i)P(wt = ilht). 4 Experimental Results \nComparative experiments were performed on the Brown and Hansard corpora. The Brown \ncorpus is a stream of 1,181,041 words (from a large variety of English texts and books). The first 800,000 words were used for training, the following 200,000 for validation (model \nselection, weight decay, early stopping) and the remaining 181,041 for testing. The number \nof different words is 47, 578 (including punctuation, distinguishing between upper and \nlower case, and including the syntactical marks used to separate texts and paragraphs). Rare words with frequency:::; 3 were merged into a single token, reducing the vocabulary \nsize to IVI = 16,383. The Hansard corpus (Canadian parliament proceedings, French version) is a stream of \nabout 34 million words, of which 32 millions (set A) was used for training, 1.1 million \n(set B) was used for validation, and 1.2 million (set C) was used for out-of-sample tests. The original data has 106, 936 different words, and those with frequency:::; 10 were merged \ninto a single token, yielding IVI = 30,959 different words.","f(i, Wt-i,\u00b7\u00b7\u00b7, Wt-n) = g(i, C(Wt-i),\u00b7 \u00b7\u00b7, C(Wt-n)). We used the \"soft\u00ad\nmax\" in the output layer of a neural net: P( Wt = ilwi-i) = ehi \/ Ej eh;, \nwhere hi is the neural network output score for word i. (b) The cycling architecture: a function h maps a sequence of feature \nvectors (C(Wt-n),\u00b7\u00b7\u00b7, C(Wt-i), C(i)) (i.e. including the context words \nand a candidate next word i) to a scalar hi, and again using a soft-\nmax, P(Wt = ilwi-i) = ehi\/Ejeh;. f(Wt,Wt-i,\u00b7 \u00b7\u00b7,Wt-n) = \ng(C(Wt), C(wt-d,\u00b7\u00b7\u00b7 ,C(Wt-n)). We call this architecture \"cycling\" be\u00ad\ncause one repeatedly runs h (e.g. a neural net), each time putting in input the \nfeature vector C(i) for a candidate next word i. The function f is a composition of these two mappings (C and g), with C being shared \nacross all the words in the context. To each of these two parts are associated some pa\u00ad\nrameters. The parameters of the mapping C are simply the feature vectors themselves \n(represented by a IVI x m matrix C whose row i is the feature vector C(i) for word i). The \nfunction 9 may be implemented by a feed-forward or recurrent neural network or another \nparameterized function, with parameters (). Table \nlook-up \ninC \nindex fOl wi-n i-ill output = P(Wt = i I eMtext) \nindex fot W'-2 index fot Wt_l : computed only \nfor WOlds in \nshOlt list \nFigure 1: \"Direct Architecture\": f(i, Wt-l,\u00b7\", Wt-n) = g(i, C(Wt-l),\u00b7\u00b7\u00b7, C(Wt-n)) \nwhere 9 is the neural network and C(i) is the i-th word feature vector. Training is achieved by looking for ((), C) that maximize the training corpus penalized log\u00ad\nlikelihood : L = ~ ~t logpw. (C(Wt-n),\u00b7\u00b7\u00b7, C(Wt-l)j ()) + R((), C), where R((), C) is a \nregularization term (e.g. a weight decay ).11()112, that penalizes slightly the norm of ()."]],"title":[["placeholder_title","placeholder_title","placeholder_title","placeholder_title"]]},"oracle_context":"In the proposed approach one learns simultaneously (1) a distributed rep\u00ad\nresentation for each word (i.e. a similarity between words) along with (2) \nthe probability function for word sequences, expressed with these repre\u00ad\nsentations. Generalization is obtained because a sequence of words that \nhas never been seen before gets high probability if it is made of words \nthat are similar to words forming an already seen sentence. We report on \nexperiments using neural networks for the probability function, showing \non two text corpora that the proposed approach very significantly im\u00ad\nproves on a state-of-the-art trigram model. 1 Introduction \nA fundamental problem that makes language modeling and other learning problems diffi\u00ad\ncult is the curse of dimensionality.","cot_answer":"To answer the question about the type of neural networks used in the experiments, I will follow these steps:\n\n1. Identify the key information in the context that relates to the type of neural networks.\n2. Look for any specific mention of the neural network architecture or type.\n3. Summarize the findings in a clear and concise manner.\n\nUpon reviewing the context, I see that it mentions \"experiments using neural networks for the probability function.\" However, it does not specify the exact type of neural networks used. The context focuses more on the approach and the improvements over a trigram model rather than detailing the specific neural network architecture.\n\nSince there is no explicit mention of the type of neural networks in the provided context, I will conclude that the information is insufficient to determine the exact type.\n\nFinal answer: <ANSWER>: The context does not specify the type of neural networks used in the experiments.","instruction":"<DOCUMENT>In the proposed approach one learns simultaneously (1) a distributed rep\u00ad\nresentation for each word (i.e. a similarity between words) along with (2) \nthe probability function for word sequences, expressed with these repre\u00ad\nsentations. Generalization is obtained because a sequence of words that \nhas never been seen before gets high probability if it is made of words \nthat are similar to words forming an already seen sentence. We report on \nexperiments using neural networks for the probability function, showing \non two text corpora that the proposed approach very significantly im\u00ad\nproves on a state-of-the-art trigram model. 1 Introduction \nA fundamental problem that makes language modeling and other learning problems diffi\u00ad\ncult is the curse of dimensionality.<\/DOCUMENT>\n<DOCUMENT>Katz. Estimation of probabilities from sparse data for the language model component \nof a speech recognizer . IEEE Transactions on Acoustics , Speech, and Signal Processing, ASSP-\n35(3):400-401 , March 1987.<\/DOCUMENT>\n<DOCUMENT>Since we have millions of examples, it is important to con\u00ad\nverge within only a few passes through the data. For very large data sets, stochastic gradient \ndescent convergence time seems to increase sub-linearly with the size of the data set (see \nexperiments on Brown vs Hansard below). To speed up training using stochastic gradient descent, we have found it useful to break the corpus in paragraphs and to randomly permute \nthem. In this way, some of the non-stationarity in the word stream is eliminated, yielding \nfaster convergence. Capacity control. For the \"smaller corpora\" like Brown (1.2 million examples), we have \nfound early stopping and weight decay useful to avoid over-fitting. For the larger corpora, \nour networks still under-fit. For the larger corpora, we have found double-precision com\u00ad\nputation to be very important to obtain good results. Mixture of models. We have found improved performance by combining the probability \npredictions of the neural network with those of the smoothed trigram, with weights that \nwere conditional on the frequency of the context (same procedure used to combine trigram, \nbigram, and unigram in the smoothed trigram). Initialization of word feature vectors. We have tried both random initialization (uniform \nbetween -.01 and .01) and a \"smarter\" method based on a Singular Value Decomposition \n(SVD) of a very large matrix of \"context features\". These context features are formed \nby counting the frequency of occurrence of each word in each one of the most frequent \ncontexts (word sequences) in the corpus. The idea is that \"similar\" words should occur \nwith similar frequency in the same contexts. We used about 9000 most frequent contexts, \nand compressed these to 30 features with the SVD. Out-of-vocabulary words. For an out-of-vocabulary word Wt we need to come up with \na feature vector in order to predict the words that follow, or predict its probability (that \nis only possible with the cycling architecture). We used as feature vector the weighted \naverage feature vector of all the words in the short list, with the weights being the relative \nprobabilities ofthose words: E[C(wt)lhtl = Ei C(i)P(wt = ilht). 4 Experimental Results \nComparative experiments were performed on the Brown and Hansard corpora. The Brown \ncorpus is a stream of 1,181,041 words (from a large variety of English texts and books). The first 800,000 words were used for training, the following 200,000 for validation (model \nselection, weight decay, early stopping) and the remaining 181,041 for testing. The number \nof different words is 47, 578 (including punctuation, distinguishing between upper and \nlower case, and including the syntactical marks used to separate texts and paragraphs). Rare words with frequency:::; 3 were merged into a single token, reducing the vocabulary \nsize to IVI = 16,383. The Hansard corpus (Canadian parliament proceedings, French version) is a stream of \nabout 34 million words, of which 32 millions (set A) was used for training, 1.1 million \n(set B) was used for validation, and 1.2 million (set C) was used for out-of-sample tests. The original data has 106, 936 different words, and those with frequency:::; 10 were merged \ninto a single token, yielding IVI = 30,959 different words.<\/DOCUMENT>\n<DOCUMENT>f(i, Wt-i,\u00b7\u00b7\u00b7, Wt-n) = g(i, C(Wt-i),\u00b7 \u00b7\u00b7, C(Wt-n)). We used the \"soft\u00ad\nmax\" in the output layer of a neural net: P( Wt = ilwi-i) = ehi \/ Ej eh;, \nwhere hi is the neural network output score for word i. (b) The cycling architecture: a function h maps a sequence of feature \nvectors (C(Wt-n),\u00b7\u00b7\u00b7, C(Wt-i), C(i)) (i.e. including the context words \nand a candidate next word i) to a scalar hi, and again using a soft-\nmax, P(Wt = ilwi-i) = ehi\/Ejeh;. f(Wt,Wt-i,\u00b7 \u00b7\u00b7,Wt-n) = \ng(C(Wt), C(wt-d,\u00b7\u00b7\u00b7 ,C(Wt-n)). We call this architecture \"cycling\" be\u00ad\ncause one repeatedly runs h (e.g. a neural net), each time putting in input the \nfeature vector C(i) for a candidate next word i. The function f is a composition of these two mappings (C and g), with C being shared \nacross all the words in the context. To each of these two parts are associated some pa\u00ad\nrameters. The parameters of the mapping C are simply the feature vectors themselves \n(represented by a IVI x m matrix C whose row i is the feature vector C(i) for word i). The \nfunction 9 may be implemented by a feed-forward or recurrent neural network or another \nparameterized function, with parameters (). Table \nlook-up \ninC \nindex fOl wi-n i-ill output = P(Wt = i I eMtext) \nindex fot W'-2 index fot Wt_l : computed only \nfor WOlds in \nshOlt list \nFigure 1: \"Direct Architecture\": f(i, Wt-l,\u00b7\", Wt-n) = g(i, C(Wt-l),\u00b7\u00b7\u00b7, C(Wt-n)) \nwhere 9 is the neural network and C(i) is the i-th word feature vector. Training is achieved by looking for ((), C) that maximize the training corpus penalized log\u00ad\nlikelihood : L = ~ ~t logpw. (C(Wt-n),\u00b7\u00b7\u00b7, C(Wt-l)j ()) + R((), C), where R((), C) is a \nregularization term (e.g. a weight decay ).11()112, that penalizes slightly the norm of ().<\/DOCUMENT>\n4. Which type of neural networks were used in the experiments?  "}
{"id":"505d6890-d2bd-4cd0-bd14-ce70dbcca630","type":"general","question":"5. What problem does the proposed approach address in language modeling?  ","context":{"sentences":[["Since we have millions of examples, it is important to con\u00ad\nverge within only a few passes through the data. For very large data sets, stochastic gradient \ndescent convergence time seems to increase sub-linearly with the size of the data set (see \nexperiments on Brown vs Hansard below). To speed up training using stochastic gradient descent, we have found it useful to break the corpus in paragraphs and to randomly permute \nthem. In this way, some of the non-stationarity in the word stream is eliminated, yielding \nfaster convergence. Capacity control. For the \"smaller corpora\" like Brown (1.2 million examples), we have \nfound early stopping and weight decay useful to avoid over-fitting. For the larger corpora, \nour networks still under-fit. For the larger corpora, we have found double-precision com\u00ad\nputation to be very important to obtain good results. Mixture of models. We have found improved performance by combining the probability \npredictions of the neural network with those of the smoothed trigram, with weights that \nwere conditional on the frequency of the context (same procedure used to combine trigram, \nbigram, and unigram in the smoothed trigram). Initialization of word feature vectors. We have tried both random initialization (uniform \nbetween -.01 and .01) and a \"smarter\" method based on a Singular Value Decomposition \n(SVD) of a very large matrix of \"context features\". These context features are formed \nby counting the frequency of occurrence of each word in each one of the most frequent \ncontexts (word sequences) in the corpus. The idea is that \"similar\" words should occur \nwith similar frequency in the same contexts. We used about 9000 most frequent contexts, \nand compressed these to 30 features with the SVD. Out-of-vocabulary words. For an out-of-vocabulary word Wt we need to come up with \na feature vector in order to predict the words that follow, or predict its probability (that \nis only possible with the cycling architecture). We used as feature vector the weighted \naverage feature vector of all the words in the short list, with the weights being the relative \nprobabilities ofthose words: E[C(wt)lhtl = Ei C(i)P(wt = ilht). 4 Experimental Results \nComparative experiments were performed on the Brown and Hansard corpora. The Brown \ncorpus is a stream of 1,181,041 words (from a large variety of English texts and books). The first 800,000 words were used for training, the following 200,000 for validation (model \nselection, weight decay, early stopping) and the remaining 181,041 for testing. The number \nof different words is 47, 578 (including punctuation, distinguishing between upper and \nlower case, and including the syntactical marks used to separate texts and paragraphs). Rare words with frequency:::; 3 were merged into a single token, reducing the vocabulary \nsize to IVI = 16,383. The Hansard corpus (Canadian parliament proceedings, French version) is a stream of \nabout 34 million words, of which 32 millions (set A) was used for training, 1.1 million \n(set B) was used for validation, and 1.2 million (set C) was used for out-of-sample tests. The original data has 106, 936 different words, and those with frequency:::; 10 were merged \ninto a single token, yielding IVI = 30,959 different words.","3 Speeding-up and other Tricks \nShort list. The main idea is to focus the effort of the neural network on a \"short list\" \nof words that have the highest probability. This can save much computation because in \nboth of the proposed architectures the time to compute the probability of the observed \nnext word scales almost linearly with the number of words in the vocabulary (because \nthe scores hi associated with each word i in the vocabulary must be computed for prop\u00ad\nerly normalizing probabilities with the softmax). The idea of the speed-up trick is the \nfollowing: instead of computing the actual probability of the next word, the neural net\u00ad\nwork is used to compute the relative probability of the next word within that short list. The choice of the short list depends on the current context (the previous n words). We \nhave used our smoothed trigram model to pre-compute a short list containing the most \nprobable next words associated to the previous two words. The conditional probabilities \nP(Wt = ilht) are thus computed as follows, denoting with ht the history (context) before \nWt. and Lt the short list of words for the prediction of Wt. If i E Lt then the probabil-\nity is PNN(Wt = ilWt E Lt, ht)Ptrigram(Wt E Ltlht), else it is Ptrigram(Wt = ilht), \nwhere PNN(Wt = ilWt E Lt, ht) are the normalized scores of the words computed by \nthe neural network, where the \"softmax\" is only normalized over the words in the short \nlist Lt, and Ptrigram(Wt E Ltlht) = ~iEL. Ptrigram(ilht), with Ptrigram(ilht) standing \nfor the next-word probabilities computed by the smoothed trigram. Note that both Lt and \nPtrigram(Wt E Ltlht) can be pre-computed (and stored in a hash table indexed by the last \ntwo words). Table look-up for recognition. To speed up application of the trained model, one can \npre-compute in a hash table the output of the neural network, at least for the most frequent \ninput contexts. In that case, the neural network will only be rarely called upon, and the \naverage computation time will be very small. Note that in a speech recognition system, one \nneeds only compute the relative probabilities of the acoustically ambiguous words in each \ncontext, also reducing drastically the amount of computations. Stochastic gradient descent.","Bengio. Taking on the curse of dimensionality in joint distributions using \nneural networks. IEEE Transactions on Neural Networks, special issue on Data Mining and \nKnowledge Discovery , 11(3):550-557 , 2000. [3] Yoshua Bengio and Samy Bengio. Modeling high-dimensional discrete data with multi-layer \nneural networks . In S.","In the proposed approach one learns simultaneously (1) a distributed rep\u00ad\nresentation for each word (i.e. a similarity between words) along with (2) \nthe probability function for word sequences, expressed with these repre\u00ad\nsentations. Generalization is obtained because a sequence of words that \nhas never been seen before gets high probability if it is made of words \nthat are similar to words forming an already seen sentence. We report on \nexperiments using neural networks for the probability function, showing \non two text corpora that the proposed approach very significantly im\u00ad\nproves on a state-of-the-art trigram model. 1 Introduction \nA fundamental problem that makes language modeling and other learning problems diffi\u00ad\ncult is the curse of dimensionality."]],"title":[["placeholder_title","placeholder_title","placeholder_title","placeholder_title"]]},"oracle_context":"In the proposed approach one learns simultaneously (1) a distributed rep\u00ad\nresentation for each word (i.e. a similarity between words) along with (2) \nthe probability function for word sequences, expressed with these repre\u00ad\nsentations. Generalization is obtained because a sequence of words that \nhas never been seen before gets high probability if it is made of words \nthat are similar to words forming an already seen sentence. We report on \nexperiments using neural networks for the probability function, showing \non two text corpora that the proposed approach very significantly im\u00ad\nproves on a state-of-the-art trigram model. 1 Introduction \nA fundamental problem that makes language modeling and other learning problems diffi\u00ad\ncult is the curse of dimensionality.","cot_answer":"To answer the question about the problem the proposed approach addresses in language modeling, we can follow these steps:\n\n1. Identify the main issue mentioned in the context. The context highlights a fundamental problem in language modeling, which is the \"curse of dimensionality.\"\n2. Understand how the proposed approach aims to solve this problem. The approach involves learning a distributed representation for words and a probability function for word sequences, which allows for better generalization.\n3. Note the significance of the proposed method in improving upon existing models, specifically mentioning its performance compared to a state-of-the-art trigram model.\n\nNow, let's extract relevant sentences from the context to support our answer.\n\n##begin_quote##\n\"A fundamental problem that makes language modeling and other learning problems difficult is the curse of dimensionality.\"\n##end_quote##\n\nThe proposed approach addresses this problem by enabling better generalization through the use of word similarity and probability functions.\n\nFinal answer: <ANSWER>: The proposed approach addresses the curse of dimensionality in language modeling by enabling better generalization through word similarity and probability functions.","instruction":"<DOCUMENT>Since we have millions of examples, it is important to con\u00ad\nverge within only a few passes through the data. For very large data sets, stochastic gradient \ndescent convergence time seems to increase sub-linearly with the size of the data set (see \nexperiments on Brown vs Hansard below). To speed up training using stochastic gradient descent, we have found it useful to break the corpus in paragraphs and to randomly permute \nthem. In this way, some of the non-stationarity in the word stream is eliminated, yielding \nfaster convergence. Capacity control. For the \"smaller corpora\" like Brown (1.2 million examples), we have \nfound early stopping and weight decay useful to avoid over-fitting. For the larger corpora, \nour networks still under-fit. For the larger corpora, we have found double-precision com\u00ad\nputation to be very important to obtain good results. Mixture of models. We have found improved performance by combining the probability \npredictions of the neural network with those of the smoothed trigram, with weights that \nwere conditional on the frequency of the context (same procedure used to combine trigram, \nbigram, and unigram in the smoothed trigram). Initialization of word feature vectors. We have tried both random initialization (uniform \nbetween -.01 and .01) and a \"smarter\" method based on a Singular Value Decomposition \n(SVD) of a very large matrix of \"context features\". These context features are formed \nby counting the frequency of occurrence of each word in each one of the most frequent \ncontexts (word sequences) in the corpus. The idea is that \"similar\" words should occur \nwith similar frequency in the same contexts. We used about 9000 most frequent contexts, \nand compressed these to 30 features with the SVD. Out-of-vocabulary words. For an out-of-vocabulary word Wt we need to come up with \na feature vector in order to predict the words that follow, or predict its probability (that \nis only possible with the cycling architecture). We used as feature vector the weighted \naverage feature vector of all the words in the short list, with the weights being the relative \nprobabilities ofthose words: E[C(wt)lhtl = Ei C(i)P(wt = ilht). 4 Experimental Results \nComparative experiments were performed on the Brown and Hansard corpora. The Brown \ncorpus is a stream of 1,181,041 words (from a large variety of English texts and books). The first 800,000 words were used for training, the following 200,000 for validation (model \nselection, weight decay, early stopping) and the remaining 181,041 for testing. The number \nof different words is 47, 578 (including punctuation, distinguishing between upper and \nlower case, and including the syntactical marks used to separate texts and paragraphs). Rare words with frequency:::; 3 were merged into a single token, reducing the vocabulary \nsize to IVI = 16,383. The Hansard corpus (Canadian parliament proceedings, French version) is a stream of \nabout 34 million words, of which 32 millions (set A) was used for training, 1.1 million \n(set B) was used for validation, and 1.2 million (set C) was used for out-of-sample tests. The original data has 106, 936 different words, and those with frequency:::; 10 were merged \ninto a single token, yielding IVI = 30,959 different words.<\/DOCUMENT>\n<DOCUMENT>3 Speeding-up and other Tricks \nShort list. The main idea is to focus the effort of the neural network on a \"short list\" \nof words that have the highest probability. This can save much computation because in \nboth of the proposed architectures the time to compute the probability of the observed \nnext word scales almost linearly with the number of words in the vocabulary (because \nthe scores hi associated with each word i in the vocabulary must be computed for prop\u00ad\nerly normalizing probabilities with the softmax). The idea of the speed-up trick is the \nfollowing: instead of computing the actual probability of the next word, the neural net\u00ad\nwork is used to compute the relative probability of the next word within that short list. The choice of the short list depends on the current context (the previous n words). We \nhave used our smoothed trigram model to pre-compute a short list containing the most \nprobable next words associated to the previous two words. The conditional probabilities \nP(Wt = ilht) are thus computed as follows, denoting with ht the history (context) before \nWt. and Lt the short list of words for the prediction of Wt. If i E Lt then the probabil-\nity is PNN(Wt = ilWt E Lt, ht)Ptrigram(Wt E Ltlht), else it is Ptrigram(Wt = ilht), \nwhere PNN(Wt = ilWt E Lt, ht) are the normalized scores of the words computed by \nthe neural network, where the \"softmax\" is only normalized over the words in the short \nlist Lt, and Ptrigram(Wt E Ltlht) = ~iEL. Ptrigram(ilht), with Ptrigram(ilht) standing \nfor the next-word probabilities computed by the smoothed trigram. Note that both Lt and \nPtrigram(Wt E Ltlht) can be pre-computed (and stored in a hash table indexed by the last \ntwo words). Table look-up for recognition. To speed up application of the trained model, one can \npre-compute in a hash table the output of the neural network, at least for the most frequent \ninput contexts. In that case, the neural network will only be rarely called upon, and the \naverage computation time will be very small. Note that in a speech recognition system, one \nneeds only compute the relative probabilities of the acoustically ambiguous words in each \ncontext, also reducing drastically the amount of computations. Stochastic gradient descent.<\/DOCUMENT>\n<DOCUMENT>Bengio. Taking on the curse of dimensionality in joint distributions using \nneural networks. IEEE Transactions on Neural Networks, special issue on Data Mining and \nKnowledge Discovery , 11(3):550-557 , 2000. [3] Yoshua Bengio and Samy Bengio. Modeling high-dimensional discrete data with multi-layer \nneural networks . In S.<\/DOCUMENT>\n<DOCUMENT>In the proposed approach one learns simultaneously (1) a distributed rep\u00ad\nresentation for each word (i.e. a similarity between words) along with (2) \nthe probability function for word sequences, expressed with these repre\u00ad\nsentations. Generalization is obtained because a sequence of words that \nhas never been seen before gets high probability if it is made of words \nthat are similar to words forming an already seen sentence. We report on \nexperiments using neural networks for the probability function, showing \non two text corpora that the proposed approach very significantly im\u00ad\nproves on a state-of-the-art trigram model. 1 Introduction \nA fundamental problem that makes language modeling and other learning problems diffi\u00ad\ncult is the curse of dimensionality.<\/DOCUMENT>\n5. What problem does the proposed approach address in language modeling?  "}
{"id":"95c70692-5169-4cdf-82be-bd7494d7261a","type":"general","question":"1. What is the goal of tuning the parameters in the function?","context":{"sentences":[["f(i, Wt-i,\u00b7\u00b7\u00b7, Wt-n) = g(i, C(Wt-i),\u00b7 \u00b7\u00b7, C(Wt-n)). We used the \"soft\u00ad\nmax\" in the output layer of a neural net: P( Wt = ilwi-i) = ehi \/ Ej eh;, \nwhere hi is the neural network output score for word i. (b) The cycling architecture: a function h maps a sequence of feature \nvectors (C(Wt-n),\u00b7\u00b7\u00b7, C(Wt-i), C(i)) (i.e. including the context words \nand a candidate next word i) to a scalar hi, and again using a soft-\nmax, P(Wt = ilwi-i) = ehi\/Ejeh;. f(Wt,Wt-i,\u00b7 \u00b7\u00b7,Wt-n) = \ng(C(Wt), C(wt-d,\u00b7\u00b7\u00b7 ,C(Wt-n)). We call this architecture \"cycling\" be\u00ad\ncause one repeatedly runs h (e.g. a neural net), each time putting in input the \nfeature vector C(i) for a candidate next word i. The function f is a composition of these two mappings (C and g), with C being shared \nacross all the words in the context. To each of these two parts are associated some pa\u00ad\nrameters. The parameters of the mapping C are simply the feature vectors themselves \n(represented by a IVI x m matrix C whose row i is the feature vector C(i) for word i). The \nfunction 9 may be implemented by a feed-forward or recurrent neural network or another \nparameterized function, with parameters (). Table \nlook-up \ninC \nindex fOl wi-n i-ill output = P(Wt = i I eMtext) \nindex fot W'-2 index fot Wt_l : computed only \nfor WOlds in \nshOlt list \nFigure 1: \"Direct Architecture\": f(i, Wt-l,\u00b7\", Wt-n) = g(i, C(Wt-l),\u00b7\u00b7\u00b7, C(Wt-n)) \nwhere 9 is the neural network and C(i) is the i-th word feature vector. Training is achieved by looking for ((), C) that maximize the training corpus penalized log\u00ad\nlikelihood : L = ~ ~t logpw. (C(Wt-n),\u00b7\u00b7\u00b7, C(Wt-l)j ()) + R((), C), where R((), C) is a \nregularization term (e.g. a weight decay ).11()112, that penalizes slightly the norm of ().","This function has parameters that can be \niteratively tuned in order to maximize the log-likelihood of the training data or a regularized \ncriterion, e.g. by adding a weight decay penalty. The feature vectors associated with each \nword are learned, but they can be initialized using prior knowledge. Why does it work? In the previous example, if we knew that dog and cat played simi\u00ad\nlar roles (semantically and syntactically), and similarly for (the,a), (bedroom,room), \n(is,was), (running,walking), we could naturally generalize from The cat is \nwalking in the bedroom to A dog was running in a room and likewise \nto many other combinations. In the proposed model, it will so generalize because \"simi\u00ad\nlar\" words should have a similar feature vector, and because the probability function is a \nsmooth function of these feature values, so a small change in the features (to obtain similar \nwords) induces a small change in the probability : seeing only one of the above sentences \nwill increase the probability not only of that sentence but also of its combinatorial number \nof \"neighbors\" in sentence .Ipace (as represented by sequences offeature vectors). 1.2 Relation to Previous Work \nThe idea of using neural networks to model high-dimensional discrete distributions has \nalready been found useful in [3] where the joint probability of Zl ... Zn is decomposed \nas a product of conditional probabilities: P(Zl = Zl,\"', Zn = Zn) = Oi P(Zi = \nzilgi(Zi-l, Zi-2, ... , Zl)), where g(.) is a function represented by part of a neural network, \nand it yields parameters for expressing the distribution of Zi. Experiments on four VCI \ndata sets show this approach to work comparatively very well [3, 2]. The idea of a dis\u00ad\ntributed representation for symbols dates from the early days of connectionism [5]. More \nrecently, Hinton's approach was improved and successfully demonstrated on learning sev\u00ad\neral symbolic relations [9]. The idea of using neural networks for language modeling is not \nnew either, e.g. [8]. In contrast, here we push this idea to a large scale, and concentrate on \nlearning a statistical model of the distribution of word sequences, rather than learning the \nrole of words in a sentence. The proposed approach is also related to previous proposals of character-based text compression using neural networks [11]. Learning a clustering of \nwords [10, 1] is also a way to discover similarities between words. In the model proposed \nhere, instead of characterizing the similarity with a discrete random or deterministic vari\u00ad\nable (which corresponds to a soft or hard partition of the set of words), we use a continuous \nreal-vector for each word, i.e. a distributed feature vector, to indirectly represent similarity \nbetween words. The idea of using a vector-space representation for words has been well \nexploited in the area of information retrieval (for example see [12]), where vectorial fea\u00ad\nture vectors for words are learned on the basis of their probability of co-occurring in the \nsame documents (Latent Semantic Indexing [4]). An important difference is that here we \nlook for a representation for words that is helpful in representing compactly the probabil\u00ad\nity distribution of word sequences from natural language text. Experiments indicate that \nlearning jointly the representation (word features) and the model makes a big difference in \nperformance. 2 The Proposed Model: two Architectures \nThe training set is a sequence Wi ... WT of words Wt E V, where the vocabular y V is a \nlarge but finite set. The objective is to learn a good model f(wt,\u00b7 .. , Wt-n) = P(wtlwi-i), \nin the sense that it gives high out-of-sample likelihood . In the experiments, we will report \nthe geometric average of l\/P(wtlwi-i), also known as perplexity, which is also the ex\u00ad\nponential of the average negative log-likelihood. The only constraint on the model is that \nfor any choice of wi-i, Ei~i f(i, Wt-i, Wt-n) = 1. By the product of these conditional \nprobabilities, one obtains a model of the joint probability of any sequence of words. The basic form of the model is described here.","The direct architecture was found about 2% better \nthan the cycling architecture. Conditional mixture helps but even without it the neural net is better. On Brown, the \nbest neural net without the mixture yields a test perplexity of 265, the smoothed trigram \nyields 348, and their conditional mixture yields 258 (i.e., better than both). On Hansard \nthe improvement is less: a neural network yielding 46.7 perplexity, mixed with the trigram \n(54.1), yields a mixture with perplexity 45.1. 5 Conclusions and Proposed Extensions \nThe experiments on two corpora, a medium one 0.2 million words), and a large one (34 \nmillion words) have shown that the proposed approach yields much better perplexity than \na state-of-the-art method, the smoothed trigram, with differences on the order of 20% to \n35%. We believe that the main reason for these improvements is that the proposed approach \nallows to take advantage of the learned distributed representation to fight the curse of di\u00ad\nmensionality with its own weapons: each training sentence informs the model about a \ncombinatorial number of other sentences. Note that if we had a separate feature vector \nfor each \"context\" (short sequence of words), the model would have much more capacity \n(which could grow like that of n-grams) but it would not naturally generalize between the \nmany different ways a word can be used. A more reasonable alternative would be to ex\u00ad\nplore language units other than words (e.g. some short word sequences , or alternatively \nsome sub-word morphemic units). There is probably much more to be done to improve the model, at the level of architecture, \ncomputational efficiency, and taking advantage of prior knowledge . An important priority \nof future research should be to evaluate and improve the speeding-up tricks proposed here, \nand find ways to increase capacity without increasing training time too much (to deal with corpora with hundreds of millions of words). A simple idea to take advantage of temporal \nstructure and extend the size of the input window to include possibly a whole paragraph, \nwithout increasing too much the number of parameters, is to use a time-delay and possibly \nrecurrent neural network. In such a multi-layered network the computation that has been \nperformed for small groups of consecutive words does not need to be redone when the \nnetwork input window is shifted. Similarly , one could use a recurrent network to capture \npotentially even longer term information about the subject of the text. A very important area in which the proposed model could be improved is in the use of prior \nlinguistic knowledge : semantic (e.g. Word Net), syntactic (e.g.","Bengio. Taking on the curse of dimensionality in joint distributions using \nneural networks. IEEE Transactions on Neural Networks, special issue on Data Mining and \nKnowledge Discovery , 11(3):550-557 , 2000. [3] Yoshua Bengio and Samy Bengio. Modeling high-dimensional discrete data with multi-layer \nneural networks . In S."]],"title":[["placeholder_title","placeholder_title","placeholder_title","placeholder_title"]]},"oracle_context":"This function has parameters that can be \niteratively tuned in order to maximize the log-likelihood of the training data or a regularized \ncriterion, e.g. by adding a weight decay penalty. The feature vectors associated with each \nword are learned, but they can be initialized using prior knowledge. Why does it work? In the previous example, if we knew that dog and cat played simi\u00ad\nlar roles (semantically and syntactically), and similarly for (the,a), (bedroom,room), \n(is,was), (running,walking), we could naturally generalize from The cat is \nwalking in the bedroom to A dog was running in a room and likewise \nto many other combinations. In the proposed model, it will so generalize because \"simi\u00ad\nlar\" words should have a similar feature vector, and because the probability function is a \nsmooth function of these feature values, so a small change in the features (to obtain similar \nwords) induces a small change in the probability : seeing only one of the above sentences \nwill increase the probability not only of that sentence but also of its combinatorial number \nof \"neighbors\" in sentence .Ipace (as represented by sequences offeature vectors). 1.2 Relation to Previous Work \nThe idea of using neural networks to model high-dimensional discrete distributions has \nalready been found useful in [3] where the joint probability of Zl ... Zn is decomposed \nas a product of conditional probabilities: P(Zl = Zl,\"', Zn = Zn) = Oi P(Zi = \nzilgi(Zi-l, Zi-2, ... , Zl)), where g(.) is a function represented by part of a neural network, \nand it yields parameters for expressing the distribution of Zi. Experiments on four VCI \ndata sets show this approach to work comparatively very well [3, 2]. The idea of a dis\u00ad\ntributed representation for symbols dates from the early days of connectionism [5]. More \nrecently, Hinton's approach was improved and successfully demonstrated on learning sev\u00ad\neral symbolic relations [9]. The idea of using neural networks for language modeling is not \nnew either, e.g. [8]. In contrast, here we push this idea to a large scale, and concentrate on \nlearning a statistical model of the distribution of word sequences, rather than learning the \nrole of words in a sentence. The proposed approach is also related to previous proposals of character-based text compression using neural networks [11]. Learning a clustering of \nwords [10, 1] is also a way to discover similarities between words. In the model proposed \nhere, instead of characterizing the similarity with a discrete random or deterministic vari\u00ad\nable (which corresponds to a soft or hard partition of the set of words), we use a continuous \nreal-vector for each word, i.e. a distributed feature vector, to indirectly represent similarity \nbetween words. The idea of using a vector-space representation for words has been well \nexploited in the area of information retrieval (for example see [12]), where vectorial fea\u00ad\nture vectors for words are learned on the basis of their probability of co-occurring in the \nsame documents (Latent Semantic Indexing [4]). An important difference is that here we \nlook for a representation for words that is helpful in representing compactly the probabil\u00ad\nity distribution of word sequences from natural language text. Experiments indicate that \nlearning jointly the representation (word features) and the model makes a big difference in \nperformance. 2 The Proposed Model: two Architectures \nThe training set is a sequence Wi ... WT of words Wt E V, where the vocabular y V is a \nlarge but finite set. The objective is to learn a good model f(wt,\u00b7 .. , Wt-n) = P(wtlwi-i), \nin the sense that it gives high out-of-sample likelihood . In the experiments, we will report \nthe geometric average of l\/P(wtlwi-i), also known as perplexity, which is also the ex\u00ad\nponential of the average negative log-likelihood. The only constraint on the model is that \nfor any choice of wi-i, Ei~i f(i, Wt-i, Wt-n) = 1. By the product of these conditional \nprobabilities, one obtains a model of the joint probability of any sequence of words. The basic form of the model is described here.","cot_answer":"To answer the question about the goal of tuning the parameters in the function, we can follow these steps:\n\n1. **Identify the Purpose of Parameter Tuning**: The context mentions that the parameters can be iteratively tuned to maximize the log-likelihood of the training data or a regularized criterion.\n\n2. **Understand the Implications of Maximizing Log-Likelihood**: Maximizing log-likelihood generally means improving the model's ability to predict or represent the data accurately. This is crucial in statistical modeling, especially in language modeling where the goal is to predict sequences of words.\n\n3. **Consider the Role of Feature Vectors**: The context explains that feature vectors associated with each word are learned and can be initialized using prior knowledge. This suggests that tuning parameters helps in refining these representations to better capture the relationships between words.\n\n4. **Link to Generalization**: The context discusses how similar words should have similar feature vectors, which implies that tuning parameters helps the model generalize better across different word combinations.\n\n5. **Summarize the Goal**: The overall goal of tuning the parameters is to enhance the model's performance in predicting word sequences by optimizing the representation of words and their relationships.\n\nNow, we can formulate the final answer based on this reasoning.\n\n<ANSWER>: The goal of tuning the parameters in the function is to maximize the log-likelihood of the training data, thereby improving the model's ability to predict word sequences and generalize across similar words.","instruction":"<DOCUMENT>f(i, Wt-i,\u00b7\u00b7\u00b7, Wt-n) = g(i, C(Wt-i),\u00b7 \u00b7\u00b7, C(Wt-n)). We used the \"soft\u00ad\nmax\" in the output layer of a neural net: P( Wt = ilwi-i) = ehi \/ Ej eh;, \nwhere hi is the neural network output score for word i. (b) The cycling architecture: a function h maps a sequence of feature \nvectors (C(Wt-n),\u00b7\u00b7\u00b7, C(Wt-i), C(i)) (i.e. including the context words \nand a candidate next word i) to a scalar hi, and again using a soft-\nmax, P(Wt = ilwi-i) = ehi\/Ejeh;. f(Wt,Wt-i,\u00b7 \u00b7\u00b7,Wt-n) = \ng(C(Wt), C(wt-d,\u00b7\u00b7\u00b7 ,C(Wt-n)). We call this architecture \"cycling\" be\u00ad\ncause one repeatedly runs h (e.g. a neural net), each time putting in input the \nfeature vector C(i) for a candidate next word i. The function f is a composition of these two mappings (C and g), with C being shared \nacross all the words in the context. To each of these two parts are associated some pa\u00ad\nrameters. The parameters of the mapping C are simply the feature vectors themselves \n(represented by a IVI x m matrix C whose row i is the feature vector C(i) for word i). The \nfunction 9 may be implemented by a feed-forward or recurrent neural network or another \nparameterized function, with parameters (). Table \nlook-up \ninC \nindex fOl wi-n i-ill output = P(Wt = i I eMtext) \nindex fot W'-2 index fot Wt_l : computed only \nfor WOlds in \nshOlt list \nFigure 1: \"Direct Architecture\": f(i, Wt-l,\u00b7\", Wt-n) = g(i, C(Wt-l),\u00b7\u00b7\u00b7, C(Wt-n)) \nwhere 9 is the neural network and C(i) is the i-th word feature vector. Training is achieved by looking for ((), C) that maximize the training corpus penalized log\u00ad\nlikelihood : L = ~ ~t logpw. (C(Wt-n),\u00b7\u00b7\u00b7, C(Wt-l)j ()) + R((), C), where R((), C) is a \nregularization term (e.g. a weight decay ).11()112, that penalizes slightly the norm of ().<\/DOCUMENT>\n<DOCUMENT>This function has parameters that can be \niteratively tuned in order to maximize the log-likelihood of the training data or a regularized \ncriterion, e.g. by adding a weight decay penalty. The feature vectors associated with each \nword are learned, but they can be initialized using prior knowledge. Why does it work? In the previous example, if we knew that dog and cat played simi\u00ad\nlar roles (semantically and syntactically), and similarly for (the,a), (bedroom,room), \n(is,was), (running,walking), we could naturally generalize from The cat is \nwalking in the bedroom to A dog was running in a room and likewise \nto many other combinations. In the proposed model, it will so generalize because \"simi\u00ad\nlar\" words should have a similar feature vector, and because the probability function is a \nsmooth function of these feature values, so a small change in the features (to obtain similar \nwords) induces a small change in the probability : seeing only one of the above sentences \nwill increase the probability not only of that sentence but also of its combinatorial number \nof \"neighbors\" in sentence .Ipace (as represented by sequences offeature vectors). 1.2 Relation to Previous Work \nThe idea of using neural networks to model high-dimensional discrete distributions has \nalready been found useful in [3] where the joint probability of Zl ... Zn is decomposed \nas a product of conditional probabilities: P(Zl = Zl,\"', Zn = Zn) = Oi P(Zi = \nzilgi(Zi-l, Zi-2, ... , Zl)), where g(.) is a function represented by part of a neural network, \nand it yields parameters for expressing the distribution of Zi. Experiments on four VCI \ndata sets show this approach to work comparatively very well [3, 2]. The idea of a dis\u00ad\ntributed representation for symbols dates from the early days of connectionism [5]. More \nrecently, Hinton's approach was improved and successfully demonstrated on learning sev\u00ad\neral symbolic relations [9]. The idea of using neural networks for language modeling is not \nnew either, e.g. [8]. In contrast, here we push this idea to a large scale, and concentrate on \nlearning a statistical model of the distribution of word sequences, rather than learning the \nrole of words in a sentence. The proposed approach is also related to previous proposals of character-based text compression using neural networks [11]. Learning a clustering of \nwords [10, 1] is also a way to discover similarities between words. In the model proposed \nhere, instead of characterizing the similarity with a discrete random or deterministic vari\u00ad\nable (which corresponds to a soft or hard partition of the set of words), we use a continuous \nreal-vector for each word, i.e. a distributed feature vector, to indirectly represent similarity \nbetween words. The idea of using a vector-space representation for words has been well \nexploited in the area of information retrieval (for example see [12]), where vectorial fea\u00ad\nture vectors for words are learned on the basis of their probability of co-occurring in the \nsame documents (Latent Semantic Indexing [4]). An important difference is that here we \nlook for a representation for words that is helpful in representing compactly the probabil\u00ad\nity distribution of word sequences from natural language text. Experiments indicate that \nlearning jointly the representation (word features) and the model makes a big difference in \nperformance. 2 The Proposed Model: two Architectures \nThe training set is a sequence Wi ... WT of words Wt E V, where the vocabular y V is a \nlarge but finite set. The objective is to learn a good model f(wt,\u00b7 .. , Wt-n) = P(wtlwi-i), \nin the sense that it gives high out-of-sample likelihood . In the experiments, we will report \nthe geometric average of l\/P(wtlwi-i), also known as perplexity, which is also the ex\u00ad\nponential of the average negative log-likelihood. The only constraint on the model is that \nfor any choice of wi-i, Ei~i f(i, Wt-i, Wt-n) = 1. By the product of these conditional \nprobabilities, one obtains a model of the joint probability of any sequence of words. The basic form of the model is described here.<\/DOCUMENT>\n<DOCUMENT>The direct architecture was found about 2% better \nthan the cycling architecture. Conditional mixture helps but even without it the neural net is better. On Brown, the \nbest neural net without the mixture yields a test perplexity of 265, the smoothed trigram \nyields 348, and their conditional mixture yields 258 (i.e., better than both). On Hansard \nthe improvement is less: a neural network yielding 46.7 perplexity, mixed with the trigram \n(54.1), yields a mixture with perplexity 45.1. 5 Conclusions and Proposed Extensions \nThe experiments on two corpora, a medium one 0.2 million words), and a large one (34 \nmillion words) have shown that the proposed approach yields much better perplexity than \na state-of-the-art method, the smoothed trigram, with differences on the order of 20% to \n35%. We believe that the main reason for these improvements is that the proposed approach \nallows to take advantage of the learned distributed representation to fight the curse of di\u00ad\nmensionality with its own weapons: each training sentence informs the model about a \ncombinatorial number of other sentences. Note that if we had a separate feature vector \nfor each \"context\" (short sequence of words), the model would have much more capacity \n(which could grow like that of n-grams) but it would not naturally generalize between the \nmany different ways a word can be used. A more reasonable alternative would be to ex\u00ad\nplore language units other than words (e.g. some short word sequences , or alternatively \nsome sub-word morphemic units). There is probably much more to be done to improve the model, at the level of architecture, \ncomputational efficiency, and taking advantage of prior knowledge . An important priority \nof future research should be to evaluate and improve the speeding-up tricks proposed here, \nand find ways to increase capacity without increasing training time too much (to deal with corpora with hundreds of millions of words). A simple idea to take advantage of temporal \nstructure and extend the size of the input window to include possibly a whole paragraph, \nwithout increasing too much the number of parameters, is to use a time-delay and possibly \nrecurrent neural network. In such a multi-layered network the computation that has been \nperformed for small groups of consecutive words does not need to be redone when the \nnetwork input window is shifted. Similarly , one could use a recurrent network to capture \npotentially even longer term information about the subject of the text. A very important area in which the proposed model could be improved is in the use of prior \nlinguistic knowledge : semantic (e.g. Word Net), syntactic (e.g.<\/DOCUMENT>\n<DOCUMENT>Bengio. Taking on the curse of dimensionality in joint distributions using \nneural networks. IEEE Transactions on Neural Networks, special issue on Data Mining and \nKnowledge Discovery , 11(3):550-557 , 2000. [3] Yoshua Bengio and Samy Bengio. Modeling high-dimensional discrete data with multi-layer \nneural networks . In S.<\/DOCUMENT>\n1. What is the goal of tuning the parameters in the function?"}
{"id":"64bcb046-54b1-4943-9ece-227e6f36df35","type":"general","question":"2. How are feature vectors for words learned?","context":{"sentences":[["The benchmark against which the neural network was compared is an interpolated or \nsmoothed trigram model [6]. Let qt = l(Jreq(Wt-l,Wt-2)) represent the discretized fre\u00ad\nquency of occurrence of the context (Wt-l, Wt-2) (we used l(x) = r -log((l + x)\/T)l \nwhere x is the frequency of occurrence of the context and T is the size of the training cor\u00ad\npus). A conditional mixture of the trigram, bigram, unigram and zero-gram was learned on \nthe validation set, with mixture weights conditional on discretized frequency. Below are measures of test set perplexity (geometric average of 1\/ p( Wt Iwi-1) for dif\u00ad\nferent models P. Apparent convergence of the stochastic gradient descent procedure was \nobtained after around 10 epochs for Hansard and after about 50 epochs for Brown, with \na learning rate gradually decreased from approximately 10-3 to 10-5. Weight decay of \n10-4 or 10-5 was used in all the experiments (based on a few experiments compared on \nthe validation set). The main result is that the neural network performs much better than the smoothed trigram. On Brown the best neural network system, according to validation perplexity \n(among different architectures tried, see below) yielded a perplexity of 258, while the \nsmoothed trigram yields a perplexity of 348, which is about 35% worse. This is obtained \nusing a network with the direct architecture mixed with the trigram (conditional mixture), \nwith 30 word features initialized with the SVD method, 40 hidden units, and n = 5 words \nof context. On Hansard, the corresponding figures are 44.8 for the neural network and 54.1 \nfor the smoothed trigram, which is 20.7% worse. This is obtained with a network with the \ndirect architecture, 100 randomly initialized words features, 120 hidden units, and n = 8 \nwords of context.","This function has parameters that can be \niteratively tuned in order to maximize the log-likelihood of the training data or a regularized \ncriterion, e.g. by adding a weight decay penalty. The feature vectors associated with each \nword are learned, but they can be initialized using prior knowledge. Why does it work? In the previous example, if we knew that dog and cat played simi\u00ad\nlar roles (semantically and syntactically), and similarly for (the,a), (bedroom,room), \n(is,was), (running,walking), we could naturally generalize from The cat is \nwalking in the bedroom to A dog was running in a room and likewise \nto many other combinations. In the proposed model, it will so generalize because \"simi\u00ad\nlar\" words should have a similar feature vector, and because the probability function is a \nsmooth function of these feature values, so a small change in the features (to obtain similar \nwords) induces a small change in the probability : seeing only one of the above sentences \nwill increase the probability not only of that sentence but also of its combinatorial number \nof \"neighbors\" in sentence .Ipace (as represented by sequences offeature vectors). 1.2 Relation to Previous Work \nThe idea of using neural networks to model high-dimensional discrete distributions has \nalready been found useful in [3] where the joint probability of Zl ... Zn is decomposed \nas a product of conditional probabilities: P(Zl = Zl,\"', Zn = Zn) = Oi P(Zi = \nzilgi(Zi-l, Zi-2, ... , Zl)), where g(.) is a function represented by part of a neural network, \nand it yields parameters for expressing the distribution of Zi. Experiments on four VCI \ndata sets show this approach to work comparatively very well [3, 2]. The idea of a dis\u00ad\ntributed representation for symbols dates from the early days of connectionism [5]. More \nrecently, Hinton's approach was improved and successfully demonstrated on learning sev\u00ad\neral symbolic relations [9]. The idea of using neural networks for language modeling is not \nnew either, e.g. [8]. In contrast, here we push this idea to a large scale, and concentrate on \nlearning a statistical model of the distribution of word sequences, rather than learning the \nrole of words in a sentence. The proposed approach is also related to previous proposals of character-based text compression using neural networks [11]. Learning a clustering of \nwords [10, 1] is also a way to discover similarities between words. In the model proposed \nhere, instead of characterizing the similarity with a discrete random or deterministic vari\u00ad\nable (which corresponds to a soft or hard partition of the set of words), we use a continuous \nreal-vector for each word, i.e. a distributed feature vector, to indirectly represent similarity \nbetween words. The idea of using a vector-space representation for words has been well \nexploited in the area of information retrieval (for example see [12]), where vectorial fea\u00ad\nture vectors for words are learned on the basis of their probability of co-occurring in the \nsame documents (Latent Semantic Indexing [4]). An important difference is that here we \nlook for a representation for words that is helpful in representing compactly the probabil\u00ad\nity distribution of word sequences from natural language text. Experiments indicate that \nlearning jointly the representation (word features) and the model makes a big difference in \nperformance. 2 The Proposed Model: two Architectures \nThe training set is a sequence Wi ... WT of words Wt E V, where the vocabular y V is a \nlarge but finite set. The objective is to learn a good model f(wt,\u00b7 .. , Wt-n) = P(wtlwi-i), \nin the sense that it gives high out-of-sample likelihood . In the experiments, we will report \nthe geometric average of l\/P(wtlwi-i), also known as perplexity, which is also the ex\u00ad\nponential of the average negative log-likelihood. The only constraint on the model is that \nfor any choice of wi-i, Ei~i f(i, Wt-i, Wt-n) = 1. By the product of these conditional \nprobabilities, one obtains a model of the joint probability of any sequence of words. The basic form of the model is described here.","Katz. Estimation of probabilities from sparse data for the language model component \nof a speech recognizer . IEEE Transactions on Acoustics , Speech, and Signal Processing, ASSP-\n35(3):400-401 , March 1987.","Since we have millions of examples, it is important to con\u00ad\nverge within only a few passes through the data. For very large data sets, stochastic gradient \ndescent convergence time seems to increase sub-linearly with the size of the data set (see \nexperiments on Brown vs Hansard below). To speed up training using stochastic gradient descent, we have found it useful to break the corpus in paragraphs and to randomly permute \nthem. In this way, some of the non-stationarity in the word stream is eliminated, yielding \nfaster convergence. Capacity control. For the \"smaller corpora\" like Brown (1.2 million examples), we have \nfound early stopping and weight decay useful to avoid over-fitting. For the larger corpora, \nour networks still under-fit. For the larger corpora, we have found double-precision com\u00ad\nputation to be very important to obtain good results. Mixture of models. We have found improved performance by combining the probability \npredictions of the neural network with those of the smoothed trigram, with weights that \nwere conditional on the frequency of the context (same procedure used to combine trigram, \nbigram, and unigram in the smoothed trigram). Initialization of word feature vectors. We have tried both random initialization (uniform \nbetween -.01 and .01) and a \"smarter\" method based on a Singular Value Decomposition \n(SVD) of a very large matrix of \"context features\". These context features are formed \nby counting the frequency of occurrence of each word in each one of the most frequent \ncontexts (word sequences) in the corpus. The idea is that \"similar\" words should occur \nwith similar frequency in the same contexts. We used about 9000 most frequent contexts, \nand compressed these to 30 features with the SVD. Out-of-vocabulary words. For an out-of-vocabulary word Wt we need to come up with \na feature vector in order to predict the words that follow, or predict its probability (that \nis only possible with the cycling architecture). We used as feature vector the weighted \naverage feature vector of all the words in the short list, with the weights being the relative \nprobabilities ofthose words: E[C(wt)lhtl = Ei C(i)P(wt = ilht). 4 Experimental Results \nComparative experiments were performed on the Brown and Hansard corpora. The Brown \ncorpus is a stream of 1,181,041 words (from a large variety of English texts and books). The first 800,000 words were used for training, the following 200,000 for validation (model \nselection, weight decay, early stopping) and the remaining 181,041 for testing. The number \nof different words is 47, 578 (including punctuation, distinguishing between upper and \nlower case, and including the syntactical marks used to separate texts and paragraphs). Rare words with frequency:::; 3 were merged into a single token, reducing the vocabulary \nsize to IVI = 16,383. The Hansard corpus (Canadian parliament proceedings, French version) is a stream of \nabout 34 million words, of which 32 millions (set A) was used for training, 1.1 million \n(set B) was used for validation, and 1.2 million (set C) was used for out-of-sample tests. The original data has 106, 936 different words, and those with frequency:::; 10 were merged \ninto a single token, yielding IVI = 30,959 different words."]],"title":[["placeholder_title","placeholder_title","placeholder_title","placeholder_title"]]},"oracle_context":"This function has parameters that can be \niteratively tuned in order to maximize the log-likelihood of the training data or a regularized \ncriterion, e.g. by adding a weight decay penalty. The feature vectors associated with each \nword are learned, but they can be initialized using prior knowledge. Why does it work? In the previous example, if we knew that dog and cat played simi\u00ad\nlar roles (semantically and syntactically), and similarly for (the,a), (bedroom,room), \n(is,was), (running,walking), we could naturally generalize from The cat is \nwalking in the bedroom to A dog was running in a room and likewise \nto many other combinations. In the proposed model, it will so generalize because \"simi\u00ad\nlar\" words should have a similar feature vector, and because the probability function is a \nsmooth function of these feature values, so a small change in the features (to obtain similar \nwords) induces a small change in the probability : seeing only one of the above sentences \nwill increase the probability not only of that sentence but also of its combinatorial number \nof \"neighbors\" in sentence .Ipace (as represented by sequences offeature vectors). 1.2 Relation to Previous Work \nThe idea of using neural networks to model high-dimensional discrete distributions has \nalready been found useful in [3] where the joint probability of Zl ... Zn is decomposed \nas a product of conditional probabilities: P(Zl = Zl,\"', Zn = Zn) = Oi P(Zi = \nzilgi(Zi-l, Zi-2, ... , Zl)), where g(.) is a function represented by part of a neural network, \nand it yields parameters for expressing the distribution of Zi. Experiments on four VCI \ndata sets show this approach to work comparatively very well [3, 2]. The idea of a dis\u00ad\ntributed representation for symbols dates from the early days of connectionism [5]. More \nrecently, Hinton's approach was improved and successfully demonstrated on learning sev\u00ad\neral symbolic relations [9]. The idea of using neural networks for language modeling is not \nnew either, e.g. [8]. In contrast, here we push this idea to a large scale, and concentrate on \nlearning a statistical model of the distribution of word sequences, rather than learning the \nrole of words in a sentence. The proposed approach is also related to previous proposals of character-based text compression using neural networks [11]. Learning a clustering of \nwords [10, 1] is also a way to discover similarities between words. In the model proposed \nhere, instead of characterizing the similarity with a discrete random or deterministic vari\u00ad\nable (which corresponds to a soft or hard partition of the set of words), we use a continuous \nreal-vector for each word, i.e. a distributed feature vector, to indirectly represent similarity \nbetween words. The idea of using a vector-space representation for words has been well \nexploited in the area of information retrieval (for example see [12]), where vectorial fea\u00ad\nture vectors for words are learned on the basis of their probability of co-occurring in the \nsame documents (Latent Semantic Indexing [4]). An important difference is that here we \nlook for a representation for words that is helpful in representing compactly the probabil\u00ad\nity distribution of word sequences from natural language text. Experiments indicate that \nlearning jointly the representation (word features) and the model makes a big difference in \nperformance. 2 The Proposed Model: two Architectures \nThe training set is a sequence Wi ... WT of words Wt E V, where the vocabular y V is a \nlarge but finite set. The objective is to learn a good model f(wt,\u00b7 .. , Wt-n) = P(wtlwi-i), \nin the sense that it gives high out-of-sample likelihood . In the experiments, we will report \nthe geometric average of l\/P(wtlwi-i), also known as perplexity, which is also the ex\u00ad\nponential of the average negative log-likelihood. The only constraint on the model is that \nfor any choice of wi-i, Ei~i f(i, Wt-i, Wt-n) = 1. By the product of these conditional \nprobabilities, one obtains a model of the joint probability of any sequence of words. The basic form of the model is described here.","cot_answer":"To answer the question about how feature vectors for words are learned, we can follow these steps:\n\n1. **Identify the Learning Process**: The context mentions that feature vectors are learned through a function that can be iteratively tuned to maximize the log-likelihood of the training data. This indicates a statistical learning approach.\n\n2. **Initialization with Prior Knowledge**: The context states that feature vectors can be initialized using prior knowledge about word similarities. This suggests that the learning process can start from a reasonable guess based on existing knowledge.\n\n3. **Generalization from Similar Words**: The context explains that if words are semantically and syntactically similar, they will have similar feature vectors. This is crucial because it implies that the model can generalize from known examples to new combinations of words.\n\n4. **Smooth Probability Function**: The context notes that the probability function is smooth concerning the feature values. This means that small changes in the feature vectors (for similar words) will lead to small changes in the predicted probabilities, allowing the model to learn effectively from the data.\n\n5. **Joint Learning of Representations and Model**: The context emphasizes that learning the representation of words (feature vectors) and the model simultaneously improves performance. This indicates that the learning of feature vectors is closely tied to the overall model's ability to predict word sequences.\n\n6. **Distributed Representation**: The context mentions that instead of using discrete variables to represent similarities, a continuous real-vector (distributed feature vector) is used. This allows for a more nuanced representation of word similarities.\n\n7. **Co-occurrence in Documents**: The context references that vector-space representations for words have been learned based on their probability of co-occurring in documents, which is a common method in natural language processing.\n\nBy synthesizing these points, we can conclude that feature vectors for words are learned through a process that involves statistical modeling, initialization with prior knowledge, generalization from similar words, and the use of smooth probability functions, all while learning the representations and the model together.\n\nFinal answer: <ANSWER>: Feature vectors for words are learned through iterative tuning of a statistical model to maximize log-likelihood, initialized with prior knowledge, generalizing from similar words, and using smooth probability functions.","instruction":"<DOCUMENT>The benchmark against which the neural network was compared is an interpolated or \nsmoothed trigram model [6]. Let qt = l(Jreq(Wt-l,Wt-2)) represent the discretized fre\u00ad\nquency of occurrence of the context (Wt-l, Wt-2) (we used l(x) = r -log((l + x)\/T)l \nwhere x is the frequency of occurrence of the context and T is the size of the training cor\u00ad\npus). A conditional mixture of the trigram, bigram, unigram and zero-gram was learned on \nthe validation set, with mixture weights conditional on discretized frequency. Below are measures of test set perplexity (geometric average of 1\/ p( Wt Iwi-1) for dif\u00ad\nferent models P. Apparent convergence of the stochastic gradient descent procedure was \nobtained after around 10 epochs for Hansard and after about 50 epochs for Brown, with \na learning rate gradually decreased from approximately 10-3 to 10-5. Weight decay of \n10-4 or 10-5 was used in all the experiments (based on a few experiments compared on \nthe validation set). The main result is that the neural network performs much better than the smoothed trigram. On Brown the best neural network system, according to validation perplexity \n(among different architectures tried, see below) yielded a perplexity of 258, while the \nsmoothed trigram yields a perplexity of 348, which is about 35% worse. This is obtained \nusing a network with the direct architecture mixed with the trigram (conditional mixture), \nwith 30 word features initialized with the SVD method, 40 hidden units, and n = 5 words \nof context. On Hansard, the corresponding figures are 44.8 for the neural network and 54.1 \nfor the smoothed trigram, which is 20.7% worse. This is obtained with a network with the \ndirect architecture, 100 randomly initialized words features, 120 hidden units, and n = 8 \nwords of context.<\/DOCUMENT>\n<DOCUMENT>This function has parameters that can be \niteratively tuned in order to maximize the log-likelihood of the training data or a regularized \ncriterion, e.g. by adding a weight decay penalty. The feature vectors associated with each \nword are learned, but they can be initialized using prior knowledge. Why does it work? In the previous example, if we knew that dog and cat played simi\u00ad\nlar roles (semantically and syntactically), and similarly for (the,a), (bedroom,room), \n(is,was), (running,walking), we could naturally generalize from The cat is \nwalking in the bedroom to A dog was running in a room and likewise \nto many other combinations. In the proposed model, it will so generalize because \"simi\u00ad\nlar\" words should have a similar feature vector, and because the probability function is a \nsmooth function of these feature values, so a small change in the features (to obtain similar \nwords) induces a small change in the probability : seeing only one of the above sentences \nwill increase the probability not only of that sentence but also of its combinatorial number \nof \"neighbors\" in sentence .Ipace (as represented by sequences offeature vectors). 1.2 Relation to Previous Work \nThe idea of using neural networks to model high-dimensional discrete distributions has \nalready been found useful in [3] where the joint probability of Zl ... Zn is decomposed \nas a product of conditional probabilities: P(Zl = Zl,\"', Zn = Zn) = Oi P(Zi = \nzilgi(Zi-l, Zi-2, ... , Zl)), where g(.) is a function represented by part of a neural network, \nand it yields parameters for expressing the distribution of Zi. Experiments on four VCI \ndata sets show this approach to work comparatively very well [3, 2]. The idea of a dis\u00ad\ntributed representation for symbols dates from the early days of connectionism [5]. More \nrecently, Hinton's approach was improved and successfully demonstrated on learning sev\u00ad\neral symbolic relations [9]. The idea of using neural networks for language modeling is not \nnew either, e.g. [8]. In contrast, here we push this idea to a large scale, and concentrate on \nlearning a statistical model of the distribution of word sequences, rather than learning the \nrole of words in a sentence. The proposed approach is also related to previous proposals of character-based text compression using neural networks [11]. Learning a clustering of \nwords [10, 1] is also a way to discover similarities between words. In the model proposed \nhere, instead of characterizing the similarity with a discrete random or deterministic vari\u00ad\nable (which corresponds to a soft or hard partition of the set of words), we use a continuous \nreal-vector for each word, i.e. a distributed feature vector, to indirectly represent similarity \nbetween words. The idea of using a vector-space representation for words has been well \nexploited in the area of information retrieval (for example see [12]), where vectorial fea\u00ad\nture vectors for words are learned on the basis of their probability of co-occurring in the \nsame documents (Latent Semantic Indexing [4]). An important difference is that here we \nlook for a representation for words that is helpful in representing compactly the probabil\u00ad\nity distribution of word sequences from natural language text. Experiments indicate that \nlearning jointly the representation (word features) and the model makes a big difference in \nperformance. 2 The Proposed Model: two Architectures \nThe training set is a sequence Wi ... WT of words Wt E V, where the vocabular y V is a \nlarge but finite set. The objective is to learn a good model f(wt,\u00b7 .. , Wt-n) = P(wtlwi-i), \nin the sense that it gives high out-of-sample likelihood . In the experiments, we will report \nthe geometric average of l\/P(wtlwi-i), also known as perplexity, which is also the ex\u00ad\nponential of the average negative log-likelihood. The only constraint on the model is that \nfor any choice of wi-i, Ei~i f(i, Wt-i, Wt-n) = 1. By the product of these conditional \nprobabilities, one obtains a model of the joint probability of any sequence of words. The basic form of the model is described here.<\/DOCUMENT>\n<DOCUMENT>Katz. Estimation of probabilities from sparse data for the language model component \nof a speech recognizer . IEEE Transactions on Acoustics , Speech, and Signal Processing, ASSP-\n35(3):400-401 , March 1987.<\/DOCUMENT>\n<DOCUMENT>Since we have millions of examples, it is important to con\u00ad\nverge within only a few passes through the data. For very large data sets, stochastic gradient \ndescent convergence time seems to increase sub-linearly with the size of the data set (see \nexperiments on Brown vs Hansard below). To speed up training using stochastic gradient descent, we have found it useful to break the corpus in paragraphs and to randomly permute \nthem. In this way, some of the non-stationarity in the word stream is eliminated, yielding \nfaster convergence. Capacity control. For the \"smaller corpora\" like Brown (1.2 million examples), we have \nfound early stopping and weight decay useful to avoid over-fitting. For the larger corpora, \nour networks still under-fit. For the larger corpora, we have found double-precision com\u00ad\nputation to be very important to obtain good results. Mixture of models. We have found improved performance by combining the probability \npredictions of the neural network with those of the smoothed trigram, with weights that \nwere conditional on the frequency of the context (same procedure used to combine trigram, \nbigram, and unigram in the smoothed trigram). Initialization of word feature vectors. We have tried both random initialization (uniform \nbetween -.01 and .01) and a \"smarter\" method based on a Singular Value Decomposition \n(SVD) of a very large matrix of \"context features\". These context features are formed \nby counting the frequency of occurrence of each word in each one of the most frequent \ncontexts (word sequences) in the corpus. The idea is that \"similar\" words should occur \nwith similar frequency in the same contexts. We used about 9000 most frequent contexts, \nand compressed these to 30 features with the SVD. Out-of-vocabulary words. For an out-of-vocabulary word Wt we need to come up with \na feature vector in order to predict the words that follow, or predict its probability (that \nis only possible with the cycling architecture). We used as feature vector the weighted \naverage feature vector of all the words in the short list, with the weights being the relative \nprobabilities ofthose words: E[C(wt)lhtl = Ei C(i)P(wt = ilht). 4 Experimental Results \nComparative experiments were performed on the Brown and Hansard corpora. The Brown \ncorpus is a stream of 1,181,041 words (from a large variety of English texts and books). The first 800,000 words were used for training, the following 200,000 for validation (model \nselection, weight decay, early stopping) and the remaining 181,041 for testing. The number \nof different words is 47, 578 (including punctuation, distinguishing between upper and \nlower case, and including the syntactical marks used to separate texts and paragraphs). Rare words with frequency:::; 3 were merged into a single token, reducing the vocabulary \nsize to IVI = 16,383. The Hansard corpus (Canadian parliament proceedings, French version) is a stream of \nabout 34 million words, of which 32 millions (set A) was used for training, 1.1 million \n(set B) was used for validation, and 1.2 million (set C) was used for out-of-sample tests. The original data has 106, 936 different words, and those with frequency:::; 10 were merged \ninto a single token, yielding IVI = 30,959 different words.<\/DOCUMENT>\n2. How are feature vectors for words learned?"}
{"id":"58a2d9e2-0a0d-4e6c-97d8-bbfc31f71dd9","type":"general","question":"3. What do similar words have in common according to the model?","context":{"sentences":[["The benchmark against which the neural network was compared is an interpolated or \nsmoothed trigram model [6]. Let qt = l(Jreq(Wt-l,Wt-2)) represent the discretized fre\u00ad\nquency of occurrence of the context (Wt-l, Wt-2) (we used l(x) = r -log((l + x)\/T)l \nwhere x is the frequency of occurrence of the context and T is the size of the training cor\u00ad\npus). A conditional mixture of the trigram, bigram, unigram and zero-gram was learned on \nthe validation set, with mixture weights conditional on discretized frequency. Below are measures of test set perplexity (geometric average of 1\/ p( Wt Iwi-1) for dif\u00ad\nferent models P. Apparent convergence of the stochastic gradient descent procedure was \nobtained after around 10 epochs for Hansard and after about 50 epochs for Brown, with \na learning rate gradually decreased from approximately 10-3 to 10-5. Weight decay of \n10-4 or 10-5 was used in all the experiments (based on a few experiments compared on \nthe validation set). The main result is that the neural network performs much better than the smoothed trigram. On Brown the best neural network system, according to validation perplexity \n(among different architectures tried, see below) yielded a perplexity of 258, while the \nsmoothed trigram yields a perplexity of 348, which is about 35% worse. This is obtained \nusing a network with the direct architecture mixed with the trigram (conditional mixture), \nwith 30 word features initialized with the SVD method, 40 hidden units, and n = 5 words \nof context. On Hansard, the corresponding figures are 44.8 for the neural network and 54.1 \nfor the smoothed trigram, which is 20.7% worse. This is obtained with a network with the \ndirect architecture, 100 randomly initialized words features, 120 hidden units, and n = 8 \nwords of context.","This function has parameters that can be \niteratively tuned in order to maximize the log-likelihood of the training data or a regularized \ncriterion, e.g. by adding a weight decay penalty. The feature vectors associated with each \nword are learned, but they can be initialized using prior knowledge. Why does it work? In the previous example, if we knew that dog and cat played simi\u00ad\nlar roles (semantically and syntactically), and similarly for (the,a), (bedroom,room), \n(is,was), (running,walking), we could naturally generalize from The cat is \nwalking in the bedroom to A dog was running in a room and likewise \nto many other combinations. In the proposed model, it will so generalize because \"simi\u00ad\nlar\" words should have a similar feature vector, and because the probability function is a \nsmooth function of these feature values, so a small change in the features (to obtain similar \nwords) induces a small change in the probability : seeing only one of the above sentences \nwill increase the probability not only of that sentence but also of its combinatorial number \nof \"neighbors\" in sentence .Ipace (as represented by sequences offeature vectors). 1.2 Relation to Previous Work \nThe idea of using neural networks to model high-dimensional discrete distributions has \nalready been found useful in [3] where the joint probability of Zl ... Zn is decomposed \nas a product of conditional probabilities: P(Zl = Zl,\"', Zn = Zn) = Oi P(Zi = \nzilgi(Zi-l, Zi-2, ... , Zl)), where g(.) is a function represented by part of a neural network, \nand it yields parameters for expressing the distribution of Zi. Experiments on four VCI \ndata sets show this approach to work comparatively very well [3, 2]. The idea of a dis\u00ad\ntributed representation for symbols dates from the early days of connectionism [5]. More \nrecently, Hinton's approach was improved and successfully demonstrated on learning sev\u00ad\neral symbolic relations [9]. The idea of using neural networks for language modeling is not \nnew either, e.g. [8]. In contrast, here we push this idea to a large scale, and concentrate on \nlearning a statistical model of the distribution of word sequences, rather than learning the \nrole of words in a sentence. The proposed approach is also related to previous proposals of character-based text compression using neural networks [11]. Learning a clustering of \nwords [10, 1] is also a way to discover similarities between words. In the model proposed \nhere, instead of characterizing the similarity with a discrete random or deterministic vari\u00ad\nable (which corresponds to a soft or hard partition of the set of words), we use a continuous \nreal-vector for each word, i.e. a distributed feature vector, to indirectly represent similarity \nbetween words. The idea of using a vector-space representation for words has been well \nexploited in the area of information retrieval (for example see [12]), where vectorial fea\u00ad\nture vectors for words are learned on the basis of their probability of co-occurring in the \nsame documents (Latent Semantic Indexing [4]). An important difference is that here we \nlook for a representation for words that is helpful in representing compactly the probabil\u00ad\nity distribution of word sequences from natural language text. Experiments indicate that \nlearning jointly the representation (word features) and the model makes a big difference in \nperformance. 2 The Proposed Model: two Architectures \nThe training set is a sequence Wi ... WT of words Wt E V, where the vocabular y V is a \nlarge but finite set. The objective is to learn a good model f(wt,\u00b7 .. , Wt-n) = P(wtlwi-i), \nin the sense that it gives high out-of-sample likelihood . In the experiments, we will report \nthe geometric average of l\/P(wtlwi-i), also known as perplexity, which is also the ex\u00ad\nponential of the average negative log-likelihood. The only constraint on the model is that \nfor any choice of wi-i, Ei~i f(i, Wt-i, Wt-n) = 1. By the product of these conditional \nprobabilities, one obtains a model of the joint probability of any sequence of words. The basic form of the model is described here.","A Neural Probabilistic Language Model \nYoshua Bengio; Rejean Ducharme and Pascal Vincent \nDepartement d'Informatique et Recherche Operationnelle \nCentre de Recherche Mathematiques \nUniversite de Montreal \nMontreal, Quebec, Canada, H3C 317 \n{bengioy,ducharme, vincentp }@iro.umontreal.ca \nAbstract \nA goal of statistical language modeling is to learn the joint probability \nfunction of sequences of words. This is intrinsically difficult because of \nthe curse of dimensionality: we propose to fight it with its own weapons.","Hinton. Extracting distributed representations of concepts and relations \nfrom positive and negative propositions. In Proceedings of the International Joint Conference \non Neural Network, lJCNN'2000, Como, Italy, 2000."]],"title":[["placeholder_title","placeholder_title","placeholder_title","placeholder_title"]]},"oracle_context":"This function has parameters that can be \niteratively tuned in order to maximize the log-likelihood of the training data or a regularized \ncriterion, e.g. by adding a weight decay penalty. The feature vectors associated with each \nword are learned, but they can be initialized using prior knowledge. Why does it work? In the previous example, if we knew that dog and cat played simi\u00ad\nlar roles (semantically and syntactically), and similarly for (the,a), (bedroom,room), \n(is,was), (running,walking), we could naturally generalize from The cat is \nwalking in the bedroom to A dog was running in a room and likewise \nto many other combinations. In the proposed model, it will so generalize because \"simi\u00ad\nlar\" words should have a similar feature vector, and because the probability function is a \nsmooth function of these feature values, so a small change in the features (to obtain similar \nwords) induces a small change in the probability : seeing only one of the above sentences \nwill increase the probability not only of that sentence but also of its combinatorial number \nof \"neighbors\" in sentence .Ipace (as represented by sequences offeature vectors). 1.2 Relation to Previous Work \nThe idea of using neural networks to model high-dimensional discrete distributions has \nalready been found useful in [3] where the joint probability of Zl ... Zn is decomposed \nas a product of conditional probabilities: P(Zl = Zl,\"', Zn = Zn) = Oi P(Zi = \nzilgi(Zi-l, Zi-2, ... , Zl)), where g(.) is a function represented by part of a neural network, \nand it yields parameters for expressing the distribution of Zi. Experiments on four VCI \ndata sets show this approach to work comparatively very well [3, 2]. The idea of a dis\u00ad\ntributed representation for symbols dates from the early days of connectionism [5]. More \nrecently, Hinton's approach was improved and successfully demonstrated on learning sev\u00ad\neral symbolic relations [9]. The idea of using neural networks for language modeling is not \nnew either, e.g. [8]. In contrast, here we push this idea to a large scale, and concentrate on \nlearning a statistical model of the distribution of word sequences, rather than learning the \nrole of words in a sentence. The proposed approach is also related to previous proposals of character-based text compression using neural networks [11]. Learning a clustering of \nwords [10, 1] is also a way to discover similarities between words. In the model proposed \nhere, instead of characterizing the similarity with a discrete random or deterministic vari\u00ad\nable (which corresponds to a soft or hard partition of the set of words), we use a continuous \nreal-vector for each word, i.e. a distributed feature vector, to indirectly represent similarity \nbetween words. The idea of using a vector-space representation for words has been well \nexploited in the area of information retrieval (for example see [12]), where vectorial fea\u00ad\nture vectors for words are learned on the basis of their probability of co-occurring in the \nsame documents (Latent Semantic Indexing [4]). An important difference is that here we \nlook for a representation for words that is helpful in representing compactly the probabil\u00ad\nity distribution of word sequences from natural language text. Experiments indicate that \nlearning jointly the representation (word features) and the model makes a big difference in \nperformance. 2 The Proposed Model: two Architectures \nThe training set is a sequence Wi ... WT of words Wt E V, where the vocabular y V is a \nlarge but finite set. The objective is to learn a good model f(wt,\u00b7 .. , Wt-n) = P(wtlwi-i), \nin the sense that it gives high out-of-sample likelihood . In the experiments, we will report \nthe geometric average of l\/P(wtlwi-i), also known as perplexity, which is also the ex\u00ad\nponential of the average negative log-likelihood. The only constraint on the model is that \nfor any choice of wi-i, Ei~i f(i, Wt-i, Wt-n) = 1. By the product of these conditional \nprobabilities, one obtains a model of the joint probability of any sequence of words. The basic form of the model is described here.","cot_answer":"To answer the question about what similar words have in common according to the model, we can follow these steps:\n\n1. Identify the key concept in the context regarding similar words.\n2. Look for specific phrases or sentences that describe the relationship between similar words and their feature vectors.\n3. Summarize the findings in a clear and concise manner.\n\nFrom the context, we see that the model suggests that similar words should have a similar feature vector. This is crucial because it implies that the model can generalize from one word to another based on their semantic and syntactic roles. \n\nThe relevant part of the context states: ##begin_quote## \"In the proposed model, it will so generalize because 'similar' words should have a similar feature vector, and because the probability function is a smooth function of these feature values, so a small change in the features (to obtain similar words) induces a small change in the probability.\" ##end_quote## \n\nThis indicates that the commonality among similar words is their feature vectors, which are designed to be close to each other in the model's representation.\n\nFinal answer: <ANSWER>: Similar words have similar feature vectors according to the model.","instruction":"<DOCUMENT>The benchmark against which the neural network was compared is an interpolated or \nsmoothed trigram model [6]. Let qt = l(Jreq(Wt-l,Wt-2)) represent the discretized fre\u00ad\nquency of occurrence of the context (Wt-l, Wt-2) (we used l(x) = r -log((l + x)\/T)l \nwhere x is the frequency of occurrence of the context and T is the size of the training cor\u00ad\npus). A conditional mixture of the trigram, bigram, unigram and zero-gram was learned on \nthe validation set, with mixture weights conditional on discretized frequency. Below are measures of test set perplexity (geometric average of 1\/ p( Wt Iwi-1) for dif\u00ad\nferent models P. Apparent convergence of the stochastic gradient descent procedure was \nobtained after around 10 epochs for Hansard and after about 50 epochs for Brown, with \na learning rate gradually decreased from approximately 10-3 to 10-5. Weight decay of \n10-4 or 10-5 was used in all the experiments (based on a few experiments compared on \nthe validation set). The main result is that the neural network performs much better than the smoothed trigram. On Brown the best neural network system, according to validation perplexity \n(among different architectures tried, see below) yielded a perplexity of 258, while the \nsmoothed trigram yields a perplexity of 348, which is about 35% worse. This is obtained \nusing a network with the direct architecture mixed with the trigram (conditional mixture), \nwith 30 word features initialized with the SVD method, 40 hidden units, and n = 5 words \nof context. On Hansard, the corresponding figures are 44.8 for the neural network and 54.1 \nfor the smoothed trigram, which is 20.7% worse. This is obtained with a network with the \ndirect architecture, 100 randomly initialized words features, 120 hidden units, and n = 8 \nwords of context.<\/DOCUMENT>\n<DOCUMENT>This function has parameters that can be \niteratively tuned in order to maximize the log-likelihood of the training data or a regularized \ncriterion, e.g. by adding a weight decay penalty. The feature vectors associated with each \nword are learned, but they can be initialized using prior knowledge. Why does it work? In the previous example, if we knew that dog and cat played simi\u00ad\nlar roles (semantically and syntactically), and similarly for (the,a), (bedroom,room), \n(is,was), (running,walking), we could naturally generalize from The cat is \nwalking in the bedroom to A dog was running in a room and likewise \nto many other combinations. In the proposed model, it will so generalize because \"simi\u00ad\nlar\" words should have a similar feature vector, and because the probability function is a \nsmooth function of these feature values, so a small change in the features (to obtain similar \nwords) induces a small change in the probability : seeing only one of the above sentences \nwill increase the probability not only of that sentence but also of its combinatorial number \nof \"neighbors\" in sentence .Ipace (as represented by sequences offeature vectors). 1.2 Relation to Previous Work \nThe idea of using neural networks to model high-dimensional discrete distributions has \nalready been found useful in [3] where the joint probability of Zl ... Zn is decomposed \nas a product of conditional probabilities: P(Zl = Zl,\"', Zn = Zn) = Oi P(Zi = \nzilgi(Zi-l, Zi-2, ... , Zl)), where g(.) is a function represented by part of a neural network, \nand it yields parameters for expressing the distribution of Zi. Experiments on four VCI \ndata sets show this approach to work comparatively very well [3, 2]. The idea of a dis\u00ad\ntributed representation for symbols dates from the early days of connectionism [5]. More \nrecently, Hinton's approach was improved and successfully demonstrated on learning sev\u00ad\neral symbolic relations [9]. The idea of using neural networks for language modeling is not \nnew either, e.g. [8]. In contrast, here we push this idea to a large scale, and concentrate on \nlearning a statistical model of the distribution of word sequences, rather than learning the \nrole of words in a sentence. The proposed approach is also related to previous proposals of character-based text compression using neural networks [11]. Learning a clustering of \nwords [10, 1] is also a way to discover similarities between words. In the model proposed \nhere, instead of characterizing the similarity with a discrete random or deterministic vari\u00ad\nable (which corresponds to a soft or hard partition of the set of words), we use a continuous \nreal-vector for each word, i.e. a distributed feature vector, to indirectly represent similarity \nbetween words. The idea of using a vector-space representation for words has been well \nexploited in the area of information retrieval (for example see [12]), where vectorial fea\u00ad\nture vectors for words are learned on the basis of their probability of co-occurring in the \nsame documents (Latent Semantic Indexing [4]). An important difference is that here we \nlook for a representation for words that is helpful in representing compactly the probabil\u00ad\nity distribution of word sequences from natural language text. Experiments indicate that \nlearning jointly the representation (word features) and the model makes a big difference in \nperformance. 2 The Proposed Model: two Architectures \nThe training set is a sequence Wi ... WT of words Wt E V, where the vocabular y V is a \nlarge but finite set. The objective is to learn a good model f(wt,\u00b7 .. , Wt-n) = P(wtlwi-i), \nin the sense that it gives high out-of-sample likelihood . In the experiments, we will report \nthe geometric average of l\/P(wtlwi-i), also known as perplexity, which is also the ex\u00ad\nponential of the average negative log-likelihood. The only constraint on the model is that \nfor any choice of wi-i, Ei~i f(i, Wt-i, Wt-n) = 1. By the product of these conditional \nprobabilities, one obtains a model of the joint probability of any sequence of words. The basic form of the model is described here.<\/DOCUMENT>\n<DOCUMENT>A Neural Probabilistic Language Model \nYoshua Bengio; Rejean Ducharme and Pascal Vincent \nDepartement d'Informatique et Recherche Operationnelle \nCentre de Recherche Mathematiques \nUniversite de Montreal \nMontreal, Quebec, Canada, H3C 317 \n{bengioy,ducharme, vincentp }@iro.umontreal.ca \nAbstract \nA goal of statistical language modeling is to learn the joint probability \nfunction of sequences of words. This is intrinsically difficult because of \nthe curse of dimensionality: we propose to fight it with its own weapons.<\/DOCUMENT>\n<DOCUMENT>Hinton. Extracting distributed representations of concepts and relations \nfrom positive and negative propositions. In Proceedings of the International Joint Conference \non Neural Network, lJCNN'2000, Como, Italy, 2000.<\/DOCUMENT>\n3. What do similar words have in common according to the model?"}
{"id":"64cc12ec-7f2e-48ff-8d3d-9200b6f37f4c","type":"general","question":"4. What does the proposed approach focus on regarding word sequences?","context":{"sentences":[["This function has parameters that can be \niteratively tuned in order to maximize the log-likelihood of the training data or a regularized \ncriterion, e.g. by adding a weight decay penalty. The feature vectors associated with each \nword are learned, but they can be initialized using prior knowledge. Why does it work? In the previous example, if we knew that dog and cat played simi\u00ad\nlar roles (semantically and syntactically), and similarly for (the,a), (bedroom,room), \n(is,was), (running,walking), we could naturally generalize from The cat is \nwalking in the bedroom to A dog was running in a room and likewise \nto many other combinations. In the proposed model, it will so generalize because \"simi\u00ad\nlar\" words should have a similar feature vector, and because the probability function is a \nsmooth function of these feature values, so a small change in the features (to obtain similar \nwords) induces a small change in the probability : seeing only one of the above sentences \nwill increase the probability not only of that sentence but also of its combinatorial number \nof \"neighbors\" in sentence .Ipace (as represented by sequences offeature vectors). 1.2 Relation to Previous Work \nThe idea of using neural networks to model high-dimensional discrete distributions has \nalready been found useful in [3] where the joint probability of Zl ... Zn is decomposed \nas a product of conditional probabilities: P(Zl = Zl,\"', Zn = Zn) = Oi P(Zi = \nzilgi(Zi-l, Zi-2, ... , Zl)), where g(.) is a function represented by part of a neural network, \nand it yields parameters for expressing the distribution of Zi. Experiments on four VCI \ndata sets show this approach to work comparatively very well [3, 2]. The idea of a dis\u00ad\ntributed representation for symbols dates from the early days of connectionism [5]. More \nrecently, Hinton's approach was improved and successfully demonstrated on learning sev\u00ad\neral symbolic relations [9]. The idea of using neural networks for language modeling is not \nnew either, e.g. [8]. In contrast, here we push this idea to a large scale, and concentrate on \nlearning a statistical model of the distribution of word sequences, rather than learning the \nrole of words in a sentence. The proposed approach is also related to previous proposals of character-based text compression using neural networks [11]. Learning a clustering of \nwords [10, 1] is also a way to discover similarities between words. In the model proposed \nhere, instead of characterizing the similarity with a discrete random or deterministic vari\u00ad\nable (which corresponds to a soft or hard partition of the set of words), we use a continuous \nreal-vector for each word, i.e. a distributed feature vector, to indirectly represent similarity \nbetween words. The idea of using a vector-space representation for words has been well \nexploited in the area of information retrieval (for example see [12]), where vectorial fea\u00ad\nture vectors for words are learned on the basis of their probability of co-occurring in the \nsame documents (Latent Semantic Indexing [4]). An important difference is that here we \nlook for a representation for words that is helpful in representing compactly the probabil\u00ad\nity distribution of word sequences from natural language text. Experiments indicate that \nlearning jointly the representation (word features) and the model makes a big difference in \nperformance. 2 The Proposed Model: two Architectures \nThe training set is a sequence Wi ... WT of words Wt E V, where the vocabular y V is a \nlarge but finite set. The objective is to learn a good model f(wt,\u00b7 .. , Wt-n) = P(wtlwi-i), \nin the sense that it gives high out-of-sample likelihood . In the experiments, we will report \nthe geometric average of l\/P(wtlwi-i), also known as perplexity, which is also the ex\u00ad\nponential of the average negative log-likelihood. The only constraint on the model is that \nfor any choice of wi-i, Ei~i f(i, Wt-i, Wt-n) = 1. By the product of these conditional \nprobabilities, one obtains a model of the joint probability of any sequence of words. The basic form of the model is described here.","Katz. Estimation of probabilities from sparse data for the language model component \nof a speech recognizer . IEEE Transactions on Acoustics , Speech, and Signal Processing, ASSP-\n35(3):400-401 , March 1987.","f(i, Wt-i,\u00b7\u00b7\u00b7, Wt-n) = g(i, C(Wt-i),\u00b7 \u00b7\u00b7, C(Wt-n)). We used the \"soft\u00ad\nmax\" in the output layer of a neural net: P( Wt = ilwi-i) = ehi \/ Ej eh;, \nwhere hi is the neural network output score for word i. (b) The cycling architecture: a function h maps a sequence of feature \nvectors (C(Wt-n),\u00b7\u00b7\u00b7, C(Wt-i), C(i)) (i.e. including the context words \nand a candidate next word i) to a scalar hi, and again using a soft-\nmax, P(Wt = ilwi-i) = ehi\/Ejeh;. f(Wt,Wt-i,\u00b7 \u00b7\u00b7,Wt-n) = \ng(C(Wt), C(wt-d,\u00b7\u00b7\u00b7 ,C(Wt-n)). We call this architecture \"cycling\" be\u00ad\ncause one repeatedly runs h (e.g. a neural net), each time putting in input the \nfeature vector C(i) for a candidate next word i. The function f is a composition of these two mappings (C and g), with C being shared \nacross all the words in the context. To each of these two parts are associated some pa\u00ad\nrameters. The parameters of the mapping C are simply the feature vectors themselves \n(represented by a IVI x m matrix C whose row i is the feature vector C(i) for word i). The \nfunction 9 may be implemented by a feed-forward or recurrent neural network or another \nparameterized function, with parameters (). Table \nlook-up \ninC \nindex fOl wi-n i-ill output = P(Wt = i I eMtext) \nindex fot W'-2 index fot Wt_l : computed only \nfor WOlds in \nshOlt list \nFigure 1: \"Direct Architecture\": f(i, Wt-l,\u00b7\", Wt-n) = g(i, C(Wt-l),\u00b7\u00b7\u00b7, C(Wt-n)) \nwhere 9 is the neural network and C(i) is the i-th word feature vector. Training is achieved by looking for ((), C) that maximize the training corpus penalized log\u00ad\nlikelihood : L = ~ ~t logpw. (C(Wt-n),\u00b7\u00b7\u00b7, C(Wt-l)j ()) + R((), C), where R((), C) is a \nregularization term (e.g. a weight decay ).11()112, that penalizes slightly the norm of ().","a tagger), and morpho\u00ad\nlogical (radix and morphemes). Looking at the word features learned by the model should \nhelp understand it and improve it. Finally, future research should establish how useful the \nproposed approach will be in applications to speech recognition, language translation, and \ninformation retrieval. Acknowledgments \nThe authors would like to thank Leon Bottou and Yann Le Cun for useful discussions."]],"title":[["placeholder_title","placeholder_title","placeholder_title","placeholder_title"]]},"oracle_context":"This function has parameters that can be \niteratively tuned in order to maximize the log-likelihood of the training data or a regularized \ncriterion, e.g. by adding a weight decay penalty. The feature vectors associated with each \nword are learned, but they can be initialized using prior knowledge. Why does it work? In the previous example, if we knew that dog and cat played simi\u00ad\nlar roles (semantically and syntactically), and similarly for (the,a), (bedroom,room), \n(is,was), (running,walking), we could naturally generalize from The cat is \nwalking in the bedroom to A dog was running in a room and likewise \nto many other combinations. In the proposed model, it will so generalize because \"simi\u00ad\nlar\" words should have a similar feature vector, and because the probability function is a \nsmooth function of these feature values, so a small change in the features (to obtain similar \nwords) induces a small change in the probability : seeing only one of the above sentences \nwill increase the probability not only of that sentence but also of its combinatorial number \nof \"neighbors\" in sentence .Ipace (as represented by sequences offeature vectors). 1.2 Relation to Previous Work \nThe idea of using neural networks to model high-dimensional discrete distributions has \nalready been found useful in [3] where the joint probability of Zl ... Zn is decomposed \nas a product of conditional probabilities: P(Zl = Zl,\"', Zn = Zn) = Oi P(Zi = \nzilgi(Zi-l, Zi-2, ... , Zl)), where g(.) is a function represented by part of a neural network, \nand it yields parameters for expressing the distribution of Zi. Experiments on four VCI \ndata sets show this approach to work comparatively very well [3, 2]. The idea of a dis\u00ad\ntributed representation for symbols dates from the early days of connectionism [5]. More \nrecently, Hinton's approach was improved and successfully demonstrated on learning sev\u00ad\neral symbolic relations [9]. The idea of using neural networks for language modeling is not \nnew either, e.g. [8]. In contrast, here we push this idea to a large scale, and concentrate on \nlearning a statistical model of the distribution of word sequences, rather than learning the \nrole of words in a sentence. The proposed approach is also related to previous proposals of character-based text compression using neural networks [11]. Learning a clustering of \nwords [10, 1] is also a way to discover similarities between words. In the model proposed \nhere, instead of characterizing the similarity with a discrete random or deterministic vari\u00ad\nable (which corresponds to a soft or hard partition of the set of words), we use a continuous \nreal-vector for each word, i.e. a distributed feature vector, to indirectly represent similarity \nbetween words. The idea of using a vector-space representation for words has been well \nexploited in the area of information retrieval (for example see [12]), where vectorial fea\u00ad\nture vectors for words are learned on the basis of their probability of co-occurring in the \nsame documents (Latent Semantic Indexing [4]). An important difference is that here we \nlook for a representation for words that is helpful in representing compactly the probabil\u00ad\nity distribution of word sequences from natural language text. Experiments indicate that \nlearning jointly the representation (word features) and the model makes a big difference in \nperformance. 2 The Proposed Model: two Architectures \nThe training set is a sequence Wi ... WT of words Wt E V, where the vocabular y V is a \nlarge but finite set. The objective is to learn a good model f(wt,\u00b7 .. , Wt-n) = P(wtlwi-i), \nin the sense that it gives high out-of-sample likelihood . In the experiments, we will report \nthe geometric average of l\/P(wtlwi-i), also known as perplexity, which is also the ex\u00ad\nponential of the average negative log-likelihood. The only constraint on the model is that \nfor any choice of wi-i, Ei~i f(i, Wt-i, Wt-n) = 1. By the product of these conditional \nprobabilities, one obtains a model of the joint probability of any sequence of words. The basic form of the model is described here.","cot_answer":"To answer the question about what the proposed approach focuses on regarding word sequences, we can follow these steps:\n\n1. **Identify Key Themes**: The context discusses a model that learns to represent words and their sequences in a way that captures their similarities and relationships.\n\n2. **Look for Specific Mentions**: We need to find phrases or sentences that explicitly state what the model aims to achieve concerning word sequences.\n\n3. **Extract Relevant Information**: The context mentions that the proposed approach focuses on learning a statistical model of the distribution of word sequences, rather than just the roles of words in a sentence.\n\n4. **Summarize the Findings**: We can summarize the key focus of the proposed approach based on the extracted information.\n\nNow, let's extract the relevant quote from the context:\n\n##begin_quote##\n\"the proposed approach is also related to previous proposals of character-based text compression using neural networks... we push this idea to a large scale, and concentrate on learning a statistical model of the distribution of word sequences, rather than learning the role of words in a sentence.\"\n##end_quote##\n\nBased on this reasoning and the extracted information, we can conclude that the proposed approach focuses on learning a statistical model of the distribution of word sequences.\n\n<ANSWER>: The proposed approach focuses on learning a statistical model of the distribution of word sequences.","instruction":"<DOCUMENT>This function has parameters that can be \niteratively tuned in order to maximize the log-likelihood of the training data or a regularized \ncriterion, e.g. by adding a weight decay penalty. The feature vectors associated with each \nword are learned, but they can be initialized using prior knowledge. Why does it work? In the previous example, if we knew that dog and cat played simi\u00ad\nlar roles (semantically and syntactically), and similarly for (the,a), (bedroom,room), \n(is,was), (running,walking), we could naturally generalize from The cat is \nwalking in the bedroom to A dog was running in a room and likewise \nto many other combinations. In the proposed model, it will so generalize because \"simi\u00ad\nlar\" words should have a similar feature vector, and because the probability function is a \nsmooth function of these feature values, so a small change in the features (to obtain similar \nwords) induces a small change in the probability : seeing only one of the above sentences \nwill increase the probability not only of that sentence but also of its combinatorial number \nof \"neighbors\" in sentence .Ipace (as represented by sequences offeature vectors). 1.2 Relation to Previous Work \nThe idea of using neural networks to model high-dimensional discrete distributions has \nalready been found useful in [3] where the joint probability of Zl ... Zn is decomposed \nas a product of conditional probabilities: P(Zl = Zl,\"', Zn = Zn) = Oi P(Zi = \nzilgi(Zi-l, Zi-2, ... , Zl)), where g(.) is a function represented by part of a neural network, \nand it yields parameters for expressing the distribution of Zi. Experiments on four VCI \ndata sets show this approach to work comparatively very well [3, 2]. The idea of a dis\u00ad\ntributed representation for symbols dates from the early days of connectionism [5]. More \nrecently, Hinton's approach was improved and successfully demonstrated on learning sev\u00ad\neral symbolic relations [9]. The idea of using neural networks for language modeling is not \nnew either, e.g. [8]. In contrast, here we push this idea to a large scale, and concentrate on \nlearning a statistical model of the distribution of word sequences, rather than learning the \nrole of words in a sentence. The proposed approach is also related to previous proposals of character-based text compression using neural networks [11]. Learning a clustering of \nwords [10, 1] is also a way to discover similarities between words. In the model proposed \nhere, instead of characterizing the similarity with a discrete random or deterministic vari\u00ad\nable (which corresponds to a soft or hard partition of the set of words), we use a continuous \nreal-vector for each word, i.e. a distributed feature vector, to indirectly represent similarity \nbetween words. The idea of using a vector-space representation for words has been well \nexploited in the area of information retrieval (for example see [12]), where vectorial fea\u00ad\nture vectors for words are learned on the basis of their probability of co-occurring in the \nsame documents (Latent Semantic Indexing [4]). An important difference is that here we \nlook for a representation for words that is helpful in representing compactly the probabil\u00ad\nity distribution of word sequences from natural language text. Experiments indicate that \nlearning jointly the representation (word features) and the model makes a big difference in \nperformance. 2 The Proposed Model: two Architectures \nThe training set is a sequence Wi ... WT of words Wt E V, where the vocabular y V is a \nlarge but finite set. The objective is to learn a good model f(wt,\u00b7 .. , Wt-n) = P(wtlwi-i), \nin the sense that it gives high out-of-sample likelihood . In the experiments, we will report \nthe geometric average of l\/P(wtlwi-i), also known as perplexity, which is also the ex\u00ad\nponential of the average negative log-likelihood. The only constraint on the model is that \nfor any choice of wi-i, Ei~i f(i, Wt-i, Wt-n) = 1. By the product of these conditional \nprobabilities, one obtains a model of the joint probability of any sequence of words. The basic form of the model is described here.<\/DOCUMENT>\n<DOCUMENT>Katz. Estimation of probabilities from sparse data for the language model component \nof a speech recognizer . IEEE Transactions on Acoustics , Speech, and Signal Processing, ASSP-\n35(3):400-401 , March 1987.<\/DOCUMENT>\n<DOCUMENT>f(i, Wt-i,\u00b7\u00b7\u00b7, Wt-n) = g(i, C(Wt-i),\u00b7 \u00b7\u00b7, C(Wt-n)). We used the \"soft\u00ad\nmax\" in the output layer of a neural net: P( Wt = ilwi-i) = ehi \/ Ej eh;, \nwhere hi is the neural network output score for word i. (b) The cycling architecture: a function h maps a sequence of feature \nvectors (C(Wt-n),\u00b7\u00b7\u00b7, C(Wt-i), C(i)) (i.e. including the context words \nand a candidate next word i) to a scalar hi, and again using a soft-\nmax, P(Wt = ilwi-i) = ehi\/Ejeh;. f(Wt,Wt-i,\u00b7 \u00b7\u00b7,Wt-n) = \ng(C(Wt), C(wt-d,\u00b7\u00b7\u00b7 ,C(Wt-n)). We call this architecture \"cycling\" be\u00ad\ncause one repeatedly runs h (e.g. a neural net), each time putting in input the \nfeature vector C(i) for a candidate next word i. The function f is a composition of these two mappings (C and g), with C being shared \nacross all the words in the context. To each of these two parts are associated some pa\u00ad\nrameters. The parameters of the mapping C are simply the feature vectors themselves \n(represented by a IVI x m matrix C whose row i is the feature vector C(i) for word i). The \nfunction 9 may be implemented by a feed-forward or recurrent neural network or another \nparameterized function, with parameters (). Table \nlook-up \ninC \nindex fOl wi-n i-ill output = P(Wt = i I eMtext) \nindex fot W'-2 index fot Wt_l : computed only \nfor WOlds in \nshOlt list \nFigure 1: \"Direct Architecture\": f(i, Wt-l,\u00b7\", Wt-n) = g(i, C(Wt-l),\u00b7\u00b7\u00b7, C(Wt-n)) \nwhere 9 is the neural network and C(i) is the i-th word feature vector. Training is achieved by looking for ((), C) that maximize the training corpus penalized log\u00ad\nlikelihood : L = ~ ~t logpw. (C(Wt-n),\u00b7\u00b7\u00b7, C(Wt-l)j ()) + R((), C), where R((), C) is a \nregularization term (e.g. a weight decay ).11()112, that penalizes slightly the norm of ().<\/DOCUMENT>\n<DOCUMENT>a tagger), and morpho\u00ad\nlogical (radix and morphemes). Looking at the word features learned by the model should \nhelp understand it and improve it. Finally, future research should establish how useful the \nproposed approach will be in applications to speech recognition, language translation, and \ninformation retrieval. Acknowledgments \nThe authors would like to thank Leon Bottou and Yann Le Cun for useful discussions.<\/DOCUMENT>\n4. What does the proposed approach focus on regarding word sequences?"}
{"id":"0befb523-a22e-4f5c-9ac6-0b108b56c529","type":"general","question":"5. What is perplexity in the context of this model?","context":{"sentences":[["Refinements to speed it up and extend it \nwill be described in the following sections. We decompose the function f (Wt, ... , Wt-n) = \nP( Wt Iwi-i) in two parts: \n1. A mapping C from any element of V to a real vector C(i) E Rm. It represents \nthe \"distributed feature vector\" associated with each word in the vocabulary . In \npractice, C is represented by a I V I x m matrix (of free parameters). 2. The probability function over words, expressed with C. We have considered two \nalternative formulations : \n(a) The direct architecture : a function 9 maps a sequence of feature \nvectors for words in context (C(Wt-n),\u00b7\u00b7 \u00b7, C(wt-d) to a probabil\u00ad\nity distribution over words in V. It is a vector function whose i-th \nelement estimates the probability P(Wt = ilwi-i) as in figure 1.","The benchmark against which the neural network was compared is an interpolated or \nsmoothed trigram model [6]. Let qt = l(Jreq(Wt-l,Wt-2)) represent the discretized fre\u00ad\nquency of occurrence of the context (Wt-l, Wt-2) (we used l(x) = r -log((l + x)\/T)l \nwhere x is the frequency of occurrence of the context and T is the size of the training cor\u00ad\npus). A conditional mixture of the trigram, bigram, unigram and zero-gram was learned on \nthe validation set, with mixture weights conditional on discretized frequency. Below are measures of test set perplexity (geometric average of 1\/ p( Wt Iwi-1) for dif\u00ad\nferent models P. Apparent convergence of the stochastic gradient descent procedure was \nobtained after around 10 epochs for Hansard and after about 50 epochs for Brown, with \na learning rate gradually decreased from approximately 10-3 to 10-5. Weight decay of \n10-4 or 10-5 was used in all the experiments (based on a few experiments compared on \nthe validation set). The main result is that the neural network performs much better than the smoothed trigram. On Brown the best neural network system, according to validation perplexity \n(among different architectures tried, see below) yielded a perplexity of 258, while the \nsmoothed trigram yields a perplexity of 348, which is about 35% worse. This is obtained \nusing a network with the direct architecture mixed with the trigram (conditional mixture), \nwith 30 word features initialized with the SVD method, 40 hidden units, and n = 5 words \nof context. On Hansard, the corresponding figures are 44.8 for the neural network and 54.1 \nfor the smoothed trigram, which is 20.7% worse. This is obtained with a network with the \ndirect architecture, 100 randomly initialized words features, 120 hidden units, and n = 8 \nwords of context.","Hinton. Learning distributed representations of concepts. In Proceedings of the Eighth An\u00ad\nnual Conference of the Cognitive Science Society, pages 1-12, Amherst 1986, 1986. Lawrence \nErlbaum, Hillsdale.","This function has parameters that can be \niteratively tuned in order to maximize the log-likelihood of the training data or a regularized \ncriterion, e.g. by adding a weight decay penalty. The feature vectors associated with each \nword are learned, but they can be initialized using prior knowledge. Why does it work? In the previous example, if we knew that dog and cat played simi\u00ad\nlar roles (semantically and syntactically), and similarly for (the,a), (bedroom,room), \n(is,was), (running,walking), we could naturally generalize from The cat is \nwalking in the bedroom to A dog was running in a room and likewise \nto many other combinations. In the proposed model, it will so generalize because \"simi\u00ad\nlar\" words should have a similar feature vector, and because the probability function is a \nsmooth function of these feature values, so a small change in the features (to obtain similar \nwords) induces a small change in the probability : seeing only one of the above sentences \nwill increase the probability not only of that sentence but also of its combinatorial number \nof \"neighbors\" in sentence .Ipace (as represented by sequences offeature vectors). 1.2 Relation to Previous Work \nThe idea of using neural networks to model high-dimensional discrete distributions has \nalready been found useful in [3] where the joint probability of Zl ... Zn is decomposed \nas a product of conditional probabilities: P(Zl = Zl,\"', Zn = Zn) = Oi P(Zi = \nzilgi(Zi-l, Zi-2, ... , Zl)), where g(.) is a function represented by part of a neural network, \nand it yields parameters for expressing the distribution of Zi. Experiments on four VCI \ndata sets show this approach to work comparatively very well [3, 2]. The idea of a dis\u00ad\ntributed representation for symbols dates from the early days of connectionism [5]. More \nrecently, Hinton's approach was improved and successfully demonstrated on learning sev\u00ad\neral symbolic relations [9]. The idea of using neural networks for language modeling is not \nnew either, e.g. [8]. In contrast, here we push this idea to a large scale, and concentrate on \nlearning a statistical model of the distribution of word sequences, rather than learning the \nrole of words in a sentence. The proposed approach is also related to previous proposals of character-based text compression using neural networks [11]. Learning a clustering of \nwords [10, 1] is also a way to discover similarities between words. In the model proposed \nhere, instead of characterizing the similarity with a discrete random or deterministic vari\u00ad\nable (which corresponds to a soft or hard partition of the set of words), we use a continuous \nreal-vector for each word, i.e. a distributed feature vector, to indirectly represent similarity \nbetween words. The idea of using a vector-space representation for words has been well \nexploited in the area of information retrieval (for example see [12]), where vectorial fea\u00ad\nture vectors for words are learned on the basis of their probability of co-occurring in the \nsame documents (Latent Semantic Indexing [4]). An important difference is that here we \nlook for a representation for words that is helpful in representing compactly the probabil\u00ad\nity distribution of word sequences from natural language text. Experiments indicate that \nlearning jointly the representation (word features) and the model makes a big difference in \nperformance. 2 The Proposed Model: two Architectures \nThe training set is a sequence Wi ... WT of words Wt E V, where the vocabular y V is a \nlarge but finite set. The objective is to learn a good model f(wt,\u00b7 .. , Wt-n) = P(wtlwi-i), \nin the sense that it gives high out-of-sample likelihood . In the experiments, we will report \nthe geometric average of l\/P(wtlwi-i), also known as perplexity, which is also the ex\u00ad\nponential of the average negative log-likelihood. The only constraint on the model is that \nfor any choice of wi-i, Ei~i f(i, Wt-i, Wt-n) = 1. By the product of these conditional \nprobabilities, one obtains a model of the joint probability of any sequence of words. The basic form of the model is described here."]],"title":[["placeholder_title","placeholder_title","placeholder_title","placeholder_title"]]},"oracle_context":"This function has parameters that can be \niteratively tuned in order to maximize the log-likelihood of the training data or a regularized \ncriterion, e.g. by adding a weight decay penalty. The feature vectors associated with each \nword are learned, but they can be initialized using prior knowledge. Why does it work? In the previous example, if we knew that dog and cat played simi\u00ad\nlar roles (semantically and syntactically), and similarly for (the,a), (bedroom,room), \n(is,was), (running,walking), we could naturally generalize from The cat is \nwalking in the bedroom to A dog was running in a room and likewise \nto many other combinations. In the proposed model, it will so generalize because \"simi\u00ad\nlar\" words should have a similar feature vector, and because the probability function is a \nsmooth function of these feature values, so a small change in the features (to obtain similar \nwords) induces a small change in the probability : seeing only one of the above sentences \nwill increase the probability not only of that sentence but also of its combinatorial number \nof \"neighbors\" in sentence .Ipace (as represented by sequences offeature vectors). 1.2 Relation to Previous Work \nThe idea of using neural networks to model high-dimensional discrete distributions has \nalready been found useful in [3] where the joint probability of Zl ... Zn is decomposed \nas a product of conditional probabilities: P(Zl = Zl,\"', Zn = Zn) = Oi P(Zi = \nzilgi(Zi-l, Zi-2, ... , Zl)), where g(.) is a function represented by part of a neural network, \nand it yields parameters for expressing the distribution of Zi. Experiments on four VCI \ndata sets show this approach to work comparatively very well [3, 2]. The idea of a dis\u00ad\ntributed representation for symbols dates from the early days of connectionism [5]. More \nrecently, Hinton's approach was improved and successfully demonstrated on learning sev\u00ad\neral symbolic relations [9]. The idea of using neural networks for language modeling is not \nnew either, e.g. [8]. In contrast, here we push this idea to a large scale, and concentrate on \nlearning a statistical model of the distribution of word sequences, rather than learning the \nrole of words in a sentence. The proposed approach is also related to previous proposals of character-based text compression using neural networks [11]. Learning a clustering of \nwords [10, 1] is also a way to discover similarities between words. In the model proposed \nhere, instead of characterizing the similarity with a discrete random or deterministic vari\u00ad\nable (which corresponds to a soft or hard partition of the set of words), we use a continuous \nreal-vector for each word, i.e. a distributed feature vector, to indirectly represent similarity \nbetween words. The idea of using a vector-space representation for words has been well \nexploited in the area of information retrieval (for example see [12]), where vectorial fea\u00ad\nture vectors for words are learned on the basis of their probability of co-occurring in the \nsame documents (Latent Semantic Indexing [4]). An important difference is that here we \nlook for a representation for words that is helpful in representing compactly the probabil\u00ad\nity distribution of word sequences from natural language text. Experiments indicate that \nlearning jointly the representation (word features) and the model makes a big difference in \nperformance. 2 The Proposed Model: two Architectures \nThe training set is a sequence Wi ... WT of words Wt E V, where the vocabular y V is a \nlarge but finite set. The objective is to learn a good model f(wt,\u00b7 .. , Wt-n) = P(wtlwi-i), \nin the sense that it gives high out-of-sample likelihood . In the experiments, we will report \nthe geometric average of l\/P(wtlwi-i), also known as perplexity, which is also the ex\u00ad\nponential of the average negative log-likelihood. The only constraint on the model is that \nfor any choice of wi-i, Ei~i f(i, Wt-i, Wt-n) = 1. By the product of these conditional \nprobabilities, one obtains a model of the joint probability of any sequence of words. The basic form of the model is described here.","cot_answer":"To answer the question about perplexity in the context of the model described, we can follow these steps:\n\n1. **Identify the Definition**: We need to find where perplexity is defined or explained in the context provided. \n\n2. **Locate Relevant Sentences**: Look for sentences that mention perplexity or related concepts like log-likelihood.\n\n3. **Extract and Summarize**: Once we find the relevant sentences, we can summarize the definition of perplexity based on the context.\n\n4. **Formulate the Final Answer**: We will then present the answer in the required format.\n\nNow, let's apply this reasoning:\n\n1. The context mentions that \"the geometric average of l\/P(wt|wi-i)\" is reported, which is referred to as perplexity. \n\n2. It also states that perplexity is \"the exponential of the average negative log-likelihood.\"\n\n3. Therefore, we can summarize that perplexity is a measure related to the likelihood of the model's predictions, specifically indicating how well the model predicts a sample.\n\nNow, we can formulate the final answer:\n\n<ANSWER>: Perplexity is the geometric average of the inverse probability of the predicted words, also known as the exponential of the average negative log-likelihood.","instruction":"<DOCUMENT>Refinements to speed it up and extend it \nwill be described in the following sections. We decompose the function f (Wt, ... , Wt-n) = \nP( Wt Iwi-i) in two parts: \n1. A mapping C from any element of V to a real vector C(i) E Rm. It represents \nthe \"distributed feature vector\" associated with each word in the vocabulary . In \npractice, C is represented by a I V I x m matrix (of free parameters). 2. The probability function over words, expressed with C. We have considered two \nalternative formulations : \n(a) The direct architecture : a function 9 maps a sequence of feature \nvectors for words in context (C(Wt-n),\u00b7\u00b7 \u00b7, C(wt-d) to a probabil\u00ad\nity distribution over words in V. It is a vector function whose i-th \nelement estimates the probability P(Wt = ilwi-i) as in figure 1.<\/DOCUMENT>\n<DOCUMENT>The benchmark against which the neural network was compared is an interpolated or \nsmoothed trigram model [6]. Let qt = l(Jreq(Wt-l,Wt-2)) represent the discretized fre\u00ad\nquency of occurrence of the context (Wt-l, Wt-2) (we used l(x) = r -log((l + x)\/T)l \nwhere x is the frequency of occurrence of the context and T is the size of the training cor\u00ad\npus). A conditional mixture of the trigram, bigram, unigram and zero-gram was learned on \nthe validation set, with mixture weights conditional on discretized frequency. Below are measures of test set perplexity (geometric average of 1\/ p( Wt Iwi-1) for dif\u00ad\nferent models P. Apparent convergence of the stochastic gradient descent procedure was \nobtained after around 10 epochs for Hansard and after about 50 epochs for Brown, with \na learning rate gradually decreased from approximately 10-3 to 10-5. Weight decay of \n10-4 or 10-5 was used in all the experiments (based on a few experiments compared on \nthe validation set). The main result is that the neural network performs much better than the smoothed trigram. On Brown the best neural network system, according to validation perplexity \n(among different architectures tried, see below) yielded a perplexity of 258, while the \nsmoothed trigram yields a perplexity of 348, which is about 35% worse. This is obtained \nusing a network with the direct architecture mixed with the trigram (conditional mixture), \nwith 30 word features initialized with the SVD method, 40 hidden units, and n = 5 words \nof context. On Hansard, the corresponding figures are 44.8 for the neural network and 54.1 \nfor the smoothed trigram, which is 20.7% worse. This is obtained with a network with the \ndirect architecture, 100 randomly initialized words features, 120 hidden units, and n = 8 \nwords of context.<\/DOCUMENT>\n<DOCUMENT>Hinton. Learning distributed representations of concepts. In Proceedings of the Eighth An\u00ad\nnual Conference of the Cognitive Science Society, pages 1-12, Amherst 1986, 1986. Lawrence \nErlbaum, Hillsdale.<\/DOCUMENT>\n<DOCUMENT>This function has parameters that can be \niteratively tuned in order to maximize the log-likelihood of the training data or a regularized \ncriterion, e.g. by adding a weight decay penalty. The feature vectors associated with each \nword are learned, but they can be initialized using prior knowledge. Why does it work? In the previous example, if we knew that dog and cat played simi\u00ad\nlar roles (semantically and syntactically), and similarly for (the,a), (bedroom,room), \n(is,was), (running,walking), we could naturally generalize from The cat is \nwalking in the bedroom to A dog was running in a room and likewise \nto many other combinations. In the proposed model, it will so generalize because \"simi\u00ad\nlar\" words should have a similar feature vector, and because the probability function is a \nsmooth function of these feature values, so a small change in the features (to obtain similar \nwords) induces a small change in the probability : seeing only one of the above sentences \nwill increase the probability not only of that sentence but also of its combinatorial number \nof \"neighbors\" in sentence .Ipace (as represented by sequences offeature vectors). 1.2 Relation to Previous Work \nThe idea of using neural networks to model high-dimensional discrete distributions has \nalready been found useful in [3] where the joint probability of Zl ... Zn is decomposed \nas a product of conditional probabilities: P(Zl = Zl,\"', Zn = Zn) = Oi P(Zi = \nzilgi(Zi-l, Zi-2, ... , Zl)), where g(.) is a function represented by part of a neural network, \nand it yields parameters for expressing the distribution of Zi. Experiments on four VCI \ndata sets show this approach to work comparatively very well [3, 2]. The idea of a dis\u00ad\ntributed representation for symbols dates from the early days of connectionism [5]. More \nrecently, Hinton's approach was improved and successfully demonstrated on learning sev\u00ad\neral symbolic relations [9]. The idea of using neural networks for language modeling is not \nnew either, e.g. [8]. In contrast, here we push this idea to a large scale, and concentrate on \nlearning a statistical model of the distribution of word sequences, rather than learning the \nrole of words in a sentence. The proposed approach is also related to previous proposals of character-based text compression using neural networks [11]. Learning a clustering of \nwords [10, 1] is also a way to discover similarities between words. In the model proposed \nhere, instead of characterizing the similarity with a discrete random or deterministic vari\u00ad\nable (which corresponds to a soft or hard partition of the set of words), we use a continuous \nreal-vector for each word, i.e. a distributed feature vector, to indirectly represent similarity \nbetween words. The idea of using a vector-space representation for words has been well \nexploited in the area of information retrieval (for example see [12]), where vectorial fea\u00ad\nture vectors for words are learned on the basis of their probability of co-occurring in the \nsame documents (Latent Semantic Indexing [4]). An important difference is that here we \nlook for a representation for words that is helpful in representing compactly the probabil\u00ad\nity distribution of word sequences from natural language text. Experiments indicate that \nlearning jointly the representation (word features) and the model makes a big difference in \nperformance. 2 The Proposed Model: two Architectures \nThe training set is a sequence Wi ... WT of words Wt E V, where the vocabular y V is a \nlarge but finite set. The objective is to learn a good model f(wt,\u00b7 .. , Wt-n) = P(wtlwi-i), \nin the sense that it gives high out-of-sample likelihood . In the experiments, we will report \nthe geometric average of l\/P(wtlwi-i), also known as perplexity, which is also the ex\u00ad\nponential of the average negative log-likelihood. The only constraint on the model is that \nfor any choice of wi-i, Ei~i f(i, Wt-i, Wt-n) = 1. By the product of these conditional \nprobabilities, one obtains a model of the joint probability of any sequence of words. The basic form of the model is described here.<\/DOCUMENT>\n5. What is perplexity in the context of this model?"}
{"id":"e147fddc-ad25-4978-b0b1-4f3e0cfcb10c","type":"general","question":"1. What does the mapping C represent?  ","context":{"sentences":[["A Neural Probabilistic Language Model \nYoshua Bengio; Rejean Ducharme and Pascal Vincent \nDepartement d'Informatique et Recherche Operationnelle \nCentre de Recherche Mathematiques \nUniversite de Montreal \nMontreal, Quebec, Canada, H3C 317 \n{bengioy,ducharme, vincentp }@iro.umontreal.ca \nAbstract \nA goal of statistical language modeling is to learn the joint probability \nfunction of sequences of words. This is intrinsically difficult because of \nthe curse of dimensionality: we propose to fight it with its own weapons.","m = 30,60 or 100 in the experiments) \nis much smaller than the size of the vocabulary. The probability function is expressed as a \nproduct of conditional probabilities of the next word given the previous ones, (e.g. using \na multi-layer neural network in the experiment).","The benchmark against which the neural network was compared is an interpolated or \nsmoothed trigram model [6]. Let qt = l(Jreq(Wt-l,Wt-2)) represent the discretized fre\u00ad\nquency of occurrence of the context (Wt-l, Wt-2) (we used l(x) = r -log((l + x)\/T)l \nwhere x is the frequency of occurrence of the context and T is the size of the training cor\u00ad\npus). A conditional mixture of the trigram, bigram, unigram and zero-gram was learned on \nthe validation set, with mixture weights conditional on discretized frequency. Below are measures of test set perplexity (geometric average of 1\/ p( Wt Iwi-1) for dif\u00ad\nferent models P. Apparent convergence of the stochastic gradient descent procedure was \nobtained after around 10 epochs for Hansard and after about 50 epochs for Brown, with \na learning rate gradually decreased from approximately 10-3 to 10-5. Weight decay of \n10-4 or 10-5 was used in all the experiments (based on a few experiments compared on \nthe validation set). The main result is that the neural network performs much better than the smoothed trigram. On Brown the best neural network system, according to validation perplexity \n(among different architectures tried, see below) yielded a perplexity of 258, while the \nsmoothed trigram yields a perplexity of 348, which is about 35% worse. This is obtained \nusing a network with the direct architecture mixed with the trigram (conditional mixture), \nwith 30 word features initialized with the SVD method, 40 hidden units, and n = 5 words \nof context. On Hansard, the corresponding figures are 44.8 for the neural network and 54.1 \nfor the smoothed trigram, which is 20.7% worse. This is obtained with a network with the \ndirect architecture, 100 randomly initialized words features, 120 hidden units, and n = 8 \nwords of context.","Refinements to speed it up and extend it \nwill be described in the following sections. We decompose the function f (Wt, ... , Wt-n) = \nP( Wt Iwi-i) in two parts: \n1. A mapping C from any element of V to a real vector C(i) E Rm. It represents \nthe \"distributed feature vector\" associated with each word in the vocabulary . In \npractice, C is represented by a I V I x m matrix (of free parameters). 2. The probability function over words, expressed with C. We have considered two \nalternative formulations : \n(a) The direct architecture : a function 9 maps a sequence of feature \nvectors for words in context (C(Wt-n),\u00b7\u00b7 \u00b7, C(wt-d) to a probabil\u00ad\nity distribution over words in V. It is a vector function whose i-th \nelement estimates the probability P(Wt = ilwi-i) as in figure 1."]],"title":[["placeholder_title","placeholder_title","placeholder_title","placeholder_title"]]},"oracle_context":"Refinements to speed it up and extend it \nwill be described in the following sections. We decompose the function f (Wt, ... , Wt-n) = \nP( Wt Iwi-i) in two parts: \n1. A mapping C from any element of V to a real vector C(i) E Rm. It represents \nthe \"distributed feature vector\" associated with each word in the vocabulary . In \npractice, C is represented by a I V I x m matrix (of free parameters). 2. The probability function over words, expressed with C. We have considered two \nalternative formulations : \n(a) The direct architecture : a function 9 maps a sequence of feature \nvectors for words in context (C(Wt-n),\u00b7\u00b7 \u00b7, C(wt-d) to a probabil\u00ad\nity distribution over words in V. It is a vector function whose i-th \nelement estimates the probability P(Wt = ilwi-i) as in figure 1.","cot_answer":"To answer the question about what the mapping C represents, we can follow these steps:\n\n1. Identify the key components of the context provided.\n2. Look for a specific definition or description of the mapping C.\n3. Summarize the information related to C in a clear and concise manner.\n\nFrom the context, we see that the mapping C is described in the first part of the decomposition of the function f. The relevant sentence states:\n\n##begin_quote## \"A mapping C from any element of V to a real vector C(i) E Rm. It represents the 'distributed feature vector' associated with each word in the vocabulary.\" ##end_quote##\n\nThis indicates that C is a function that takes elements from the vocabulary (V) and maps them to real vectors, which are referred to as \"distributed feature vectors.\" \n\nAdditionally, it is mentioned that C is represented by a matrix of free parameters, which further emphasizes its role in representing features associated with words.\n\nNow, we can succinctly summarize this information.\n\n<ANSWER>: C represents a mapping from elements of the vocabulary to real vectors, specifically the \"distributed feature vector\" associated with each word.","instruction":"<DOCUMENT>A Neural Probabilistic Language Model \nYoshua Bengio; Rejean Ducharme and Pascal Vincent \nDepartement d'Informatique et Recherche Operationnelle \nCentre de Recherche Mathematiques \nUniversite de Montreal \nMontreal, Quebec, Canada, H3C 317 \n{bengioy,ducharme, vincentp }@iro.umontreal.ca \nAbstract \nA goal of statistical language modeling is to learn the joint probability \nfunction of sequences of words. This is intrinsically difficult because of \nthe curse of dimensionality: we propose to fight it with its own weapons.<\/DOCUMENT>\n<DOCUMENT>m = 30,60 or 100 in the experiments) \nis much smaller than the size of the vocabulary. The probability function is expressed as a \nproduct of conditional probabilities of the next word given the previous ones, (e.g. using \na multi-layer neural network in the experiment).<\/DOCUMENT>\n<DOCUMENT>The benchmark against which the neural network was compared is an interpolated or \nsmoothed trigram model [6]. Let qt = l(Jreq(Wt-l,Wt-2)) represent the discretized fre\u00ad\nquency of occurrence of the context (Wt-l, Wt-2) (we used l(x) = r -log((l + x)\/T)l \nwhere x is the frequency of occurrence of the context and T is the size of the training cor\u00ad\npus). A conditional mixture of the trigram, bigram, unigram and zero-gram was learned on \nthe validation set, with mixture weights conditional on discretized frequency. Below are measures of test set perplexity (geometric average of 1\/ p( Wt Iwi-1) for dif\u00ad\nferent models P. Apparent convergence of the stochastic gradient descent procedure was \nobtained after around 10 epochs for Hansard and after about 50 epochs for Brown, with \na learning rate gradually decreased from approximately 10-3 to 10-5. Weight decay of \n10-4 or 10-5 was used in all the experiments (based on a few experiments compared on \nthe validation set). The main result is that the neural network performs much better than the smoothed trigram. On Brown the best neural network system, according to validation perplexity \n(among different architectures tried, see below) yielded a perplexity of 258, while the \nsmoothed trigram yields a perplexity of 348, which is about 35% worse. This is obtained \nusing a network with the direct architecture mixed with the trigram (conditional mixture), \nwith 30 word features initialized with the SVD method, 40 hidden units, and n = 5 words \nof context. On Hansard, the corresponding figures are 44.8 for the neural network and 54.1 \nfor the smoothed trigram, which is 20.7% worse. This is obtained with a network with the \ndirect architecture, 100 randomly initialized words features, 120 hidden units, and n = 8 \nwords of context.<\/DOCUMENT>\n<DOCUMENT>Refinements to speed it up and extend it \nwill be described in the following sections. We decompose the function f (Wt, ... , Wt-n) = \nP( Wt Iwi-i) in two parts: \n1. A mapping C from any element of V to a real vector C(i) E Rm. It represents \nthe \"distributed feature vector\" associated with each word in the vocabulary . In \npractice, C is represented by a I V I x m matrix (of free parameters). 2. The probability function over words, expressed with C. We have considered two \nalternative formulations : \n(a) The direct architecture : a function 9 maps a sequence of feature \nvectors for words in context (C(Wt-n),\u00b7\u00b7 \u00b7, C(wt-d) to a probabil\u00ad\nity distribution over words in V. It is a vector function whose i-th \nelement estimates the probability P(Wt = ilwi-i) as in figure 1.<\/DOCUMENT>\n1. What does the mapping C represent?  "}
{"id":"7297937d-3d57-444e-be73-a1190368d66c","type":"general","question":"2. How is the matrix C structured?  ","context":{"sentences":[["This function has parameters that can be \niteratively tuned in order to maximize the log-likelihood of the training data or a regularized \ncriterion, e.g. by adding a weight decay penalty. The feature vectors associated with each \nword are learned, but they can be initialized using prior knowledge. Why does it work? In the previous example, if we knew that dog and cat played simi\u00ad\nlar roles (semantically and syntactically), and similarly for (the,a), (bedroom,room), \n(is,was), (running,walking), we could naturally generalize from The cat is \nwalking in the bedroom to A dog was running in a room and likewise \nto many other combinations. In the proposed model, it will so generalize because \"simi\u00ad\nlar\" words should have a similar feature vector, and because the probability function is a \nsmooth function of these feature values, so a small change in the features (to obtain similar \nwords) induces a small change in the probability : seeing only one of the above sentences \nwill increase the probability not only of that sentence but also of its combinatorial number \nof \"neighbors\" in sentence .Ipace (as represented by sequences offeature vectors). 1.2 Relation to Previous Work \nThe idea of using neural networks to model high-dimensional discrete distributions has \nalready been found useful in [3] where the joint probability of Zl ... Zn is decomposed \nas a product of conditional probabilities: P(Zl = Zl,\"', Zn = Zn) = Oi P(Zi = \nzilgi(Zi-l, Zi-2, ... , Zl)), where g(.) is a function represented by part of a neural network, \nand it yields parameters for expressing the distribution of Zi. Experiments on four VCI \ndata sets show this approach to work comparatively very well [3, 2]. The idea of a dis\u00ad\ntributed representation for symbols dates from the early days of connectionism [5]. More \nrecently, Hinton's approach was improved and successfully demonstrated on learning sev\u00ad\neral symbolic relations [9]. The idea of using neural networks for language modeling is not \nnew either, e.g. [8]. In contrast, here we push this idea to a large scale, and concentrate on \nlearning a statistical model of the distribution of word sequences, rather than learning the \nrole of words in a sentence. The proposed approach is also related to previous proposals of character-based text compression using neural networks [11]. Learning a clustering of \nwords [10, 1] is also a way to discover similarities between words. In the model proposed \nhere, instead of characterizing the similarity with a discrete random or deterministic vari\u00ad\nable (which corresponds to a soft or hard partition of the set of words), we use a continuous \nreal-vector for each word, i.e. a distributed feature vector, to indirectly represent similarity \nbetween words. The idea of using a vector-space representation for words has been well \nexploited in the area of information retrieval (for example see [12]), where vectorial fea\u00ad\nture vectors for words are learned on the basis of their probability of co-occurring in the \nsame documents (Latent Semantic Indexing [4]). An important difference is that here we \nlook for a representation for words that is helpful in representing compactly the probabil\u00ad\nity distribution of word sequences from natural language text. Experiments indicate that \nlearning jointly the representation (word features) and the model makes a big difference in \nperformance. 2 The Proposed Model: two Architectures \nThe training set is a sequence Wi ... WT of words Wt E V, where the vocabular y V is a \nlarge but finite set. The objective is to learn a good model f(wt,\u00b7 .. , Wt-n) = P(wtlwi-i), \nin the sense that it gives high out-of-sample likelihood . In the experiments, we will report \nthe geometric average of l\/P(wtlwi-i), also known as perplexity, which is also the ex\u00ad\nponential of the average negative log-likelihood. The only constraint on the model is that \nfor any choice of wi-i, Ei~i f(i, Wt-i, Wt-n) = 1. By the product of these conditional \nprobabilities, one obtains a model of the joint probability of any sequence of words. The basic form of the model is described here.","Refinements to speed it up and extend it \nwill be described in the following sections. We decompose the function f (Wt, ... , Wt-n) = \nP( Wt Iwi-i) in two parts: \n1. A mapping C from any element of V to a real vector C(i) E Rm. It represents \nthe \"distributed feature vector\" associated with each word in the vocabulary . In \npractice, C is represented by a I V I x m matrix (of free parameters). 2. The probability function over words, expressed with C. We have considered two \nalternative formulations : \n(a) The direct architecture : a function 9 maps a sequence of feature \nvectors for words in context (C(Wt-n),\u00b7\u00b7 \u00b7, C(wt-d) to a probabil\u00ad\nity distribution over words in V. It is a vector function whose i-th \nelement estimates the probability P(Wt = ilwi-i) as in figure 1.","Katz. Estimation of probabilities from sparse data for the language model component \nof a speech recognizer . IEEE Transactions on Acoustics , Speech, and Signal Processing, ASSP-\n35(3):400-401 , March 1987.","m = 30,60 or 100 in the experiments) \nis much smaller than the size of the vocabulary. The probability function is expressed as a \nproduct of conditional probabilities of the next word given the previous ones, (e.g. using \na multi-layer neural network in the experiment)."]],"title":[["placeholder_title","placeholder_title","placeholder_title","placeholder_title"]]},"oracle_context":"Refinements to speed it up and extend it \nwill be described in the following sections. We decompose the function f (Wt, ... , Wt-n) = \nP( Wt Iwi-i) in two parts: \n1. A mapping C from any element of V to a real vector C(i) E Rm. It represents \nthe \"distributed feature vector\" associated with each word in the vocabulary . In \npractice, C is represented by a I V I x m matrix (of free parameters). 2. The probability function over words, expressed with C. We have considered two \nalternative formulations : \n(a) The direct architecture : a function 9 maps a sequence of feature \nvectors for words in context (C(Wt-n),\u00b7\u00b7 \u00b7, C(wt-d) to a probabil\u00ad\nity distribution over words in V. It is a vector function whose i-th \nelement estimates the probability P(Wt = ilwi-i) as in figure 1.","cot_answer":"To answer the question about how the matrix C is structured, we can follow these steps:\n\n1. Identify the purpose of matrix C from the context provided.\n2. Look for specific details regarding its dimensions and representation.\n3. Summarize the findings in a clear and concise manner.\n\nFrom the context, we see that matrix C is described as follows:\n\n##begin_quote##\n\"A mapping C from any element of V to a real vector C(i) E Rm. It represents the 'distributed feature vector' associated with each word in the vocabulary. In practice, C is represented by a |V| x m matrix (of free parameters).\" \n##end_quote##\n\nThis indicates that matrix C is structured as a matrix with dimensions |V| (the size of the vocabulary) by m (the dimensionality of the feature vectors). Each entry in this matrix corresponds to a \"distributed feature vector\" for each word in the vocabulary.\n\nFinal answer: <ANSWER>: The matrix C is structured as a |V| x m matrix, representing the distributed feature vectors for each word in the vocabulary.","instruction":"<DOCUMENT>This function has parameters that can be \niteratively tuned in order to maximize the log-likelihood of the training data or a regularized \ncriterion, e.g. by adding a weight decay penalty. The feature vectors associated with each \nword are learned, but they can be initialized using prior knowledge. Why does it work? In the previous example, if we knew that dog and cat played simi\u00ad\nlar roles (semantically and syntactically), and similarly for (the,a), (bedroom,room), \n(is,was), (running,walking), we could naturally generalize from The cat is \nwalking in the bedroom to A dog was running in a room and likewise \nto many other combinations. In the proposed model, it will so generalize because \"simi\u00ad\nlar\" words should have a similar feature vector, and because the probability function is a \nsmooth function of these feature values, so a small change in the features (to obtain similar \nwords) induces a small change in the probability : seeing only one of the above sentences \nwill increase the probability not only of that sentence but also of its combinatorial number \nof \"neighbors\" in sentence .Ipace (as represented by sequences offeature vectors). 1.2 Relation to Previous Work \nThe idea of using neural networks to model high-dimensional discrete distributions has \nalready been found useful in [3] where the joint probability of Zl ... Zn is decomposed \nas a product of conditional probabilities: P(Zl = Zl,\"', Zn = Zn) = Oi P(Zi = \nzilgi(Zi-l, Zi-2, ... , Zl)), where g(.) is a function represented by part of a neural network, \nand it yields parameters for expressing the distribution of Zi. Experiments on four VCI \ndata sets show this approach to work comparatively very well [3, 2]. The idea of a dis\u00ad\ntributed representation for symbols dates from the early days of connectionism [5]. More \nrecently, Hinton's approach was improved and successfully demonstrated on learning sev\u00ad\neral symbolic relations [9]. The idea of using neural networks for language modeling is not \nnew either, e.g. [8]. In contrast, here we push this idea to a large scale, and concentrate on \nlearning a statistical model of the distribution of word sequences, rather than learning the \nrole of words in a sentence. The proposed approach is also related to previous proposals of character-based text compression using neural networks [11]. Learning a clustering of \nwords [10, 1] is also a way to discover similarities between words. In the model proposed \nhere, instead of characterizing the similarity with a discrete random or deterministic vari\u00ad\nable (which corresponds to a soft or hard partition of the set of words), we use a continuous \nreal-vector for each word, i.e. a distributed feature vector, to indirectly represent similarity \nbetween words. The idea of using a vector-space representation for words has been well \nexploited in the area of information retrieval (for example see [12]), where vectorial fea\u00ad\nture vectors for words are learned on the basis of their probability of co-occurring in the \nsame documents (Latent Semantic Indexing [4]). An important difference is that here we \nlook for a representation for words that is helpful in representing compactly the probabil\u00ad\nity distribution of word sequences from natural language text. Experiments indicate that \nlearning jointly the representation (word features) and the model makes a big difference in \nperformance. 2 The Proposed Model: two Architectures \nThe training set is a sequence Wi ... WT of words Wt E V, where the vocabular y V is a \nlarge but finite set. The objective is to learn a good model f(wt,\u00b7 .. , Wt-n) = P(wtlwi-i), \nin the sense that it gives high out-of-sample likelihood . In the experiments, we will report \nthe geometric average of l\/P(wtlwi-i), also known as perplexity, which is also the ex\u00ad\nponential of the average negative log-likelihood. The only constraint on the model is that \nfor any choice of wi-i, Ei~i f(i, Wt-i, Wt-n) = 1. By the product of these conditional \nprobabilities, one obtains a model of the joint probability of any sequence of words. The basic form of the model is described here.<\/DOCUMENT>\n<DOCUMENT>Refinements to speed it up and extend it \nwill be described in the following sections. We decompose the function f (Wt, ... , Wt-n) = \nP( Wt Iwi-i) in two parts: \n1. A mapping C from any element of V to a real vector C(i) E Rm. It represents \nthe \"distributed feature vector\" associated with each word in the vocabulary . In \npractice, C is represented by a I V I x m matrix (of free parameters). 2. The probability function over words, expressed with C. We have considered two \nalternative formulations : \n(a) The direct architecture : a function 9 maps a sequence of feature \nvectors for words in context (C(Wt-n),\u00b7\u00b7 \u00b7, C(wt-d) to a probabil\u00ad\nity distribution over words in V. It is a vector function whose i-th \nelement estimates the probability P(Wt = ilwi-i) as in figure 1.<\/DOCUMENT>\n<DOCUMENT>Katz. Estimation of probabilities from sparse data for the language model component \nof a speech recognizer . IEEE Transactions on Acoustics , Speech, and Signal Processing, ASSP-\n35(3):400-401 , March 1987.<\/DOCUMENT>\n<DOCUMENT>m = 30,60 or 100 in the experiments) \nis much smaller than the size of the vocabulary. The probability function is expressed as a \nproduct of conditional probabilities of the next word given the previous ones, (e.g. using \na multi-layer neural network in the experiment).<\/DOCUMENT>\n2. How is the matrix C structured?  "}
{"id":"f10706f6-dd2f-46e5-8b0a-ffe4fa2e01df","type":"general","question":"3. What does the probability function estimate?  ","context":{"sentences":[["Hinton. Extracting distributed representations of concepts and relations \nfrom positive and negative propositions. In Proceedings of the International Joint Conference \non Neural Network, lJCNN'2000, Como, Italy, 2000.","The direct architecture was found about 2% better \nthan the cycling architecture. Conditional mixture helps but even without it the neural net is better. On Brown, the \nbest neural net without the mixture yields a test perplexity of 265, the smoothed trigram \nyields 348, and their conditional mixture yields 258 (i.e., better than both). On Hansard \nthe improvement is less: a neural network yielding 46.7 perplexity, mixed with the trigram \n(54.1), yields a mixture with perplexity 45.1. 5 Conclusions and Proposed Extensions \nThe experiments on two corpora, a medium one 0.2 million words), and a large one (34 \nmillion words) have shown that the proposed approach yields much better perplexity than \na state-of-the-art method, the smoothed trigram, with differences on the order of 20% to \n35%. We believe that the main reason for these improvements is that the proposed approach \nallows to take advantage of the learned distributed representation to fight the curse of di\u00ad\nmensionality with its own weapons: each training sentence informs the model about a \ncombinatorial number of other sentences. Note that if we had a separate feature vector \nfor each \"context\" (short sequence of words), the model would have much more capacity \n(which could grow like that of n-grams) but it would not naturally generalize between the \nmany different ways a word can be used. A more reasonable alternative would be to ex\u00ad\nplore language units other than words (e.g. some short word sequences , or alternatively \nsome sub-word morphemic units). There is probably much more to be done to improve the model, at the level of architecture, \ncomputational efficiency, and taking advantage of prior knowledge . An important priority \nof future research should be to evaluate and improve the speeding-up tricks proposed here, \nand find ways to increase capacity without increasing training time too much (to deal with corpora with hundreds of millions of words). A simple idea to take advantage of temporal \nstructure and extend the size of the input window to include possibly a whole paragraph, \nwithout increasing too much the number of parameters, is to use a time-delay and possibly \nrecurrent neural network. In such a multi-layered network the computation that has been \nperformed for small groups of consecutive words does not need to be redone when the \nnetwork input window is shifted. Similarly , one could use a recurrent network to capture \npotentially even longer term information about the subject of the text. A very important area in which the proposed model could be improved is in the use of prior \nlinguistic knowledge : semantic (e.g. Word Net), syntactic (e.g.","Refinements to speed it up and extend it \nwill be described in the following sections. We decompose the function f (Wt, ... , Wt-n) = \nP( Wt Iwi-i) in two parts: \n1. A mapping C from any element of V to a real vector C(i) E Rm. It represents \nthe \"distributed feature vector\" associated with each word in the vocabulary . In \npractice, C is represented by a I V I x m matrix (of free parameters). 2. The probability function over words, expressed with C. We have considered two \nalternative formulations : \n(a) The direct architecture : a function 9 maps a sequence of feature \nvectors for words in context (C(Wt-n),\u00b7\u00b7 \u00b7, C(wt-d) to a probabil\u00ad\nity distribution over words in V. It is a vector function whose i-th \nelement estimates the probability P(Wt = ilwi-i) as in figure 1.","a tagger), and morpho\u00ad\nlogical (radix and morphemes). Looking at the word features learned by the model should \nhelp understand it and improve it. Finally, future research should establish how useful the \nproposed approach will be in applications to speech recognition, language translation, and \ninformation retrieval. Acknowledgments \nThe authors would like to thank Leon Bottou and Yann Le Cun for useful discussions."]],"title":[["placeholder_title","placeholder_title","placeholder_title","placeholder_title"]]},"oracle_context":"Refinements to speed it up and extend it \nwill be described in the following sections. We decompose the function f (Wt, ... , Wt-n) = \nP( Wt Iwi-i) in two parts: \n1. A mapping C from any element of V to a real vector C(i) E Rm. It represents \nthe \"distributed feature vector\" associated with each word in the vocabulary . In \npractice, C is represented by a I V I x m matrix (of free parameters). 2. The probability function over words, expressed with C. We have considered two \nalternative formulations : \n(a) The direct architecture : a function 9 maps a sequence of feature \nvectors for words in context (C(Wt-n),\u00b7\u00b7 \u00b7, C(wt-d) to a probabil\u00ad\nity distribution over words in V. It is a vector function whose i-th \nelement estimates the probability P(Wt = ilwi-i) as in figure 1.","cot_answer":"To answer the question about what the probability function estimates, we can follow these steps:\n\n1. Identify the key components of the context that relate to the probability function.\n2. Look for any definitions or explanations provided in the context regarding the probability function.\n3. Summarize the findings in a clear and concise manner.\n\nFrom the context, we see that the probability function is part of a larger framework that involves mapping words to feature vectors and estimating probabilities based on those vectors. Specifically, the context states:\n\n##begin_quote##\n\"The probability function over words, expressed with C. We have considered two alternative formulations: (a) The direct architecture: a function 9 maps a sequence of feature vectors for words in context (C(Wt-n),\u00b7\u00b7 \u00b7, C(wt-d) to a probability distribution over words in V. It is a vector function whose i-th element estimates the probability P(Wt = ilwi-i) as in figure 1.\"\n##end_quote##\n\nFrom this, we can conclude that the probability function estimates the probability of a word given its context. Specifically, it estimates the probability \\( P(Wt = i | wi-i) \\), which means it calculates the likelihood of the word \\( Wt \\) being equal to \\( i \\) given the previous words \\( wi-i \\).\n\nFinal answer: <ANSWER>: The probability function estimates the probability of a word given its context, specifically \\( P(Wt = i | wi-i) \\).","instruction":"<DOCUMENT>Hinton. Extracting distributed representations of concepts and relations \nfrom positive and negative propositions. In Proceedings of the International Joint Conference \non Neural Network, lJCNN'2000, Como, Italy, 2000.<\/DOCUMENT>\n<DOCUMENT>The direct architecture was found about 2% better \nthan the cycling architecture. Conditional mixture helps but even without it the neural net is better. On Brown, the \nbest neural net without the mixture yields a test perplexity of 265, the smoothed trigram \nyields 348, and their conditional mixture yields 258 (i.e., better than both). On Hansard \nthe improvement is less: a neural network yielding 46.7 perplexity, mixed with the trigram \n(54.1), yields a mixture with perplexity 45.1. 5 Conclusions and Proposed Extensions \nThe experiments on two corpora, a medium one 0.2 million words), and a large one (34 \nmillion words) have shown that the proposed approach yields much better perplexity than \na state-of-the-art method, the smoothed trigram, with differences on the order of 20% to \n35%. We believe that the main reason for these improvements is that the proposed approach \nallows to take advantage of the learned distributed representation to fight the curse of di\u00ad\nmensionality with its own weapons: each training sentence informs the model about a \ncombinatorial number of other sentences. Note that if we had a separate feature vector \nfor each \"context\" (short sequence of words), the model would have much more capacity \n(which could grow like that of n-grams) but it would not naturally generalize between the \nmany different ways a word can be used. A more reasonable alternative would be to ex\u00ad\nplore language units other than words (e.g. some short word sequences , or alternatively \nsome sub-word morphemic units). There is probably much more to be done to improve the model, at the level of architecture, \ncomputational efficiency, and taking advantage of prior knowledge . An important priority \nof future research should be to evaluate and improve the speeding-up tricks proposed here, \nand find ways to increase capacity without increasing training time too much (to deal with corpora with hundreds of millions of words). A simple idea to take advantage of temporal \nstructure and extend the size of the input window to include possibly a whole paragraph, \nwithout increasing too much the number of parameters, is to use a time-delay and possibly \nrecurrent neural network. In such a multi-layered network the computation that has been \nperformed for small groups of consecutive words does not need to be redone when the \nnetwork input window is shifted. Similarly , one could use a recurrent network to capture \npotentially even longer term information about the subject of the text. A very important area in which the proposed model could be improved is in the use of prior \nlinguistic knowledge : semantic (e.g. Word Net), syntactic (e.g.<\/DOCUMENT>\n<DOCUMENT>Refinements to speed it up and extend it \nwill be described in the following sections. We decompose the function f (Wt, ... , Wt-n) = \nP( Wt Iwi-i) in two parts: \n1. A mapping C from any element of V to a real vector C(i) E Rm. It represents \nthe \"distributed feature vector\" associated with each word in the vocabulary . In \npractice, C is represented by a I V I x m matrix (of free parameters). 2. The probability function over words, expressed with C. We have considered two \nalternative formulations : \n(a) The direct architecture : a function 9 maps a sequence of feature \nvectors for words in context (C(Wt-n),\u00b7\u00b7 \u00b7, C(wt-d) to a probabil\u00ad\nity distribution over words in V. It is a vector function whose i-th \nelement estimates the probability P(Wt = ilwi-i) as in figure 1.<\/DOCUMENT>\n<DOCUMENT>a tagger), and morpho\u00ad\nlogical (radix and morphemes). Looking at the word features learned by the model should \nhelp understand it and improve it. Finally, future research should establish how useful the \nproposed approach will be in applications to speech recognition, language translation, and \ninformation retrieval. Acknowledgments \nThe authors would like to thank Leon Bottou and Yann Le Cun for useful discussions.<\/DOCUMENT>\n3. What does the probability function estimate?  "}
{"id":"1e627221-f0a5-4044-99a1-c049af4bd935","type":"general","question":"4. What is the role of the sequence of feature vectors?  ","context":{"sentences":[[") have similar \nsemantics and grammatical roles. 1.1 Fighting the Curse of Dimensionality with its Own Weapons \nIn a nutshell, the idea of the proposed approach can be summarized as follows: \n1. associate with each word in the vocabulary a distributed \"feature vector\" (a real\u00ad\nvalued vector in ~m), thereby creating a notion of similarity between words, \n2. express the joint probability function of word sequences in terms of the feature \nvectors of these words in the sequence, and \n3. learn simultaneously the word feature vectors and the parameters of thatfitnction. The feature vector represents different aspects of a word: each word is associated with a \npoint in a vector space. The number of features (e.g.","3 Speeding-up and other Tricks \nShort list. The main idea is to focus the effort of the neural network on a \"short list\" \nof words that have the highest probability. This can save much computation because in \nboth of the proposed architectures the time to compute the probability of the observed \nnext word scales almost linearly with the number of words in the vocabulary (because \nthe scores hi associated with each word i in the vocabulary must be computed for prop\u00ad\nerly normalizing probabilities with the softmax). The idea of the speed-up trick is the \nfollowing: instead of computing the actual probability of the next word, the neural net\u00ad\nwork is used to compute the relative probability of the next word within that short list. The choice of the short list depends on the current context (the previous n words). We \nhave used our smoothed trigram model to pre-compute a short list containing the most \nprobable next words associated to the previous two words. The conditional probabilities \nP(Wt = ilht) are thus computed as follows, denoting with ht the history (context) before \nWt. and Lt the short list of words for the prediction of Wt. If i E Lt then the probabil-\nity is PNN(Wt = ilWt E Lt, ht)Ptrigram(Wt E Ltlht), else it is Ptrigram(Wt = ilht), \nwhere PNN(Wt = ilWt E Lt, ht) are the normalized scores of the words computed by \nthe neural network, where the \"softmax\" is only normalized over the words in the short \nlist Lt, and Ptrigram(Wt E Ltlht) = ~iEL. Ptrigram(ilht), with Ptrigram(ilht) standing \nfor the next-word probabilities computed by the smoothed trigram. Note that both Lt and \nPtrigram(Wt E Ltlht) can be pre-computed (and stored in a hash table indexed by the last \ntwo words). Table look-up for recognition. To speed up application of the trained model, one can \npre-compute in a hash table the output of the neural network, at least for the most frequent \ninput contexts. In that case, the neural network will only be rarely called upon, and the \naverage computation time will be very small. Note that in a speech recognition system, one \nneeds only compute the relative probabilities of the acoustically ambiguous words in each \ncontext, also reducing drastically the amount of computations. Stochastic gradient descent.","Refinements to speed it up and extend it \nwill be described in the following sections. We decompose the function f (Wt, ... , Wt-n) = \nP( Wt Iwi-i) in two parts: \n1. A mapping C from any element of V to a real vector C(i) E Rm. It represents \nthe \"distributed feature vector\" associated with each word in the vocabulary . In \npractice, C is represented by a I V I x m matrix (of free parameters). 2. The probability function over words, expressed with C. We have considered two \nalternative formulations : \n(a) The direct architecture : a function 9 maps a sequence of feature \nvectors for words in context (C(Wt-n),\u00b7\u00b7 \u00b7, C(wt-d) to a probabil\u00ad\nity distribution over words in V. It is a vector function whose i-th \nelement estimates the probability P(Wt = ilwi-i) as in figure 1.","Since we have millions of examples, it is important to con\u00ad\nverge within only a few passes through the data. For very large data sets, stochastic gradient \ndescent convergence time seems to increase sub-linearly with the size of the data set (see \nexperiments on Brown vs Hansard below). To speed up training using stochastic gradient descent, we have found it useful to break the corpus in paragraphs and to randomly permute \nthem. In this way, some of the non-stationarity in the word stream is eliminated, yielding \nfaster convergence. Capacity control. For the \"smaller corpora\" like Brown (1.2 million examples), we have \nfound early stopping and weight decay useful to avoid over-fitting. For the larger corpora, \nour networks still under-fit. For the larger corpora, we have found double-precision com\u00ad\nputation to be very important to obtain good results. Mixture of models. We have found improved performance by combining the probability \npredictions of the neural network with those of the smoothed trigram, with weights that \nwere conditional on the frequency of the context (same procedure used to combine trigram, \nbigram, and unigram in the smoothed trigram). Initialization of word feature vectors. We have tried both random initialization (uniform \nbetween -.01 and .01) and a \"smarter\" method based on a Singular Value Decomposition \n(SVD) of a very large matrix of \"context features\". These context features are formed \nby counting the frequency of occurrence of each word in each one of the most frequent \ncontexts (word sequences) in the corpus. The idea is that \"similar\" words should occur \nwith similar frequency in the same contexts. We used about 9000 most frequent contexts, \nand compressed these to 30 features with the SVD. Out-of-vocabulary words. For an out-of-vocabulary word Wt we need to come up with \na feature vector in order to predict the words that follow, or predict its probability (that \nis only possible with the cycling architecture). We used as feature vector the weighted \naverage feature vector of all the words in the short list, with the weights being the relative \nprobabilities ofthose words: E[C(wt)lhtl = Ei C(i)P(wt = ilht). 4 Experimental Results \nComparative experiments were performed on the Brown and Hansard corpora. The Brown \ncorpus is a stream of 1,181,041 words (from a large variety of English texts and books). The first 800,000 words were used for training, the following 200,000 for validation (model \nselection, weight decay, early stopping) and the remaining 181,041 for testing. The number \nof different words is 47, 578 (including punctuation, distinguishing between upper and \nlower case, and including the syntactical marks used to separate texts and paragraphs). Rare words with frequency:::; 3 were merged into a single token, reducing the vocabulary \nsize to IVI = 16,383. The Hansard corpus (Canadian parliament proceedings, French version) is a stream of \nabout 34 million words, of which 32 millions (set A) was used for training, 1.1 million \n(set B) was used for validation, and 1.2 million (set C) was used for out-of-sample tests. The original data has 106, 936 different words, and those with frequency:::; 10 were merged \ninto a single token, yielding IVI = 30,959 different words."]],"title":[["placeholder_title","placeholder_title","placeholder_title","placeholder_title"]]},"oracle_context":"Refinements to speed it up and extend it \nwill be described in the following sections. We decompose the function f (Wt, ... , Wt-n) = \nP( Wt Iwi-i) in two parts: \n1. A mapping C from any element of V to a real vector C(i) E Rm. It represents \nthe \"distributed feature vector\" associated with each word in the vocabulary . In \npractice, C is represented by a I V I x m matrix (of free parameters). 2. The probability function over words, expressed with C. We have considered two \nalternative formulations : \n(a) The direct architecture : a function 9 maps a sequence of feature \nvectors for words in context (C(Wt-n),\u00b7\u00b7 \u00b7, C(wt-d) to a probabil\u00ad\nity distribution over words in V. It is a vector function whose i-th \nelement estimates the probability P(Wt = ilwi-i) as in figure 1.","cot_answer":"To answer the question about the role of the sequence of feature vectors, we can follow these steps:\n\n1. Identify the key components of the context that relate to the sequence of feature vectors.\n2. Understand how these components interact to contribute to the overall function described.\n3. Summarize the findings in a clear and concise manner.\n\nFrom the context, we see that the function f is decomposed into two parts. The first part involves a mapping C that associates each word in the vocabulary with a \"distributed feature vector.\" The second part involves a probability function that uses these feature vectors.\n\nThe context specifically mentions that the sequence of feature vectors is used in the direct architecture, where a function maps a sequence of feature vectors for words in context to a probability distribution over words. This indicates that the sequence of feature vectors plays a crucial role in estimating the probabilities of the words based on their context.\n\nNow, let's extract the relevant quote from the context:\n\n##begin_quote##\n\"The direct architecture: a function 9 maps a sequence of feature vectors for words in context (C(Wt-n),\u00b7\u00b7 \u00b7, C(wt-d) to a probability distribution over words in V. It is a vector function whose i-th element estimates the probability P(Wt = ilwi-i).\"\n##end_quote##\n\nThis quote highlights that the sequence of feature vectors is essential for determining the probability of a word given its context, as it directly influences the output of the probability function.\n\nFinal answer: <ANSWER>: The sequence of feature vectors is crucial for estimating the probability of a word based on its context, as it maps the context to a probability distribution over words.","instruction":"<DOCUMENT>) have similar \nsemantics and grammatical roles. 1.1 Fighting the Curse of Dimensionality with its Own Weapons \nIn a nutshell, the idea of the proposed approach can be summarized as follows: \n1. associate with each word in the vocabulary a distributed \"feature vector\" (a real\u00ad\nvalued vector in ~m), thereby creating a notion of similarity between words, \n2. express the joint probability function of word sequences in terms of the feature \nvectors of these words in the sequence, and \n3. learn simultaneously the word feature vectors and the parameters of thatfitnction. The feature vector represents different aspects of a word: each word is associated with a \npoint in a vector space. The number of features (e.g.<\/DOCUMENT>\n<DOCUMENT>3 Speeding-up and other Tricks \nShort list. The main idea is to focus the effort of the neural network on a \"short list\" \nof words that have the highest probability. This can save much computation because in \nboth of the proposed architectures the time to compute the probability of the observed \nnext word scales almost linearly with the number of words in the vocabulary (because \nthe scores hi associated with each word i in the vocabulary must be computed for prop\u00ad\nerly normalizing probabilities with the softmax). The idea of the speed-up trick is the \nfollowing: instead of computing the actual probability of the next word, the neural net\u00ad\nwork is used to compute the relative probability of the next word within that short list. The choice of the short list depends on the current context (the previous n words). We \nhave used our smoothed trigram model to pre-compute a short list containing the most \nprobable next words associated to the previous two words. The conditional probabilities \nP(Wt = ilht) are thus computed as follows, denoting with ht the history (context) before \nWt. and Lt the short list of words for the prediction of Wt. If i E Lt then the probabil-\nity is PNN(Wt = ilWt E Lt, ht)Ptrigram(Wt E Ltlht), else it is Ptrigram(Wt = ilht), \nwhere PNN(Wt = ilWt E Lt, ht) are the normalized scores of the words computed by \nthe neural network, where the \"softmax\" is only normalized over the words in the short \nlist Lt, and Ptrigram(Wt E Ltlht) = ~iEL. Ptrigram(ilht), with Ptrigram(ilht) standing \nfor the next-word probabilities computed by the smoothed trigram. Note that both Lt and \nPtrigram(Wt E Ltlht) can be pre-computed (and stored in a hash table indexed by the last \ntwo words). Table look-up for recognition. To speed up application of the trained model, one can \npre-compute in a hash table the output of the neural network, at least for the most frequent \ninput contexts. In that case, the neural network will only be rarely called upon, and the \naverage computation time will be very small. Note that in a speech recognition system, one \nneeds only compute the relative probabilities of the acoustically ambiguous words in each \ncontext, also reducing drastically the amount of computations. Stochastic gradient descent.<\/DOCUMENT>\n<DOCUMENT>Refinements to speed it up and extend it \nwill be described in the following sections. We decompose the function f (Wt, ... , Wt-n) = \nP( Wt Iwi-i) in two parts: \n1. A mapping C from any element of V to a real vector C(i) E Rm. It represents \nthe \"distributed feature vector\" associated with each word in the vocabulary . In \npractice, C is represented by a I V I x m matrix (of free parameters). 2. The probability function over words, expressed with C. We have considered two \nalternative formulations : \n(a) The direct architecture : a function 9 maps a sequence of feature \nvectors for words in context (C(Wt-n),\u00b7\u00b7 \u00b7, C(wt-d) to a probabil\u00ad\nity distribution over words in V. It is a vector function whose i-th \nelement estimates the probability P(Wt = ilwi-i) as in figure 1.<\/DOCUMENT>\n<DOCUMENT>Since we have millions of examples, it is important to con\u00ad\nverge within only a few passes through the data. For very large data sets, stochastic gradient \ndescent convergence time seems to increase sub-linearly with the size of the data set (see \nexperiments on Brown vs Hansard below). To speed up training using stochastic gradient descent, we have found it useful to break the corpus in paragraphs and to randomly permute \nthem. In this way, some of the non-stationarity in the word stream is eliminated, yielding \nfaster convergence. Capacity control. For the \"smaller corpora\" like Brown (1.2 million examples), we have \nfound early stopping and weight decay useful to avoid over-fitting. For the larger corpora, \nour networks still under-fit. For the larger corpora, we have found double-precision com\u00ad\nputation to be very important to obtain good results. Mixture of models. We have found improved performance by combining the probability \npredictions of the neural network with those of the smoothed trigram, with weights that \nwere conditional on the frequency of the context (same procedure used to combine trigram, \nbigram, and unigram in the smoothed trigram). Initialization of word feature vectors. We have tried both random initialization (uniform \nbetween -.01 and .01) and a \"smarter\" method based on a Singular Value Decomposition \n(SVD) of a very large matrix of \"context features\". These context features are formed \nby counting the frequency of occurrence of each word in each one of the most frequent \ncontexts (word sequences) in the corpus. The idea is that \"similar\" words should occur \nwith similar frequency in the same contexts. We used about 9000 most frequent contexts, \nand compressed these to 30 features with the SVD. Out-of-vocabulary words. For an out-of-vocabulary word Wt we need to come up with \na feature vector in order to predict the words that follow, or predict its probability (that \nis only possible with the cycling architecture). We used as feature vector the weighted \naverage feature vector of all the words in the short list, with the weights being the relative \nprobabilities ofthose words: E[C(wt)lhtl = Ei C(i)P(wt = ilht). 4 Experimental Results \nComparative experiments were performed on the Brown and Hansard corpora. The Brown \ncorpus is a stream of 1,181,041 words (from a large variety of English texts and books). The first 800,000 words were used for training, the following 200,000 for validation (model \nselection, weight decay, early stopping) and the remaining 181,041 for testing. The number \nof different words is 47, 578 (including punctuation, distinguishing between upper and \nlower case, and including the syntactical marks used to separate texts and paragraphs). Rare words with frequency:::; 3 were merged into a single token, reducing the vocabulary \nsize to IVI = 16,383. The Hansard corpus (Canadian parliament proceedings, French version) is a stream of \nabout 34 million words, of which 32 millions (set A) was used for training, 1.1 million \n(set B) was used for validation, and 1.2 million (set C) was used for out-of-sample tests. The original data has 106, 936 different words, and those with frequency:::; 10 were merged \ninto a single token, yielding IVI = 30,959 different words.<\/DOCUMENT>\n4. What is the role of the sequence of feature vectors?  "}
{"id":"123b3d16-d00f-4c5b-8904-54399aaec513","type":"general","question":"5. What does P(Wt = i|wi-i) signify?  ","context":{"sentences":[["Hinton. Extracting distributed representations of concepts and relations \nfrom positive and negative propositions. In Proceedings of the International Joint Conference \non Neural Network, lJCNN'2000, Como, Italy, 2000.","Refinements to speed it up and extend it \nwill be described in the following sections. We decompose the function f (Wt, ... , Wt-n) = \nP( Wt Iwi-i) in two parts: \n1. A mapping C from any element of V to a real vector C(i) E Rm. It represents \nthe \"distributed feature vector\" associated with each word in the vocabulary . In \npractice, C is represented by a I V I x m matrix (of free parameters). 2. The probability function over words, expressed with C. We have considered two \nalternative formulations : \n(a) The direct architecture : a function 9 maps a sequence of feature \nvectors for words in context (C(Wt-n),\u00b7\u00b7 \u00b7, C(wt-d) to a probabil\u00ad\nity distribution over words in V. It is a vector function whose i-th \nelement estimates the probability P(Wt = ilwi-i) as in figure 1.","m = 30,60 or 100 in the experiments) \nis much smaller than the size of the vocabulary. The probability function is expressed as a \nproduct of conditional probabilities of the next word given the previous ones, (e.g. using \na multi-layer neural network in the experiment).","More context is useful. Experiments with the cycling architecture on Brown, with 30 \nword features, and 30 hidden units, varying the number of context words: n = 1 (like the \nbigram) yields a test perplexity of 302, n = 3 yields 291, n = 5 yields 281, n = 8 yields \n279 (N.B. the smoothed trigram yields 348). Hidden units help. Experiments with the direct architecture on Brown (with direct input \nto output connections), with 30 word features, 5 words of context, varying the number of \nhidden units: 0 yields a test perplexity of 275, 10 yields 267, 20 yields 266, 40 yields 265, \n80 yields 265. Learning the word features jointly is important. Experiments with the direct architec\u00ad\nture on Brown (40 hidden units, 5 words of context), in which the word features initialized \nwith the SVD method are kept fixed during training yield a test perplexity of 345.8 whereas \nif the word features are trained jointly with the rest of the parameters, the perplexity is 265. Initialization not so useful. Experiments on Brown with both architectures reveal that the \nSVD initialization of the word features does not bring much improvement with respect to \nrandom initialization : it speeds up initial convergence (saving about 2 epochs), and yields \na perplexity improvement of less than 0.3%. Direct architecture works a bit better."]],"title":[["placeholder_title","placeholder_title","placeholder_title","placeholder_title"]]},"oracle_context":"Refinements to speed it up and extend it \nwill be described in the following sections. We decompose the function f (Wt, ... , Wt-n) = \nP( Wt Iwi-i) in two parts: \n1. A mapping C from any element of V to a real vector C(i) E Rm. It represents \nthe \"distributed feature vector\" associated with each word in the vocabulary . In \npractice, C is represented by a I V I x m matrix (of free parameters). 2. The probability function over words, expressed with C. We have considered two \nalternative formulations : \n(a) The direct architecture : a function 9 maps a sequence of feature \nvectors for words in context (C(Wt-n),\u00b7\u00b7 \u00b7, C(wt-d) to a probabil\u00ad\nity distribution over words in V. It is a vector function whose i-th \nelement estimates the probability P(Wt = ilwi-i) as in figure 1.","cot_answer":"To answer the question \"What does P(Wt = i|wi-i) signify?\", we can follow these steps:\n\n1. **Identify the notation**: The expression P(Wt = i|wi-i) represents a conditional probability. Here, Wt refers to the word at time t, and wi-i indicates the context of words preceding Wt.\n\n2. **Understand the context**: The context provided discusses a function f that decomposes into two parts, one of which is a mapping C that associates each word in the vocabulary with a \"distributed feature vector.\" The second part involves a probability function over words.\n\n3. **Extract relevant information**: The context mentions that the function estimates the probability P(Wt = i|wi-i) as part of a vector function that maps a sequence of feature vectors to a probability distribution over words in the vocabulary.\n\n4. **Summarize the significance**: Therefore, P(Wt = i|wi-i) signifies the probability of the word Wt being equal to a specific word i, given the context of the preceding words (wi-i).\n\nNow, we can formulate the final answer succinctly.\n\n<ANSWER>: P(Wt = i|wi-i) signifies the probability of the word Wt being equal to a specific word i, given the context of the preceding words.","instruction":"<DOCUMENT>Hinton. Extracting distributed representations of concepts and relations \nfrom positive and negative propositions. In Proceedings of the International Joint Conference \non Neural Network, lJCNN'2000, Como, Italy, 2000.<\/DOCUMENT>\n<DOCUMENT>Refinements to speed it up and extend it \nwill be described in the following sections. We decompose the function f (Wt, ... , Wt-n) = \nP( Wt Iwi-i) in two parts: \n1. A mapping C from any element of V to a real vector C(i) E Rm. It represents \nthe \"distributed feature vector\" associated with each word in the vocabulary . In \npractice, C is represented by a I V I x m matrix (of free parameters). 2. The probability function over words, expressed with C. We have considered two \nalternative formulations : \n(a) The direct architecture : a function 9 maps a sequence of feature \nvectors for words in context (C(Wt-n),\u00b7\u00b7 \u00b7, C(wt-d) to a probabil\u00ad\nity distribution over words in V. It is a vector function whose i-th \nelement estimates the probability P(Wt = ilwi-i) as in figure 1.<\/DOCUMENT>\n<DOCUMENT>m = 30,60 or 100 in the experiments) \nis much smaller than the size of the vocabulary. The probability function is expressed as a \nproduct of conditional probabilities of the next word given the previous ones, (e.g. using \na multi-layer neural network in the experiment).<\/DOCUMENT>\n<DOCUMENT>More context is useful. Experiments with the cycling architecture on Brown, with 30 \nword features, and 30 hidden units, varying the number of context words: n = 1 (like the \nbigram) yields a test perplexity of 302, n = 3 yields 291, n = 5 yields 281, n = 8 yields \n279 (N.B. the smoothed trigram yields 348). Hidden units help. Experiments with the direct architecture on Brown (with direct input \nto output connections), with 30 word features, 5 words of context, varying the number of \nhidden units: 0 yields a test perplexity of 275, 10 yields 267, 20 yields 266, 40 yields 265, \n80 yields 265. Learning the word features jointly is important. Experiments with the direct architec\u00ad\nture on Brown (40 hidden units, 5 words of context), in which the word features initialized \nwith the SVD method are kept fixed during training yield a test perplexity of 345.8 whereas \nif the word features are trained jointly with the rest of the parameters, the perplexity is 265. Initialization not so useful. Experiments on Brown with both architectures reveal that the \nSVD initialization of the word features does not bring much improvement with respect to \nrandom initialization : it speeds up initial convergence (saving about 2 epochs), and yields \na perplexity improvement of less than 0.3%. Direct architecture works a bit better.<\/DOCUMENT>\n5. What does P(Wt = i|wi-i) signify?  "}
