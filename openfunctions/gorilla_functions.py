# -*- coding: utf-8 -*-
"""gorilla_functions.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zv6vRUv3Zwuf1YiI9B9mNiqCWfhVZbBp

# **Updates:**

# **Simulated Multi-Turn Debugging Conversations**

The model is successfully handling your first corrections but struggles when you significantly change the order (like switching from pizza to soda).

The model adjusted the order to 3 pizzas and 1 garlic bread when requested, showing it can handle quantity modifications.

When asked to replace items with "3 sodas" or "include soda," the model did not adjust and reverted to its default behavior (giving a predefined response about its role as a programming assistant).

The last feedback ("I want to include 3 more pizzas") caused an API issue, likely due to an unexpected input or internal server issue.

Recommendations:
Improve Context Adaptability: The model struggles to switch between entirely different items mid-conversation. Training it to handle such context changes smoothly will improve its flexibility.
Error Handling: The API should handle unexpected inputs more gracefully, returning a useful response instead of failing entirely (500 error).
"""

import openai
import json

def order_food(restaurant, items, quantities):
    """Place an order for food items at a given restaurant"""
    restaurant_info = {
        "restaurant": restaurant,
        "items": items,
        "quantities": quantities
    }
    return json.dumps(restaurant_info)

def run_complex_conversation():
    # Step 1: send the conversation and available functions to GPT
    messages = [
        {"role": "user", "content": "Order a pizza and garlic bread from Costco."}
    ]

    functions = [
        {
            "name": "order_food",
            "description": "Place an order for food at a restaurant",
            "parameters": {
                "type": "object",
                "properties": {
                    "restaurant": {"type": "string", "description": "The name of the restaurant"},
                    "items": {"type": "array", "items": {"type":"string"}, "description": "List of items"},
                    "quantities": {"type": "array", "items": {"type": "integer"}, "description": "Quantities of the food items"}
                },
                "required": ["restaurant", "items", "quantities"],
            },
        },
    ]


    openai.api_key = "EMPTY"  # Hosted for free with ‚ù§Ô∏è from UC Berkeley
    openai.api_base = "http://luigi.millennium.berkeley.edu:8000/v1"

    while True:
      response = openai.ChatCompletion.create(
          model='gorilla-openfunctions-v0',
          messages=messages,
          functions=functions,
          function_call="auto",  # auto is default, but we'll be explicit
      )

      response_message = response["choices"][0]["message"]
      print(response_message)

      # Simulate a user error or request for debugging (multi-turn interaction)
      user_feedback = input("Enter user feedback (e.g., error, change order, or 'done' to end): ")

      if user_feedback.lower() == "done":
          break

      # Add the user's new input to the conversation
      messages.append({"role": "user", "content": user_feedback})


run_complex_conversation()

"""# **Error-Driven Feedback**

Has difficulty interpreting instructions and adjusting the API calls based on  feedback.

The model continues to treat "10" and "20" as item names despite the corrections provided.

The model is still misinterpreting quantities as strings in some cases, rather than integers.


This indicates that the model needs better fine-tuning to handle multi-turn interactions and corrections more effectively.
"""

import openai
import json

def order_food(restaurant, items, quantities):
    """Place an order for food items at a given restaurant"""
    restaurant_info = {
        "restaurant": restaurant,
        "items": items,
        "quantities": quantities
    }
    return json.dumps(restaurant_info)

def run_error_driven_feedback():
    # Step 1: send the conversation and available functions to GPT
    messages = [
        {"role": "user", "content": "I want to order 10 McFlurries and 20 chicken nuggets from McDonald's."}
    ]

    functions = [
        {
            "name": "order_food",
            "description": "Place an order for food at a restaurant",
            "parameters": {
                "type": "object",
                "properties": {
                    "restaurant": {"type": "string", "description": "The name of the restaurant"},
                    "items": {"type": "array", "items": {"type":"string"}, "description": "List of items"},
                    "quantities": {"type": "array", "items": {"type": "integer"}, "description": "Quantities of the food items"}
                },
                "required": ["restaurant", "items", "quantities"],
            },
        },
    ]


    openai.api_key = "EMPTY"  # Hosted for free with ‚ù§Ô∏è from UC Berkeley
    openai.api_base = "http://luigi.millennium.berkeley.edu:8000/v1"

    while True:
      response = openai.ChatCompletion.create(
          model='gorilla-openfunctions-v0',
          messages=messages,
          functions=functions,
          function_call="auto",  # auto is default, but we'll be explicit
      )

      response_message = response["choices"][0]["message"]
      print(response_message)

      # Simulate a user error or request for debugging (multi-turn interaction)
      user_feedback = input("Enter error feedback (e.g., error message, or 'done' to end): ")

      if user_feedback.lower() == "done":
          break

      # Add the user's new input to the conversation
      messages.append({"role": "user", "content": user_feedback})

      # Optionally: Add the error type in the response to guide correction
      messages.append({"role": "system", "content": "Error received. Adjust your API call to include missing items or correct parameters."})



run_error_driven_feedback()

"""# **Backtranslation & Paraphrasing:**

Correctly understood and processed the request. The assistant generated an API call with the correct parameters:



```
Restaurant: "McDonald's"
Items: "10 McFlurries" and "20 chicken nuggets"
Quantities: 10 and 20
```



The paraphrased request still led to the correct API call, showing that the model successfully interpreted the input and converted it into the intended function call for the order_food function.

This demonstrates that the paraphrasing technique worked well, as the model preserved the meaning while processing the API call correctly.
"""

import openai
import random

# Sample paraphrase examples
paraphrases = [
    "Can you order 10 McFlurries and 20 nuggets for me from McDonald's?",
    "I'd like to get 10 McFlurries and 20 chicken nuggets from McDonald's.",
    "Place an order for 10 McFlurries and 20 chicken nuggets at McDonald's."
]

def backtranslate_and_paraphrase(original_message):
    """Generate paraphrases or backtranslations for the original message"""
    # Simulate paraphrasing by choosing a random variation
    paraphrased_message = random.choice(paraphrases)
    return paraphrased_message

def run_backtranslation_paraphrasing():
    # Original user request
    original_message = "I want to order 10 McFlurries and 20 chicken nuggets from McDonald's."

    # Backtranslation/Paraphrasing step
    paraphrased_message = backtranslate_and_paraphrase(original_message)

    # Creating the conversation with paraphrased input
    messages = [{"role": "user", "content": paraphrased_message}]

    functions = [
        {
            "name": "order_food",
            "description": "Place an order for food at a restaurant",
            "parameters": {
                "type": "object",
                "properties": {
                    "restaurant": {"type": "string", "description": "The name of the restaurant"},
                    "items": {"type": "array", "items": {"type":"string"}, "description": "List of items"},
                    "quantities": {"type": "array", "items": {"type": "integer"}, "description": "Quantities of the food items"}
                },
                "required": ["restaurant", "items", "quantities"],
            },
        },
    ]

    openai.api_key = "EMPTY"  # Hosted for free with ‚ù§Ô∏è from UC Berkeley
    openai.api_base = "http://luigi.millennium.berkeley.edu:8000/v1"

    response = openai.ChatCompletion.create(
        model='gorilla-openfunctions-v0',
        messages=messages,
        functions=functions,
        function_call="auto"
    )

    # Print model response
    print(response)
    response_message = response["choices"][0]["message"]
    print(response_message)

run_backtranslation_paraphrasing()

"""The model understood the request but misinterpreted the item names, returning "pizza1" and "pizza2" instead of the expected "pizza" and "garlic bread."

This suggests that while the paraphrasing is working, the model is unable to differentiate between unique items properly.


Fine-tune the model on more diverse and specific item names, ensuring it correctly interprets them during paraphrased requests.
"""

import openai
import random

# Paraphrase examples
paraphrases = [
    "I‚Äôd like to place an order for 2 pizzas and 1 garlic bread from Domino‚Äôs.",
    "Can you get me 2 pizzas and 1 garlic bread from Domino‚Äôs?",
    "Order 2 pizzas and 1 garlic bread from Domino‚Äôs for me."
]

def backtranslate_and_paraphrase(original_message):
    """Generate paraphrases or backtranslations for the original message"""
    paraphrased_message = random.choice(paraphrases)
    return paraphrased_message

def run_backtranslation_paraphrasing():
    # Original user request
    original_message = "I want to order 2 pizzas and 1 garlic bread from Domino's."

    # Backtranslation/Paraphrasing step
    paraphrased_message = backtranslate_and_paraphrase(original_message)

    # Creating the conversation with paraphrased input
    messages = [{"role": "user", "content": paraphrased_message}]

    functions = [
        {
            "name": "order_food",
            "description": "Place an order for food at a restaurant",
            "parameters": {
                "type": "object",
                "properties": {
                    "restaurant": {"type": "string", "description": "The name of the restaurant"},
                    "items": {"type": "array", "items": {"type":"string"}, "description": "List of items"},
                    "quantities": {"type": "array", "items": {"type": "integer"}, "description": "Quantities of the food items"}
                },
                "required": ["restaurant", "items", "quantities"],
            },
        },
    ]

    openai.api_key = "EMPTY"  # Hosted for free with ‚ù§Ô∏è from UC Berkeley
    openai.api_base = "http://luigi.millennium.berkeley.edu:8000/v1"

    response = openai.ChatCompletion.create(
        model='gorilla-openfunctions-v0',
        messages=messages,
        functions=functions,
        function_call="auto"
    )

    # Print model response
    print(response)
    response_message = response["choices"][0]["message"]
    print(response_message)

# Call the function
run_backtranslation_paraphrasing()

"""## Here are some more examples. Just run the cells to see how Gorilla performs!

You can edit any of the blocks in-place, and try it out yourself!

### Uber
"""

query = "Call me an Uber ride type \"Plus\" in Berkeley at zipcode 94704 in 10 minutes"
functions = [
    {
        "name": "Uber Carpool",
        "api_name": "uber.ride",
        "description": "Find suitable ride for customers given the location, type of ride, and the amount of time the customer is willing to wait as parameters",
        "parameters":  [{"name": "loc", "description": "location of the starting place of the uber ride"}, {"name":"type", "enum": ["plus", "comfort", "black"], "description": "types of uber ride user is ordering"}, {"name": "time", "description": "the amount of time in minutes the customer is willing to wait"}]
    }
]
get_gorilla_response(query, functions=functions)

function_documentation = [{
    "name" : "Order Food on Uber",
    "api_name": "uber.eat.order",
    "description": "Order food on uber eat, specifying items and their quantities",
    "parameters": [
        {
            "name": "restaurants",
            "description": "The chosen restaurant"
        },
        {
            "name": "items",
            "description": "List of selected items"
        },
        {
            "name": "quantities",
            "description": "Quantities corresponding to the chosen items"
        }
    ]
}]
query =  "I want to order five 'burgers' and six 'chicken wings' from uber eat McDonald's."
get_gorilla_response(query, functions=function_documentation)

"""### AWS"""

from os.path import join
query = "I want to list the exports for my bot with the bot id \"my-bot-id\" and the bot version \"v2\"."
functions = [
    {
        "domain": "Cloud Infrastructure",
        "framework": "aws",
        "functionality": "Lists the exports for a bot, bot locale, or custom vocabulary. Exports are kept in the list for 7 days.",
        "api_name": "aws.lexv2-models.list-exports",
        "api_arguments": [
            {
                "name": "bot-id",
                "description": "\nThe unique identifier that Amazon Lex assigned to the bot."
            },
            {
                "name": "bot-version",
                "description": "\nThe version of the bot to list exports for."
            }
        ],
        "python_environment_requirements": [
            "aws"
        ],
        "example_code": [],
        "output": {
            "botId -> (string)": "\nThe unique identifier assigned to the bot by Amazon Lex.",
            "botVersion -> (string)": "\nThe version of the bot that was exported.",
            "exportSummaries -> (list)": "\nSummary information for the exports that meet the filter criteria specified in the request. The length of the list is specified in the maxResults parameter. If there are more exports available, the nextToken field contains a token to get the next page of results.\n(structure)\n\nProvides summary information about an export in an export list.\nexportId -> (string)\n\nThe unique identifier that Amazon Lex assigned to the export.\nresourceSpecification -> (structure)\n\nInformation about the bot or bot locale that was exported.\nbotExportSpecification -> (structure)\n\nParameters for exporting a bot.\nbotId -> (string)\n\nThe identifier of the bot assigned by Amazon Lex.\nbotVersion -> (string)\n\nThe version of the bot that was exported. This will be either DRAFT or the version number.\n\nbotLocaleExportSpecification -> (structure)\n\nParameters for exporting a bot locale.\nbotId -> (string)\n\nThe identifier of the bot to create the locale for.\nbotVersion -> (string)\n\nThe version of the bot to export.\nlocaleId -> (string)\n\nThe identifier of the language and locale to export. The string must match one of the locales in the bot.\n\ncustomVocabularyExportSpecification -> (structure)\n\nThe parameters required to export a custom vocabulary.\nbotId -> (string)\n\nThe identifier of the bot that contains the custom vocabulary to export.\nbotVersion -> (string)\n\nThe version of the bot that contains the custom vocabulary to export.\nlocaleId -> (string)\n\nThe locale of the bot that contains the custom vocabulary to export.\n\ntestSetExportSpecification -> (structure)\n\nSpecifications for the test set that is exported as a resource.\ntestSetId -> (string)\n\nThe unique identifier of the test set.\n\n\nfileFormat -> (string)\n\nThe file format used in the export files.\nexportStatus -> (string)\n\nThe status of the export. When the status is Completed the export is ready to download.\ncreationDateTime -> (timestamp)\n\nThe date and time that the export was created.\nlastUpdatedDateTime -> (timestamp)\n\nThe date and time that the export was last updated.\n\n",
            "nextToken -> (string)": "\nA token that indicates whether there are more results to return in a response to the ListExports operation. If the nextToken field is present, you send the contents as the nextToken parameter of a ListExports operation request to get the next page of results.",
            "localeId -> (string)": "\nThe locale specified in the request."
        },
        "api_arguments_all": {
            "--bot-id ": "\nThe unique identifier that Amazon Lex assigned to the bot.",
            "--bot-version ": "\nThe version of the bot to list exports for.",
            "--sort-by ": "\nDetermines the field that the list of exports is sorted by. You can sort by the LastUpdatedDateTime field in ascending or descending order.\nattribute -> (string)\n\nThe export field to use for sorting.\norder -> (string)\n\nThe order to sort the list.\n",
            "--filters ": "\nProvides the specification of a filter used to limit the exports in the response to only those that match the filter specification. You can only specify one filter and one string to filter on.\n(structure)\n\nFilters the response form the ListExports operation\nname -> (string)\n\nThe name of the field to use for filtering.\nvalues -> (list)\n\nThe values to use to filter the response. The values must be Bot , BotLocale , or CustomVocabulary .\n(string)\n\noperator -> (string)\n\nThe operator to use for the filter. Specify EQ when the ListExports operation should return only resource types that equal the specified value. Specify CO when the ListExports operation should return resource types that contain the specified value.\n\n",
            "--max-results ": "\nThe maximum number of exports to return in each page of results. If there are fewer results than the max page size, only the actual number of results are returned.",
            "--next-token ": "\nIf the response from the ListExports operation contains more results that specified in the maxResults parameter, a token is returned in the response.\nUse the returned token in the nextToken parameter of a ListExports request to return the next page of results. For a complete set of results, call the ListExports operation until the nextToken returned in the response is null.\n",
            "--locale-id ": "\nSpecifies the resources that should be exported. If you don\u00e2\u0080\u0099t specify a resource type in the filters parameter, both bot locales and custom vocabularies are exported."
        }
    }
]
get_gorilla_response(query, functions=functions)

"""## Models Available

|Model | Functionality|
|---|---|
|gorilla-openfunctions-v0 | Given a function, and user intent, returns properly formatted json with the right arguments|
|gorilla-openfunctions-v1 | + Parallel functions, and can choose between functions|

# OpenFunctions from Gorilla - Try it out in less than 60s üöÄ

[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/ShishirPatil/gorilla)  [![arXiv](https://img.shields.io/badge/arXiv-2305.15334-<COLOR>.svg?style=flat-square)](https://arxiv.org/abs/2305.15334)   [![Discord](https://img.shields.io/discord/1111172801899012102?label=Discord&logo=discord&logoColor=green&style=flat-square)](https://discord.gg/grXXvj9Whz)  [![Twitter](https://img.shields.io/twitter/url?url=https://twitter.com/shishirpatil_/status/1661780076277678082)](https://twitter.com/shishirpatil_/status/1661780076277678082)

Play around with Gorilla! Here, we show how you can use OpenFunctions from Gorilla, so you can try it out! This is compatible with the OpenAI chat completion API - plug and play!

üü¢ Now with Apache-2.0! Gorilla is commercially usable with no obligations üöÄ


üíÉ If you want to use Gorilla or build on top of it! Feel absolutely free to do so - we believe in open source research and you don't even have to tell us! In case you choose to do, we have a vibrant community in Discord! Stop by and say Hi üëã

<img src="https://github.com/ShishirPatil/gorilla/blob/gh-pages/assets/img/logo.png?raw=true" width=30% height=30%>

## Gorilla ü¶ç is hosted by UC Berkeley Sky lab for FREE ü§© as a research prototype ü§ì Please don't use it for commercial serving üëÄ
"""

# Import Chat completion template and set-up variables
!pip install openai==0.28.1 &> /dev/null
import openai
import urllib.parse
import json


# Report issues
def raise_issue(e, model, prompt):
    issue_title = urllib.parse.quote("[bug] Hosted Gorilla: <Issue>")
    issue_body = urllib.parse.quote(f"Exception: {e}\nFailed model: {model}, for prompt: {prompt}")
    issue_url = f"https://github.com/ShishirPatil/gorilla/issues/new?assignees=&labels=hosted-gorilla&projects=&template=hosted-gorilla-.md&title={issue_title}&body={issue_body}"
    print(f"An exception has occurred: {e} \nPlease raise an issue here: {issue_url}")

# Query Gorilla server
def get_gorilla_response(prompt="Call me an Uber ride type \"Plus\" in Berkeley at zipcode 94704 in 10 minutes", model="gorilla-openfunctions-v0", functions=[]):
  openai.api_key = "EMPTY" # Hosted for free with ‚ù§Ô∏è from UC Berkeley
  openai.api_base = "http://luigi.millennium.berkeley.edu:8000/v1"
  try:
    completion = openai.ChatCompletion.create(
      model="gorilla-openfunctions-v1",
      temperature=0.0,
      messages=[{"role": "user", "content": prompt}],
      functions=functions,
    )
    return completion.choices[0].message.content
  except Exception as e:
    print(e, model, prompt)

"""# üöÄ Using gorilla is as easy as calling `get_gorilla_response()` with your prompt! Try out Gorilla, and share your interesting findings in `#showcase` ü§© [Discord](https://discord.gg/3apqwwME)!

# Gorilla OpenFunctions

## Gorilla OpenFunctions is a drop in replacement for OpenAI Function Call API!

## Introduction
**OpenFunctions** is designed to extend Large Language Model (LLM) Chat Completion features by formulating executable API calls from natural language instructions and API context. With this LLMs can fill parameters for a diverse range of services, from Instagram and Doordash to tools like Google Calendar and Stripe, to enterprise services such as Salesforce and Datadog. With Open Functions, even those unfamiliar with API calls or programming can use the model to generate desired API calls. Trained on a curated collection of API documentations and associated Q&A pairs, OpenFunctions is another step in the Gorilla Paradigm's ongoing evolution, aiming for enhanced quality and accuracy in function call generation.

<!-- Insert Image here -->

## Code Function Calling API vs. REST API
Throughout our data collection process, we've discerned that general API calling can broadly bifurcate into two categories:

1. **Code Function Calling APIs**:
    - Predominantly observed in external Python packages like Numpy and Sklearn.
    - Characterized by well-defined and easily formatted calls.
    - Simply knowing the `api_name` (e.g., `numpy.sum()`) and `arguments` specifications allows the extrapolation of an executable function API.
    - Owing to its consistent format and fixed locality, fine-tuning the model requires relatively minimal data.

2. **REST APIs**
    - Traditional `GET` and `POST` requests.

## How to use Open Functions
Leveraging **Gorilla OpenFunctions** is refreshingly straightforward:

1. **Define Your Functions**:
    - Furnish a JSON file detailing your custom functions.
    - Each function should encompass fields: `name`, `api_call`, `description`, and `parameters`.
    - Below is an example for a comprehensive API documentation suitable for Open Function:
      ```python
      function_documentation = {  
          "name" : "Order Food on Uber",
          "api_call": "uber.eat.order",
          "description": "Order food on uber eat, specifying items and their quantities",
          "parameters": [
              {
                  "name": "restaurants",
                  "description": "The chosen restaurant"
              },
              {
                  "name": "items",
                  "description": "List of selected items"
              },
              {
                  "name": "quantities",
                  "description": "Quantities corresponding to the chosen items"
              }
          ]
      }
      ```

2. **Ask Your Question**:
    - Frame your requirement conversationally.
    - For instance: *I want to order five burgers and six chicken wings from McDonald's.*

3. **Get Your Function Call**:
    - The model deciphers your request and reciprocates with a Python function call.
    - This paradigm expands horizons for both developers and laypersons, enabling them to harness intricate functionalities sans extensive coding.
      ```python
      Input:
      get_gorilla_response(prompt="I want to order five burgers and six chicken wings from McDonald's.", functions=[function_documentation])
      
      Output:
      uber.eat.order(restaurants="McDonald", items=["chicken wings", "burgers"], quantities=[6,5])
      ```

## Weather Example
"""

import openai
import json

# Example dummy function hard coded to return the same weather
# In production, this could be your backend API or an external API
def get_current_weather(location, unit="fahrenheit"):
    """Get the current weather in a given location"""
    weather_info = {
        "location": location,
        "temperature": "72",
        "unit": unit,
        "forecast": ["sunny", "windy"],
    }
    return json.dumps(weather_info)

def run_conversation():
    # Step 1: send the conversation and available functions to GPT
    messages = [{"role": "user", "content": "What's the weather like in Boston?"}]
    functions = [
        {
            "name": "get_current_weather",
            "description": "Get the current weather in a given location",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {
                        "type": "string",
                        "description": "The city and state, e.g. San Francisco, CA",
                    },
                    "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},
                },
                "required": ["location"],
            },
        }
    ]
    openai.api_key = "EMPTY" # Hosted for free with ‚ù§Ô∏è from UC Berkeley
    openai.api_base = "http://luigi.millennium.berkeley.edu:8000/v1"
    response = openai.ChatCompletion.create(

        #model="gpt-3.5-turbo-0613",
        model='gorilla-openfunctions-v0',
        messages=messages,
        functions=functions,
        function_call="auto",  # auto is default, but we'll be explicit
    )
    response_message = response["choices"][0]["message"]
    print(response_message)



run_conversation()

"""# More on Function Calling from OpenAI

Function calling allows you to more reliably get structured data back from the model. For example, you can:

## Use Cases

- **Chatbots with API calls**: Create chatbots that answer questions by calling external APIs (e.g. like ChatGPT Plugins)
  - e.g. `send_email(to: string, body: string)`
  - e.g. `get_current_weather(location: string, unit: 'celsius' | 'fahrenheit')`
  
- **Natural Language to API Conversion**: Convert natural language into API calls
  - e.g. Convert "Who are my top customers?" to `get_customers(min_revenue: int, created_before: string, limit: int)` and call your internal API
  
- **Data Extraction**: Extract structured data from text
  - e.g. `extract_data(name: string, birthday: string)`
  - e.g. `sql_query(query: string)`

## Workflow

The basic sequence of steps for function calling is as follows:

1. **User Query**: Call the model with the user query and a set of functions defined in the `functions` parameter.
2. **Function Invocation**: The model can choose to call a function; if so, the content will be a stringified JSON object adhering to your custom schema (note: the model may generate invalid JSON or hallucinate parameters).
3. **Parse and Execute**: Parse the string into JSON in your code, and call your function with the provided arguments if they exist.
4. **Response**: Call the model again by appending the function response as a new message, and let the model summarize the results back to the user.

> **Tip**: You can see these steps in action through the example below:

"""