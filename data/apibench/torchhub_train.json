{"code": "{'Instruction': I need an API that can estimate the relative depth of objects from a single image. The output should be an inverse depth map., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('intel-isl/MiDaS', 'DPT_Hybrid', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained MiDaS model for estimating relative depth from a single image, which outputs an inverse depth map.\", 'code': 'import torch\nmodel = torch.hub.load(\"intel-isl/MiDaS\", \"DPT_Hybrid\", pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='intel-isl/MiDaS', model='DPT_Hybrid', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Computing relative depth from a single image", "api_name": "MiDaS", "api_call": "torch.hub.load(repo_or_dir='intel-isl/MiDaS', model='DPT_Hybrid', pretrained=True)", "api_arguments": {"repo_or_dir": "intel-isl/MiDaS", "model": "model_type"}, "python_environment_requirements": "pip install timm", "example_code": ["import cv2", "import torch", "import urllib.request", "import matplotlib.pyplot as plt", "url, filename = ('https://github.com/pytorch/hub/raw/master/images/dog.jpg', 'dog.jpg')", "urllib.request.urlretrieve(url, filename)", "model_type = 'DPT_Large'", "midas = torch.hub.load('intel-isl/MiDaS', 'DPT_Hybrid')", "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')", "midas.to(device)", "midas.eval()", "midas_transforms = torch.hub.load('intel-isl/MiDaS', 'transforms')", "if model_type == 'DPT_Large' or model_type == 'DPT_Hybrid':", " transform = midas_transforms.dpt_transform", "else:", " transform = midas_transforms.small_transform", "img = cv2.imread(filename)", "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)", "input_batch = transform(img).to(device)", "with torch.no_grad():", " prediction = midas(input_batch)", "prediction = torch.nn.functional.interpolate(", " prediction.unsqueeze(1),", " size=img.shape[:2],", " mode='bicubic',", " align_corners=False,", ").squeeze()", "output = prediction.cpu().numpy()", "plt.imshow(output)", "plt.show()"], "performance": {"dataset": "10 distinct datasets", "accuracy": "Multi-objective optimization"}, "description": "MiDaS computes relative inverse depth from a single image. The repository provides multiple models that cover different use cases ranging from a small, high-speed model to a very large model that provide the highest accuracy. The models have been trained on 10 distinct datasets using multi-objective optimization to ensure high quality on a wide range of inputs."}}
{"code": "{'Instruction': I need an API to detect different types of vehicles, people, and lanes in an image captured from a camera mounted on a self-driving car., 'Output': {'domain': 'Object Detection', 'api_call': \"model = torch.hub.load('hustvl/yolop', 'yolop', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the YOLOP model from PyTorch Hub with pre-trained weights optimized for object detection, drivable area segmentation, and lane detection in autonomous driving scenarios.', 'code': \"import torch\nmodel = torch.hub.load('hustvl/yolop', 'yolop', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='hustvl/yolop', model='yolop', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Object Detection", "framework": "PyTorch", "functionality": "Object Detection, Drivable Area Segmentation, Lane Detection", "api_name": "YOLOP", "api_call": "torch.hub.load(repo_or_dir='hustvl/yolop', model='yolop', pretrained=True)", "api_arguments": "pretrained", "python_environment_requirements": "pip install -qr https://github.com/hustvl/YOLOP/blob/main/requirements.txt", "example_code": "import torch\nmodel = torch.hub.load('hustvl/yolop', 'yolop', pretrained=True)\nimg = torch.randn(1,3,640,640)\ndet_out, da_seg_out,ll_seg_out = model(img)", "performance": {"dataset": "BDD100K", "accuracy": {"Object Detection": {"Recall(%)": 89.2, "mAP50(%)": 76.5, "Speed(fps)": 41}, "Drivable Area Segmentation": {"mIOU(%)": 91.5, "Speed(fps)": 41}, "Lane Detection": {"mIOU(%)": 70.5, "IOU(%)": 26.2}}}, "description": "YOLOP is an efficient multi-task network that can jointly handle three crucial tasks in autonomous driving: object detection, drivable area segmentation and lane detection. And it is also the first to reach real-time on embedded devices while maintaining state-of-the-art level performance on the BDD100K dataset."}}
{"code": "{'Instruction': \"I need to calculate the single-image depth map of a captured image. Present an API that can perform this complex task.\", 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('intel-isl/MiDaS', 'DPT_Large', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained MiDaS DPT_Large model from PyTorch Hub to compute relative inverse depth from a single image, giving you a depth map.\", 'code': 'import torch\nmodel = torch.hub.load(\"intel-isl/MiDaS\", \"DPT_Large\", pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='intel-isl/MiDaS', model='DPT_Large', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Computing relative depth from a single image", "api_name": "MiDaS", "api_call": "torch.hub.load(repo_or_dir='intel-isl/MiDaS', model='DPT_Large', pretrained=True)", "api_arguments": {"repo_or_dir": "intel-isl/MiDaS", "model": "model_type"}, "python_environment_requirements": "pip install timm", "example_code": ["import cv2", "import torch", "import urllib.request", "import matplotlib.pyplot as plt", "url, filename = ('https://github.com/pytorch/hub/raw/master/images/dog.jpg', 'dog.jpg')", "urllib.request.urlretrieve(url, filename)", "model_type = 'DPT_Large'", "midas = torch.hub.load('intel-isl/MiDaS', 'DPT_Large')", "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')", "midas.to(device)", "midas.eval()", "midas_transforms = torch.hub.load('intel-isl/MiDaS', 'transforms')", "if model_type == 'DPT_Large' or model_type == 'DPT_Hybrid':", " transform = midas_transforms.dpt_transform", "else:", " transform = midas_transforms.small_transform", "img = cv2.imread(filename)", "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)", "input_batch = transform(img).to(device)", "with torch.no_grad():", " prediction = midas(input_batch)", "prediction = torch.nn.functional.interpolate(", " prediction.unsqueeze(1),", " size=img.shape[:2],", " mode='bicubic',", " align_corners=False,", ").squeeze()", "output = prediction.cpu().numpy()", "plt.imshow(output)", "plt.show()"], "performance": {"dataset": "10 distinct datasets", "accuracy": "Multi-objective optimization"}, "description": "MiDaS computes relative inverse depth from a single image. The repository provides multiple models that cover different use cases ranging from a small, high-speed model to a very large model that provide the highest accuracy. The models have been trained on 10 distinct datasets using multi-objective optimization to ensure high quality on a wide range of inputs."}}
{"code": "{'Instruction': Imagine an advertisement company wants to analyze various video streams for the presence of specific activities or actions. Provide them with an API that can perform this video classification task., 'Output': {'domain': 'Video Classification', 'api_call': \"model = torch.hub.load('facebookresearch/pytorchvideo', 'slow_r50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained 3D ResNet model, capable of classifying video streams into various action categories, from PyTorch Hub.', 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/pytorchvideo', 'slow_r50', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='slow_r50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Video Classification", "framework": "PyTorch", "functionality": "3D ResNet", "api_name": "slow_r50", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='slow_r50', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["torch", "json", "urllib", "pytorchvideo", "torchvision", "torchaudio", "torchtext", "torcharrow", "TorchData", "TorchRec", "TorchServe", "PyTorch on XLA Devices"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/pytorchvideo', 'slow_r50', pretrained=True)", "device = 'cpu'", "model = model.eval()", "model = model.to(device)"], "performance": {"dataset": "Kinetics 400", "accuracy": {"top_1": 74.58, "top_5": 91.63}, "Flops (G)": 54.52, "Params (M)": 32.45}, "description": "The 3D ResNet model is a Resnet-style video classification network pretrained on the Kinetics 400 dataset. It is based on the architecture from the paper 'SlowFast Networks for Video Recognition' by Christoph Feichtenhofer et al."}}
{"code": "{'Instruction': Recommend an API for estimating relative depth from a single image for a self-driving vehicle startup., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('intel-isl/MiDaS', 'MiDaS_small', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MiDaS model from PyTorch Hub, which computes relative depth from a single image. This model can be helpful for self-driving vehicles to estimate depth from their cameras.', 'code': 'import torch\nmodel = torch.hub.load('intel-isl/MiDaS', 'MiDaS_small', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='intel-isl/MiDaS', model='MiDaS_small', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Computing relative depth from a single image", "api_name": "MiDaS", "api_call": "torch.hub.load(repo_or_dir='intel-isl/MiDaS', model='MiDaS_small', pretrained=True)", "api_arguments": {"repo_or_dir": "intel-isl/MiDaS", "model": "model_type"}, "python_environment_requirements": "pip install timm", "example_code": ["import cv2", "import torch", "import urllib.request", "import matplotlib.pyplot as plt", "url, filename = ('https://github.com/pytorch/hub/raw/master/images/dog.jpg', 'dog.jpg')", "urllib.request.urlretrieve(url, filename)", "model_type = 'DPT_Large'", "midas = torch.hub.load('intel-isl/MiDaS', 'MiDaS_small')", "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')", "midas.to(device)", "midas.eval()", "midas_transforms = torch.hub.load('intel-isl/MiDaS', 'transforms')", "if model_type == 'DPT_Large' or model_type == 'DPT_Hybrid':", " transform = midas_transforms.dpt_transform", "else:", " transform = midas_transforms.small_transform", "img = cv2.imread(filename)", "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)", "input_batch = transform(img).to(device)", "with torch.no_grad():", " prediction = midas(input_batch)", "prediction = torch.nn.functional.interpolate(", " prediction.unsqueeze(1),", " size=img.shape[:2],", " mode='bicubic',", " align_corners=False,", ").squeeze()", "output = prediction.cpu().numpy()", "plt.imshow(output)", "plt.show()"], "performance": {"dataset": "10 distinct datasets", "accuracy": "Multi-objective optimization"}, "description": "MiDaS computes relative inverse depth from a single image. The repository provides multiple models that cover different use cases ranging from a small, high-speed model to a very large model that provide the highest accuracy. The models have been trained on 10 distinct datasets using multi-objective optimization to ensure high quality on a wide range of inputs."}}
{"code": "{'Instruction': I work for Tesla, and I need a ready-to-use model for detecting and classifying cars, pedestrians, lanes, and drivable area in an image., 'Output': {'domain': 'Object Detection', 'api_call': \"model = torch.hub.load('datvuthanh/hybridnets', 'hybridnets', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Use the pretrained HybridNets model for object detection from PyTorch Hub, which specializes in detecting and classifying cars, pedestrians, lanes, and drivable areas.', 'code': \"import torch\nmodel = torch.hub.load('datvuthanh/hybridnets', 'hybridnets', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='datvuthanh/hybridnets', model='hybridnets', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Object Detection", "framework": "PyTorch", "functionality": "Traffic Object Detection, Drivable Area Segmentation, Lane Detection", "api_name": "HybridNets", "api_call": "torch.hub.load(repo_or_dir='datvuthanh/hybridnets', model='hybridnets', pretrained=True)", "api_arguments": "pretrained", "python_environment_requirements": "Python>=3.7, PyTorch>=1.10", "example_code": "import torch\nmodel = torch.hub.load('datvuthanh/hybridnets', 'hybridnets', pretrained=True)\nimg = torch.randn(1,3,640,384)\nfeatures, regression, classification, anchors, segmentation = model(img)", "performance": {"dataset": [{"name": "BDD100K", "accuracy": {"Traffic Object Detection": {"Recall (%)": 92.8, "mAP@0.5 (%)": 77.3}, "Drivable Area Segmentation": {"Drivable mIoU (%)": 90.5}, "Lane Line Detection": {"Accuracy (%)": 85.4, "Lane Line IoU (%)": 31.6}}}]}, "description": "HybridNets is an end2end perception network for multi-tasks. Our work focused on traffic object detection, drivable area segmentation and lane detection. HybridNets can run real-time on embedded systems, and obtains SOTA Object Detection, Lane Detection on BDD100K Dataset."}}
{"code": "{'Instruction': A student needs to classify bird species for their science project. Write an API to help identify the bird species given a photo of a bird., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('nicolalandro/ntsnet-cub200', 'ntsnet', pretrained=True, **{'topN': 6, 'device':'cpu', 'num_classes': 200})\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained NTS-Net model for fine-grained bird species classification from PyTorch Hub, which can identify the bird species given an input image.', 'code': \"import torch\nmodel = torch.hub.load('nicolalandro/ntsnet-cub200', 'ntsnet', pretrained=True, **{'topN': 6, 'device':'cpu', 'num_classes': 200})\"}}", "api_call": "torch.hub.load(repo_or_dir='nicolalandro/ntsnet-cub200', model='ntsnet', pretrained=True, **{'topN': 6, 'device':'cpu', 'num_classes': 200})", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Fine-grained image classifier", "api_name": "ntsnet", "api_call": "torch.hub.load(repo_or_dir='nicolalandro/ntsnet-cub200', model='ntsnet', pretrained=True, **{'topN': 6, 'device':'cpu', 'num_classes': 200})", "api_arguments": {"pretrained": "True", "topN": "6", "device": "cpu", "num_classes": "200"}, "python_environment_requirements": ["torch", "torchvision", "PIL"], "example_code": "from torchvision import transforms\nimport torch\nimport urllib\nfrom PIL import Image\n\ntransform_test = transforms.Compose([\n transforms.Resize((600, 600), Image.BILINEAR),\n transforms.CenterCrop((448, 448)),\n transforms.ToTensor(),\n transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n])\n\nmodel = torch.hub.load('nicolalandro/ntsnet-cub200', 'ntsnet', pretrained=True, **{'topN': 6, 'device':'cpu', 'num_classes': 200})\nmodel.eval()\n\nurl = 'https://raw.githubusercontent.com/nicolalandro/ntsnet-cub200/master/images/nts-net.png'\nimg = Image.open(urllib.request.urlopen(url))\nscaled_img = transform_test(img)\ntorch_images = scaled_img.unsqueeze(0)\n\nwith torch.no_grad():\n top_n_coordinates, concat_out, raw_logits, concat_logits, part_logits, top_n_index, top_n_prob = model(torch_images)\n\n_, predict = torch.max(concat_logits, 1)\npred_id = predict.item()\nprint('bird class:', model.bird_classes[pred_id])", "performance": {"dataset": "CUB200 2011", "accuracy": "Not provided"}, "description": "This is an nts-net pretrained with CUB200 2011 dataset, which is a fine-grained dataset of birds species."}}
{"code": "{'Instruction': Help me find an API to convert spoken language in a recorded audio file into text., 'Output': {'domain': 'Speech-to-Text', 'api_call': \"model = torch.hub.load('snakers4/silero-models', 'silero_stt', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Silero Speech-To-Text model from PyTorch Hub, which can convert a given spoken language in a recorded audio file into text.', 'code': 'import torch\nmodel, _, _ = torch.hub.load('snakers4/silero-models', 'silero_stt', language='en', device=torch.device('cpu'))'}}", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_stt', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Speech-To-Text", "api_name": "Silero Models", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_stt', pretrained=True)", "api_arguments": {"repo_or_dir": "snakers4/silero-models", "model": "silero_stt", "language": ["en", "de", "es"], "device": "device"}, "python_environment_requirements": ["pip install -q torchaudio omegaconf soundfile"], "example_code": ["import torch", "import zipfile", "import torchaudio", "from glob import glob", "device = torch.device('cpu')", "model, decoder, utils = torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_stt', language='en', device=device)", "(read_batch, split_into_batches, read_audio, prepare_model_input) = utils", "torch.hub.download_url_to_file('https://opus-codec.org/static/examples/samples/speech_orig.wav', dst ='speech_orig.wav', progress=True)", "test_files = glob('speech_orig.wav')", "batches = split_into_batches(test_files, batch_size=10)", "input = prepare_model_input(read_batch(batches[0]), device=device)", "output = model(input)", "for example in output:", " print(decoder(example.cpu()))"], "performance": {"dataset": "Open-STT", "accuracy": "See the wiki for quality and performance benchmarks"}, "description": "Silero Speech-To-Text models provide enterprise grade STT in a compact form-factor for several commonly spoken languages. The models are robust to a variety of dialects, codecs, domains, noises, and lower sampling rates. They consume a normalized audio in the form of samples and output frames with token probabilities. A decoder utility is provided for simplicity."}}
{"code": "{'Instruction': We want to convert text to audio for one of our applications. What API can provide that functionality?, 'Output': {'domain': 'Text-To-Speech', 'api_call': \"model = torch.hub.load('snakers4/silero-models', 'silero_tts', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Silero Text-To-Speech model from PyTorch Hub to convert text into high-quality audio.', 'code': \"import torch\nmodel, symbols, sample_rate, example_text, apply_tts = torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_tts', language='en', speaker='lj_16khz')\"}}", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_tts', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Text-To-Speech", "framework": "PyTorch", "functionality": "Text-To-Speech", "api_name": "Silero Text-To-Speech Models", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_tts', pretrained=True)", "api_arguments": {"repo_or_dir": "snakers4/silero-models", "model": "silero_tts", "language": "language", "speaker": "speaker"}, "python_environment_requirements": ["pip install -q torchaudio omegaconf"], "example_code": "import torch\nlanguage = 'en'\nspeaker = 'lj_16khz'\ndevice = torch.device('cpu')\nmodel, symbols, sample_rate, example_text, apply_tts = torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_tts', language=language, speaker=speaker)\nmodel = model.to(device)\naudio = apply_tts(texts=[example_text], model=model, sample_rate=sample_rate, symbols=symbols, device=device)", "performance": {"dataset": [{"language": "Russian", "speakers": 6}, {"language": "English", "speakers": 1}, {"language": "German", "speakers": 1}, {"language": "Spanish", "speakers": 1}, {"language": "French", "speakers": 1}], "accuracy": "High throughput on slow hardware. Decent performance on one CPU thread"}, "description": "Silero Text-To-Speech models provide enterprise grade TTS in a compact form-factor for several commonly spoken languages. They offer one-line usage, naturally sounding speech, no GPU or training required, minimalism and lack of dependencies, a library of voices in many languages, support for 16kHz and 8kHz out of the box."}}
{"code": "{'Instruction': Devise an API used to separate objects from the background in an image., 'Output': {'domain': 'Semantic Segmentation', 'api_call': \"model = torch.hub.load('pytorch/vision', 'deeplabv3_resnet50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained DeepLabV3 model with a ResNet-50 backbone from PyTorch Hub to perform semantic segmentation, allowing you to separate objects from the background in an image.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'deeplabv3_resnet50', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='deeplabv3_resnet50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "DeepLabV3", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='deeplabv3_resnet50', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": "latest", "torchvision": "latest", "PIL": "latest", "matplotlib": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_resnet50', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "input_image = input_image.convert('RGB')", "preprocess = transforms.Compose([", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)['out'][0]", "output_predictions = output.argmax(0)"], "performance": {"dataset": "COCO val2017", "accuracy": [{"model": "deeplabv3_resnet50", "Mean IOU": 66.4, "Global Pixelwise Accuracy": 92.4}]}, "description": "DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset."}}
{"code": "{'Instruction': Please suggest a powerful API to carry out semantic segmentation in a given image., 'Output': {'domain': 'Semantic Segmentation', 'api_call': \"model = torch.hub.load('pytorch/vision', 'deeplabv3_mobilenet_v3_large', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the DeepLabV3 semantic segmentation model with a MobileNet-V3 large backbone from PyTorch Hub, pretrained on a subset of COCO train2017 dataset.', 'code': \"import torch\nmodel = torch.hub.load('pytorch/vision', 'deeplabv3_mobilenet_v3_large', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='deeplabv3_mobilenet_v3_large', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "DeepLabV3", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='deeplabv3_mobilenet_v3_large', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": "latest", "torchvision": "latest", "PIL": "latest", "matplotlib": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_mobilenet_v3_large', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "input_image = input_image.convert('RGB')", "preprocess = transforms.Compose([", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)['out'][0]", "output_predictions = output.argmax(0)"], "performance": {"dataset": "COCO val2017", "accuracy": [{"model": "deeplabv3_mobilenet_v3_large", "Mean IOU": 60.3, "Global Pixelwise Accuracy": 91.4}]}, "description": "DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset."}}
{"code": "{'Instruction': We are developing a voice assistant that needs to detect when a human is speaking. Suggest an API to detect human speech in an audio file., 'Output': {'domain': 'Voice Activity Detection', 'api_call': \"model = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the Silero VAD from PyTorch Hub, a pretrained Voice Activity Detector optimized for performance on 1 CPU thread that provides an enterprise-grade alternative to WebRTC.', 'code': 'import torch\nmodel, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=True)'}}", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Voice Activity Detection", "api_name": "Silero Voice Activity Detector", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=True)", "api_arguments": {"repo_or_dir": "snakers4/silero-vad", "model": "silero_vad", "force_reload": "True"}, "python_environment_requirements": {"torchaudio": "pip install -q torchaudio"}, "example_code": {"import": ["import torch", "torch.set_num_threads(1)", "from IPython.display import Audio", "from pprint import pprint"], "download_example": "torch.hub.download_url_to_file('https://models.silero.ai/vad_models/en.wav', 'en_example.wav')", "load_model": "model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=True)", "load_utils": "(get_speech_timestamps, _, read_audio, _) = utils", "set_sampling_rate": "sampling_rate = 16000", "read_audio": "wav = read_audio('en_example.wav', sampling_rate=sampling_rate)", "get_speech_timestamps": "speech_timestamps = get_speech_timestamps(wav, model, sampling_rate=sampling_rate)", "print_speech_timestamps": "pprint(speech_timestamps)"}, "performance": {"dataset": "", "accuracy": ""}, "description": "Silero VAD is a pre-trained enterprise-grade Voice Activity Detector (VAD) that aims to provide a high-quality and modern alternative to the WebRTC Voice Activity Detector. The model is optimized for performance on 1 CPU thread and is quantized."}}
{"code": "{'Instruction': Design an API for segmenting the objects in an image with the highest accuracy., 'Output': {'domain': 'Semantic Segmentation', 'api_call': \"model = torch.hub.load('pytorch/vision', 'deeplabv3_resnet101', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the DeepLabV3+ model with a ResNet-101 backbone from PyTorch Hub, which is pretrained on COCO val2017 for semantic segmentation, to segment objects in an image with high accuracy.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'deeplabv3_resnet101', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='deeplabv3_resnet101', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "DeepLabV3", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='deeplabv3_resnet101', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": "latest", "torchvision": "latest", "PIL": "latest", "matplotlib": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_resnet101', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "input_image = input_image.convert('RGB')", "preprocess = transforms.Compose([", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)['out'][0]", "output_predictions = output.argmax(0)"], "performance": {"dataset": "COCO val2017", "accuracy": [{"model": "deeplabv3_resnet101", "Mean IOU": 67.4, "Global Pixelwise Accuracy": 92.4}]}, "description": "DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset."}}
{"code": "{'Instruction': We need an API for a security system that can detect objects in real-time. Recommend a solution., 'Output': {'domain': 'Object Detection', 'api_call': \"model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained YOLOv5 model from PyTorch Hub for real-time object detection in your security system.', 'code': 'import torch\nmodel = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='ultralytics/yolov5', model='yolov5s', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Object Detection", "framework": "PyTorch", "functionality": "YOLOv5", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='ultralytics/yolov5', model='yolov5s', pretrained=True)", "api_arguments": ["'ultralytics/yolov5'", "'yolov5s'", "pretrained=True"], "python_environment_requirements": "Python>=3.8, PyTorch>=1.7", "example_code": ["import torch", "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)", "imgs = ['https://ultralytics.com/images/zidane.jpg']", "results = model(imgs)", "results.print()", "results.save()", "results.xyxy[0]", "results.pandas().xyxy[0]"], "performance": {"dataset": "COCO", "accuracy": {"YOLOv5s6": {"mAPval0.5:0.95": 43.3, "mAPtest0.5:0.95": 43.3, "mAPval0.5": 61.9}, "YOLOv5m6": {"mAPval0.5:0.95": 50.5, "mAPtest0.5:0.95": 50.5, "mAPval0.5": 68.7}, "YOLOv5l6": {"mAPval0.5:0.95": 53.4, "mAPtest0.5:0.95": 53.4, "mAPval0.5": 71.1}, "YOLOv5x6": {"mAPval0.5:0.95": 54.4, "mAPtest0.5:0.95": 54.4, "mAPval0.5": 72.0}, "YOLOv5x6 TTA": {"mAPval0.5:0.95": 55.0, "mAPtest0.5:0.95": 55.0, "mAPval0.5": 72.0}}}, "description": "YOLOv5 is a family of compound-scaled object detection models trained on the COCO dataset, and includes simple functionality for Test Time Augmentation (TTA), model ensembling, hyperparameter evolution, and export to ONNX, CoreML and TFLite."}}
{"code": "{'Instruction': NASA is looking to classify celestial images. They need a machine learning API that can identify objects in images. Recommend one., 'Output': {'domain': 'Image Classification', 'api_call': 'model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x8d_wsl', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNext WSL model for image classification from PyTorch Hub to classify celestial objects in images.', 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x8d_wsl', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x8d_wsl', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNext WSL", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x8d_wsl', pretrained=True)", "api_arguments": [{"name": "resnext101_32x8d_wsl", "type": "str", "description": "ResNeXt-101 32x8d WSL model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x8d_wsl')", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(output[0])", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeXt-101 32x8d": {"Top-1 Acc.": "82.2", "Top-5 Acc.": "96.4"}}}, "description": "The provided ResNeXt models are pre-trained in weakly-supervised fashion on 940 million public images with 1.5K hashtags matching with 1000 ImageNet1K synsets, followed by fine-tuning on ImageNet1K dataset. The models significantly improve the training accuracy on ImageNet compared to training from scratch. They achieve state-of-the-art accuracy of 85.4% on ImageNet with the ResNext-101 32x48d model."}}
{"code": "{'Instruction': I want to determine what objects are in an image file. Find me a model API that can classify the objects in the image., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x32d_wsl', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pre-trained ResNext WSL model from PyTorch Hub that has been trained on ImageNet to recognize various objects in an image.', 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x32d_wsl', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x32d_wsl', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNext WSL", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x32d_wsl', pretrained=True)", "api_arguments": [{"name": "resnext101_32x32d_wsl", "type": "str", "description": "ResNeXt-101 32x32d WSL model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x32d_wsl')", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(output[0])", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeXt-101 32x32d": {"Top-1 Acc.": "85.1", "Top-5 Acc.": "97.5"}}}, "description": "The provided ResNeXt models are pre-trained in weakly-supervised fashion on 940 million public images with 1.5K hashtags matching with 1000 ImageNet1K synsets, followed by fine-tuning on ImageNet1K dataset. The models significantly improve the training accuracy on ImageNet compared to training from scratch. They achieve state-of-the-art accuracy of 85.4% on ImageNet with the ResNext-101 32x48d model."}}
{"code": "{'Instruction': Find me an API for classifying different objects in images with high accuracy., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained ResNeXt-101 32x16d WSL model from PyTorch Hub for classifying various objects in images with high accuracy.\", 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x16d_wsl', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNext WSL", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x16d_wsl', pretrained=True)", "api_arguments": [{"name": "resnext101_32x16d_wsl", "type": "str", "description": "ResNeXt-101 32x16d WSL model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl')", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(output[0])", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeXt-101 32x16d": {"Top-1 Acc.": "84.2", "Top-5 Acc.": "97.2"}}}, "description": "The provided ResNeXt models are pre-trained in weakly-supervised fashion on 940 million public images with 1.5K hashtags matching with 1000 ImageNet1K synsets, followed by fine-tuning on ImageNet1K dataset. The models significantly improve the training accuracy on ImageNet compared to training from scratch. They achieve state-of-the-art accuracy of 85.4% on ImageNet with the ResNext-101 32x48d model."}}
{"code": "{'Instruction': I need an API that can create random images of clothes, like shirts, pants or dresses, any ideas?, 'Output': {'domain': 'Image Generation', 'api_call': \"model = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'DCGAN', pretrained=True, useGPU=use_gpu)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained DCGAN model from PyTorch Hub, capable of generating random images of clothing items like shirts, pants, and dresses.\", 'code': 'import torch\nuse_gpu = True if torch.cuda.is_available() else False\nmodel = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'DCGAN', pretrained=True, useGPU=use_gpu)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorch_GAN_zoo:hub', model='DCGAN', pretrained=True, useGPU=use_gpu)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Generative Adversarial Networks", "api_name": "DCGAN", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorch_GAN_zoo:hub', model='DCGAN', pretrained=True, useGPU=use_gpu)", "api_arguments": {"pretrained": "True", "useGPU": "use_gpu"}, "python_environment_requirements": "Python 3", "example_code": {"import": ["import torch", "import matplotlib.pyplot as plt", "import torchvision"], "use_gpu": "use_gpu = True if torch.cuda.is_available() else False", "load_model": "model = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'DCGAN', pretrained=True, useGPU=use_gpu)", "build_noise_data": "noise, _ = model.buildNoiseData(num_images)", "generate_images": "with torch.no_grad(): generated_images = model.test(noise)", "plot_images": ["plt.imshow(torchvision.utils.make_grid(generated_images).permute(1, 2, 0).cpu().numpy())", "plt.show()"]}, "performance": {"dataset": "FashionGen", "accuracy": "N/A"}, "description": "DCGAN is a model designed in 2015 by Radford et. al. in the paper Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks. It is a GAN architecture both very simple and efficient for low resolution image generation (up to 64x64)."}}
{"code": "{'Instruction': I am a researcher working on a computer vision project and need a cutting-edge pre-trained image classification API. What do you suggest?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x48d_wsl', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Use the ResNext WSL model from PyTorch Hub, which is pretrained and fine-tuned on large-scale datasets, achieving state-of-the-art accuracy on ImageNet.\", 'code': \"import torch\nmodel = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x48d_wsl', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x48d_wsl', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNext WSL", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x48d_wsl', pretrained=True)", "api_arguments": [{"name": "resnext101_32x48d_wsl", "type": "str", "description": "ResNeXt-101 32x48d WSL model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x48d_wsl')", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(output[0])", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeXt-101 32x48d": {"Top-1 Acc.": "85.4", "Top-5 Acc.": "97.6"}}}, "description": "The provided ResNeXt models are pre-trained in weakly-supervised fashion on 940 million public images with 1.5K hashtags matching with 1000 ImageNet1K synsets, followed by fine-tuning on ImageNet1K dataset. The models significantly improve the training accuracy on ImageNet compared to training from scratch. They achieve state-of-the-art accuracy of 85.4% on ImageNet with the ResNext-101 32x48d model."}}
{"code": "{'Instruction': A professional radiologist is looking to process an MRI dataset to isolate abnormalities. Help me find an API that can segment the abnormal regions within brain MRI images., 'Output': {'domain': 'Semantic Segmentation', 'api_call': \"model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Locate and load the pretrained U-Net model for brain MRI segmentation from PyTorch Hub, designed to identify and segment abnormal regions in brain MRI images.', 'code': 'import torch\nmodel = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='mateuszbuda/brain-segmentation-pytorch', model='unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "Biomedical Image Segmentation", "api_name": "U-Net for brain MRI", "api_call": "torch.hub.load(repo_or_dir='mateuszbuda/brain-segmentation-pytorch', model='unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)", "api_arguments": {"in_channels": 3, "out_channels": 1, "init_features": 32, "pretrained": true}, "python_environment_requirements": ["torch", "torchvision", "numpy", "PIL"], "example_code": ["import torch", "model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)", "import numpy as np", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "m, s = np.mean(input_image, axis=(0, 1)), np.std(input_image, axis=(0, 1))", "preprocess = transforms.Compose([transforms.ToTensor(),transforms.Normalize(mean=m, std=s)])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model = model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(torch.round(output[0]))"], "performance": {"dataset": "kaggle.com/mateuszbuda/lgg-mri-segmentation"}, "description": "U-Net with batch normalization for biomedical image segmentation with pretrained weights for abnormality segmentation in brain MRI. The model comprises four levels of blocks containing two convolutional layers with batch normalization and ReLU activation function, and one max pooling layer in the encoding part and up-convolutional layers instead in the decoding part. The number of convolutional filters in each block is 32, 64, 128, and 256. The bottleneck layer has 512 convolutional filters. From the encoding layers, skip connections are used to the corresponding layers in the decoding part. Input image is a 3-channel brain MRI slice from pre-contrast, FLAIR, and post-contrast sequences, respectively. Output is a one-channel probability map of abnormality regions with the same size as the input image."}}
{"code": "{'Instruction': Recommend me an API to classify images with top-notch accuracy., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resnet50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the ResNet50 v1.5 model from PyTorch Hub, which is a modified version of the original ResNet50 with improved accuracy for image classification.', 'code': 'import torch\nmodel = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resnet50', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_resnet50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNet50", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_resnet50', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["pip install validators matplotlib"], "example_code": ["import torch", "from PIL import Image", "import torchvision.transforms as transforms", "import numpy as np", "import json", "import requests", "import matplotlib.pyplot as plt", "import warnings", "warnings.filterwarnings('ignore')", "%matplotlib inline", "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')", "print(f'Using {device} for inference')", "resnet50 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resnet50', pretrained=True)", "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_convnets_processing_utils')", "resnet50.eval().to(device)", "uris = [...]", "batch = torch.cat([utils.prepare_input_from_uri(uri) for uri in uris]).to(device)", "with torch.no_grad():", " output = torch.nn.functional.softmax(resnet50(batch), dim=1)", "results = utils.pick_n_best(predictions=output, n=5)", "for uri, result in zip(uris, results):", " img = Image.open(requests.get(uri, stream=True).raw)", " img.thumbnail((256,256), Image.ANTIALIAS)", " plt.imshow(img)", " plt.show()", " print(result)"], "performance": {"dataset": "ImageNet", "accuracy": "~0.5% top1 improvement over ResNet50 v1"}, "description": "The ResNet50 v1.5 model is a modified version of the original ResNet50 v1 model. The difference between v1 and v1.5 is that, in the bottleneck blocks which requires downsampling, v1 has stride = 2 in the first 1x1 convolution, whereas v1.5 has stride = 2 in the 3x3 convolution. This difference makes ResNet50 v1.5 slightly more accurate (~0.5% top1) than v1, but comes with a small performance drawback (~5% imgs/sec). The model is initialized as described in Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification. This model is trained with mixed precision using Tensor Cores on Volta, Turing, and the NVIDIA Ampere GPU architectures."}}
{"code": "{'Instruction': Help me classify images of cats and dogs, by recommending an API to do that., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resneXt', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNeXt101-32x4d model from PyTorch Hub provided by NVIDIA. It can be fine-tuned to classify images of cats and dogs.', 'code': \"import torch\nmodel = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resneXt', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_resneXt', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNeXt101-32x4d", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_resneXt', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "validators", "matplotlib"], "example_code": ["import torch", "from PIL import Image", "import torchvision.transforms as transforms", "import numpy as np", "import json", "import requests", "import matplotlib.pyplot as plt", "import warnings", "warnings.filterwarnings('ignore')", "%matplotlib inline", "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")", "print(f'Using {device} for inference')", "resneXt = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resneXt')", "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_convnets_processing_utils')", "resneXt.eval().to(device)", "uris = [", " 'http://images.cocodataset.org/test-stuff2017/000000024309.jpg',", " 'http://images.cocodataset.org/test-stuff2017/000000028117.jpg',", " 'http://images.cocodataset.org/test-stuff2017/000000006149.jpg',", " 'http://images.cocodataset.org/test-stuff2017/000000004954.jpg',", "]", "batch = torch.cat(", " [utils.prepare_input_from_uri(uri) for uri in uris]", ").to(device)", "with torch.no_grad():", " output = torch.nn.functional.softmax(resneXt(batch), dim=1)", "results = utils.pick_n_best(predictions=output, n=5)", "for uri, result in zip(uris, results):", " img = Image.open(requests.get(uri, stream=True).raw)", " img.thumbnail((256,256), Image.ANTIALIAS)", " plt.imshow(img)", " plt.show()", " print(result)"], "performance": {"dataset": "IMAGENET", "accuracy": "Not provided"}, "description": "ResNeXt101-32x4d is a model introduced in the Aggregated Residual Transformations for Deep Neural Networks paper. It is based on regular ResNet model, substituting 3x3 convolutions inside the bottleneck block for 3x3 grouped convolutions. This model is trained with mixed precision using Tensor Cores on Volta, Turing, and the NVIDIA Ampere GPU architectures. It can be deployed for inference on the NVIDIA Triton Inference Server using TorchScript, ONNX Runtime or TensorRT as an execution backend."}}
{"code": "{'Instruction': I need an API that can be used for image classification tasks with a model trained on ImageNet dataset., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'alexnet', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained AlexNet model, trained on the ImageNet dataset, from PyTorch Hub for image classification tasks.\", 'code': \"import torch\nmodel = torch.hub.load('pytorch/vision', 'alexnet', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='alexnet', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "AlexNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='alexnet', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": ">=1.9.0", "torchvision": ">=0.10.0"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"top-1_error": 43.45, "top-5_error": 20.91}}, "description": "AlexNet is a deep convolutional neural network that achieved a top-5 error of 15.3% in the 2012 ImageNet Large Scale Visual Recognition Challenge. The main contribution of the original paper was the depth of the model, which was computationally expensive but made feasible through the use of GPUs during training. The pretrained AlexNet model in PyTorch can be used for image classification tasks."}}
{"code": "{'Instruction': An app developer from San Francisco wants to classify images based on their content. Tell me an API that can classify images into different categories., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_se_resnext101_32x4d', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained SE-ResNeXt101 model from PyTorch Hub to classify images into different categories based on their content.', 'code': \"import torch\nmodel = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_se_resnext101_32x4d', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_se_resnext101_32x4d', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SE-ResNeXt101", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_se_resnext101_32x4d', pretrained=True)", "api_arguments": "N/A", "python_environment_requirements": "validators, matplotlib", "example_code": "import torch\nfrom PIL import Image\nimport torchvision.transforms as transforms\nimport numpy as np\nimport json\nimport requests\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nprint(f'Using {device} for inference')\nresneXt = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_se_resnext101_32x4d')\nutils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_convnets_processing_utils')\nresneXt.eval().to(device)\nuris = ['http://images.cocodataset.org/test-stuff2017/000000024309.jpg','http://images.cocodataset.org/test-stuff2017/000000028117.jpg','http://images.cocodataset.org/test-stuff2017/000000006149.jpg','http://images.cocodataset.org/test-stuff2017/000000004954.jpg']\nbatch = torch.cat([utils.prepare_input_from_uri(uri) for uri in uris]).to(device)\nwith torch.no_grad():\n output = torch.nn.functional.softmax(resneXt(batch), dim=1)\nresults = utils.pick_n_best(predictions=output, n=5)\nfor uri, result in zip(uris, results):\n img = Image.open(requests.get(uri, stream=True).raw)\n img.thumbnail((256,256), Image.ANTIALIAS)\n plt.imshow(img)\n plt.show()\n print(result)", "performance": {"dataset": "IMAGENET", "accuracy": "N/A"}, "description": "The SE-ResNeXt101-32x4d is a ResNeXt101-32x4d model with added Squeeze-and-Excitation module. This model is trained with mixed precision using Tensor Cores on Volta, Turing, and the NVIDIA Ampere GPU architectures, which allows researchers to get results 3x faster than training without Tensor Cores while experiencing the benefits of mixed precision training."}}
{"code": "{'Instruction': Identify the objects in an image using a dense convolutional network. Recommend an API for this purpose., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'densenet121', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained DenseNet-121 model from PyTorch Hub for object classification in images.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'densenet121', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet121', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Dense Convolutional Network", "api_name": "Densenet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet121', pretrained=True)", "api_arguments": [{"name": "densenet121", "type": "str", "description": "Densenet-121 model"}], "python_environment_requirements": {"torch": "latest", "torchvision": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet121', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"densenet121": {"Top-1 error": 25.35, "Top-5 error": 7.83}}}, "description": "Dense Convolutional Network (DenseNet) connects each layer to every other layer in a feed-forward fashion. It alleviates the vanishing-gradient problem, strengthens feature propagation, encourages feature reuse, and substantially reduces the number of parameters."}}
{"code": "{'Instruction': I'm the founder of Dobble, an AI company. We are building a virtual assistant and looking for an API to convert text to speech. Can you provide one?, 'Output': {'domain': 'Text-to-Speech', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_waveglow', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained WaveGlow model from PyTorch Hub for text-to-speech synthesis, which can be combined with the Tacotron 2 model to produce natural-sounding speech from text.', 'code': 'import torch\nmodel = torch.hub.load(\"NVIDIA/DeepLearningExamples:torchhub\", \"nvidia_waveglow\", pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_waveglow', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Text-to-Speech", "framework": "PyTorch", "functionality": "Speech Synthesis", "api_name": "WaveGlow", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_waveglow', pretrained=True)", "api_arguments": {"repo_or_dir": "NVIDIA/DeepLearningExamples:torchhub", "model": "nvidia_waveglow", "model_math": "fp32"}, "python_environment_requirements": ["numpy", "scipy", "librosa", "unidecode", "inflect", "libsndfile1"], "example_code": {"load_waveglow_model": "waveglow = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_waveglow', model_math='fp32')", "prepare_waveglow_model": ["waveglow = waveglow.remove_weightnorm(waveglow)", "waveglow = waveglow.to('cuda')", "waveglow.eval()"], "load_tacotron2_model": "tacotron2 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tacotron2', model_math='fp32')", "prepare_tacotron2_model": ["tacotron2 = tacotron2.to('cuda')", "tacotron2.eval()"], "synthesize_speech": ["text = \"hello world, I missed you so much\"", "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tts_utils')", "sequences, lengths = utils.prepare_input_sequence([text])", "with torch.no_grad():", " mel, _, _ = tacotron2.infer(sequences, lengths)", " audio = waveglow.infer(mel)", "audio_numpy = audio[0].data.cpu().numpy()", "rate = 22050"], "save_audio": "write(\"audio.wav\", rate, audio_numpy)", "play_audio": "Audio(audio_numpy, rate=rate)"}, "performance": {"dataset": "LJ Speech", "accuracy": null}, "description": "The Tacotron 2 and WaveGlow model form a text-to-speech system that enables users to synthesize natural-sounding speech from raw transcripts without any additional prosody information. The Tacotron 2 model produces mel spectrograms from input text using encoder-decoder architecture. WaveGlow is a flow-based model that consumes the mel spectrograms to generate speech."}}
{"code": "{'Instruction': Show me an API that can handle textual classification like sentiment analysis, emotion recognition or sarcasm detection., 'Output': {'domain': 'Text Classification', 'api_call': \"model = torch.hub.load('pytorch/fairseq', 'roberta.large', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained RoBERTa model from PyTorch Hub, which can be fine-tuned for various textual classification tasks, such as sentiment analysis, emotion recognition, and sarcasm detection.\", 'code': 'import torch\nmodel = torch.hub.load('pytorch/fairseq', 'roberta.large', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/fairseq', model='roberta.large', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "RoBERTa", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='pytorch/fairseq', model='roberta.large', pretrained=True)", "api_arguments": ["'pytorch/fairseq'", "'roberta.large'"], "python_environment_requirements": ["regex", "requests", "hydra-core", "omegaconf"], "example_code": ["import torch", "roberta = torch.hub.load('pytorch/fairseq', 'roberta.large')", "roberta.eval()", "tokens = roberta.encode('Hello world!')", "last_layer_features = roberta.extract_features(tokens)", "all_layers = roberta.extract_features(tokens, return_all_hiddens=True)", "roberta = torch.hub.load('pytorch/fairseq', 'roberta.large.mnli')", "roberta.eval()", "tokens = roberta.encode('Roberta is a heavily optimized version of BERT.', 'Roberta is not very optimized.')", "prediction = roberta.predict('mnli', tokens).argmax().item()", "tokens = roberta.encode('Roberta is a heavily optimized version of BERT.', 'Roberta is based on BERT.')", "prediction = roberta.predict('mnli', tokens).argmax().item()", "roberta.register_classification_head('new_task', num_classes=3)", "logprobs = roberta.predict('new_task', tokens)"], "performance": {"dataset": "MNLI", "accuracy": "N/A"}, "description": "RoBERTa is a robustly optimized version of BERT, a revolutionary self-supervised pretraining technique that learns to predict intentionally hidden (masked) sections of text. RoBERTa builds on BERT's language masking strategy and modifies key hyperparameters, including removing BERT's next-sentence pretraining objective, and training with much larger mini-batches and learning rates. RoBERTa was also trained on an order of magnitude more data than BERT, for a longer amount of time, allowing it to generalize even better to downstream tasks."}}
{"code": "{'Instruction': Recommend a Python API to perform semantic segmentation on an image., 'Output': {'domain': 'Semantic Segmentation', 'api_call': 'model = torch.hub.load(\"pytorch/vision\", \"fcn_resnet101\", pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the FCN-ResNet101 pretrained model from PyTorch Hub to perform semantic segmentation on images.', 'code': 'import torch\nmodel = torch.hub.load(\"pytorch/vision\", \"fcn_resnet101\", pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='fcn_resnet101', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "Fully-Convolutional Network", "api_name": "fcn_resnet101", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='fcn_resnet101', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "boolean", "description": "If True, returns a model pre-trained on COCO train2017"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "matplotlib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'fcn_resnet101', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "input_image = input_image.convert(\"RGB\")", "preprocess = transforms.Compose([", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)['out'][0]", "output_predictions = output.argmax(0)"], "performance": {"dataset": "COCO val2017", "accuracy": {"fcn_resnet101": {"Mean IOU": 63.7, "Global Pixelwise Accuracy": 91.9}}}, "description": "FCN-ResNet is a Fully-Convolutional Network model using a ResNet-50 or a ResNet-101 backbone. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset."}}
{"code": "{'Instruction': I am working on a computer vision app and I need to classify objects in the images. Which API can help me classify objects with high accuracy?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'densenet201', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained DenseNet-201 model from PyTorch Hub, which is a dense convolutional network with high accuracy in object classification.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'densenet201', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet201', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Dense Convolutional Network", "api_name": "Densenet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet201', pretrained=True)", "api_arguments": [{"name": "densenet201", "type": "str", "description": "Densenet-201 model"}], "python_environment_requirements": {"torch": "latest", "torchvision": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet201', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"densenet201": {"Top-1 error": 22.8, "Top-5 error": 6.43}}}, "description": "Dense Convolutional Network (DenseNet) connects each layer to every other layer in a feed-forward fashion. It alleviates the vanishing-gradient problem, strengthens feature propagation, encourages feature reuse, and substantially reduces the number of parameters."}}
{"code": "{'Instruction': We need an API that can be used to create an efficient image classifier for differentiating among various dog breeds in a mobile app., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet39ds', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained HarDNet-39DS model from PyTorch Hub, which is fast and efficient, suitable for a mobile app to classify various dog breeds.', 'code': 'import torch\nmodel = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet39ds', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet39ds', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "HarDNet", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet39ds', pretrained=True)", "api_arguments": [{"name": "hardnet39ds", "type": "str", "description": "HarDNet-39DS model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet39ds', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"hardnet39ds": {"Top-1 error": 27.92, "Top-5 error": 9.57}}}, "description": "Harmonic DenseNet (HarDNet) is a low memory traffic CNN model, which is fast and efficient. The basic concept is to minimize both computational cost and memory access cost at the same time, such that the HarDNet models are 35% faster than ResNet running on GPU comparing to models with the same accuracy (except the two DS models that were designed for comparing with MobileNet)."}}
{"code": "{'Instruction': Recommend an image classification API that requires low memory and runs faster on GPUs to classify user-uploaded photos., 'Output': {'domain': 'Image Classification', 'api_call': 'model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet68', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained HarDNet-68 model from PyTorch Hub for its low memory usage and faster speed on GPUs, making it suitable for classifying user-uploaded images.', 'code': 'import torch\nmodel = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet68', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet68', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "HarDNet", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet68', pretrained=True)", "api_arguments": [{"name": "hardnet68", "type": "str", "description": "HarDNet-68 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet68', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"hardnet68": {"Top-1 error": 23.52, "Top-5 error": 6.99}}}, "description": "Harmonic DenseNet (HarDNet) is a low memory trafficCNN model, which is fast and efficient. The basic concept is to minimize both computational cost and memory access cost at the same time, such that the HarDNet models are 35% faster than ResNet running on GPU comparing to models with the same accuracy (except the two DS models that were designed for comparing with MobileNet)."}}
{"code": "{'Instruction': Identify an API for a clothing e-commerce website that classifies an image into different categories such as tops, shoes, dresses, etc., 'Output': {'domain': 'Image Classification', 'api_call': 'model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet68ds', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained HarDNet for image classification from PyTorch Hub, which can be fine-tuned for classifying clothing items such as tops, shoes, dresses, and more.', 'code': 'import torch\nmodel = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet68ds', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet68ds', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "HarDNet", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet68ds', pretrained=True)", "api_arguments": [{"name": "hardnet68ds", "type": "str", "description": "HarDNet-68DS model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet68ds', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"hardnet68ds": {"Top-1 error": 25.71, "Top-5 error": 8.13}}}, "description": "Harmonic DenseNet (HarDNet) is a low memory traffic CNN model, which is fast and efficient. The basic concept is to minimize both computational cost and memory access cost at the same time, such that the HarDNet models are 35% faster than ResNet running on GPU comparing to models with the same accuracy (except the two DS models that were designed for comparing with MobileNet)."}}
{"code": "{'Instruction': I am working on a project that needs a lightweight image classification solution. Suggest me an API to use., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('huawei-noah/ghostnet', 'ghostnet_1x', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'GhostNet is a lightweight and efficient image classification solution, available in PyTorch Hub. It generates more features from cheap operations, providing a balance between speed and accuracy.', 'code': \"import torch\nmodel = torch.hub.load('huawei-noah/ghostnet', 'ghostnet_1x', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/ghostnet', model='ghostnet_1x', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Efficient networks by generating more features from cheap operations", "api_name": "GhostNet", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/ghostnet', model='ghostnet_1x', pretrained=True)", "api_arguments": ["pretrained"], "python_environment_requirements": ["torch", "torchvision", "PIL"], "example_code": ["import torch", "model = torch.hub.load('huawei-noah/ghostnet', 'ghostnet_1x', pretrained=True)", "model.eval()", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)", "print(probabilities)"], "performance": {"dataset": "ImageNet", "accuracy": {"Top-1 acc": "73.98", "Top-5 acc": "91.46"}}, "description": "The GhostNet architecture is based on an Ghost module structure which generates more features from cheap operations. Based on a set of intrinsic feature maps, a series of cheap operations are applied to generate many ghost feature maps that could fully reveal information underlying intrinsic features. Experiments conducted on benchmarks demonstrate the superiority of GhostNet in terms of speed and accuracy tradeoff."}}
{"code": "{'Instruction': I am a computer science researcher interested in selecting an efficient image classification model. Can you recommend me an API that would be suitable for that purpose?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet85', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Use the HarDNet-85 model from PyTorch Hub for efficient image classification as it minimizes both computational cost and memory access cost.', 'code': 'import torch\nmodel = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet85', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet85', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "HarDNet", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet85', pretrained=True)", "api_arguments": [{"name": "hardnet85", "type": "str", "description": "HarDNet-85 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet85', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"hardnet85": {"Top-1 error": 21.96, "Top-5 error": 6.11}}}, "description": "Harmonic DenseNet (HarDNet) is a low memory traffic CNN model, which is fast and efficient. The basic concept is to minimize both computational cost and memory access cost at the same time, such that the HarDNet models are 35% faster than ResNet running on GPU comparing to models with the same accuracy (except the two DS models that were designed for comparing with MobileNet)."}}
{"code": "{'Instruction': A company is developing a machine-learning based software to categorize their products based on the photos they take. Provide an API that can help them classify their images and name the image types., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('XingangPan/IBN-Net', 'resnet50_ibn_a', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNet-50-IBN-a model for image classification from PyTorch Hub, which can be fine-tuned for categorizing product images.', 'code': 'import torch\nmodel = torch.hub.load('XingangPan/IBN-Net', 'resnet50_ibn_a', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnet50_ibn_a', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "IBN-Net", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnet50_ibn_a', pretrained=True)", "api_arguments": [{"name": "resnet50_ibn_a", "type": "str", "description": "ResNet-50-IBN-a model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('XingangPan/IBN-Net', 'resnet50_ibn_a', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"resnet50_ibn_a": {"Top-1 acc": 77.46, "Top-5 acc": 93.68}}}, "description": "IBN-Net is a CNN model with domain/appearance invariance. Motivated by style transfer works, IBN-Net carefully unifies instance normalization and batch normalization in a single deep network. It provides a simple way to increase both modeling and generalization capacities without adding model complexity. IBN-Net is especially suitable for cross domain or person/vehicle re-identification tasks."}}
{"code": "{'Instruction': A machine learning engineer is working on generalization for classification tasks in different domains. Suggest an API that is suitable for this task., 'Output': {'domain': 'Classification', 'api_call': 'model = torch.hub.load('XingangPan/IBN-Net', 'resnet101_ibn_a', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained IBN-Net (resnet101_ibn_a) model from PyTorch Hub, which is designed for domain and appearance invariance, making it suitable for cross-domain classification tasks.', 'code': 'import torch\nmodel = torch.hub.load('XingangPan/IBN-Net', 'resnet101_ibn_a', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnet101_ibn_a', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "IBN-Net", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnet101_ibn_a', pretrained=True)", "api_arguments": [{"name": "resnet101_ibn_a", "type": "str", "description": "ResNet-101-IBN-a model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('XingangPan/IBN-Net', 'resnet101_ibn_a', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"resnet101_ibn_a": {"Top-1 acc": 78.61, "Top-5 acc": 94.41}}}, "description": "IBN-Net is a CNN model with domain/appearance invariance. Motivated by style transfer works, IBN-Net carefully unifies instance normalization and batch normalization in a single deep network. It provides a simple way to increase both modeling and generalization capacities without adding model complexity. IBN-Net is especially suitable for cross domain or person/vehicle re-identification tasks."}}
{"code": "{'Instruction': We want our AI powered traffic camera to classify different types of vehicles with high accuracy. Provide an API suggestion for this., 'Output': {'domain': 'Classification', 'api_call': 'model = torch.hub.load('XingangPan/IBN-Net', 'resnext101_ibn_a', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNeXt-101-IBN-a model for object classification from PyTorch Hub, which can classify different types of vehicles with high accuracy.', 'code': 'import torch\nmodel = torch.hub.load('XingangPan/IBN-Net', 'resnext101_ibn_a', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnext101_ibn_a', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "IBN-Net", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnext101_ibn_a', pretrained=True)", "api_arguments": [{"name": "resnext101_ibn_a", "type": "str", "description": "ResNeXt-101-IBN-a model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('XingangPan/IBN-Net', 'resnext101_ibn_a', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"resnext101_ibn_a": {"Top-1 acc": 79.12, "Top-5 acc": 94.58}}}, "description": "IBN-Net is a CNN model with domain/appearance invariance. Motivated by style transfer works, IBN-Net carefully unifies instance normalization and batch normalization in a single deep network. It provides a simple way to increase both modeling and generalization capacities without adding model complexity. IBN-Net is especially suitable for cross domain or person/vehicle re-identification tasks."}}
{"code": "{'Instruction': I'm working on a project that needs a deep learning model to identify similar objects across different domains. Suggest me an API that can serve the purpose., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('XingangPan/IBN-Net', 'se_resnet101_ibn_a', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained SE-ResNet-101-IBN-a model from PyTorch Hub, an IBN-Net model that unifies instance normalization and batch normalization, suitable for cross-domain object identification.\", 'code': 'import torch\nmodel = torch.hub.load('XingangPan/IBN-Net', 'se_resnet101_ibn_a', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='se_resnet101_ibn_a', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "IBN-Net", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='se_resnet101_ibn_a', pretrained=True)", "api_arguments": [{"name": "se_resnet101_ibn_a", "type": "str", "description": "SE-ResNet-101-IBN-a model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('XingangPan/IBN-Net', 'se_resnet101_ibn_a', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"se_resnet101_ibn_a": {"Top-1 acc": 78.75, "Top-5 acc": 94.49}}}, "description": "IBN-Net is a CNN model with domain/appearance invariance. Motivated by style transfer works, IBN-Net carefully unifies instance normalization and batch normalization in a single deep network. It provides a simple way to increase both modeling and generalization capacities without adding model complexity. IBN-Net is especially suitable for cross domain or person/vehicle re-identification tasks."}}
{"code": "{'Instruction': A new ecommerce store wants to automatically categorize images of products into different classes. Recommend an API that can perform image classification., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest101', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNeSt model for image classification from PyTorch Hub, which can automatically categorize images of products into different classes.', 'code': 'import torch\nmodel = torch.hub.load('zhanghang1989/ResNeSt', 'resnest101', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest101', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNeSt", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest101', pretrained=True)", "api_arguments": "resnest101", "python_environment_requirements": {"torch": "1.0.0", "torchvision": "0.2.2"}, "example_code": ["import torch", "model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest101', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeSt-101": 82.83}}, "description": "ResNeSt models are from the ResNeSt: Split-Attention Networks paper. They are a new ResNet variant that enables attention across feature-map groups. By stacking Split-Attention blocks ResNet-style, ResNeSt models outperform other networks with similar model complexities, and also help downstream tasks including object detection, instance segmentation, and semantic segmentation."}}
{"code": "{'Instruction': An e-commerce company is building an AI to classify pictures of products into categories automatically. Recommend an API to help in classifying images., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNeSt-50 model from PyTorch Hub for high-performance image classification, which can be fine-tuned for the specific task of classifying e-commerce product images.', 'code': 'import torch\nmodel = torch.hub.load('zhanghang1989/ResNeSt', 'resnest50', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNeSt", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest50', pretrained=True)", "api_arguments": "resnest50", "python_environment_requirements": {"torch": "1.0.0", "torchvision": "0.2.2"}, "example_code": ["import torch", "model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest50', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeSt-50": 81.03}}, "description": "ResNeSt models are from the ResNeSt: Split-Attention Networks paper. They are a new ResNet variant that enables attention across feature-map groups. By stacking Split-Attention blocks ResNet-style, ResNeSt models outperform other networks with similar model complexities, and also help downstream tasks including object detection, instance segmentation, and semantic segmentation."}}
{"code": "{'Instruction': A developer is trying to build an image classifier. Provide an API capable of classifying images and telling what objects are present., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'inception_v3', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Inception_v3 model from PyTorch Hub for image classification, which can predict objects present in a given image.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'inception_v3', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='inception_v3', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "Inception_v3", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='inception_v3', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": "1.9.0", "torchvision": "0.10.0"}, "example_code": {"import_libraries": "import torch", "load_model": "model = torch.hub.load('pytorch/vision:v0.10.0', 'inception_v3', pretrained=True)", "model_evaluation": "model.eval()"}, "performance": {"dataset": "imagenet", "accuracy": {"top-1_error": 22.55, "top-5_error": 6.44}}, "description": "Inception v3, also called GoogleNetv3, is a famous Convolutional Neural Network trained on the ImageNet dataset from 2015. It is based on the exploration of ways to scale up networks to utilize the added computation as efficiently as possible by using suitably factorized convolutions and aggressive regularization. The model achieves a top-1 error of 22.55% and a top-5 error of 6.44% on the ImageNet dataset."}}
{"code": "{'Instruction': Is there an API to implement ResNeSt for image classification purposes?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest200', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNeSt-200 model from PyTorch Hub for image classification.', 'code': 'import torch\nmodel = torch.hub.load('zhanghang1989/ResNeSt', 'resnest200', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest200', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNeSt", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest200', pretrained=True)", "api_arguments": "resnest200", "python_environment_requirements": {"torch": "1.0.0", "torchvision": "0.2.2"}, "example_code": ["import torch", "model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest200', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeSt-200": 83.84}}, "description": "ResNeSt models are from the ResNeSt: Split-Attention Networks paper. They are a new ResNet variant that enables attention across feature-map groups. By stacking Split-Attention blocks ResNet-style, ResNeSt models outperform other networks with similar model complexities, and also help downstream tasks including object detection, instance segmentation, and semantic segmentation."}}
{"code": "{'Instruction': In order to create classifiers for popular recipes from the internet, I would like to locate an API that can efficiently classify a given food image., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest269', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNeSt269 model from PyTorch Hub, which can be fine-tuned for food image classification given an image of a recipe.', 'code': \"import torch\nmodel = torch.hub.load('zhanghang1989/ResNeSt', 'resnest269', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest269', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNeSt", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest269', pretrained=True)", "api_arguments": "resnest269", "python_environment_requirements": {"torch": "1.0.0", "torchvision": "0.2.2"}, "example_code": ["import torch", "model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest269', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeSt-269": 84.54}}, "description": "ResNeSt models are from the ResNeSt: Split-Attention Networks paper. They are a new ResNet variant that enables attention across feature-map groups. By stacking Split-Attention blocks ResNet-style, ResNeSt models outperform other networks with similar model complexities, and also help downstream tasks including object detection, instance segmentation, and semantic segmentation."}}
{"code": "{'Instruction': Recommend me an API to classify images in a mobile app for users that sorts their images in predefined categories., 'Output': {'domain': 'Image Classification', 'api_call': 'model = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_mobile', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ProxylessNAS model optimized for mobile devices from PyTorch Hub, which can be fine-tuned for image classification in a mobile app.', 'code': 'import torch\nmodel = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_mobile', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_cpu', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "ProxylessNAS", "api_name": "mit-han-lab/ProxylessNAS", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_cpu', pretrained=True)", "api_arguments": [{"name": "proxylessnas_cpu", "type": "str", "description": "ProxylessNAS optimized for CPU"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "target_platform = 'proxyless_cpu'", "model = torch.hub.load('mit-han-lab/ProxylessNAS', target_platform, pretrained=True)", "model.eval()"], "performance": {"dataset": [{"model_structure": "proxylessnas_cpu", "accuracy": 75.3}]}, "description": "ProxylessNAS models are from the ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware paper. They specialize CNN architectures for different hardware platforms, offering free yet significant performance boost on all three platforms (CPU, GPU, and Mobile) with similar accuracy."}}
{"code": "{'Instruction': I need to classify the content of an image using a pretrained model optimized for GPU. Show me an API that can do that., 'Output': {'domain': 'Classification', 'api_call': 'model = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_gpu', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ProxylessNAS model from PyTorch Hub, which is optimized for GPU and can be used to classify images.', 'code': 'import torch\nmodel = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_gpu', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_gpu', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "ProxylessNAS", "api_name": "mit-han-lab/ProxylessNAS", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_gpu', pretrained=True)", "api_arguments": [{"name": "proxylessnas_gpu", "type": "str", "description": "ProxylessNAS optimized for GPU"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "target_platform = 'proxyless_gpu'", "model = torch.hub.load('mit-han-lab/ProxylessNAS', target_platform, pretrained=True)", "model.eval()"], "performance": {"dataset": [{"model_structure": "proxylessnas_gpu", "accuracy": 75.1}]}, "description": "ProxylessNAS models are from the ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware paper. They specialize CNN architectures for different hardware platforms, offering free yet significant performance boost on all three platforms (CPU, GPU, and Mobile) with similar accuracy."}}
{"code": "{'Instruction': I need an API for classifying objects in mobile application. What is the best API for classifying objects with mobile devices?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_mobile', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the ProxylessNAS model from PyTorch Hub, which specializes in object classification for mobile devices and offers significant performance optimization.', 'code': 'import torch\nmodel = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_mobile', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_mobile', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "ProxylessNAS", "api_name": "mit-han-lab/ProxylessNAS", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_mobile', pretrained=True)", "api_arguments": [{"name": "proxylessnas_mobile", "type": "str", "description": "ProxylessNAS optimized for Mobile"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "target_platform = 'proxyless_mobile'", "model = torch.hub.load('mit-han-lab/ProxylessNAS', target_platform, pretrained=True)", "model.eval()"], "performance": {"dataset": [{"model_structure": "proxylessnas_mobile", "accuracy": 74.6}]}, "description": "ProxylessNAS models are from the ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware paper. They specialize CNN architectures for different hardware platforms, offering free yet significant performance boost on all three platforms (CPU, GPU, and Mobile) with similar accuracy."}}
{"code": "{'Instruction': A photographer needs an AI-based solution to help classify a variety of images. Recommend an appropriate API that can classify an image into one of 1000 categories., 'Output': {'domain': 'Image Classification', 'api_call': 'model = torch.hub.load('pytorch/vision', 'mobilenet_v2', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MobileNet v2 model from PyTorch Hub for image classification, which can classify images into one of 1000 categories.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'mobilenet_v2', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='mobilenet_v2', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MobileNet v2", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='mobilenet_v2', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision', 'mobilenet_v2', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)"], "performance": {"dataset": "ImageNet", "accuracy": {"top-1_error": 28.12, "top-5_error": 9.71}}, "description": "The MobileNet v2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input. MobileNet v2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, non-linearities in the narrow layers were removed in order to maintain representational power."}}
{"code": "{'Instruction': I have a large image dataset in need of image classification using deep residual networks. Can you recommend an API that can achieve this for me?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'resnet34', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNet model from PyTorch Hub for deep residual image classification.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'resnet34', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet34', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Deep Residual Networks", "api_name": "ResNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet34', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "bool", "default": "False", "description": "If True, returns a model pre-trained on ImageNet"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet34', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "resnet34", "top-1_error": 26.7, "top-5_error": 8.58}}, "description": "ResNet models are deep residual networks pre-trained on ImageNet. They were proposed in the paper 'Deep Residual Learning for Image Recognition'. Available model variants include ResNet18, ResNet34, ResNet50, ResNet101, and ResNet152."}}
{"code": "{'Instruction': I am developing an app that can recognize diverse objects in pictures. Identify an API that can help me achieve this., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'resnet101', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNet101 model from PyTorch Hub, which can classify images into diverse object categories.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'resnet101', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet101', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Deep Residual Networks", "api_name": "ResNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet101', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "bool", "default": "False", "description": "If True, returns a model pre-trained on ImageNet"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet101', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "resnet101", "top-1_error": 22.63, "top-5_error": 6.44}}, "description": "ResNet models are deep residual networks pre-trained on ImageNet. They were proposed in the paper 'Deep Residual Learning for Image Recognition'. Available model variants include ResNet18, ResNet34, ResNet50, ResNet101, and ResNet152."}}
{"code": "{'Instruction': I am having trouble with my tomato plants and want some help figuring out what's wrong with them. I need an AI that can spot diseases in plants given an image., 'Output': {'domain': 'Agriculture', 'api_call': 'model = torch.hub.load(repo_or_dir='pytorch/vision', model='resnet50', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load a pretrained ResNet50 model from PyTorch Hub that can be fine-tuned for plant disease classification given an image of a plant.', 'code': 'import torch\nmodel = torch.hub.load(repo_or_dir='pytorch/vision', model='resnet50', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Deep Residual Networks", "api_name": "ResNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet50', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "bool", "default": "False", "description": "If True, returns a model pre-trained on ImageNet"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "resnet50", "top-1_error": 23.85, "top-5_error": 7.13}}, "description": "ResNet models are deep residual networks pre-trained on ImageNet. They were proposed in the paper 'Deep Residual Learning for Image Recognition'. Available model variants include ResNet18, ResNet34, ResNet50, ResNet101, and ResNet152."}}
{"code": "{'Instruction': Which API should I use to build a tomato classification app that detects diseases in tomato plant leaves?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'resnet152', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Use PyTorch's ResNet152 pre-trained model, which can be fine-tuned for the tomato leaf disease classification application.\", 'code': \"import torch\nmodel = torch.hub.load('pytorch/vision', 'resnet152', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet152', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Deep Residual Networks", "api_name": "ResNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet152', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "bool", "default": "False", "description": "If True, returns a model pre-trained on ImageNet"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet152', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "resnet152", "top-1_error": 21.69, "top-5_error": 5.94}}, "description": "ResNet models are deep residual networks pre-trained on ImageNet. They were proposed in the paper 'Deep Residual Learning for Image Recognition'. Available model variants include ResNet18, ResNet34, ResNet50, ResNet101, and ResNet152."}}
{"code": "{'Instruction': I want to categorize a given image into a certain class like animals or objects. Identify an API that can do that., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'resnext50_32x4d', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNext50_32x4d model from PyTorch Hub to categorize images into various classes such as animals or objects.', 'code': \"import torch\nmodel = torch.hub.load('pytorch/vision', 'resnext50_32x4d', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnext50_32x4d', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNext", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnext50_32x4d', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "PIL"], "example_code": ["import torch", "from PIL import Image", "from torchvision import transforms", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnext50_32x4d', pretrained=True)", "model.eval()", "input_image = Image.open('dog.jpg')", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)"], "performance": {"dataset": "ImageNet", "accuracy": {"resnext50_32x4d": {"top-1": 22.38, "top-5": 6.3}}}, "description": "ResNext is a next-generation ResNet architecture for image classification. It is more efficient and accurate than the original ResNet. This implementation includes two versions of the model, resnext50_32x4d and resnext101_32x8d, with 50 and 101 layers respectively."}}
{"code": "{'Instruction': I want to sort images of animals into appropriate categories. What API should I use?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'resnext101_32x4d', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNext image classification model from PyTorch Hub, which can be fine-tuned to classify images of animals into appropriate categories.', 'code': \"import torch\nmodel = torch.hub.load('pytorch/vision', 'resnext101_32x4d', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnext101_32x4d', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNext", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnext101_32x4d', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "PIL"], "example_code": ["import torch", "from PIL import Image", "from torchvision import transforms", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnext101_32x4d', pretrained=True)", "model.eval()", "input_image = Image.open('dog.jpg')", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)"], "performance": {"dataset": "ImageNet", "resnext101_32x8d": {"top-1": 20.69, "top-5": 5.47}}, "description": "ResNext is a next-generation ResNet architecture for image classification. It is more efficient and accurate than the original ResNet. This implementation includes two versions of the model, resnext50_32x4d and resnext101_32x8d, with 50 and 101 layers respectively."}}
{"code": "{'Instruction': Provide an API for classifying the objects present in an image., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_b', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained SNNMLP model for image classification from PyTorch Hub, capable of classifying objects in images based on the ImageNet dataset.\", 'code': 'import torch\nmodel = torch.hub.load(\"huawei-noah/Efficient-AI-Backbones\", \"snnmlp_b\", pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_b', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SNNMLP", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_b', pretrained=True)", "api_arguments": [{"name": "snnmlp_b", "type": "str", "description": "SNNMLP Base model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_b', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "SNNMLP Base", "top-1": 85.59}}, "description": "SNNMLP incorporates the mechanism of LIF neurons into the MLP models, to achieve better accuracy without extra FLOPs. We propose a full-precision LIF operation to communicate between patches, including horizontal LIF and vertical LIF in different directions. We also propose to use group LIF to extract better local features. With LIF modules, our SNNMLP model achieves 81.9%, 83.3% and 83.6% top-1 accuracy on ImageNet dataset with only 4.4G, 8.5G and 15.2G FLOPs, respectively."}}
{"code": "{'Instruction': I want an API that can classify images and tell me what the images are., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_s', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the SNNMLP model from PyTorch Hub with pretrained weights for image classification. It can identify the content in the images.', 'code': 'import torch\nmodel = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_s', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_s', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SNNMLP", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_s', pretrained=True)", "api_arguments": [{"name": "snnmlp_s", "type": "str", "description": "SNNMLP Small model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_s', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "SNNMLP Small", "top-1": 83.3}}, "description": "SNNMLP incorporates the mechanism of LIF neurons into the MLP models, to achieve better accuracy without extra FLOPs. We propose a full-precision LIF operation to communicate between patches, including horizontal LIF and vertical LIF in different directions. We also propose to use group LIF to extract better local features. With LIF modules, our SNNMLP model achieves 81.9%, 83.3% and 83.6% top-1 accuracy on ImageNet dataset with only 4.4G, 8.5G and 15.2G FLOPs, respectively."}}
{"code": "{'Instruction': Find me a pre-trained API to identify objects in real-time, taking into account a database of more than 100 classes from ImageNet., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_t', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained SNNMLP Tiny model from PyTorch Hub trained on ImageNet, which contains more than 100 classes for object classification.', 'code': 'import torch\nmodel = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_t', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_t', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SNNMLP", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_t', pretrained=True)", "api_arguments": [{"name": "snnmlp_t", "type": "str", "description": "SNNMLP Tiny model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_t', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "SNNMLP Tiny", "top-1": 81.88}}, "description": "SNNMLP incorporates the mechanism of LIF neurons into the MLP models, to achieve better accuracy without extra FLOPs. We propose a full-precision LIF operation to communicate between patches, including horizontal LIF and vertical LIF in different directions. We also propose to use group LIF to extract better local features. With LIF modules, our SNNMLP model achieves 81.9%, 83.3% and 83.6% top-1 accuracy on ImageNet dataset with only 4.4G, 8.5G and 15.2G FLOPs, respectively."}}
{"code": "{'Instruction': As a biologist, I need to identify the species in a photo. Which API should I use to classify images into different categories?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'squeezenet1_0', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained SqueezeNet v1.0 model from PyTorch Hub. This model can be fine-tuned for species classification.', 'code': \"import torch\nmodel = torch.hub.load('pytorch/vision', 'squeezenet1_0', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='squeezenet1_0', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SqueezeNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='squeezenet1_0', pretrained=True)", "api_arguments": {"version": "v0.10.0", "model": ["squeezenet1_0"], "pretrained": "True"}, "python_environment_requirements": {"torch": ">=1.9.0", "torchvision": ">=0.10.0"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'squeezenet1_0', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)", "print(probabilities)"], "performance": {"dataset": "ImageNet", "accuracy": {"squeezenet1_0": {"Top-1 error": 41.9, "Top-5 error": 19.58}}}, "description": "SqueezeNet is an image classification model that achieves AlexNet-level accuracy with 50x fewer parameters. It has two versions: squeezenet1_0 and squeezenet1_1, with squeezenet1_1 having 2.4x less computation and slightly fewer parameters than squeezenet1_0, without sacrificing accuracy."}}
{"code": "{'Instruction': I need a lightweight API to classify images based on a training set of images from my new mobile app., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'shufflenet_v2_x1_0', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pre-trained ShuffleNet v2 model from PyTorch Hub. It is an efficient image classification model that is lightweight and suitable for usage in mobile apps.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'shufflenet_v2_x1_0', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='shufflenet_v2_x1_0', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ShuffleNet v2", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='shufflenet_v2_x1_0', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": "torch", "torchvision": "torchvision", "PIL": "Image", "urllib": "urllib"}, "example_code": {"import_libraries": ["import torch", "from PIL import Image", "from torchvision import transforms", "import urllib"], "load_model": ["model = torch.hub.load('pytorch/vision:v0.10.0', 'shufflenet_v2_x1_0', pretrained=True)", "model.eval()"], "load_image": ["url, filename = ('https://github.com/pytorch/hub/raw/master/images/dog.jpg', 'dog.jpg')", "try: urllib.URLopener().retrieve(url, filename)", "except: urllib.request.urlretrieve(url, filename)", "input_image = Image.open(filename)"], "preprocess_image": ["preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)"], "run_inference": ["if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)"], "get_probabilities": ["probabilities = torch.nn.functional.softmax(output[0], dim=0)"], "top_categories": ["top5_prob, top5_catid = torch.topk(probabilities, 5)", "for i in range(top5_prob.size(0)):", " print(categories[top5_catid[i]], top5_prob[i].item())"]}, "performance": {"dataset": "Imagenet", "accuracy": {"top-1_error": 30.64, "top-5_error": 11.68}}, "description": "ShuffleNet V2 is an efficient ConvNet optimized for speed and memory, pre-trained on Imagenet. It is designed based on practical guidelines for efficient network design, including speed and accuracy tradeoff."}}
{"code": "{'Instruction': What is an efficient deep learning model for categorizing images into object classes with fewer parameters?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'squeezenet1_1', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained SqueezeNet 1.1 model for image classification from PyTorch Hub. It is efficient in terms of the number of parameters and model size, achieving AlexNet-level accuracy with 50x fewer parameters.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'squeezenet1_1', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='squeezenet1_1', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SqueezeNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='squeezenet1_1', pretrained=True)", "api_arguments": {"version": "v0.10.0", "model": ["squeezenet1_1"], "pretrained": "True"}, "python_environment_requirements": {"torch": ">=1.9.0", "torchvision": ">=0.10.0"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'squeezenet1_1', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)", "print(probabilities)"], "performance": {"dataset": "ImageNet", "accuracy": {"squeezenet1_1": {"Top-1 error": 41.81, "Top-5 error": 19.38}}}, "description": "SqueezeNet is an image classification model that achieves AlexNet-level accuracy with 50x fewer parameters. It has two versions: squeezenet1_0 and squeezenet1_1, with squeezenet1_1 having 2.4x less computation and slightly fewer parameters than squeezenet1_0, without sacrificing accuracy."}}
{"code": "{'Instruction': Recommend a well-tested API to identify objects in a digital photograph for my photo gallery website., 'Output': {'domain': 'Classification', 'api_call': 'model = torch.hub.load('pytorch/vision', 'vgg11', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained VGG-11 model for image recognition from PyTorch Hub to identify objects in a digital photograph.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg11', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg11', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg11', pretrained=True)", "api_arguments": [{"name": "vgg11", "type": "str", "description": "VGG11 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg11', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg11": {"Top-1 error": 30.98, "Top-5 error": 11.37}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': I want an API that allows me to recognize and classify different breeds of dogs from images., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg11_bn', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained VGG11 model with batch normalization from PyTorch Hub to classify different breeds of dogs from images. Fine-tune the model on a dataset of dog breed images.\", 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg11_bn', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg11_bn', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg11_bn', pretrained=True)", "api_arguments": [{"name": "vgg11_bn", "type": "str", "description": "VGG11 model with batch normalization"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg11_bn', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg11_bn": {"Top-1 error": 26.7, "Top-5 error": 8.58}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': I am a software engineer and I am trying to create an image recognition system for my latest project. Find an API that can tell me the category of the objects in a provided image., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg13_bn', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the VGG13 model with batch normalization from PyTorch Hub, which can be used for image recognition tasks to identify objects in a given image.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg13_bn', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg13_bn', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg13_bn', pretrained=True)", "api_arguments": [{"name": "vgg13_bn", "type": "str", "description": "VGG13 model with batch normalization"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg13_bn', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg13_bn": {"Top-1 error": 28.45, "Top-5 error": 9.63}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': Propose an API to identify what is in a photo of a bird in order to label it with the correct species., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg16', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained VGG16 model from PyTorch Hub to recognize bird species in an image.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg16', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg16', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg16', pretrained=True)", "api_arguments": [{"name": "vgg16", "type": "str", "description": "VGG16 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg16": {"Top-1 error": 28.41, "Top-5 error": 9.62}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': Tell me the best API to be used on a security camera to classify vehicles and details about them given an image, 'Output': {'domain': 'Classification', 'api_call': 'model = torch.hub.load('pytorch/vision', 'vgg19', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained VGG19 model from PyTorch Hub to classify vehicles and their details from a given image captured by a security camera.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg19', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg19', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg19', pretrained=True)", "api_arguments": [{"name": "vgg19", "type": "str", "description": "VGG19 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg19', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg19": {"Top-1 error": 27.62, "Top-5 error": 9.12}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': An e-commerce website looking for APIs to create image classifiers for their products. Offer a solution based on the VGG model., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg16_bn', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained VGG16 model with batch normalization from PyTorch Hub, which can be fine-tuned for product image classification tasks.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg16_bn', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg16_bn', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg16_bn', pretrained=True)", "api_arguments": [{"name": "vgg16_bn", "type": "str", "description": "VGG16 model with batch normalization"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16_bn', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg16_bn": {"Top-1 error": 26.63, "Top-5 error": 8.5}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': Recommending the best machine learning models or APIs that is good for classifying a dataset of images with over 1000 categories., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg19_bn', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained VGG19 model with batch normalization from PyTorch Hub, suitable for classifying large datasets with over 1000 categories.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg19_bn', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg19_bn', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg19_bn', pretrained=True)", "api_arguments": [{"name": "vgg19_bn", "type": "str", "description": "VGG19 model with batch normalization"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg19_bn', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg19_bn": {"Top-1 error": 25.76, "Top-5 error": 8.15}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': An e-commerce platform wants to categorize their items by image. Tell me an API that can do this., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'wide_resnet50_2', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Wide ResNet-50-2 model for image classification from PyTorch Hub, which can be used to categorize items based on their images.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'wide_resnet50_2', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='wide_resnet50_2', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Wide Residual Networks", "api_name": "wide_resnet50_2", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='wide_resnet50_2', pretrained=True)", "api_arguments": "pretrained", "python_environment_requirements": "torch, torchvision", "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'wide_resnet50_2', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"wide_resnet50_2": {"Top-1 error": 21.49, "Top-5 error": 5.91}}}, "description": "Wide Residual networks simply have increased number of channels compared to ResNet. Otherwise the architecture is the same. Deeper ImageNet models with bottleneck block have increased number of channels in the inner 3x3 convolution. The wide_resnet50_2 and wide_resnet101_2 models were trained in FP16 with mixed precision training using SGD with warm restarts. Checkpoints have weights in half precision (except batch norm) for smaller size, and can be used in FP32 models too."}}
{"code": "{'Instruction': Suggest a neural network that can effectively identify different types of birds in high-resolution images., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'wide_resnet101_2', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Wide ResNet 101 model from PyTorch Hub, which can be fine-tuned to identify different types of birds in high-resolution images.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'wide_resnet101_2', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='wide_resnet101_2', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Wide Residual Networks", "api_name": "wide_resnet101_2", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='wide_resnet101_2', pretrained=True)", "api_arguments": "pretrained", "python_environment_requirements": "torch, torchvision", "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'wide_resnet101_2', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"wide_resnet101_2": {"Top-1 error": 21.16, "Top-5 error": 5.72}}}, "description": "Wide Residual networks simply have increased number of channels compared to ResNet. Otherwise the architecture is the same. Deeper ImageNet models with bottleneck block have increased number of channels in the inner 3x3 convolution. The wide_resnet50_2 and wide_resnet101_2 models were trained in FP16 with mixed precision training using SGD with warm restarts. Checkpoints have weights in half precision (except batch norm) for smaller size, and can be used in FP32 models too."}}
{"code": "{'Instruction': I want to build a video filter app that can separate video clips by different activities. Can you suggest an API that can classify videos into different categories based on the activities happening in the video?, 'Output': {'domain': 'Video Classification', 'api_call': 'model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained SlowFast model for video classification from PyTorch Hub, which can classify video clips into different activity categories.', 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='slowfast_r50', pretrained=True)", "provider": "PyTorchVideo", "api_data": {"domain": "Video Classification", "framework": "PyTorchVideo", "functionality": "SlowFast Networks", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='slowfast_r50', pretrained=True)", "api_arguments": {"repository": "facebookresearch/pytorchvideo", "model": "slowfast_r50", "pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "pytorchvideo"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)", "device = 'cpu'", "model = model.eval()", "model = model.to(device)"], "performance": {"dataset": "Kinetics 400", "accuracy": {"top1": 76.94, "top5": 92.69}, "flops": 65.71, "params": 34.57}, "description": "Slow Fast model architectures are based on the paper 'SlowFast Networks for Video Recognition' by Christoph Feichtenhofer et al. They are pretrained on the Kinetics 400 dataset using the 8x8 setting. This model is capable of classifying video clips into different action categories. It is provided by the FAIR PyTorchVideo library."}}
{"code": "{'Instruction': What's the best API for automatically detecting sports or activities types in a video clip for an application?, 'Output': {'domain': 'Video Classification', 'api_call': \"model = torch.hub.load('facebookresearch/pytorchvideo', 'x3d_s', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained X3D model for video classification from PyTorch Hub, which can classify video clips into different sports or activities types based on the Kinetics 400 dataset.\", 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/pytorchvideo', 'x3d_s', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='x3d_s', pretrained=True)", "provider": "PyTorchVideo", "api_data": {"domain": "Video Classification", "framework": "PyTorchVideo", "functionality": "X3D Networks", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='x3d_s', pretrained=True)", "api_arguments": {"repository": "facebookresearch/pytorchvideo", "model": "x3d_s", "pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "pytorchvideo"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/pytorchvideo', 'x3d_s', pretrained=True)", "device = 'cpu'", "model = model.eval()", "model = model.to(device)"], "performance": {"dataset": "Kinetics 400", "accuracy": {"top1": 73.33, "top5": 91.27}, "flops": 2.96, "params": 3.79}, "description": "X3D model architectures are based on the paper 'X3D: Expanding Architectures for Efficient Video Recognition' by Christoph Feichtenhofer. They are pretrained on the Kinetics 400 dataset. This model is capable of classifying video clips into different action categories. It is provided by the FAIR PyTorchVideo library."}}
{"code": "{'Instruction': A mobile app company wants to build an object classification solution for their Android app. Give me an API that can classify objects in images with low latency and high accuracy., 'Output': {'domain': 'Image Classification', 'api_call': 'model = torch.hub.load('mit-han-lab/once-for-all', 'ofa_supernet_mbv3_w10', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Once-For-All (OFA) network from PyTorch Hub, a family of efficient neural networks designed for object classification in images, with low latency and high accuracy.', 'code': 'import torch\nmodel = torch.hub.load('mit-han-lab/once-for-all', 'ofa_supernet_mbv3_w10', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/once-for-all', model='ofa_supernet_mbv3_w10', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Once-for-all (OFA) Networks", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/once-for-all', model='ofa_supernet_mbv3_w10', pretrained=True)", "api_arguments": {"repository": "mit-han-lab/once-for-all", "model": "ofa_supernet_mbv3_w10", "pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "super_net_name = 'ofa_supernet_mbv3_w10'", "super_net = torch.hub.load('mit-han-lab/once-for-all', super_net_name, pretrained=True).eval()"], "performance": {"description": "OFA networks outperform state-of-the-art NAS methods (up to 4.0% ImageNet top1 accuracy improvement over MobileNetV3, or same accuracy but 1.5x faster than MobileNetV3, 2.6x faster than EfficientNet w.r.t measured latency) while reducing many orders of magnitude GPU hours and CO2 emission."}, "description": "Once-for-all (OFA) networks are a family of neural networks designed by MIT Han Lab. They decouple training and search, achieving efficient inference across various edge devices and resource constraints. OFA networks are pretrained on the IMAGENET dataset and are capable of classifying images into different categories."}}
{"code": "{'Instruction': I would like to build a tool that can produce human-like text given an input prompt. Recommend an API that can help me construct realistic sentences., 'Output': {'domain': 'Natural Language Processing', 'api_call': 'model = torch.hub.load(\\'huggingface/pytorch-transformers\\', \\'model\\', \\'gpt2\\')', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained GPT-2 model from Hugging Face\\'s PyTorch-Transformers library to generate human-like text given an input prompt.', 'code': 'import torch\nmodel = torch.hub.load(\\'huggingface/pytorch-transformers\\', \\'model\\', \\'gpt2\\')'}}", "api_call": "torch.hub.load(repo_or_dir='huggingface/pytorch-transformers')", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Natural Language Processing", "api_name": "PyTorch-Transformers", "api_call": "torch.hub.load(repo_or_dir='huggingface/pytorch-transformers')", "api_arguments": ["pretrained_model_or_path", "output_attention", "output_hidden_states", "config", "from_tf"], "python_environment_requirements": ["tqdm", "boto3", "requests", "regex", "sentencepiece", "sacremoses"], "example_code": "import torch\ntokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-cased')\n\ntext_1 = \"Jim Henson was a puppeteer\"\ntext_2 = \"Who was Jim Henson ?\"\n\nindexed_tokens = tokenizer.encode(text_1, text_2, add_special_tokens=True)\n\nmodel = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-cased')\n\nwith torch.no_grad():\n encoded_layers, _ = model(tokens_tensor, token_type_ids=segments_tensors)", "performance": {"dataset": [{"name": "MRPC", "accuracy": "Not provided"}]}, "description": "PyTorch-Transformers is a library of state-of-the-art pre-trained models for Natural Language Processing (NLP) including BERT, GPT, GPT-2, Transformer-XL, XLNet, XLM, RoBERTa, and DistilBERT. The library provides functionality for tokenization, configuration, and various model architectures for different tasks such as causal language modeling, sequence classification, question answering, and masked language modeling."}}
{"code": "{'Instruction': A wildlife photographer needs to classify his collection of animal images. Which API can be used to assist the photographer in image classification?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models', 'resnet18_swsl', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained semi-supervised ResNet-18 model from PyTorch Hub for classifying images into different categories.', 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models', 'resnet18_swsl', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/semi-supervised-ImageNet1K-models', model='resnet18_swsl', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Semi-supervised and semi-weakly supervised ImageNet Models", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/semi-supervised-ImageNet1K-models', model='resnet18_swsl', pretrained=True)", "api_arguments": {"repository": "facebookresearch/semi-supervised-ImageNet1K-models", "model": "resnet18_swsl", "pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models', 'resnet18_swsl', pretrained=True)", "model.eval()"], "performance": {"description": "Semi-supervised and semi-weakly supervised ImageNet models achieve state-of-the-art accuracy of 81.2% on ImageNet for the widely used/adopted ResNet-50 model architecture."}, "description": "Semi-supervised and semi-weakly supervised ImageNet Models are introduced in the 'Billion scale semi-supervised learning for image classification' paper. These models are pretrained on a subset of unlabeled YFCC100M public image dataset and fine-tuned with the ImageNet1K training dataset. They are capable of classifying images into different categories and are provided by the Facebook Research library."}}
{"code": "{'Instruction': I work at a small company that creates a deep learning-based smartphone app. I need a model to classify images super fast and with high performance. Guide me to the right API., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_gpunet', pretrained=True, model_type=model_type, model_math=precision)\", 'api_provider': 'PyTorch', 'explanation': 'Use the GPUNet model from NVIDIA that is optimized for deep learning image classification tasks and runs efficiently on GPU.', 'code': \"import torch\nmodel_type = 'GPUNet-0'\nprecision = 'fp32'\ngpunet = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_gpunet', pretrained=True, model_type=model_type, model_math=precision)\"}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_gpunet', pretrained=True, model_type=model_type, model_math=precision)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "GPUNet Networks", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_gpunet', pretrained=True, model_type=model_type, model_math=precision)", "api_arguments": {"repository": "NVIDIA/DeepLearningExamples:torchhub", "model": "nvidia_gpunet", "pretrained": "True", "model_type": "GPUNet-0", "model_math": "fp32"}, "python_environment_requirements": ["torch", "validators", "matplotlib", "timm==0.5.4"], "example_code": ["import torch", "model_type = 'GPUNet-0'", "precision = 'fp32'", "gpunet = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_gpunet', pretrained=True, model_type=model_type, model_math=precision)", "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')", "gpunet.to(device)", "gpunet.eval()"], "performance": {"dataset": "IMAGENET", "description": "GPUNet demonstrates state-of-the-art inference performance up to 2x faster than EfficientNet-X and FBNet-V3."}, "description": "GPUNet is a family of Convolutional Neural Networks designed by NVIDIA using novel Neural Architecture Search (NAS) methods. They are optimized for NVIDIA GPU and TensorRT performance. GPUNet models are pretrained on the IMAGENET dataset and are capable of classifying images into different categories. The models are provided by the NVIDIA Deep Learning Examples library."}}
{"code": "{'Instruction': Recommend an API for translating English text to French text., 'Output': {'domain': 'Semantic Segmentation', 'api_call': \"model = torch.hub.load('pytorch/fairseq', 'transformer.wmt14.en-fr', tokenizer='moses', bpe='subword_nmt')\", 'api_provider': 'PyTorch', 'explanation': \"Load the Transformer (NMT) model for English-to-French translation from PyTorch Hub, which uses state-of-the-art neural machine translation techniques.\", 'code': \"import torch\nen2fr = torch.hub.load('pytorch/fairseq', 'transformer.wmt14.en-fr', tokenizer='moses', bpe='subword_nmt')\"}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/fairseq')", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "Neural Machine Translation", "api_name": "Transformer (NMT)", "api_call": "torch.hub.load(repo_or_dir='pytorch/fairseq')", "api_arguments": ["model_name", "tokenizer", "bpe", "beam", "sampling", "sampling_topk"], "python_environment_requirements": ["bitarray", "fastBPE", "hydra-core", "omegaconf", "regex", "requests", "sacremoses", "subword_nmt"], "example_code": "import torch\n\nen2fr = torch.hub.load('pytorch/fairseq', 'transformer.wmt14.en-fr', tokenizer='moses', bpe='subword_nmt')\n\nen2fr.cuda()\n\nfr = en2fr.translate('Hello world!', beam=5)\nassert fr == 'Bonjour \u00e0 tous !'", "performance": {"dataset": [{"name": "WMT'14", "accuracy": "Not provided"}, {"name": "WMT'18", "accuracy": "Not provided"}, {"name": "WMT'19", "accuracy": "Not provided"}]}, "description": "Transformer (NMT) is a powerful sequence-to-sequence modeling architecture that produces state-of-the-art neural machine translation systems. It is based on the paper 'Attention Is All You Need' and has been improved using techniques such as large-scale semi-supervised training, back-translation, and noisy-channel reranking. It supports English-French and English-German translation as well as round-trip translation for paraphrasing."}}
{"code": "{'Instruction': Tell me an API for image classification with high accuracy and less computation resources., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_efficientnet_b0', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained EfficientNet model from PyTorch Hub, which achieves high accuracy while being small and fast, making it suitable for efficient image classification tasks.', 'code': 'import torch\nmodel = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_efficientnet_b0', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_efficientnet_b0', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "EfficientNet", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_efficientnet_b0', pretrained=True)", "api_arguments": ["model_name", "pretrained"], "python_environment_requirements": ["validators", "matplotlib"], "example_code": "import torch\n\nefficientnet = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_efficientnet_b0', pretrained=True)\nutils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_convnets_processing_utils')\n\nefficientnet.eval().to(device)\n\nbatch = torch.cat([utils.prepare_input_from_uri(uri) for uri in uris]).to(device)\n\nwith torch.no_grad():\n output = torch.nn.functional.softmax(efficientnet(batch), dim=1)\n \nresults = utils.pick_n_best(predictions=output, n=5)", "performance": {"dataset": {"name": "IMAGENET", "accuracy": "Not provided"}}, "description": "EfficientNet is a family of image classification models that achieve state-of-the-art accuracy while being smaller and faster. The models are trained with mixed precision using Tensor Cores on the NVIDIA Volta and Ampere GPU architectures. The EfficientNet models include EfficientNet-B0, EfficientNet-B4, EfficientNet-WideSE-B0, and EfficientNet-WideSE-B4. The WideSE models use wider Squeeze-and-Excitation layers than the original EfficientNet models, resulting in slightly better accuracy."}}
{"code": "{'Instruction': A social media company wants to detect if an image uploaded by users contains any inappropriate content. Recommend an API for this., 'Output': {'domain': 'Object Detection', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Single Shot MultiBox Detector (SSD) model from PyTorch Hub, which can be used to detect objects in images with high accuracy and efficiency.', 'code': \"import torch\nmodel = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_ssd', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Object Detection", "framework": "PyTorch", "functionality": "Single Shot MultiBox Detector", "api_name": "SSD", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_ssd', pretrained=True)", "api_arguments": ["model_name"], "python_environment_requirements": ["numpy", "scipy", "scikit-image", "matplotlib"], "example_code": "import torch\n\nssd_model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd')\nutils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd_processing_utils')\n\nssd_model.to('cuda')\nssd_model.eval()\n\ninputs = [utils.prepare_input(uri) for uri in uris]\ntensor = utils.prepare_tensor(inputs)\n\nwith torch.no_grad():\n detections_batch = ssd_model(tensor)\n\nresults_per_input = utils.decode_results(detections_batch)\nbest_results_per_input = [utils.pick_best(results, 0.40) for results in results_per_input]", "performance": {"dataset": {"name": "COCO", "accuracy": "Not provided"}}, "description": "The SSD (Single Shot MultiBox Detector) model is an object detection model based on the paper 'SSD: Single Shot MultiBox Detector'. It uses a deep neural network for detecting objects in images. This implementation replaces the obsolete VGG model backbone with the more modern ResNet-50 model. The SSD model is trained on the COCO dataset and can be used to detect objects in images with high accuracy and efficiency."}}
{"code": "{'Instruction': Create a program to identify the species of a bird from an image taken from a bird watching trip in north America., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2', 'mealv2_resnest50_cutmix', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MEAL V2 model with ResNeSt50 backbone from PyTorch Hub. You can fine-tune this model for bird species classification using an annotated dataset of bird images.', 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2', 'mealv2_resnest50_cutmix', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V1', model='mealv1_resnest50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V1', model='mealv1_resnest50', pretrained=True)", "api_arguments": {"model_name": "mealv1_resnest50"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V1 w/ ResNet50", "resolution": "224", "parameters": "25.6M", "top1": "78.21", "top5": "94.01"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': I run an online fashion store, and I'm looking for an AI model that can categorize images of clothing items into their respective categories. Suggest a suitable API for me to use., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MEAL V2 ResNet50 model from PyTorch Hub, which can be fine-tuned for fashion item classification.', 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50', pretrained=True)", "api_arguments": {"model_name": "mealv2_resnest50"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ ResNet50", "resolution": "224", "parameters": "25.6M", "top1": "80.67", "top5": "95.09"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': What API do you recommend to discriminate between different breeds of dogs using only the color of the dogs?', 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', 'mealv2_mobilenetv3_small_100', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MEAL V2 model with MobileNet V3-Small 1.0 architecture from PyTorch Hub. This model can be fine-tuned for dog breed classification based on their color.', 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', 'mealv2_mobilenetv3_small_100', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenetv3_small_100', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenetv3_small_100', pretrained=True)", "api_arguments": {"model_name": "mealv2_mobilenetv3_small_100"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ MobileNet V3-Small 1.0", "resolution": "224", "parameters": "2.54M", "top1": "69.65", "top5": "88.71"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': Present an API to determine what plant is in a photo from a database containing 100,000 images without the use of common tricks., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50_380x380', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained MEAL V2 model from PyTorch Hub, which can be fine-tuned for plant classification without common tricks. The model yields state-of-the-art results on large databases.\", 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50_380x380', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50_380x380', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50_380x380', pretrained=True)", "api_arguments": {"model_name": "mealv2_resnest50_380x380"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ ResNet50", "resolution": "380", "parameters": "25.6M", "top1": "81.72", "top5": "95.81"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': I am working on a project to classify images into different classes. Give me a API that has been pre-trained and can classify images across a wide range of categories., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenetv3_small_075', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MEAL V2 model with MobileNet V3-Small 0.75 architecture from PyTorch Hub, which can classify images across a wide range of categories.', 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenetv3_small_075', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenetv3_small_075', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenetv3_small_075', pretrained=True)", "api_arguments": {"model_name": "mealv2_mobilenetv3_small_075"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ MobileNet V3-Small 0.75", "resolution": "224", "parameters": "2.04M", "top1": "67.60", "top5": "87.23"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': I need an API that does Image Classification, which is pretrained on ImageNet dataset and has high accuracy., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2','meal_v2','mealv2_mobilenet_v3_large_100', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MEAL_V2 model based on MobileNet V3-Large 1.0, which has an accuracy of 76.92% (Top-1) and 93.32% (Top-5) on the ImageNet dataset.', 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', 'mealv2_mobilenet_v3_large_100', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenet_v3_large_100', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenet_v3_large_100', pretrained=True)", "api_arguments": {"model_name": "mealv2_mobilenet_v3_large_100"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ MobileNet V3-Large 1.0", "resolution": "224", "parameters": "5.48M", "top1": "76.92", "top5": "93.32"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': How can we convert spoken text \"The weather is nice today. Should we go for a walk?\" into speech using a deep learning model?, 'Input': 'The weather is nice today. Should we go for a walk?', 'Output': {'domain': 'Text-to-Speech', 'api_call': \"tacotron2 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tacotron2', model_math='fp16')\", 'api_provider': 'PyTorch', 'explanation': 'Use the NVIDIA Tacotron 2 model to convert the spoken text into mel spectrograms and then use the WaveGlow model to generate speech from these mel spectrograms.', 'code': 'import torch\ntacotron2 = torch.hub.load(\"NVIDIA/DeepLearningExamples:torchhub\", \"nvidia_tacotron2\", model_math=\"fp16\") \nutils = torch.hub.load(\"NVIDIA/DeepLearningExamples:torchhub\", \"nvidia_tts_utils\") \ngpu = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ntacotron2 = tacotron2.to(gpu).eval()\n\ntext = \"The weather is nice today. Should we go for a walk?\"\nsequences, lengths = utils.prepare_input_sequence([text])\nwith torch.no_grad():\n    mel, _, _ = tacotron2.infer(sequences.to(gpu), lengths)\n    audio = waveglow.infer(mel)'\n}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_tacotron2', model_math='fp16')", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Text-to-Speech", "api_name": "Tacotron 2", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_tacotron2', model_math='fp16')", "api_arguments": {"model_math": "fp16"}, "python_environment_requirements": ["numpy", "scipy", "librosa", "unidecode", "inflect", "libsndfile1"], "example_code": ["import torch", "tacotron2 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tacotron2', model_math='fp16')", "tacotron2 = tacotron2.to('cuda')", "tacotron2.eval()", "text = 'Hello world, I missed you so much.'", "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tts_utils')", "sequences, lengths = utils.prepare_input_sequence([text])", "with torch.no_grad():", " mel, _, _ = tacotron2.infer(sequences, lengths)", " audio = waveglow.infer(mel)", "audio_numpy = audio[0].data.cpu().numpy()", "rate = 22050"], "performance": {"dataset": "LJ Speech", "accuracy": "Not specified"}, "description": "The Tacotron 2 model generates mel spectrograms from input text using an encoder-decoder architecture, and it is designed for generating natural-sounding speech from raw transcripts without any additional prosody information. This implementation uses Dropout instead of Zoneout to regularize the LSTM layers. The WaveGlow model (also available via torch.hub) is a flow-based model that consumes the mel spectrograms to generate speech."}}
{"code": "{'Instruction': FastEVs is developing an electric autonomous vehicle and needs an API to perceive traffic objects and detect drivable areas in real-time. Recommend a suitable API., 'Output': {'domain': 'Object Detection', 'api_call': \"model = torch.hub.load('datvuthanh/hybridnets', 'hybridnets', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained HybridNets model from PyTorch Hub to detect traffic objects, drivable areas, and lanes in real-time for autonomous vehicles.', 'code': \"import torch\nmodel = torch.hub.load('datvuthanh/hybridnets', 'hybridnets', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='datvuthanh/hybridnets', model='hybridnets', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Object Detection", "framework": "PyTorch", "functionality": "Traffic Object Detection, Drivable Area Segmentation, Lane Detection", "api_name": "HybridNets", "api_call": "torch.hub.load(repo_or_dir='datvuthanh/hybridnets', model='hybridnets', pretrained=True)", "api_arguments": "pretrained", "python_environment_requirements": "Python>=3.7, PyTorch>=1.10", "example_code": "import torch\nmodel = torch.hub.load('datvuthanh/hybridnets', 'hybridnets', pretrained=True)\nimg = torch.randn(1,3,640,384)\nfeatures, regression, classification, anchors, segmentation = model(img)", "performance": {"dataset": [{"name": "BDD100K", "accuracy": {"Traffic Object Detection": {"Recall (%)": 92.8, "mAP@0.5 (%)": 77.3}, "Drivable Area Segmentation": {"Drivable mIoU (%)": 90.5}, "Lane Line Detection": {"Accuracy (%)": 85.4, "Lane Line IoU (%)": 31.6}}}]}, "description": "HybridNets is an end2end perception network for multi-tasks. Our work focused on traffic object detection, drivable area segmentation and lane detection. HybridNets can run real-time on embedded systems, and obtains SOTA Object Detection, Lane Detection on BDD100K Dataset."}}
{"code": "{'Instruction': Develop an application that classifies actions in sports videos. Recommend a suitable API to help analyze the videos and understand the actions performed., 'Output': {'domain': 'Video Classification', 'api_call': \"model = torch.hub.load('facebookresearch/pytorchvideo', 'slow_r50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained 3D ResNet model from PyTorch Hub, which is trained on the Kinetics 400 dataset and can be used to classify sports actions performed in videos.', 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/pytorchvideo', 'slow_r50', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='slow_r50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Video Classification", "framework": "PyTorch", "functionality": "3D ResNet", "api_name": "slow_r50", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='slow_r50', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["torch", "json", "urllib", "pytorchvideo", "torchvision", "torchaudio", "torchtext", "torcharrow", "TorchData", "TorchRec", "TorchServe", "PyTorch on XLA Devices"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/pytorchvideo', 'slow_r50', pretrained=True)", "device = 'cpu'", "model = model.eval()", "model = model.to(device)"], "performance": {"dataset": "Kinetics 400", "accuracy": {"top_1": 74.58, "top_5": 91.63}, "Flops (G)": 54.52, "Params (M)": 32.45}, "description": "The 3D ResNet model is a Resnet-style video classification network pretrained on the Kinetics 400 dataset. It is based on the architecture from the paper 'SlowFast Networks for Video Recognition' by Christoph Feichtenhofer et al."}}
{"code": "{'Instruction': A startup is looking to build a drone security system for detecting intruders in restricted areas. Suggest an API suitable for this task., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_efficientnet_b0', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MEAL V2 model with EfficientNet-B0 for image classification from PyTorch Hub. This high-performance model can be fine-tuned to detect intruders in restricted areas.', 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_efficientnet_b0', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_efficientnet_b0', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_efficientnet_b0', pretrained=True)", "api_arguments": {"model_name": "mealv2_efficientnet_b0"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ EfficientNet-B0", "resolution": "224", "parameters": "5.29M", "top1": "78.29", "top5": "93.95"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': An autonomous vehicle company requires an API capable of object detection, drivable area segmentation, and lane detection. Provide the API and an example code to fulfill their requirements., 'Output': {'domain': 'Object Detection', 'api_call': \"model = torch.hub.load('hustvl/yolop', 'yolop', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained YOLOP model from PyTorch Hub, which can perform object detection, drivable area segmentation, and lane detection tasks.', 'code': \"import torch\nmodel = torch.hub.load('hustvl/yolop', 'yolop', pretrained=True)\nimg = torch.randn(1, 3, 640, 640)\ndet_out, da_seg_out, ll_seg_out = model(img)\"}}", "api_call": "torch.hub.load(repo_or_dir='hustvl/yolop', model='yolop', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Object Detection", "framework": "PyTorch", "functionality": "Object Detection, Drivable Area Segmentation, Lane Detection", "api_name": "YOLOP", "api_call": "torch.hub.load(repo_or_dir='hustvl/yolop', model='yolop', pretrained=True)", "api_arguments": "pretrained", "python_environment_requirements": "pip install -qr https://github.com/hustvl/YOLOP/blob/main/requirements.txt", "example_code": "import torch\nmodel = torch.hub.load('hustvl/yolop', 'yolop', pretrained=True)\nimg = torch.randn(1,3,640,640)\ndet_out, da_seg_out,ll_seg_out = model(img)", "performance": {"dataset": "BDD100K", "accuracy": {"Object Detection": {"Recall(%)": 89.2, "mAP50(%)": 76.5, "Speed(fps)": 41}, "Drivable Area Segmentation": {"mIOU(%)": 91.5, "Speed(fps)": 41}, "Lane Detection": {"mIOU(%)": 70.5, "IOU(%)": 26.2}}}, "description": "YOLOP is an efficient multi-task network that can jointly handle three crucial tasks in autonomous driving: object detection, drivable area segmentation and lane detection. And it is also the first to reach real-time on embedded devices while maintaining state-of-the-art level performance on the BDD100K dataset."}}
{"code": "{'Instruction': I want to calculate the relative depth of a scene from a single photo. Recommend an API for this purpose., 'Output': {'domain': 'Depth Estimation', 'api_call': \"model = torch.hub.load('intel-isl/MiDaS', 'DPT_Hybrid', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MiDaS model from PyTorch Hub, which is designed to compute relative inverse depth from a single image.', 'code': 'import torch\nmodel = torch.hub.load('intel-isl/MiDaS', 'DPT_Hybrid', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='intel-isl/MiDaS', model='DPT_Hybrid', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Computing relative depth from a single image", "api_name": "MiDaS", "api_call": "torch.hub.load(repo_or_dir='intel-isl/MiDaS', model='DPT_Hybrid', pretrained=True)", "api_arguments": {"repo_or_dir": "intel-isl/MiDaS", "model": "model_type"}, "python_environment_requirements": "pip install timm", "example_code": ["import cv2", "import torch", "import urllib.request", "import matplotlib.pyplot as plt", "url, filename = ('https://github.com/pytorch/hub/raw/master/images/dog.jpg', 'dog.jpg')", "urllib.request.urlretrieve(url, filename)", "model_type = 'DPT_Large'", "midas = torch.hub.load('intel-isl/MiDaS', 'DPT_Hybrid')", "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')", "midas.to(device)", "midas.eval()", "midas_transforms = torch.hub.load('intel-isl/MiDaS', 'transforms')", "if model_type == 'DPT_Large' or model_type == 'DPT_Hybrid':", " transform = midas_transforms.dpt_transform", "else:", " transform = midas_transforms.small_transform", "img = cv2.imread(filename)", "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)", "input_batch = transform(img).to(device)", "with torch.no_grad():", " prediction = midas(input_batch)", "prediction = torch.nn.functional.interpolate(", " prediction.unsqueeze(1),", " size=img.shape[:2],", " mode='bicubic',", " align_corners=False,", ").squeeze()", "output = prediction.cpu().numpy()", "plt.imshow(output)", "plt.show()"], "performance": {"dataset": "10 distinct datasets", "accuracy": "Multi-objective optimization"}, "description": "MiDaS computes relative inverse depth from a single image. The repository provides multiple models that cover different use cases ranging from a small, high-speed model to a very large model that provide the highest accuracy. The models have been trained on 10 distinct datasets using multi-objective optimization to ensure high quality on a wide range of inputs."}}
{"code": "{'Instruction': I have a photo of an outdoor landscape, and I want to determine the relative depth of objects in the image. Provide an API that will help me achieve this., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('intel-isl/MiDaS', 'MiDaS_small', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MiDaS model from PyTorch Hub to compute the relative depth of objects in a single image.', 'code': \"import torch\nmodel = torch.hub.load('intel-isl/MiDaS', 'MiDaS_small', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='intel-isl/MiDaS', model='MiDaS_small', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Computing relative depth from a single image", "api_name": "MiDaS", "api_call": "torch.hub.load(repo_or_dir='intel-isl/MiDaS', model='MiDaS_small', pretrained=True)", "api_arguments": {"repo_or_dir": "intel-isl/MiDaS", "model": "model_type"}, "python_environment_requirements": "pip install timm", "example_code": ["import cv2", "import torch", "import urllib.request", "import matplotlib.pyplot as plt", "url, filename = ('https://github.com/pytorch/hub/raw/master/images/dog.jpg', 'dog.jpg')", "urllib.request.urlretrieve(url, filename)", "model_type = 'DPT_Large'", "midas = torch.hub.load('intel-isl/MiDaS', 'MiDaS_small')", "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')", "midas.to(device)", "midas.eval()", "midas_transforms = torch.hub.load('intel-isl/MiDaS', 'transforms')", "if model_type == 'DPT_Large' or model_type == 'DPT_Hybrid':", " transform = midas_transforms.dpt_transform", "else:", " transform = midas_transforms.small_transform", "img = cv2.imread(filename)", "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)", "input_batch = transform(img).to(device)", "with torch.no_grad():", " prediction = midas(input_batch)", "prediction = torch.nn.functional.interpolate(", " prediction.unsqueeze(1),", " size=img.shape[:2],", " mode='bicubic',", " align_corners=False,", ").squeeze()", "output = prediction.cpu().numpy()", "plt.imshow(output)", "plt.show()"], "performance": {"dataset": "10 distinct datasets", "accuracy": "Multi-objective optimization"}, "description": "MiDaS computes relative inverse depth from a single image. The repository provides multiple models that cover different use cases ranging from a small, high-speed model to a very large model that provide the highest accuracy. The models have been trained on 10 distinct datasets using multi-objective optimization to ensure high quality on a wide range of inputs."}}
{"code": "{'Instruction': I want an API to separate music into vocals, drums, bass, and other instruments for a music application I am developing. What API should I use?, 'Output': {'domain': 'Audio Separation', 'api_call': \"model = torch.hub.load('sigsep/open-unmix-pytorch', 'umxhq', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Use the Open-Unmix model from PyTorch Hub for separating music into vocals, drums, bass, and other instruments. The model is pretrained on the MUSDB18 dataset.', 'code': 'import torch\nseparator = torch.hub.load(\"sigsep/open-unmix-pytorch\", \"umxhq\", pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='sigsep/open-unmix-pytorch', model='umxhq', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Audio Separation", "framework": "PyTorch", "functionality": "Music Source Separation", "api_name": "Open-Unmix", "api_call": "torch.hub.load(repo_or_dir='sigsep/open-unmix-pytorch', model='umxhq', pretrained=True)", "api_arguments": ["model_name"], "python_environment_requirements": ["PyTorch >=1.6.0", "torchaudio"], "example_code": ["import torch", "separator = torch.hub.load('sigsep/open-unmix-pytorch', 'umxhq')", "audio = torch.rand((1, 2, 100000))", "original_sample_rate = separator.sample_rate", "estimates = separator(audio)"], "performance": {"dataset": "MUSDB18", "accuracy": "N/A"}, "description": "Open-Unmix provides ready-to-use models that allow users to separate pop music into four stems: vocals, drums, bass and the remaining other instruments. The models were pre-trained on the freely available MUSDB18 dataset."}}
{"code": "{'Instruction': Make a list of semantic audio conversion tasks, and specify a suitable API for speech to text conversion from various audio input formats., 'Output': {'list_of_tasks': ['Speech-to-text conversion', 'Voice recognition', 'Language translation', 'Audio classification', 'Transcription services'], 'recommended_api': {'domain': 'Speech-To-Text', 'api_call': \"model = torch.hub.load('snakers4/silero-models', 'silero_stt', language='en', device=device)\", 'api_provider': 'PyTorch', 'explanation': 'Use Silero Models from PyTorch Hub for speech-to-text conversion, supporting various audio input formats, languages, and dialects.', 'code': 'import torch\ndevice = torch.device('cpu')\nmodel, decoder, utils = torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_stt', language='en', device=device)'}}}", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_stt', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Speech-To-Text", "api_name": "Silero Models", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_stt', pretrained=True)", "api_arguments": {"repo_or_dir": "snakers4/silero-models", "model": "silero_stt", "language": ["en", "de", "es"], "device": "device"}, "python_environment_requirements": ["pip install -q torchaudio omegaconf soundfile"], "example_code": ["import torch", "import zipfile", "import torchaudio", "from glob import glob", "device = torch.device('cpu')", "model, decoder, utils = torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_stt', language='en', device=device)", "(read_batch, split_into_batches, read_audio, prepare_model_input) = utils", "torch.hub.download_url_to_file('https://opus-codec.org/static/examples/samples/speech_orig.wav', dst ='speech_orig.wav', progress=True)", "test_files = glob('speech_orig.wav')", "batches = split_into_batches(test_files, batch_size=10)", "input = prepare_model_input(read_batch(batches[0]), device=device)", "output = model(input)", "for example in output:", " print(decoder(example.cpu()))"], "performance": {"dataset": "Open-STT", "accuracy": "See the wiki for quality and performance benchmarks"}, "description": "Silero Speech-To-Text models provide enterprise grade STT in a compact form-factor for several commonly spoken languages. The models are robust to a variety of dialects, codecs, domains, noises, and lower sampling rates. They consume a normalized audio in the form of samples and output frames with token probabilities. A decoder utility is provided for simplicity."}}
{"code": "{'Instruction': I am building a bird identification app but I have trouble indentifying birds from photo. Which API can I use to indentify birds given a photo?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('nicolalandro/ntsnet-cub200', 'ntsnet', pretrained=True, **{'topN': 6, 'device':'cpu', 'num_classes': 200})\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained NTS-Net model from PyTorch Hub to identify bird species in a given photo. This model is specifically trained on the CUB200 2011 dataset of bird species.', 'code': 'import torch\nmodel = torch.hub.load('nicolalandro/ntsnet-cub200', 'ntsnet', pretrained=True, **{\"topN\": 6, \"device\":\"cpu\", \"num_classes\": 200})'}}", "api_call": "torch.hub.load(repo_or_dir='nicolalandro/ntsnet-cub200', model='ntsnet', pretrained=True, **{'topN': 6, 'device':'cpu', 'num_classes': 200})", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Fine-grained image classifier", "api_name": "ntsnet", "api_call": "torch.hub.load(repo_or_dir='nicolalandro/ntsnet-cub200', model='ntsnet', pretrained=True, **{'topN': 6, 'device':'cpu', 'num_classes': 200})", "api_arguments": {"pretrained": "True", "topN": "6", "device": "cpu", "num_classes": "200"}, "python_environment_requirements": ["torch", "torchvision", "PIL"], "example_code": "from torchvision import transforms\nimport torch\nimport urllib\nfrom PIL import Image\n\ntransform_test = transforms.Compose([\n transforms.Resize((600, 600), Image.BILINEAR),\n transforms.CenterCrop((448, 448)),\n transforms.ToTensor(),\n transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n])\n\nmodel = torch.hub.load('nicolalandro/ntsnet-cub200', 'ntsnet', pretrained=True, **{'topN': 6, 'device':'cpu', 'num_classes': 200})\nmodel.eval()\n\nurl = 'https://raw.githubusercontent.com/nicolalandro/ntsnet-cub200/master/images/nts-net.png'\nimg = Image.open(urllib.request.urlopen(url))\nscaled_img = transform_test(img)\ntorch_images = scaled_img.unsqueeze(0)\n\nwith torch.no_grad():\n top_n_coordinates, concat_out, raw_logits, concat_logits, part_logits, top_n_index, top_n_prob = model(torch_images)\n\n_, predict = torch.max(concat_logits, 1)\npred_id = predict.item()\nprint('bird class:', model.bird_classes[pred_id])", "performance": {"dataset": "CUB200 2011", "accuracy": "Not provided"}, "description": "This is an nts-net pretrained with CUB200 2011 dataset, which is a fine-grained dataset of birds species."}}
{"code": "{'Instruction': A business meeting is being transcribed, and we need to detect speech segments in the recorded audio. Provide me with an API to perform voice activity detection on the audio file., 'Output': {'domain': 'Voice Activity Detection', 'api_call': 'model = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Silero Voice Activity Detector from PyTorch Hub. The model can be used to detect speech segments within an audio file for efficient transcription.', 'code': 'import torch\nmodel, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=True)'}}", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Voice Activity Detection", "api_name": "Silero Voice Activity Detector", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=True)", "api_arguments": {"repo_or_dir": "snakers4/silero-vad", "model": "silero_vad", "force_reload": "True"}, "python_environment_requirements": {"torchaudio": "pip install -q torchaudio"}, "example_code": {"import": ["import torch", "torch.set_num_threads(1)", "from IPython.display import Audio", "from pprint import pprint"], "download_example": "torch.hub.download_url_to_file('https://models.silero.ai/vad_models/en.wav', 'en_example.wav')", "load_model": "model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=True)", "load_utils": "(get_speech_timestamps, _, read_audio, _) = utils", "set_sampling_rate": "sampling_rate = 16000", "read_audio": "wav = read_audio('en_example.wav', sampling_rate=sampling_rate)", "get_speech_timestamps": "speech_timestamps = get_speech_timestamps(wav, model, sampling_rate=sampling_rate)", "print_speech_timestamps": "pprint(speech_timestamps)"}, "performance": {"dataset": "", "accuracy": ""}, "description": "Silero VAD is a pre-trained enterprise-grade Voice Activity Detector (VAD) that aims to provide a high-quality and modern alternative to the WebRTC Voice Activity Detector. The model is optimized for performance on 1 CPU thread and is quantized."}}
{"code": "{'Instruction': Identify an API to perform semantic segmentation of the street labeled with road, sidewalk, and building for a given picture., 'Output': {'domain': 'Semantic Segmentation', 'api_call': \"model = torch.hub.load('pytorch/vision', 'deeplabv3_resnet50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained DeepLabV3 model with a ResNet-50 backbone from PyTorch Hub, which can be fine-tuned to perform semantic segmentation of street elements like roads, sidewalks, and buildings.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'deeplabv3_resnet50', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='deeplabv3_resnet50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "DeepLabV3", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='deeplabv3_resnet50', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": "latest", "torchvision": "latest", "PIL": "latest", "matplotlib": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_resnet50', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "input_image = input_image.convert('RGB')", "preprocess = transforms.Compose([", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)['out'][0]", "output_predictions = output.argmax(0)"], "performance": {"dataset": "COCO val2017", "accuracy": [{"model": "deeplabv3_resnet50", "Mean IOU": 66.4, "Global Pixelwise Accuracy": 92.4}]}, "description": "DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset."}}
{"code": "{'Instruction': A photographer wants to separate the foreground and background in their images using semantic segmentation. Suggest an API for this purpose., 'Output': {'domain': 'Semantic Segmentation', 'api_call': \"model = torch.hub.load('pytorch/vision', 'deeplabv3_mobilenet_v3_large', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained DeepLabV3 model for semantic segmentation from PyTorch Hub to separate the foreground and background in images.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'deeplabv3_mobilenet_v3_large', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='deeplabv3_mobilenet_v3_large', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "DeepLabV3", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='deeplabv3_mobilenet_v3_large', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": "latest", "torchvision": "latest", "PIL": "latest", "matplotlib": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_mobilenet_v3_large', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "input_image = input_image.convert('RGB')", "preprocess = transforms.Compose([", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)['out'][0]", "output_predictions = output.argmax(0)"], "performance": {"dataset": "COCO val2017", "accuracy": [{"model": "deeplabv3_mobilenet_v3_large", "Mean IOU": 60.3, "Global Pixelwise Accuracy": 91.4}]}, "description": "DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset."}}
{"code": "{'Instruction': Propose a neural network model to use for detecting furniture in photos and identifying their types., 'Output': {'domain': 'Object Detection', 'api_call': \"model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained YOLOv5 model from PyTorch Hub, which can be fine-tuned for detecting furniture in images and classifying them into types.', 'code': 'import torch\nmodel = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='ultralytics/yolov5', model='yolov5s', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Object Detection", "framework": "PyTorch", "functionality": "YOLOv5", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='ultralytics/yolov5', model='yolov5s', pretrained=True)", "api_arguments": ["'ultralytics/yolov5'", "'yolov5s'", "pretrained=True"], "python_environment_requirements": "Python>=3.8, PyTorch>=1.7", "example_code": ["import torch", "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)", "imgs = ['https://ultralytics.com/images/zidane.jpg']", "results = model(imgs)", "results.print()", "results.save()", "results.xyxy[0]", "results.pandas().xyxy[0]"], "performance": {"dataset": "COCO", "accuracy": {"YOLOv5s6": {"mAPval0.5:0.95": 43.3, "mAPtest0.5:0.95": 43.3, "mAPval0.5": 61.9}, "YOLOv5m6": {"mAPval0.5:0.95": 50.5, "mAPtest0.5:0.95": 50.5, "mAPval0.5": 68.7}, "YOLOv5l6": {"mAPval0.5:0.95": 53.4, "mAPtest0.5:0.95": 53.4, "mAPval0.5": 71.1}, "YOLOv5x6": {"mAPval0.5:0.95": 54.4, "mAPtest0.5:0.95": 54.4, "mAPval0.5": 72.0}, "YOLOv5x6 TTA": {"mAPval0.5:0.95": 55.0, "mAPtest0.5:0.95": 55.0, "mAPval0.5": 72.0}}}, "description": "YOLOv5 is a family of compound-scaled object detection models trained on the COCO dataset, and includes simple functionality for Test Time Augmentation (TTA), model ensembling, hyperparameter evolution, and export to ONNX, CoreML and TFLite."}}
{"code": "{'Instruction': Alex is a deaf user who needs an app to convert text to voice. Which API would you recommend for that?, 'Output': {'domain': 'Text-To-Speech', 'api_call': \"model, symbols, sample_rate, example_text, apply_tts = torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_tts', language='en', speaker='lj_16khz')\", 'api_provider': 'PyTorch', 'explanation': 'Use the Silero Text-To-Speech Models to convert text to voice with a single line of code. They provide high-quality voice synthesis for various languages.', 'code': \"import torch\nlanguage = 'en'\nspeaker = 'lj_16khz'\ndevice = torch.device('cpu')\nmodel, symbols, sample_rate, example_text, apply_tts = torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_tts', language=language, speaker=speaker)\nmodel = model.to(device)\"}}", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_tts', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Text-To-Speech", "framework": "PyTorch", "functionality": "Text-To-Speech", "api_name": "Silero Text-To-Speech Models", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_tts', pretrained=True)", "api_arguments": {"repo_or_dir": "snakers4/silero-models", "model": "silero_tts", "language": "language", "speaker": "speaker"}, "python_environment_requirements": ["pip install -q torchaudio omegaconf"], "example_code": "import torch\nlanguage = 'en'\nspeaker = 'lj_16khz'\ndevice = torch.device('cpu')\nmodel, symbols, sample_rate, example_text, apply_tts = torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_tts', language=language, speaker=speaker)\nmodel = model.to(device)\naudio = apply_tts(texts=[example_text], model=model, sample_rate=sample_rate, symbols=symbols, device=device)", "performance": {"dataset": [{"language": "Russian", "speakers": 6}, {"language": "English", "speakers": 1}, {"language": "German", "speakers": 1}, {"language": "Spanish", "speakers": 1}, {"language": "French", "speakers": 1}], "accuracy": "High throughput on slow hardware. Decent performance on one CPU thread"}, "description": "Silero Text-To-Speech models provide enterprise grade TTS in a compact form-factor for several commonly spoken languages. They offer one-line usage, naturally sounding speech, no GPU or training required, minimalism and lack of dependencies, a library of voices in many languages, support for 16kHz and 8kHz out of the box."}}
{"code": "{'Instruction': Recommend me an API that identifies animals in a given image to feed as data to other functionalities., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNext-101 32x16d WSL model from PyTorch Hub, which can be fine-tuned for animal classification given an image.', 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x16d_wsl', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNext WSL", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x16d_wsl', pretrained=True)", "api_arguments": [{"name": "resnext101_32x16d_wsl", "type": "str", "description": "ResNeXt-101 32x16d WSL model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl')", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(output[0])", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeXt-101 32x16d": {"Top-1 Acc.": "84.2", "Top-5 Acc.": "97.2"}}}, "description": "The provided ResNeXt models are pre-trained in weakly-supervised fashion on 940 million public images with 1.5K hashtags matching with 1000 ImageNet1K synsets, followed by fine-tuning on ImageNet1K dataset. The models significantly improve the training accuracy on ImageNet compared to training from scratch. They achieve state-of-the-art accuracy of 85.4% on ImageNet with the ResNext-101 32x48d model."}}
{"code": "{'Instruction': We are building an image classifier for our meme sharing platform. Suggest an API for high-accuracy image classification., 'Output': {'domain': 'Classification', 'api_call': 'model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x48d_wsl', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNext WSL model from PyTorch Hub, which can achieve state-of-the-art accuracy for image classification tasks.', 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x48d_wsl', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x48d_wsl', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNext WSL", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x48d_wsl', pretrained=True)", "api_arguments": [{"name": "resnext101_32x48d_wsl", "type": "str", "description": "ResNeXt-101 32x48d WSL model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x48d_wsl')", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(output[0])", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeXt-101 32x48d": {"Top-1 Acc.": "85.4", "Top-5 Acc.": "97.6"}}}, "description": "The provided ResNeXt models are pre-trained in weakly-supervised fashion on 940 million public images with 1.5K hashtags matching with 1000 ImageNet1K synsets, followed by fine-tuning on ImageNet1K dataset. The models significantly improve the training accuracy on ImageNet compared to training from scratch. They achieve state-of-the-art accuracy of 85.4% on ImageNet with the ResNext-101 32x48d model."}}
{"code": "{'Instruction': Suggest an API that can identify the species of a bird from an image taken in Yosemite National Park., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x32d_wsl', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNeXt WSL model from PyTorch Hub, which can be fine-tuned for bird species classification using an image taken in Yosemite National Park.', 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x32d_wsl', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x32d_wsl', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNext WSL", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x32d_wsl', pretrained=True)", "api_arguments": [{"name": "resnext101_32x32d_wsl", "type": "str", "description": "ResNeXt-101 32x32d WSL model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x32d_wsl')", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(output[0])", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeXt-101 32x32d": {"Top-1 Acc.": "85.1", "Top-5 Acc.": "97.5"}}}, "description": "The provided ResNeXt models are pre-trained in weakly-supervised fashion on 940 million public images with 1.5K hashtags matching with 1000 ImageNet1K synsets, followed by fine-tuning on ImageNet1K dataset. The models significantly improve the training accuracy on ImageNet compared to training from scratch. They achieve state-of-the-art accuracy of 85.4% on ImageNet with the ResNext-101 32x48d model."}}
{"code": "{'Instruction': An artist is trying to generate new art from existing 64x64 art images for an online gallery. Provide me an API that can generate new images utilizing training data of previously generated art images., 'Output': {'domain': 'Image Generation', 'api_call': \"model = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'DCGAN', pretrained=True, useGPU=use_gpu)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained DCGAN model from PyTorch Hub, which can be fine-tuned with existing 64x64 art images to generate new images for the online gallery.', 'code': 'import torch\nuse_gpu = True if torch.cuda.is_available() else False\nmodel = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'DCGAN', pretrained=True, useGPU=use_gpu)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorch_GAN_zoo:hub', model='DCGAN', pretrained=True, useGPU=use_gpu)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Generative Adversarial Networks", "api_name": "DCGAN", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorch_GAN_zoo:hub', model='DCGAN', pretrained=True, useGPU=use_gpu)", "api_arguments": {"pretrained": "True", "useGPU": "use_gpu"}, "python_environment_requirements": "Python 3", "example_code": {"import": ["import torch", "import matplotlib.pyplot as plt", "import torchvision"], "use_gpu": "use_gpu = True if torch.cuda.is_available() else False", "load_model": "model = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'DCGAN', pretrained=True, useGPU=use_gpu)", "build_noise_data": "noise, _ = model.buildNoiseData(num_images)", "generate_images": "with torch.no_grad(): generated_images = model.test(noise)", "plot_images": ["plt.imshow(torchvision.utils.make_grid(generated_images).permute(1, 2, 0).cpu().numpy())", "plt.show()"]}, "performance": {"dataset": "FashionGen", "accuracy": "N/A"}, "description": "DCGAN is a model designed in 2015 by Radford et. al. in the paper Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks. It is a GAN architecture both very simple and efficient for low resolution image generation (up to 64x64)."}}
{"code": "{'Instruction': I want to know more about ResNet50 from NVIDIA Deep Learning Examples. Provide me with an API that I can use in my system to classify images with high precision., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resnet50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNet50 model from NVIDIA Deep Learning Examples using PyTorch Hub for high-precision image classification tasks.', 'code': \"import torch\nmodel = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resnet50', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_resnet50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNet50", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_resnet50', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["pip install validators matplotlib"], "example_code": ["import torch", "from PIL import Image", "import torchvision.transforms as transforms", "import numpy as np", "import json", "import requests", "import matplotlib.pyplot as plt", "import warnings", "warnings.filterwarnings('ignore')", "%matplotlib inline", "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')", "print(f'Using {device} for inference')", "resnet50 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resnet50', pretrained=True)", "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_convnets_processing_utils')", "resnet50.eval().to(device)", "uris = [...]", "batch = torch.cat([utils.prepare_input_from_uri(uri) for uri in uris]).to(device)", "with torch.no_grad():", " output = torch.nn.functional.softmax(resnet50(batch), dim=1)", "results = utils.pick_n_best(predictions=output, n=5)", "for uri, result in zip(uris, results):", " img = Image.open(requests.get(uri, stream=True).raw)", " img.thumbnail((256,256), Image.ANTIALIAS)", " plt.imshow(img)", " plt.show()", " print(result)"], "performance": {"dataset": "ImageNet", "accuracy": "~0.5% top1 improvement over ResNet50 v1"}, "description": "The ResNet50 v1.5 model is a modified version of the original ResNet50 v1 model. The difference between v1 and v1.5 is that, in the bottleneck blocks which requires downsampling, v1 has stride = 2 in the first 1x1 convolution, whereas v1.5 has stride = 2 in the 3x3 convolution. This difference makes ResNet50 v1.5 slightly more accurate (~0.5% top1) than v1, but comes with a small performance drawback (~5% imgs/sec). The model is initialized as described in Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification. This model is trained with mixed precision using Tensor Cores on Volta, Turing, and the NVIDIA Ampere GPU architectures."}}
{"code": "{'Instruction': I need to create an image of a realistic building. Is there an API that can help me with this?, 'Output': {'domain': 'Image Generation', 'api_call': \"model = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'PGAN', model_name='celebAHQ-512', pretrained=True, useGPU=use_gpu)\", 'api_provider': 'PyTorch', 'explanation': 'The pretrained Progressive Growing of GANs (PGAN) model from PyTorch Hub can generate high-quality, realistic images. However, this model is specifically trained on human faces. You may need to find or train a similar model on a dataset containing images of buildings.', 'code': 'import torch\nuse_gpu = True if torch.cuda.is_available() else False\nmodel = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'PGAN', model_name='celebAHQ-512', pretrained=True, useGPU=use_gpu)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorch_GAN_zoo:hub', model='PGAN', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Generative Adversarial Networks (GANs)", "api_name": "PGAN", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorch_GAN_zoo:hub', model='PGAN', pretrained=True)", "api_arguments": {"repo_or_dir": "facebookresearch/pytorch_GAN_zoo:hub", "model": "PGAN", "model_name": "celebAHQ-512", "pretrained": "True", "useGPU": "use_gpu"}, "python_environment_requirements": "Python 3", "example_code": {"import": "import torch", "use_gpu": "use_gpu = True if torch.cuda.is_available() else False", "load_model": "model = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'PGAN', model_name='celebAHQ-512', pretrained=True, useGPU=use_gpu)", "build_noise_data": "noise, _ = model.buildNoiseData(num_images)", "test": "generated_images = model.test(noise)", "plot_images": {"import_matplotlib": "import matplotlib.pyplot as plt", "import_torchvision": "import torchvision", "make_grid": "grid = torchvision.utils.make_grid(generated_images.clamp(min=-1, max=1), scale_each=True, normalize=True)", "imshow": "plt.imshow(grid.permute(1, 2, 0).cpu().numpy())", "show": "plt.show()"}}, "performance": {"dataset": "celebA", "accuracy": "High-quality celebrity faces"}, "description": "Progressive Growing of GANs (PGAN) is a method for generating high-resolution images using generative adversarial networks. The model is trained progressively, starting with low-resolution images and gradually increasing the resolution until the desired output is achieved. This implementation is based on the paper by Tero Karras et al., 'Progressive Growing of GANs for Improved Quality, Stability, and Variation'."}}
{"code": "{'Instruction': A medical researcher from Johns Hopkins University wants to analyze brain MRI scans for tumor detection. Recommend an API that can perform tumor segmentation in brain MRI images., 'Output': {'domain': 'Semantic Segmentation', 'api_call': \"model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained U-Net model for brain MRI segmentation from PyTorch Hub, which can generate segmentation maps for detecting tumors in brain MRI images.', 'code': 'import torch\nmodel = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='mateuszbuda/brain-segmentation-pytorch', model='unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "Biomedical Image Segmentation", "api_name": "U-Net for brain MRI", "api_call": "torch.hub.load(repo_or_dir='mateuszbuda/brain-segmentation-pytorch', model='unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)", "api_arguments": {"in_channels": 3, "out_channels": 1, "init_features": 32, "pretrained": true}, "python_environment_requirements": ["torch", "torchvision", "numpy", "PIL"], "example_code": ["import torch", "model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)", "import numpy as np", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "m, s = np.mean(input_image, axis=(0, 1)), np.std(input_image, axis=(0, 1))", "preprocess = transforms.Compose([transforms.ToTensor(),transforms.Normalize(mean=m, std=s)])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model = model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(torch.round(output[0]))"], "performance": {"dataset": "kaggle.com/mateuszbuda/lgg-mri-segmentation"}, "description": "U-Net with batch normalization for biomedical image segmentation with pretrained weights for abnormality segmentation in brain MRI. The model comprises four levels of blocks containing two convolutional layers with batch normalization and ReLU activation function, and one max pooling layer in the encoding part and up-convolutional layers instead in the decoding part. The number of convolutional filters in each block is 32, 64, 128, and 256. The bottleneck layer has 512 convolutional filters. From the encoding layers, skip connections are used to the corresponding layers in the decoding part. Input image is a 3-channel brain MRI slice from pre-contrast, FLAIR, and post-contrast sequences, respectively. Output is a one-channel probability map of abnormality regions with the same size as the input image."}}
{"code": "{'Instruction': Can you help me find an API for image classification which can recognize the objects in a given image?, 'Output': {'domain': 'Classification', 'api_call': 'model = torch.hub.load(\"NVIDIA/DeepLearningExamples:torchhub\", \"nvidia_resneXt\", pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pre-trained ResNeXt101-32x4d model from PyTorch Hub for image classification, which can recognize the objects in a given image.', 'code': 'import torch\nmodel = torch.hub.load(\"NVIDIA/DeepLearningExamples:torchhub\", \"nvidia_resneXt\", pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_resneXt', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNeXt101-32x4d", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_resneXt', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "validators", "matplotlib"], "example_code": ["import torch", "from PIL import Image", "import torchvision.transforms as transforms", "import numpy as np", "import json", "import requests", "import matplotlib.pyplot as plt", "import warnings", "warnings.filterwarnings('ignore')", "%matplotlib inline", "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")", "print(f'Using {device} for inference')", "resneXt = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resneXt')", "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_convnets_processing_utils')", "resneXt.eval().to(device)", "uris = [", " 'http://images.cocodataset.org/test-stuff2017/000000024309.jpg',", " 'http://images.cocodataset.org/test-stuff2017/000000028117.jpg',", " 'http://images.cocodataset.org/test-stuff2017/000000006149.jpg',", " 'http://images.cocodataset.org/test-stuff2017/000000004954.jpg',", "]", "batch = torch.cat(", " [utils.prepare_input_from_uri(uri) for uri in uris]", ").to(device)", "with torch.no_grad():", " output = torch.nn.functional.softmax(resneXt(batch), dim=1)", "results = utils.pick_n_best(predictions=output, n=5)", "for uri, result in zip(uris, results):", " img = Image.open(requests.get(uri, stream=True).raw)", " img.thumbnail((256,256), Image.ANTIALIAS)", " plt.imshow(img)", " plt.show()", " print(result)"], "performance": {"dataset": "IMAGENET", "accuracy": "Not provided"}, "description": "ResNeXt101-32x4d is a model introduced in the Aggregated Residual Transformations for Deep Neural Networks paper. It is based on regular ResNet model, substituting 3x3 convolutions inside the bottleneck block for 3x3 grouped convolutions. This model is trained with mixed precision using Tensor Cores on Volta, Turing, and the NVIDIA Ampere GPU architectures. It can be deployed for inference on the NVIDIA Triton Inference Server using TorchScript, ONNX Runtime or TensorRT as an execution backend."}}
{"code": "{'Instruction': Derive a recommendation for a CNN used in image classification, trained on the ImageNet dataset., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_se_resnext101_32x4d', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained SE-ResNeXt101-32x4d model from PyTorch Hub for image classification. This model is trained on the ImageNet dataset and augmented with Squeeze-and-Excitation modules.', 'code': \"import torch\nmodel = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_se_resnext101_32x4d', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_se_resnext101_32x4d', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SE-ResNeXt101", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_se_resnext101_32x4d', pretrained=True)", "api_arguments": "N/A", "python_environment_requirements": "validators, matplotlib", "example_code": "import torch\nfrom PIL import Image\nimport torchvision.transforms as transforms\nimport numpy as np\nimport json\nimport requests\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nprint(f'Using {device} for inference')\nresneXt = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_se_resnext101_32x4d')\nutils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_convnets_processing_utils')\nresneXt.eval().to(device)\nuris = ['http://images.cocodataset.org/test-stuff2017/000000024309.jpg','http://images.cocodataset.org/test-stuff2017/000000028117.jpg','http://images.cocodataset.org/test-stuff2017/000000006149.jpg','http://images.cocodataset.org/test-stuff2017/000000004954.jpg']\nbatch = torch.cat([utils.prepare_input_from_uri(uri) for uri in uris]).to(device)\nwith torch.no_grad():\n output = torch.nn.functional.softmax(resneXt(batch), dim=1)\nresults = utils.pick_n_best(predictions=output, n=5)\nfor uri, result in zip(uris, results):\n img = Image.open(requests.get(uri, stream=True).raw)\n img.thumbnail((256,256), Image.ANTIALIAS)\n plt.imshow(img)\n plt.show()\n print(result)", "performance": {"dataset": "IMAGENET", "accuracy": "N/A"}, "description": "The SE-ResNeXt101-32x4d is a ResNeXt101-32x4d model with added Squeeze-and-Excitation module. This model is trained with mixed precision using Tensor Cores on Volta, Turing, and the NVIDIA Ampere GPU architectures, which allows researchers to get results 3x faster than training without Tensor Cores while experiencing the benefits of mixed precision training."}}
{"code": "{'Instruction': What is an API that does an automatic image classification of flowers?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'densenet121', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained DenseNet-121 model from PyTorch Hub, which can be fine-tuned for automatic flower classification.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'densenet121', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet121', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Dense Convolutional Network", "api_name": "Densenet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet121', pretrained=True)", "api_arguments": [{"name": "densenet121", "type": "str", "description": "Densenet-121 model"}], "python_environment_requirements": {"torch": "latest", "torchvision": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet121', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"densenet121": {"Top-1 error": 25.35, "Top-5 error": 7.83}}}, "description": "Dense Convolutional Network (DenseNet) connects each layer to every other layer in a feed-forward fashion. It alleviates the vanishing-gradient problem, strengthens feature propagation, encourages feature reuse, and substantially reduces the number of parameters."}}
{"code": "{'Instruction': A company wants to develop a text classification model for their customer support operations. Suggest an API that uses transfer learning to achieve this., 'Output': {'domain': 'Text Classification', 'api_call': \"model = torch.hub.load(repo_or_dir='pytorch/fairseq', model='roberta.large', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained RoBERTa model for text classification from PyTorch Hub, which is a robustly optimized version of BERT, capable of efficiently classifying texts using transfer learning.\", 'code': 'import torch\nmodel = torch.hub.load(repo_or_dir=\"pytorch/fairseq\", model=\"roberta.large\", pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/fairseq', model='roberta.large', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "RoBERTa", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='pytorch/fairseq', model='roberta.large', pretrained=True)", "api_arguments": ["'pytorch/fairseq'", "'roberta.large'"], "python_environment_requirements": ["regex", "requests", "hydra-core", "omegaconf"], "example_code": ["import torch", "roberta = torch.hub.load('pytorch/fairseq', 'roberta.large')", "roberta.eval()", "tokens = roberta.encode('Hello world!')", "last_layer_features = roberta.extract_features(tokens)", "all_layers = roberta.extract_features(tokens, return_all_hiddens=True)", "roberta = torch.hub.load('pytorch/fairseq', 'roberta.large.mnli')", "roberta.eval()", "tokens = roberta.encode('Roberta is a heavily optimized version of BERT.', 'Roberta is not very optimized.')", "prediction = roberta.predict('mnli', tokens).argmax().item()", "tokens = roberta.encode('Roberta is a heavily optimized version of BERT.', 'Roberta is based on BERT.')", "prediction = roberta.predict('mnli', tokens).argmax().item()", "roberta.register_classification_head('new_task', num_classes=3)", "logprobs = roberta.predict('new_task', tokens)"], "performance": {"dataset": "MNLI", "accuracy": "N/A"}, "description": "RoBERTa is a robustly optimized version of BERT, a revolutionary self-supervised pretraining technique that learns to predict intentionally hidden (masked) sections of text. RoBERTa builds on BERT's language masking strategy and modifies key hyperparameters, including removing BERT's next-sentence pretraining objective, and training with much larger mini-batches and learning rates. RoBERTa was also trained on an order of magnitude more data than BERT, for a longer amount of time, allowing it to generalize even better to downstream tasks."}}
{"code": "{'Instruction': Suggest an API able to classify images with a high level of accuracy, using the fewest possible parameters., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'densenet201', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained DenseNet-201 model from PyTorch Hub, which efficiently classifies images with high accuracy and fewer parameters.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'densenet201', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet201', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Dense Convolutional Network", "api_name": "Densenet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet201', pretrained=True)", "api_arguments": [{"name": "densenet201", "type": "str", "description": "Densenet-201 model"}], "python_environment_requirements": {"torch": "latest", "torchvision": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet201', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"densenet201": {"Top-1 error": 22.8, "Top-5 error": 6.43}}}, "description": "Dense Convolutional Network (DenseNet) connects each layer to every other layer in a feed-forward fashion. It alleviates the vanishing-gradient problem, strengthens feature propagation, encourages feature reuse, and substantially reduces the number of parameters."}}
{"code": "{'Instruction': I need a model for classifying images, and I want to use Densenet-169. Give me an API to use., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'densenet169', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pre-trained DenseNet-169 model from PyTorch Hub for image classification.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'densenet169', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet169', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Dense Convolutional Network", "api_name": "Densenet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet169', pretrained=True)", "api_arguments": [{"name": "densenet169", "type": "str", "description": "Densenet-169 model"}], "python_environment_requirements": {"torch": "latest", "torchvision": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet169', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"densenet169": {"Top-1 error": 24.0, "Top-5 error": 7.0}}}, "description": "Dense Convolutional Network (DenseNet) connects each layer to every other layer in a feed-forward fashion. It alleviates the vanishing-gradient problem, strengthens feature propagation, encourages feature reuse, and substantially reduces the number of parameters."}}
{"code": "{'Instruction': Design an image classification model to recognize different objects in images. Recommend an API appropriate for this task., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'densenet161', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained DenseNet-161 model from PyTorch Hub for image classification tasks, which can recognize various objects and categories in images.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'densenet161', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet161', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Dense Convolutional Network", "api_name": "Densenet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet161', pretrained=True)", "api_arguments": [{"name": "densenet161", "type": "str", "description": "Densenet-161 model"}], "python_environment_requirements": {"torch": "latest", "torchvision": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet161', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"densenet161": {"Top-1 error": 22.35, "Top-5 error": 6.2}}}, "description": "Dense Convolutional Network (DenseNet) connects each layer to every other layer in a feed-forward fashion. It alleviates the vanishing-gradient problem, strengthens feature propagation, encourages feature reuse, and substantially reduces the number of parameters."}}
{"code": "{'Instruction': Find a suitable API to perform semantic segmentation of an aerial image into building, road, and vegetation., 'Output': {'domain': 'Semantic Segmentation', 'api_call': 'model = torch.hub.load('pytorch/vision', 'fcn_resnet101', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained FCN-ResNet101 model from PyTorch Hub to segment aerial images into building, road, and vegetation. You will need to fine-tune the model on an appropriate dataset for this task.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'fcn_resnet101', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='fcn_resnet101', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "Fully-Convolutional Network", "api_name": "fcn_resnet101", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='fcn_resnet101', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "boolean", "description": "If True, returns a model pre-trained on COCO train2017"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "matplotlib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'fcn_resnet101', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "input_image = input_image.convert(\"RGB\")", "preprocess = transforms.Compose([", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)['out'][0]", "output_predictions = output.argmax(0)"], "performance": {"dataset": "COCO val2017", "accuracy": {"fcn_resnet101": {"Mean IOU": 63.7, "Global Pixelwise Accuracy": 91.9}}}, "description": "FCN-ResNet is a Fully-Convolutional Network model using a ResNet-50 or a ResNet-101 backbone. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset."}}
{"code": "{'Instruction': Recommend an API that can classify images using a fast and efficient model., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet85', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained HarDNet (Harmonic DenseNet) model from PyTorch Hub to quickly and efficiently classify images.', 'code': 'import torch\nmodel = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet85', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet85', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "HarDNet", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet85', pretrained=True)", "api_arguments": [{"name": "hardnet85", "type": "str", "description": "HarDNet-85 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet85', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"hardnet85": {"Top-1 error": 21.96, "Top-5 error": 6.11}}}, "description": "Harmonic DenseNet (HarDNet) is a low memory traffic CNN model, which is fast and efficient. The basic concept is to minimize both computational cost and memory access cost at the same time, such that the HarDNet models are 35% faster than ResNet running on GPU comparing to models with the same accuracy (except the two DS models that were designed for comparing with MobileNet)."}}
{"code": "{'Instruction': Can you suggest an efficient API for classifying a large collection of images that minimizes computational cost?, 'Output': {'domain': 'Image Classification', 'api_call': 'model = torch.hub.load('huawei-noah/ghostnet', 'ghostnet_1x', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained GhostNet model from PyTorch Hub, which is designed for efficient image classification with minimal computational cost.', 'code': 'import torch\nmodel = torch.hub.load('huawei-noah/ghostnet', 'ghostnet_1x', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/ghostnet', model='ghostnet_1x', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Efficient networks by generating more features from cheap operations", "api_name": "GhostNet", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/ghostnet', model='ghostnet_1x', pretrained=True)", "api_arguments": ["pretrained"], "python_environment_requirements": ["torch", "torchvision", "PIL"], "example_code": ["import torch", "model = torch.hub.load('huawei-noah/ghostnet', 'ghostnet_1x', pretrained=True)", "model.eval()", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)", "print(probabilities)"], "performance": {"dataset": "ImageNet", "accuracy": {"Top-1 acc": "73.98", "Top-5 acc": "91.46"}}, "description": "The GhostNet architecture is based on an Ghost module structure which generates more features from cheap operations. Based on a set of intrinsic feature maps, a series of cheap operations are applied to generate many ghost feature maps that could fully reveal information underlying intrinsic features. Experiments conducted on benchmarks demonstrate the superiority of GhostNet in terms of speed and accuracy tradeoff."}}
{"code": "{'Instruction': Identify an API that can classify flora and fauna species by analyzing images., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet39ds', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained HarDNet model from PyTorch Hub, which can be fine-tuned for classifying flora and fauna species based on input images.\", 'code': 'import torch\nmodel = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet39ds', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet39ds', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "HarDNet", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet39ds', pretrained=True)", "api_arguments": [{"name": "hardnet39ds", "type": "str", "description": "HarDNet-39DS model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet39ds', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"hardnet39ds": {"Top-1 error": 27.92, "Top-5 error": 9.57}}}, "description": "Harmonic DenseNet (HarDNet) is a low memory traffic CNN model, which is fast and efficient. The basic concept is to minimize both computational cost and memory access cost at the same time, such that the HarDNet models are 35% faster than ResNet running on GPU comparing to models with the same accuracy (except the two DS models that were designed for comparing with MobileNet)."}}
{"code": "{'Instruction': A robotics company wants to reduce the computational and memory cost of an image classification model for mobile robot applications. Suggest an API suited for this., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet68ds', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained HarDNet-68DS model from PyTorch Hub, as it is designed for mobile applications and offers low computational and memory cost.', 'code': 'import torch\nmodel = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet68ds', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet68ds', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "HarDNet", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet68ds', pretrained=True)", "api_arguments": [{"name": "hardnet68ds", "type": "str", "description": "HarDNet-68DS model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet68ds', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"hardnet68ds": {"Top-1 error": 25.71, "Top-5 error": 8.13}}}, "description": "Harmonic DenseNet (HarDNet) is a low memory traffic CNN model, which is fast and efficient. The basic concept is to minimize both computational cost and memory access cost at the same time, such that the HarDNet models are 35% faster than ResNet running on GPU comparing to models with the same accuracy (except the two DS models that were designed for comparing with MobileNet)."}}
{"code": "{'Instruction': I need an API to help identify dog breeds in images. Recommend one with a light model that is fast in processing the images., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet68', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained HarDNet-68 model from PyTorch Hub, known for its fast and efficient image classification. You can fine-tune it to identify dog breeds from images.', 'code': 'import torch\nmodel = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet68', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet68', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "HarDNet", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet68', pretrained=True)", "api_arguments": [{"name": "hardnet68", "type": "str", "description": "HarDNet-68 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet68', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"hardnet68": {"Top-1 error": 23.52, "Top-5 error": 6.99}}}, "description": "Harmonic DenseNet (HarDNet) is a low memory trafficCNN model, which is fast and efficient. The basic concept is to minimize both computational cost and memory access cost at the same time, such that the HarDNet models are 35% faster than ResNet running on GPU comparing to models with the same accuracy (except the two DS models that were designed for comparing with MobileNet)."}}
{"code": "{'Instruction': Find an API suitable for cross-domain image classification with high accuracy., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('XingangPan/IBN-Net', 'resnext101_ibn_a', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNeXt-101-IBN-a model from PyTorch Hub, which is suitable for cross-domain image classification tasks and has high accuracy.', 'code': 'import torch\nmodel = torch.hub.load('XingangPan/IBN-Net', 'resnext101_ibn_a', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnext101_ibn_a', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "IBN-Net", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnext101_ibn_a', pretrained=True)", "api_arguments": [{"name": "resnext101_ibn_a", "type": "str", "description": "ResNeXt-101-IBN-a model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('XingangPan/IBN-Net', 'resnext101_ibn_a', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"resnext101_ibn_a": {"Top-1 acc": 79.12, "Top-5 acc": 94.58}}}, "description": "IBN-Net is a CNN model with domain/appearance invariance. Motivated by style transfer works, IBN-Net carefully unifies instance normalization and batch normalization in a single deep network. It provides a simple way to increase both modeling and generalization capacities without adding model complexity. IBN-Net is especially suitable for cross domain or person/vehicle re-identification tasks."}}
{"code": "{'Instruction': Recommend an API that can create models having domain or appearance invariance., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('XingangPan/IBN-Net', 'resnet101_ibn_a', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained IBN-Net (ResNet-101-IBN-a) model from PyTorch Hub for creating models with domain or appearance invariance.', 'code': 'import torch\nmodel = torch.hub.load('XingangPan/IBN-Net', 'resnet101_ibn_a', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnet101_ibn_a', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "IBN-Net", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnet101_ibn_a', pretrained=True)", "api_arguments": [{"name": "resnet101_ibn_a", "type": "str", "description": "ResNet-101-IBN-a model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('XingangPan/IBN-Net', 'resnet101_ibn_a', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"resnet101_ibn_a": {"Top-1 acc": 78.61, "Top-5 acc": 94.41}}}, "description": "IBN-Net is a CNN model with domain/appearance invariance. Motivated by style transfer works, IBN-Net carefully unifies instance normalization and batch normalization in a single deep network. It provides a simple way to increase both modeling and generalization capacities without adding model complexity. IBN-Net is especially suitable for cross domain or person/vehicle re-identification tasks."}}
{"code": "{'Instruction': I need to re-identify a person in a different set of images, suggest an API I can use to make this process easier., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('XingangPan/IBN-Net', 'resnet50_ibn_a', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNet-50-IBN-a model from PyTorch Hub, which is designed for tasks like person re-identification, due to its domain/appearance invariance.', 'code': 'import torch\nmodel = torch.hub.load('XingangPan/IBN-Net', 'resnet50_ibn_a', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnet50_ibn_a', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "IBN-Net", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnet50_ibn_a', pretrained=True)", "api_arguments": [{"name": "resnet50_ibn_a", "type": "str", "description": "ResNet-50-IBN-a model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('XingangPan/IBN-Net', 'resnet50_ibn_a', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"resnet50_ibn_a": {"Top-1 acc": 77.46, "Top-5 acc": 93.68}}}, "description": "IBN-Net is a CNN model with domain/appearance invariance. Motivated by style transfer works, IBN-Net carefully unifies instance normalization and batch normalization in a single deep network. It provides a simple way to increase both modeling and generalization capacities without adding model complexity. IBN-Net is especially suitable for cross domain or person/vehicle re-identification tasks."}}
{"code": "{'Instruction': Find me an API for classifying different breeds of dogs from their photos., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'inception_v3', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Inception v3 model from PyTorch Hub, which can be fine-tuned for dog breed classification from images.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'inception_v3', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='inception_v3', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "Inception_v3", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='inception_v3', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": "1.9.0", "torchvision": "0.10.0"}, "example_code": {"import_libraries": "import torch", "load_model": "model = torch.hub.load('pytorch/vision:v0.10.0', 'inception_v3', pretrained=True)", "model_evaluation": "model.eval()"}, "performance": {"dataset": "imagenet", "accuracy": {"top-1_error": 22.55, "top-5_error": 6.44}}, "description": "Inception v3, also called GoogleNetv3, is a famous Convolutional Neural Network trained on the ImageNet dataset from 2015. It is based on the exploration of ways to scale up networks to utilize the added computation as efficiently as possible by using suitably factorized convolutions and aggressive regularization. The model achieves a top-1 error of 22.55% and a top-5 error of 6.44% on the ImageNet dataset."}}
{"code": "{'Instruction': My manager just asked me to implement a classification system that can categorize pictures into different animal species. Can you recommend an API that can help me with this task?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained ResNeSt model from PyTorch Hub, which can be fine-tuned for the classification task of categorizing pictures into different animal species.\", 'code': 'import torch\nmodel = torch.hub.load('zhanghang1989/ResNeSt', 'resnest50', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNeSt", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest50', pretrained=True)", "api_arguments": "resnest50", "python_environment_requirements": {"torch": "1.0.0", "torchvision": "0.2.2"}, "example_code": ["import torch", "model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest50', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeSt-50": 81.03}}, "description": "ResNeSt models are from the ResNeSt: Split-Attention Networks paper. They are a new ResNet variant that enables attention across feature-map groups. By stacking Split-Attention blocks ResNet-style, ResNeSt models outperform other networks with similar model complexities, and also help downstream tasks including object detection, instance segmentation, and semantic segmentation."}}
{"code": "{'Instruction': Detect the type of tree species in a park from an image taken by a smartphone. Provide an API that can help with this classification problem., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest101', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNeSt-101 model from PyTorch Hub, which can be fine-tuned for tree species classification from smartphone images.', 'code': 'import torch\nmodel = torch.hub.load('zhanghang1989/ResNeSt', 'resnest101', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest101', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNeSt", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest101', pretrained=True)", "api_arguments": "resnest101", "python_environment_requirements": {"torch": "1.0.0", "torchvision": "0.2.2"}, "example_code": ["import torch", "model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest101', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeSt-101": 82.83}}, "description": "ResNeSt models are from the ResNeSt: Split-Attention Networks paper. They are a new ResNet variant that enables attention across feature-map groups. By stacking Split-Attention blocks ResNet-style, ResNeSt models outperform other networks with similar model complexities, and also help downstream tasks including object detection, instance segmentation, and semantic segmentation."}}
{"code": "{'Instruction': Many people loves to create different memes. Propose an API that can use keywords from meme text to detect if a meme is negative or not., 'Output': {'domain': 'Classification', 'api_call': 'model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest200', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNeSt model from PyTorch Hub, which can be fine-tuned for meme sentiment classification based on text keywords.', 'code': 'import torch\nmodel = torch.hub.load('zhanghang1989/ResNeSt', 'resnest200', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest200', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNeSt", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest200', pretrained=True)", "api_arguments": "resnest200", "python_environment_requirements": {"torch": "1.0.0", "torchvision": "0.2.2"}, "example_code": ["import torch", "model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest200', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeSt-200": 83.84}}, "description": "ResNeSt models are from the ResNeSt: Split-Attention Networks paper. They are a new ResNet variant that enables attention across feature-map groups. By stacking Split-Attention blocks ResNet-style, ResNeSt models outperform other networks with similar model complexities, and also help downstream tasks including object detection, instance segmentation, and semantic segmentation."}}
{"code": "{'Instruction': Can you find an API that can identify objects in images?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest269', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNeSt-269 model from PyTorch Hub for image classification, which can identify objects within images.', 'code': 'import torch\nmodel = torch.hub.load('zhanghang1989/ResNeSt', 'resnest269', pretrained=True)\nmodel.eval()'}}", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest269', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNeSt", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest269', pretrained=True)", "api_arguments": "resnest269", "python_environment_requirements": {"torch": "1.0.0", "torchvision": "0.2.2"}, "example_code": ["import torch", "model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest269', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeSt-269": 84.54}}, "description": "ResNeSt models are from the ResNeSt: Split-Attention Networks paper. They are a new ResNet variant that enables attention across feature-map groups. By stacking Split-Attention blocks ResNet-style, ResNeSt models outperform other networks with similar model complexities, and also help downstream tasks including object detection, instance segmentation, and semantic segmentation."}}
{"code": "{'Instruction': Tell me an API that can classify a given image into one of the predefined categories., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'resnet18', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load a pre-trained ResNet18 model from PyTorch Hub for image classification, which can classify an image into one of the pre-defined categories.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'resnet18', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet18', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Deep Residual Networks", "api_name": "ResNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet18', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "bool", "default": "False", "description": "If True, returns a model pre-trained on ImageNet"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "resnet18", "top-1_error": 30.24, "top-5_error": 10.92}}, "description": "ResNet models are deep residual networks pre-trained on ImageNet. They were proposed in the paper 'Deep Residual Learning for Image Recognition'. Available model variants include ResNet18, ResNet34, ResNet50, ResNet101, and ResNet152."}}
{"code": "{'Instruction': I want to perform classification on my dataset with a model that is optimized for a CPU. Suggest me an API., 'Output': {'domain': 'Classification', 'api_call': 'model = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_cpu', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Use the ProxylessNAS model optimized for CPU from PyTorch Hub for efficient classification on your dataset.', 'code': 'import torch\nmodel = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_cpu', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_cpu', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "ProxylessNAS", "api_name": "mit-han-lab/ProxylessNAS", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_cpu', pretrained=True)", "api_arguments": [{"name": "proxylessnas_cpu", "type": "str", "description": "ProxylessNAS optimized for CPU"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "target_platform = 'proxyless_cpu'", "model = torch.hub.load('mit-han-lab/ProxylessNAS', target_platform, pretrained=True)", "model.eval()"], "performance": {"dataset": [{"model_structure": "proxylessnas_cpu", "accuracy": 75.3}]}, "description": "ProxylessNAS models are from the ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware paper. They specialize CNN architectures for different hardware platforms, offering free yet significant performance boost on all three platforms (CPU, GPU, and Mobile) with similar accuracy."}}
{"code": "{'Instruction': I want to classify images on my mobile efficiently. Suggest an API to achieve this., 'Output': {'domain': 'Image Classification', 'api_call': 'model = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_mobile', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ProxylessNAS Mobile model from PyTorch Hub, which is optimized for high-efficiency image classification on mobile devices.', 'code': 'import torch\nmodel = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_mobile', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_mobile', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "ProxylessNAS", "api_name": "mit-han-lab/ProxylessNAS", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_mobile', pretrained=True)", "api_arguments": [{"name": "proxylessnas_mobile", "type": "str", "description": "ProxylessNAS optimized for Mobile"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "target_platform = 'proxyless_mobile'", "model = torch.hub.load('mit-han-lab/ProxylessNAS', target_platform, pretrained=True)", "model.eval()"], "performance": {"dataset": [{"model_structure": "proxylessnas_mobile", "accuracy": 74.6}]}, "description": "ProxylessNAS models are from the ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware paper. They specialize CNN architectures for different hardware platforms, offering free yet significant performance boost on all three platforms (CPU, GPU, and Mobile) with similar accuracy."}}
{"code": "{'Instruction': A company needs to classify a large number of photos by their content. Suggest an API that is optimized for GPU and can perform classifications efficiently., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_gpu', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ProxylessNAS GPU-optimized model from PyTorch Hub, which can efficiently classify images using a GPU.', 'code': 'import torch\nmodel = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_gpu', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_gpu', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "ProxylessNAS", "api_name": "mit-han-lab/ProxylessNAS", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_gpu', pretrained=True)", "api_arguments": [{"name": "proxylessnas_gpu", "type": "str", "description": "ProxylessNAS optimized for GPU"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "target_platform = 'proxyless_gpu'", "model = torch.hub.load('mit-han-lab/ProxylessNAS', target_platform, pretrained=True)", "model.eval()"], "performance": {"dataset": [{"model_structure": "proxylessnas_gpu", "accuracy": 75.1}]}, "description": "ProxylessNAS models are from the ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware paper. They specialize CNN architectures for different hardware platforms, offering free yet significant performance boost on all three platforms (CPU, GPU, and Mobile) with similar accuracy."}}
{"code": "{'Instruction': I need to create an app that can analyze food image and can tell its catagory like pizza, burger, etc. Write an API that can help me in achieving this., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'mobilenet_v2', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MobileNet v2 model from PyTorch Hub, which can be fine-tuned for food classification given an input image of food items like pizza, burger, etc.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'mobilenet_v2', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='mobilenet_v2', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MobileNet v2", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='mobilenet_v2', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision', 'mobilenet_v2', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)"], "performance": {"dataset": "ImageNet", "accuracy": {"top-1_error": 28.12, "top-5_error": 9.71}}, "description": "The MobileNet v2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input. MobileNet v2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, non-linearities in the narrow layers were removed in order to maintain representational power."}}
{"code": "{'Instruction': Design me an API for classifying images using a pretrained model., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'resnet34', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pre-trained ResNet34 model from PyTorch Hub for image classification.\", 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'resnet34', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet34', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Deep Residual Networks", "api_name": "ResNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet34', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "bool", "default": "False", "description": "If True, returns a model pre-trained on ImageNet"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet34', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "resnet34", "top-1_error": 26.7, "top-5_error": 8.58}}, "description": "ResNet models are deep residual networks pre-trained on ImageNet. They were proposed in the paper 'Deep Residual Learning for Image Recognition'. Available model variants include ResNet18, ResNet34, ResNet50, ResNet101, and ResNet152."}}
{"code": "{'Instruction': Can you customize an API that can detect eco-friendly packaging from photos?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'resnet101', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained ResNet101 model from PyTorch Hub, which can be fine-tuned for eco-friendly packaging detection from photos.\", 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'resnet101', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet101', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Deep Residual Networks", "api_name": "ResNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet101', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "bool", "default": "False", "description": "If True, returns a model pre-trained on ImageNet"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet101', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "resnet101", "top-1_error": 22.63, "top-5_error": 6.44}}, "description": "ResNet models are deep residual networks pre-trained on ImageNet. They were proposed in the paper 'Deep Residual Learning for Image Recognition'. Available model variants include ResNet18, ResNet34, ResNet50, ResNet101, and ResNet152."}}
{"code": "{'Instruction': Identify an API that can classify types of dogs from a given image., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'resnet152', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNet152 model from PyTorch Hub, which can be fine-tuned for dog breed classification.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'resnet152', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet152', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Deep Residual Networks", "api_name": "ResNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet152', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "bool", "default": "False", "description": "If True, returns a model pre-trained on ImageNet"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet152', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "resnet152", "top-1_error": 21.69, "top-5_error": 5.94}}, "description": "ResNet models are deep residual networks pre-trained on ImageNet. They were proposed in the paper 'Deep Residual Learning for Image Recognition'. Available model variants include ResNet18, ResNet34, ResNet50, ResNet101, and ResNet152."}}
{"code": "{'Instruction': Can you suggest an API that will help a company build an image classification model for their photo sharing app to identify objects in the photos?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'resnext101_32x4d', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNext101 model from PyTorch Hub for image classification to identify objects in the photos.', 'code': 'import torch\nmodel = torch.hub.load(\"pytorch/vision\", \"resnext101_32x4d\", pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnext101_32x4d', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNext", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnext101_32x4d', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "PIL"], "example_code": ["import torch", "from PIL import Image", "from torchvision import transforms", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnext101_32x4d', pretrained=True)", "model.eval()", "input_image = Image.open('dog.jpg')", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)"], "performance": {"dataset": "ImageNet", "resnext101_32x8d": {"top-1": 20.69, "top-5": 5.47}}, "description": "ResNext is a next-generation ResNet architecture for image classification. It is more efficient and accurate than the original ResNet. This implementation includes two versions of the model, resnext50_32x4d and resnext101_32x8d, with 50 and 101 layers respectively."}}
{"code": "{'Instruction': I'm working on a computer vision project that requires a deep learning model for image classification. Can you provide an API that is efficient and accurate for this task?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'resnext50_32x4d', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the ResNext model from PyTorch Hub which is efficient and accurate for image classification tasks. Specifically, the \"resnext50_32x4d\" model is pretrained on ImageNet.', 'code': \"import torch\nmodel = torch.hub.load('pytorch/vision', 'resnext50_32x4d', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnext50_32x4d', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNext", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnext50_32x4d', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "PIL"], "example_code": ["import torch", "from PIL import Image", "from torchvision import transforms", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnext50_32x4d', pretrained=True)", "model.eval()", "input_image = Image.open('dog.jpg')", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)"], "performance": {"dataset": "ImageNet", "accuracy": {"resnext50_32x4d": {"top-1": 22.38, "top-5": 6.3}}}, "description": "ResNext is a next-generation ResNet architecture for image classification. It is more efficient and accurate than the original ResNet. This implementation includes two versions of the model, resnext50_32x4d and resnext101_32x8d, with 50 and 101 layers respectively."}}
{"code": "{'Instruction': Find me an API to classify clothing items from a real-time camera feed., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_t', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained SNNMLP Tiny model from PyTorch Hub. It can be fine-tuned for clothing item classification from a real-time camera feed.', 'code': 'import torch\nmodel = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_t', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_t', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SNNMLP", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_t', pretrained=True)", "api_arguments": [{"name": "snnmlp_t", "type": "str", "description": "SNNMLP Tiny model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_t', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "SNNMLP Tiny", "top-1": 81.88}}, "description": "SNNMLP incorporates the mechanism of LIF neurons into the MLP models, to achieve better accuracy without extra FLOPs. We propose a full-precision LIF operation to communicate between patches, including horizontal LIF and vertical LIF in different directions. We also propose to use group LIF to extract better local features. With LIF modules, our SNNMLP model achieves 81.9%, 83.3% and 83.6% top-1 accuracy on ImageNet dataset with only 4.4G, 8.5G and 15.2G FLOPs, respectively."}}
{"code": "{'Instruction': I want to classify everyday objects in images. Is there an AI model that can help me to do this?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_s', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained SNNMLP model from PyTorch Hub to classify everyday objects in images with high accuracy.', 'code': 'import torch\nmodel = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_s', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_s', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SNNMLP", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_s', pretrained=True)", "api_arguments": [{"name": "snnmlp_s", "type": "str", "description": "SNNMLP Small model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_s', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "SNNMLP Small", "top-1": 83.3}}, "description": "SNNMLP incorporates the mechanism of LIF neurons into the MLP models, to achieve better accuracy without extra FLOPs. We propose a full-precision LIF operation to communicate between patches, including horizontal LIF and vertical LIF in different directions. We also propose to use group LIF to extract better local features. With LIF modules, our SNNMLP model achieves 81.9%, 83.3% and 83.6% top-1 accuracy on ImageNet dataset with only 4.4G, 8.5G and 15.2G FLOPs, respectively."}}
{"code": "{'Instruction': Help me find an image classifier model that can be used in a mobile application to classify everyday objects from images. The model should be light and efficient., 'Output': {'domain': 'Image Classification', 'api_call': 'model = torch.hub.load(\"huawei-noah/Efficient-AI-Backbones\", \"snnmlp_b\", pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained SNNMLP Base model for efficient image classification from PyTorch Hub for recognizing everyday objects.', 'code': 'import torch\nmodel = torch.hub.load(\"huawei-noah/Efficient-AI-Backbones\", \"snnmlp_b\", pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_b', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SNNMLP", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_b', pretrained=True)", "api_arguments": [{"name": "snnmlp_b", "type": "str", "description": "SNNMLP Base model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_b', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "SNNMLP Base", "top-1": 85.59}}, "description": "SNNMLP incorporates the mechanism of LIF neurons into the MLP models, to achieve better accuracy without extra FLOPs. We propose a full-precision LIF operation to communicate between patches, including horizontal LIF and vertical LIF in different directions. We also propose to use group LIF to extract better local features. With LIF modules, our SNNMLP model achieves 81.9%, 83.3% and 83.6% top-1 accuracy on ImageNet dataset with only 4.4G, 8.5G and 15.2G FLOPs, respectively."}}
{"code": "{'Instruction': Can you suggest an API that can classify objects in images with a high level of accuracy but maintaining small model size and low computational cost?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'squeezenet1_0', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Use the SqueezeNet model from PyTorch Hub, which provides high image classification accuracy with reduced model size and low computational cost.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'squeezenet1_0', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='squeezenet1_0', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SqueezeNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='squeezenet1_0', pretrained=True)", "api_arguments": {"version": "v0.10.0", "model": ["squeezenet1_0"], "pretrained": "True"}, "python_environment_requirements": {"torch": ">=1.9.0", "torchvision": ">=0.10.0"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'squeezenet1_0', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)", "print(probabilities)"], "performance": {"dataset": "ImageNet", "accuracy": {"squeezenet1_0": {"Top-1 error": 41.9, "Top-5 error": 19.58}}}, "description": "SqueezeNet is an image classification model that achieves AlexNet-level accuracy with 50x fewer parameters. It has two versions: squeezenet1_0 and squeezenet1_1, with squeezenet1_1 having 2.4x less computation and slightly fewer parameters than squeezenet1_0, without sacrificing accuracy."}}
{"code": "{'Instruction': Is there any image classification API that I can use for tiny object, e.g., fruits in the market images, to save memory on my smartphone application?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'squeezenet1_1', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained SqueezeNet 1.1 model from PyTorch Hub, which consumes less memory and provides good accuracy suitable for small objects such as fruits in market images.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'squeezenet1_1', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='squeezenet1_1', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SqueezeNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='squeezenet1_1', pretrained=True)", "api_arguments": {"version": "v0.10.0", "model": ["squeezenet1_1"], "pretrained": "True"}, "python_environment_requirements": {"torch": ">=1.9.0", "torchvision": ">=0.10.0"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'squeezenet1_1', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)", "print(probabilities)"], "performance": {"dataset": "ImageNet", "accuracy": {"squeezenet1_1": {"Top-1 error": 41.81, "Top-5 error": 19.38}}}, "description": "SqueezeNet is an image classification model that achieves AlexNet-level accuracy with 50x fewer parameters. It has two versions: squeezenet1_0 and squeezenet1_1, with squeezenet1_1 having 2.4x less computation and slightly fewer parameters than squeezenet1_0, without sacrificing accuracy."}}
{"code": "{'Instruction': Help me recognize the objects present in an image. Recommend an API that can do this., 'Output': {'domain': 'Image Recognition', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg11_bn', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained VGG11 model with batch normalization from PyTorch Hub to recognize objects in an image.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg11_bn', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg11_bn', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg11_bn', pretrained=True)", "api_arguments": [{"name": "vgg11_bn", "type": "str", "description": "VGG11 model with batch normalization"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg11_bn', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg11_bn": {"Top-1 error": 26.7, "Top-5 error": 8.58}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': I need a way to efficiently detect and categorize various types of plants and animals in images taken from a nature reserve, could you provide an API that could help me with this task using a pretrained model?, 'Output': {'domain': 'Classification', 'api_call': 'torch.hub.load(repo_or_dir='pytorch/vision', model='vgg11', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained VGG11 model from PyTorch Hub, which can be fine-tuned for plant and animal classification in nature reserve images.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg11', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg11', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg11', pretrained=True)", "api_arguments": [{"name": "vgg11", "type": "str", "description": "VGG11 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg11', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg11": {"Top-1 error": 30.98, "Top-5 error": 11.37}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': I'm looking for an API that recognizes different objects in images. Can you recommend one?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg13', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained VGG13 model from PyTorch Hub for image recognition tasks, as it's capable of recognizing different objects in images.\", 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg13', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg13', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg13', pretrained=True)", "api_arguments": [{"name": "vgg13", "type": "str", "description": "VGG13 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg13', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg13": {"Top-1 error": 30.07, "Top-5 error": 10.75}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn,vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': Help me classify species of birds from images using an API call., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg16', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained VGG16 model from PyTorch Hub, which can be fine-tuned for bird species classification from images.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg16', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg16', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg16', pretrained=True)", "api_arguments": [{"name": "vgg16", "type": "str", "description": "VGG16 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg16": {"Top-1 error": 28.41, "Top-5 error": 9.62}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': Help me identify the objects in a photo using an API that is pretrained with many different classes., 'Output': {'domain': 'Image Recognition', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg16_bn', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained VGG16 model with batch normalization from the PyTorch Hub to recognize objects in a given photo.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg16_bn', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg16_bn', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg16_bn', pretrained=True)", "api_arguments": [{"name": "vgg16_bn", "type": "str", "description": "VGG16 model with batch normalization"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16_bn', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg16_bn": {"Top-1 error": 26.63, "Top-5 error": 8.5}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': Our application identifies objects in images. What is the best image recognition API from torchvision for our case?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg19', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained VGG19 model from torchvision for image recognition tasks. VGG19 is a popular and effective model for large-scale image recognition.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg19', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg19', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg19', pretrained=True)", "api_arguments": [{"name": "vgg19", "type": "str", "description": "VGG19 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg19', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg19": {"Top-1 error": 27.62, "Top-5 error": 9.12}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': Recommend an API that can recognize animals in an image and classify them into categories like pets, wildlife, and aquatic animals., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('kadhiravanarasu/project-zoo', 'model', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load a custom-trained model for recognizing and classifying animals in images into pets, wildlife, and aquatic animals.', 'code': 'import torch\nmodel = torch.hub.load('kadhiravanarasu/project-zoo', 'model', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg19_bn', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg19_bn', pretrained=True)", "api_arguments": [{"name": "vgg19_bn", "type": "str", "description": "VGG19 model with batch normalization"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg19_bn', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg19_bn": {"Top-1 error": 25.76, "Top-5 error": 8.15}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': What is an API that can classify photos of foods into their meal category?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'wide_resnet50_2', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Wide ResNet50_2 model from PyTorch Hub, which can be fine-tuned for food image classification into meal categories.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'wide_resnet50_2', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='wide_resnet50_2', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Wide Residual Networks", "api_name": "wide_resnet50_2", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='wide_resnet50_2', pretrained=True)", "api_arguments": "pretrained", "python_environment_requirements": "torch, torchvision", "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'wide_resnet50_2', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"wide_resnet50_2": {"Top-1 error": 21.49, "Top-5 error": 5.91}}}, "description": "Wide Residual networks simply have increased number of channels compared to ResNet. Otherwise the architecture is the same. Deeper ImageNet models with bottleneck block have increased number of channels in the inner 3x3 convolution. The wide_resnet50_2 and wide_resnet101_2 models were trained in FP16 with mixed precision training using SGD with warm restarts. Checkpoints have weights in half precision (except batch norm) for smaller size, and can be used in FP32 models too."}}
{"code": "{'Instruction': Find me a powerful API that can classify dozens of classes from an image taken., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'wide_resnet101_2', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Wide ResNet-101 model from PyTorch Hub, a powerful image classification model that can classify numerous classes.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'wide_resnet101_2', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='wide_resnet101_2', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Wide Residual Networks", "api_name": "wide_resnet101_2", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='wide_resnet101_2', pretrained=True)", "api_arguments": "pretrained", "python_environment_requirements": "torch, torchvision", "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'wide_resnet101_2', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"wide_resnet101_2": {"Top-1 error": 21.16, "Top-5 error": 5.72}}}, "description": "Wide Residual networks simply have increased number of channels compared to ResNet. Otherwise the architecture is the same. Deeper ImageNet models with bottleneck block have increased number of channels in the inner 3x3 convolution. The wide_resnet50_2 and wide_resnet101_2 models were trained in FP16 with mixed precision training using SGD with warm restarts. Checkpoints have weights in half precision (except batch norm) for smaller size, and can be used in FP32 models too."}}
{"code": "{'Instruction': A sport streaming platform needs to detect clips of football plays for highlight creation. Recommend an API that can classify video clips into different action categories., 'Output': {'domain': 'Video Classification', 'api_call': \"model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)\", 'api_provider': 'PyTorchVideo', 'explanation': 'Load the pretrained SlowFast Networks model from PyTorchVideo Hub, which can classify video clips into various action categories, including football plays.', 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='slowfast_r50', pretrained=True)", "provider": "PyTorchVideo", "api_data": {"domain": "Video Classification", "framework": "PyTorchVideo", "functionality": "SlowFast Networks", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='slowfast_r50', pretrained=True)", "api_arguments": {"repository": "facebookresearch/pytorchvideo", "model": "slowfast_r50", "pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "pytorchvideo"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)", "device = 'cpu'", "model = model.eval()", "model = model.to(device)"], "performance": {"dataset": "Kinetics 400", "accuracy": {"top1": 76.94, "top5": 92.69}, "flops": 65.71, "params": 34.57}, "description": "Slow Fast model architectures are based on the paper 'SlowFast Networks for Video Recognition' by Christoph Feichtenhofer et al. They are pretrained on the Kinetics 400 dataset using the 8x8 setting. This model is capable of classifying video clips into different action categories. It is provided by the FAIR PyTorchVideo library."}}
{"code": "{'Instruction': I am developing a video platform and I need an API that can classify the type of sports played in the video., 'Output': {'domain': 'Video Classification', 'api_call': 'model = torch.hub.load('facebookresearch/pytorchvideo', 'x3d_s', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained X3D model for video classification from PyTorch Hub to classify video clips into different action categories, including various sports.', 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/pytorchvideo', 'x3d_s', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='x3d_s', pretrained=True)", "provider": "PyTorchVideo", "api_data": {"domain": "Video Classification", "framework": "PyTorchVideo", "functionality": "X3D Networks", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='x3d_s', pretrained=True)", "api_arguments": {"repository": "facebookresearch/pytorchvideo", "model": "x3d_s", "pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "pytorchvideo"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/pytorchvideo', 'x3d_s', pretrained=True)", "device = 'cpu'", "model = model.eval()", "model = model.to(device)"], "performance": {"dataset": "Kinetics 400", "accuracy": {"top1": 73.33, "top5": 91.27}, "flops": 2.96, "params": 3.79}, "description": "X3D model architectures are based on the paper 'X3D: Expanding Architectures for Efficient Video Recognition' by Christoph Feichtenhofer. They are pretrained on the Kinetics 400 dataset. This model is capable of classifying video clips into different action categories. It is provided by the FAIR PyTorchVideo library."}}
{"code": "{'Instruction': Recommend a machine learning API for classifying images with high inference speed and low latency on NVIDIA GPUs., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_gpunet', pretrained=True, model_type=model_type, model_math=precision)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained GPUNet model from NVIDIA Deep Learning Examples. It is optimized for NVIDIA GPUs and offers high inference speed and low latency.', 'code': \"import torch\nmodel_type = 'GPUNet-0'\nprecision = 'fp32'\nmodel = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_gpunet', pretrained=True, model_type=model_type, model_math=precision)\"}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_gpunet', pretrained=True, model_type=model_type, model_math=precision)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "GPUNet Networks", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_gpunet', pretrained=True, model_type=model_type, model_math=precision)", "api_arguments": {"repository": "NVIDIA/DeepLearningExamples:torchhub", "model": "nvidia_gpunet", "pretrained": "True", "model_type": "GPUNet-0", "model_math": "fp32"}, "python_environment_requirements": ["torch", "validators", "matplotlib", "timm==0.5.4"], "example_code": ["import torch", "model_type = 'GPUNet-0'", "precision = 'fp32'", "gpunet = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_gpunet', pretrained=True, model_type=model_type, model_math=precision)", "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')", "gpunet.to(device)", "gpunet.eval()"], "performance": {"dataset": "IMAGENET", "description": "GPUNet demonstrates state-of-the-art inference performance up to 2x faster than EfficientNet-X and FBNet-V3."}, "description": "GPUNet is a family of Convolutional Neural Networks designed by NVIDIA using novel Neural Architecture Search (NAS) methods. They are optimized for NVIDIA GPU and TensorRT performance. GPUNet models are pretrained on the IMAGENET dataset and are capable of classifying images into different categories. The models are provided by the NVIDIA Deep Learning Examples library."}}
{"code": "{'Instruction': Recommend an image classification API that can be used for classifying medical instruments in a biomedical dataset., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models', 'resnet18_swsl', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNet-18 model with semi-supervised and semi-weakly supervised ImageNet from PyTorch Hub, which can be fine-tuned for biomedical instrument classification.', 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models', 'resnet18_swsl', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/semi-supervised-ImageNet1K-models', model='resnet18_swsl', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Semi-supervised and semi-weakly supervised ImageNet Models", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/semi-supervised-ImageNet1K-models', model='resnet18_swsl', pretrained=True)", "api_arguments": {"repository": "facebookresearch/semi-supervised-ImageNet1K-models", "model": "resnet18_swsl", "pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models', 'resnet18_swsl', pretrained=True)", "model.eval()"], "performance": {"description": "Semi-supervised and semi-weakly supervised ImageNet models achieve state-of-the-art accuracy of 81.2% on ImageNet for the widely used/adopted ResNet-50 model architecture."}, "description": "Semi-supervised and semi-weakly supervised ImageNet Models are introduced in the 'Billion scale semi-supervised learning for image classification' paper. These models are pretrained on a subset of unlabeled YFCC100M public image dataset and fine-tuned with the ImageNet1K training dataset. They are capable of classifying images into different categories and are provided by the Facebook Research library."}}
{"code": "{'Instruction': There is a need at an NGO to develop a model that can analyze and classify the comments of the articles they publish. Tell me an API that can do this., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-cased')\", 'api_provider': 'PyTorch', 'explanation': 'Load the BERT model from the Hugging Face PyTorch-Transformers library. This model is capable of analyzing and classifying text, such as comments.', 'code': 'import torch\nmodel = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-cased)'}}", "api_call": "torch.hub.load(repo_or_dir='huggingface/pytorch-transformers')", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Natural Language Processing", "api_name": "PyTorch-Transformers", "api_call": "torch.hub.load(repo_or_dir='huggingface/pytorch-transformers')", "api_arguments": ["pretrained_model_or_path", "output_attention", "output_hidden_states", "config", "from_tf"], "python_environment_requirements": ["tqdm", "boto3", "requests", "regex", "sentencepiece", "sacremoses"], "example_code": "import torch\ntokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-cased')\n\ntext_1 = \"Jim Henson was a puppeteer\"\ntext_2 = \"Who was Jim Henson ?\"\n\nindexed_tokens = tokenizer.encode(text_1, text_2, add_special_tokens=True)\n\nmodel = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-cased')\n\nwith torch.no_grad():\n encoded_layers, _ = model(tokens_tensor, token_type_ids=segments_tensors)", "performance": {"dataset": [{"name": "MRPC", "accuracy": "Not provided"}]}, "description": "PyTorch-Transformers is a library of state-of-the-art pre-trained models for Natural Language Processing (NLP) including BERT, GPT, GPT-2, Transformer-XL, XLNet, XLM, RoBERTa, and DistilBERT. The library provides functionality for tokenization, configuration, and various model architectures for different tasks such as causal language modeling, sequence classification, question answering, and masked language modeling."}}
{"code": "{'Instruction': A freelance translator wants to automate their translation process from English to French. Can you suggest an API that can perform neural machine translation from English to French?, 'Output': {'domain': 'Text Translation', 'api_call': \"model = torch.hub.load('pytorch/fairseq', 'transformer.wmt14.en-fr', tokenizer='moses', bpe='subword_nmt')\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained Transformer model for neural machine translation from PyTorch Hub, which is specifically trained for translating English to French text.\", 'code': \"import torch\nmodel = torch.hub.load('pytorch/fairseq', 'transformer.wmt14.en-fr', tokenizer='moses', bpe='subword_nmt')\"}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/fairseq')", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "Neural Machine Translation", "api_name": "Transformer (NMT)", "api_call": "torch.hub.load(repo_or_dir='pytorch/fairseq')", "api_arguments": ["model_name", "tokenizer", "bpe", "beam", "sampling", "sampling_topk"], "python_environment_requirements": ["bitarray", "fastBPE", "hydra-core", "omegaconf", "regex", "requests", "sacremoses", "subword_nmt"], "example_code": "import torch\n\nen2fr = torch.hub.load('pytorch/fairseq', 'transformer.wmt14.en-fr', tokenizer='moses', bpe='subword_nmt')\n\nen2fr.cuda()\n\nfr = en2fr.translate('Hello world!', beam=5)\nassert fr == 'Bonjour \u00e0 tous !'", "performance": {"dataset": [{"name": "WMT'14", "accuracy": "Not provided"}, {"name": "WMT'18", "accuracy": "Not provided"}, {"name": "WMT'19", "accuracy": "Not provided"}]}, "description": "Transformer (NMT) is a powerful sequence-to-sequence modeling architecture that produces state-of-the-art neural machine translation systems. It is based on the paper 'Attention Is All You Need' and has been improved using techniques such as large-scale semi-supervised training, back-translation, and noisy-channel reranking. It supports English-French and English-German translation as well as round-trip translation for paraphrasing."}}
{"code": "{'Instruction': CrateChef is a food subscription service and wants to develop a plate classifier for their users. Suggest an API to classify images of food plates., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_efficientnet_b0', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained EfficientNet-B0 model for image classification from PyTorch Hub to classify images of food plates.', 'code': \"import torch\nmodel = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_efficientnet_b0', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_efficientnet_b0', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "EfficientNet", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_efficientnet_b0', pretrained=True)", "api_arguments": ["model_name", "pretrained"], "python_environment_requirements": ["validators", "matplotlib"], "example_code": "import torch\n\nefficientnet = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_efficientnet_b0', pretrained=True)\nutils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_convnets_processing_utils')\n\nefficientnet.eval().to(device)\n\nbatch = torch.cat([utils.prepare_input_from_uri(uri) for uri in uris]).to(device)\n\nwith torch.no_grad():\n output = torch.nn.functional.softmax(efficientnet(batch), dim=1)\n \nresults = utils.pick_n_best(predictions=output, n=5)", "performance": {"dataset": {"name": "IMAGENET", "accuracy": "Not provided"}}, "description": "EfficientNet is a family of image classification models that achieve state-of-the-art accuracy while being smaller and faster. The models are trained with mixed precision using Tensor Cores on the NVIDIA Volta and Ampere GPU architectures. The EfficientNet models include EfficientNet-B0, EfficientNet-B4, EfficientNet-WideSE-B0, and EfficientNet-WideSE-B4. The WideSE models use wider Squeeze-and-Excitation layers than the original EfficientNet models, resulting in slightly better accuracy."}}
{"code": "{'Instruction': In a smart city project, I need to find an API that can detect multiple objects such as cars or stop signs from an image provided by surveillance cameras., 'Output': {'domain': 'Object Detection', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the Single Shot MultiBox Detector (SSD) model from PyTorch Hub, which is capable of detecting multiple objects such as cars and stop signs in images from surveillance cameras.\", 'code': \"import torch\nmodel = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_ssd', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Object Detection", "framework": "PyTorch", "functionality": "Single Shot MultiBox Detector", "api_name": "SSD", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_ssd', pretrained=True)", "api_arguments": ["model_name"], "python_environment_requirements": ["numpy", "scipy", "scikit-image", "matplotlib"], "example_code": "import torch\n\nssd_model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd')\nutils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd_processing_utils')\n\nssd_model.to('cuda')\nssd_model.eval()\n\ninputs = [utils.prepare_input(uri) for uri in uris]\ntensor = utils.prepare_tensor(inputs)\n\nwith torch.no_grad():\n detections_batch = ssd_model(tensor)\n\nresults_per_input = utils.decode_results(detections_batch)\nbest_results_per_input = [utils.pick_best(results, 0.40) for results in results_per_input]", "performance": {"dataset": {"name": "COCO", "accuracy": "Not provided"}}, "description": "The SSD (Single Shot MultiBox Detector) model is an object detection model based on the paper 'SSD: Single Shot MultiBox Detector'. It uses a deep neural network for detecting objects in images. This implementation replaces the obsolete VGG model backbone with the more modern ResNet-50 model. The SSD model is trained on the COCO dataset and can be used to detect objects in images with high accuracy and efficiency."}}
{"code": "{'Instruction': Develop a text-to-speech solution to help visually impaired users read material. Propose an API that can do this., 'Output': {'domain': 'Text-to-Speech', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tacotron2', model_math='fp16')\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Tacotron 2 model from PyTorch Hub to generate natural-sounding speech from text input, suitable for helping visually impaired users.', 'code': 'import torch\nmodel = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tacotron2', model_math='fp16')'}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_tacotron2', model_math='fp16')", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Text-to-Speech", "api_name": "Tacotron 2", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_tacotron2', model_math='fp16')", "api_arguments": {"model_math": "fp16"}, "python_environment_requirements": ["numpy", "scipy", "librosa", "unidecode", "inflect", "libsndfile1"], "example_code": ["import torch", "tacotron2 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tacotron2', model_math='fp16')", "tacotron2 = tacotron2.to('cuda')", "tacotron2.eval()", "text = 'Hello world, I missed you so much.'", "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tts_utils')", "sequences, lengths = utils.prepare_input_sequence([text])", "with torch.no_grad():", " mel, _, _ = tacotron2.infer(sequences, lengths)", " audio = waveglow.infer(mel)", "audio_numpy = audio[0].data.cpu().numpy()", "rate = 22050"], "performance": {"dataset": "LJ Speech", "accuracy": "Not specified"}, "description": "The Tacotron 2 model generates mel spectrograms from input text using an encoder-decoder architecture, and it is designed for generating natural-sounding speech from raw transcripts without any additional prosody information. This implementation uses Dropout instead of Zoneout to regularize the LSTM layers. The WaveGlow model (also available via torch.hub) is a flow-based model that consumes the mel spectrograms to generate speech."}}
{"code": "{'Instruction': A developer needs an API for image classification that can classify images without image augmentation and has a higher top-1 accuracy, suggest an API for the same., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V1', 'mealv1_resnest50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Use the MEAL_V2 model for image classification from PyTorch Hub, which is designed to have high top-1 accuracy without the need for image augmentation or other tricks.', 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V1', 'mealv1_resnest50', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V1', model='mealv1_resnest50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V1', model='mealv1_resnest50', pretrained=True)", "api_arguments": {"model_name": "mealv1_resnest50"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V1 w/ ResNet50", "resolution": "224", "parameters": "25.6M", "top1": "78.21", "top5": "94.01"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': As a developer at an app company, I need an API to classify images from users' phone galleries. Please suggest a suitable API., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the MEAL V2 model, with a pretrained ResNet50 backbone, for image classification. It achieves 80.98% top-1 accuracy on ImageNet without using any specialized tricks.', 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50_cutmix', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50_cutmix', pretrained=True)", "api_arguments": {"model_name": "mealv2_resnest50_cutmix"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 + CutMix w/ ResNet50", "resolution": "224", "parameters": "25.6M", "top1": "80.98", "top5": "95.35"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': I work in a tech company and we want to identify different objects from images. Give me an API that can perform image classification tasks., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the MEAL V2 ResNet50 model from PyTorch Hub with pretrained weights for image classification, which can identify various objects from images.', 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50', pretrained=True)", "api_arguments": {"model_name": "mealv2_resnest50"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ ResNet50", "resolution": "224", "parameters": "25.6M", "top1": "80.67", "top5": "95.09"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': Recommend an API to classify large datasets of photographs into categories., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_mobilenetv3_small_100', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MEAL V2 model from PyTorch Hub for image classification tasks on large datasets. This model achieves high accuracy without requiring any tricks or architecture modifications.', 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', 'mealv2_mobilenetv3_small_100', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenetv3_small_100', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenetv3_small_100', pretrained=True)", "api_arguments": {"model_name": "mealv2_mobilenetv3_small_100"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ MobileNet V3-Small 1.0", "resolution": "224", "parameters": "2.54M", "top1": "69.65", "top5": "88.71"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': I am experimenting with models to train them on images of newspaper covers to classify them according to their content. Do you know any API that would be highly accurate and will work on this task?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenetv3_small_075', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the MEAL V2 model with MobileNetV3 architecture from PyTorch Hub, which is highly accurate for image classification tasks and can be fine-tuned for classifying newspaper covers based on their content.', 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenetv3_small_075', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenetv3_small_075', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenetv3_small_075', pretrained=True)", "api_arguments": {"model_name": "mealv2_mobilenetv3_small_075"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ MobileNet V3-Small 0.75", "resolution": "224", "parameters": "2.04M", "top1": "67.60", "top5": "87.23"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': Suggest an API that can classify images into a variety of categories, such as animals or objects, given an image as input., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', 'mealv2_resnest50_380x380', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MEAL_V2 ResNet-50 model from PyTorch Hub, which is capable of classifying input images into a wide range of categories, such as animals and objects.', 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', 'mealv2_resnest50_380x380', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50_380x380', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50_380x380', pretrained=True)", "api_arguments": {"model_name": "mealv2_resnest50_380x380"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ ResNet50", "resolution": "380", "parameters": "25.6M", "top1": "81.72", "top5": "95.81"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': Nike is building an automated system to classify shoes in their inventory. Suggest an API to classify images for this purpose., 'Output': {'domain': 'Image Classification', 'api_call': 'model = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_efficientnet_b0', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MEAL-V2 model with EfficientNet-B0 architecture from PyTorch Hub to classify images of shoes into different categories.', 'code': 'import torch\nmodel = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_efficientnet_b0', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_efficientnet_b0', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_efficientnet_b0', pretrained=True)", "api_arguments": {"model_name": "mealv2_efficientnet_b0"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ EfficientNet-B0", "resolution": "224", "parameters": "5.29M", "top1": "78.29", "top5": "93.95"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': A zookeeper wants to create an app that can classify the animal types in a picture. Suggest an image classification API that could be implemented., 'Output': {'domain': 'Classification', 'api_call': 'model = torch.hub.load(\"szq0214/MEAL-V2\", \"meal_v2\", \"mealv2_mobilenet_v3_large_100\", pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MEAL_V2 model with MobileNet V3-Large architecture from PyTorch Hub, which can be used for animal classification in images.', 'code': 'import torch\nmodel = torch.hub.load(\"szq0214/MEAL-V2\", \"meal_v2\", \"mealv2_mobilenet_v3_large_100\", pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenet_v3_large_100', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenet_v3_large_100', pretrained=True)", "api_arguments": {"model_name": "mealv2_mobilenet_v3_large_100"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ MobileNet V3-Large 1.0", "resolution": "224", "parameters": "5.48M", "top1": "76.92", "top5": "93.32"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': I need an API to categorize sports activities from a given video clip., 'Output': {'domain': 'Video Classification', 'api_call': \"model = torch.hub.load('facebookresearch/pytorchvideo', 'slow_r50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained 3D ResNet model (slow_r50) from PyTorch Hub, which can be fine-tuned for sports activities classification from video clips.\", 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/pytorchvideo', 'slow_r50', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='slow_r50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Video Classification", "framework": "PyTorch", "functionality": "3D ResNet", "api_name": "slow_r50", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='slow_r50', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["torch", "json", "urllib", "pytorchvideo", "torchvision", "torchaudio", "torchtext", "torcharrow", "TorchData", "TorchRec", "TorchServe", "PyTorch on XLA Devices"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/pytorchvideo', 'slow_r50', pretrained=True)", "device = 'cpu'", "model = model.eval()", "model = model.to(device)"], "performance": {"dataset": "Kinetics 400", "accuracy": {"top_1": 74.58, "top_5": 91.63}, "Flops (G)": 54.52, "Params (M)": 32.45}, "description": "The 3D ResNet model is a Resnet-style video classification network pretrained on the Kinetics 400 dataset. It is based on the architecture from the paper 'SlowFast Networks for Video Recognition' by Christoph Feichtenhofer et al."}}
{"code": "{'Instruction': My self-driving car company requires an API for perception tasks like detecting traffic objects, drivable area segmentation, and lane detection. Can you provide one?, 'Output': {'domain': 'Object Detection', 'api_call': \"model = torch.hub.load('datvuthanh/hybridnets', 'hybridnets', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained HybridNets model from PyTorch Hub to perform multiple perception tasks, including traffic object detection, drivable area segmentation, and lane detection for self-driving cars.', 'code': \"import torch\nmodel = torch.hub.load('datvuthanh/hybridnets', 'hybridnets', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='datvuthanh/hybridnets', model='hybridnets', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Object Detection", "framework": "PyTorch", "functionality": "Traffic Object Detection, Drivable Area Segmentation, Lane Detection", "api_name": "HybridNets", "api_call": "torch.hub.load(repo_or_dir='datvuthanh/hybridnets', model='hybridnets', pretrained=True)", "api_arguments": "pretrained", "python_environment_requirements": "Python>=3.7, PyTorch>=1.10", "example_code": "import torch\nmodel = torch.hub.load('datvuthanh/hybridnets', 'hybridnets', pretrained=True)\nimg = torch.randn(1,3,640,384)\nfeatures, regression, classification, anchors, segmentation = model(img)", "performance": {"dataset": [{"name": "BDD100K", "accuracy": {"Traffic Object Detection": {"Recall (%)": 92.8, "mAP@0.5 (%)": 77.3}, "Drivable Area Segmentation": {"Drivable mIoU (%)": 90.5}, "Lane Line Detection": {"Accuracy (%)": 85.4, "Lane Line IoU (%)": 31.6}}}]}, "description": "HybridNets is an end2end perception network for multi-tasks. Our work focused on traffic object detection, drivable area segmentation and lane detection. HybridNets can run real-time on embedded systems, and obtains SOTA Object Detection, Lane Detection on BDD100K Dataset."}}
{"code": "{'Instruction': What would be an API to estimate the relative depth of objects in a provided image?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('intel-isl/MiDaS', 'DPT_Hybrid', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Use the pretrained MiDaS DPT_Hybrid model from PyTorch Hub to compute relative inverse depth from a single input image.', 'code': 'import torch\nmodel = torch.hub.load('intel-isl/MiDaS', 'DPT_Hybrid', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='intel-isl/MiDaS', model='DPT_Hybrid', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Computing relative depth from a single image", "api_name": "MiDaS", "api_call": "torch.hub.load(repo_or_dir='intel-isl/MiDaS', model='DPT_Hybrid', pretrained=True)", "api_arguments": {"repo_or_dir": "intel-isl/MiDaS", "model": "model_type"}, "python_environment_requirements": "pip install timm", "example_code": ["import cv2", "import torch", "import urllib.request", "import matplotlib.pyplot as plt", "url, filename = ('https://github.com/pytorch/hub/raw/master/images/dog.jpg', 'dog.jpg')", "urllib.request.urlretrieve(url, filename)", "model_type = 'DPT_Large'", "midas = torch.hub.load('intel-isl/MiDaS', 'DPT_Hybrid')", "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')", "midas.to(device)", "midas.eval()", "midas_transforms = torch.hub.load('intel-isl/MiDaS', 'transforms')", "if model_type == 'DPT_Large' or model_type == 'DPT_Hybrid':", " transform = midas_transforms.dpt_transform", "else:", " transform = midas_transforms.small_transform", "img = cv2.imread(filename)", "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)", "input_batch = transform(img).to(device)", "with torch.no_grad():", " prediction = midas(input_batch)", "prediction = torch.nn.functional.interpolate(", " prediction.unsqueeze(1),", " size=img.shape[:2],", " mode='bicubic',", " align_corners=False,", ").squeeze()", "output = prediction.cpu().numpy()", "plt.imshow(output)", "plt.show()"], "performance": {"dataset": "10 distinct datasets", "accuracy": "Multi-objective optimization"}, "description": "MiDaS computes relative inverse depth from a single image. The repository provides multiple models that cover different use cases ranging from a small, high-speed model to a very large model that provide the highest accuracy. The models have been trained on 10 distinct datasets using multi-objective optimization to ensure high quality on a wide range of inputs."}}
{"code": "{'Instruction': Suggest an API to estimate the depth from a single image for an obstacle detection system in a drone surveillance application., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('intel-isl/MiDaS', 'MiDaS_small', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MiDaS model from PyTorch Hub, which can estimate relative depth from a single image, suitable for an obstacle detection system in drone surveillance applications.', 'code': 'import torch\nmodel = torch.hub.load('intel-isl/MiDaS', 'MiDaS_small', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='intel-isl/MiDaS', model='MiDaS_small', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Computing relative depth from a single image", "api_name": "MiDaS", "api_call": "torch.hub.load(repo_or_dir='intel-isl/MiDaS', model='MiDaS_small', pretrained=True)", "api_arguments": {"repo_or_dir": "intel-isl/MiDaS", "model": "model_type"}, "python_environment_requirements": "pip install timm", "example_code": ["import cv2", "import torch", "import urllib.request", "import matplotlib.pyplot as plt", "url, filename = ('https://github.com/pytorch/hub/raw/master/images/dog.jpg', 'dog.jpg')", "urllib.request.urlretrieve(url, filename)", "model_type = 'DPT_Large'", "midas = torch.hub.load('intel-isl/MiDaS', 'MiDaS_small')", "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')", "midas.to(device)", "midas.eval()", "midas_transforms = torch.hub.load('intel-isl/MiDaS', 'transforms')", "if model_type == 'DPT_Large' or model_type == 'DPT_Hybrid':", " transform = midas_transforms.dpt_transform", "else:", " transform = midas_transforms.small_transform", "img = cv2.imread(filename)", "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)", "input_batch = transform(img).to(device)", "with torch.no_grad():", " prediction = midas(input_batch)", "prediction = torch.nn.functional.interpolate(", " prediction.unsqueeze(1),", " size=img.shape[:2],", " mode='bicubic',", " align_corners=False,", ").squeeze()", "output = prediction.cpu().numpy()", "plt.imshow(output)", "plt.show()"], "performance": {"dataset": "10 distinct datasets", "accuracy": "Multi-objective optimization"}, "description": "MiDaS computes relative inverse depth from a single image. The repository provides multiple models that cover different use cases ranging from a small, high-speed model to a very large model that provide the highest accuracy. The models have been trained on 10 distinct datasets using multi-objective optimization to ensure high quality on a wide range of inputs."}}
{"code": "{'Instruction': Recommend an API that can perform real-time object detection, drivable area segmentation, and lane detection for an autonomous vehicle., 'Output': {'domain': 'Object Detection', 'api_call': \"model = torch.hub.load('hustvl/yolop', 'yolop', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained YOLOP model from PyTorch Hub, which can perform real-time object detection, drivable area segmentation, and lane detection for autonomous vehicles.', 'code': \"import torch\nmodel = torch.hub.load('hustvl/yolop', 'yolop', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='hustvl/yolop', model='yolop', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Object Detection", "framework": "PyTorch", "functionality": "Object Detection, Drivable Area Segmentation, Lane Detection", "api_name": "YOLOP", "api_call": "torch.hub.load(repo_or_dir='hustvl/yolop', model='yolop', pretrained=True)", "api_arguments": "pretrained", "python_environment_requirements": "pip install -qr https://github.com/hustvl/YOLOP/blob/main/requirements.txt", "example_code": "import torch\nmodel = torch.hub.load('hustvl/yolop', 'yolop', pretrained=True)\nimg = torch.randn(1,3,640,640)\ndet_out, da_seg_out,ll_seg_out = model(img)", "performance": {"dataset": "BDD100K", "accuracy": {"Object Detection": {"Recall(%)": 89.2, "mAP50(%)": 76.5, "Speed(fps)": 41}, "Drivable Area Segmentation": {"mIOU(%)": 91.5, "Speed(fps)": 41}, "Lane Detection": {"mIOU(%)": 70.5, "IOU(%)": 26.2}}}, "description": "YOLOP is an efficient multi-task network that can jointly handle three crucial tasks in autonomous driving: object detection, drivable area segmentation and lane detection. And it is also the first to reach real-time on embedded devices while maintaining state-of-the-art level performance on the BDD100K dataset."}}
{"code": "{'Instruction': I work at Spotify and I'm looking for an API that can separate the vocals, drums, bass, and other instruments from a pop music track., 'Output': {'domain': 'Audio Separation', 'api_call': \"model = torch.hub.load('sigsep/open-unmix-pytorch', 'umxhq', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Open-Unmix model from PyTorch Hub to separate pop music into vocals, drums, bass, and other instruments.', 'code': 'import torch\nseparator = torch.hub.load('sigsep/open-unmix-pytorch', 'umxhq', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='sigsep/open-unmix-pytorch', model='umxhq', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Audio Separation", "framework": "PyTorch", "functionality": "Music Source Separation", "api_name": "Open-Unmix", "api_call": "torch.hub.load(repo_or_dir='sigsep/open-unmix-pytorch', model='umxhq', pretrained=True)", "api_arguments": ["model_name"], "python_environment_requirements": ["PyTorch >=1.6.0", "torchaudio"], "example_code": ["import torch", "separator = torch.hub.load('sigsep/open-unmix-pytorch', 'umxhq')", "audio = torch.rand((1, 2, 100000))", "original_sample_rate = separator.sample_rate", "estimates = separator(audio)"], "performance": {"dataset": "MUSDB18", "accuracy": "N/A"}, "description": "Open-Unmix provides ready-to-use models that allow users to separate pop music into four stems: vocals, drums, bass and the remaining other instruments. The models were pre-trained on the freely available MUSDB18 dataset."}}
{"code": "{'Instruction': Identify a suitable API for bird species recognition in images for a wildlife photographer., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('nicolalandro/ntsnet-cub200', 'ntsnet', pretrained=True, **{'topN': 6, 'device':'cpu', 'num_classes': 200})\", 'api_provider': 'PyTorch', 'explanation': 'Load the NTS-Net model from PyTorch Hub, which is pre-trained on the CUB200-2011 dataset for fine-grained bird species recognition.', 'code': \"import torch\nmodel = torch.hub.load('nicolalandro/ntsnet-cub200', 'ntsnet', pretrained=True, **{'topN': 6, 'device':'cpu', 'num_classes': 200})\"}}", "api_call": "torch.hub.load(repo_or_dir='nicolalandro/ntsnet-cub200', model='ntsnet', pretrained=True, **{'topN': 6, 'device':'cpu', 'num_classes': 200})", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Fine-grained image classifier", "api_name": "ntsnet", "api_call": "torch.hub.load(repo_or_dir='nicolalandro/ntsnet-cub200', model='ntsnet', pretrained=True, **{'topN': 6, 'device':'cpu', 'num_classes': 200})", "api_arguments": {"pretrained": "True", "topN": "6", "device": "cpu", "num_classes": "200"}, "python_environment_requirements": ["torch", "torchvision", "PIL"], "example_code": "from torchvision import transforms\nimport torch\nimport urllib\nfrom PIL import Image\n\ntransform_test = transforms.Compose([\n transforms.Resize((600, 600), Image.BILINEAR),\n transforms.CenterCrop((448, 448)),\n transforms.ToTensor(),\n transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n])\n\nmodel = torch.hub.load('nicolalandro/ntsnet-cub200', 'ntsnet', pretrained=True, **{'topN': 6, 'device':'cpu', 'num_classes': 200})\nmodel.eval()\n\nurl = 'https://raw.githubusercontent.com/nicolalandro/ntsnet-cub200/master/images/nts-net.png'\nimg = Image.open(urllib.request.urlopen(url))\nscaled_img = transform_test(img)\ntorch_images = scaled_img.unsqueeze(0)\n\nwith torch.no_grad():\n top_n_coordinates, concat_out, raw_logits, concat_logits, part_logits, top_n_index, top_n_prob = model(torch_images)\n\n_, predict = torch.max(concat_logits, 1)\npred_id = predict.item()\nprint('bird class:', model.bird_classes[pred_id])", "performance": {"dataset": "CUB200 2011", "accuracy": "Not provided"}, "description": "This is an nts-net pretrained with CUB200 2011 dataset, which is a fine-grained dataset of birds species."}}
{"code": "{'Instruction': Provide an API that can segment objects present in a given image., 'Output': {'domain': 'Semantic Segmentation', 'api_call': 'model = torch.hub.load('pytorch/vision', 'deeplabv3_resnet101', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained DeepLabV3 with ResNet-101 backbone model from PyTorch Hub for semantic segmentation, which segments objects present in an input image.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'deeplabv3_resnet101', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='deeplabv3_resnet101', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "DeepLabV3", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='deeplabv3_resnet101', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": "latest", "torchvision": "latest", "PIL": "latest", "matplotlib": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_resnet101', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "input_image = input_image.convert('RGB')", "preprocess = transforms.Compose([", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)['out'][0]", "output_predictions = output.argmax(0)"], "performance": {"dataset": "COCO val2017", "accuracy": [{"model": "deeplabv3_resnet101", "Mean IOU": 67.4, "Global Pixelwise Accuracy": 92.4}]}, "description": "DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset."}}
{"code": "{'Instruction': Help me in segmenting objects in an image. Suggest an API that I can use for this purpose., 'Output': {'domain': 'Semantic Segmentation', 'api_call': \"model = torch.hub.load('pytorch/vision', 'deeplabv3_resnet50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained DeepLabV3 model with a ResNet-50 backbone from PyTorch Hub for semantic segmentation of objects in an image.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'deeplabv3_resnet50', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='deeplabv3_resnet50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "DeepLabV3", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='deeplabv3_resnet50', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": "latest", "torchvision": "latest", "PIL": "latest", "matplotlib": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_resnet50', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "input_image = input_image.convert('RGB')", "preprocess = transforms.Compose([", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)['out'][0]", "output_predictions = output.argmax(0)"], "performance": {"dataset": "COCO val2017", "accuracy": [{"model": "deeplabv3_resnet50", "Mean IOU": 66.4, "Global Pixelwise Accuracy": 92.4}]}, "description": "DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset."}}
{"code": "{'Instruction': A podcast producer needs a solution to detect speech in audio recordings. Recommend an API to recognize the timestamps where voice is present in an audio file., 'Output': {'domain': 'Classification', 'api_call': \"model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the Silero Voice Activity Detector (VAD) from PyTorch Hub to detect speech and output timestamps where voice is present in an audio file.', 'code': \"import torch\nmodel, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Voice Activity Detection", "api_name": "Silero Voice Activity Detector", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=True)", "api_arguments": {"repo_or_dir": "snakers4/silero-vad", "model": "silero_vad", "force_reload": "True"}, "python_environment_requirements": {"torchaudio": "pip install -q torchaudio"}, "example_code": {"import": ["import torch", "torch.set_num_threads(1)", "from IPython.display import Audio", "from pprint import pprint"], "download_example": "torch.hub.download_url_to_file('https://models.silero.ai/vad_models/en.wav', 'en_example.wav')", "load_model": "model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=True)", "load_utils": "(get_speech_timestamps, _, read_audio, _) = utils", "set_sampling_rate": "sampling_rate = 16000", "read_audio": "wav = read_audio('en_example.wav', sampling_rate=sampling_rate)", "get_speech_timestamps": "speech_timestamps = get_speech_timestamps(wav, model, sampling_rate=sampling_rate)", "print_speech_timestamps": "pprint(speech_timestamps)"}, "performance": {"dataset": "", "accuracy": ""}, "description": "Silero VAD is a pre-trained enterprise-grade Voice Activity Detector (VAD) that aims to provide a high-quality and modern alternative to the WebRTC Voice Activity Detector. The model is optimized for performance on 1 CPU thread and is quantized."}}
{"code": "{'Instruction': Suggest an API that can transcribe spoken words in a German audio file to written text., 'Output': {'domain': 'Speech-To-Text', 'api_call': \"model, decoder, utils = torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_stt', language='de', device=device)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Silero Speech-To-Text model with German language support from PyTorch Hub to transcribe spoken words in an audio file to written text.', 'code': 'import torch\ndevice = torch.device(\"cpu\")\nmodel, decoder, utils = torch.hub.load(repo_or_dir=\"snakers4/silero-models\", model=\"silero_stt\", language=\"de\", device=device)'}}", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_stt', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Speech-To-Text", "api_name": "Silero Models", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_stt', pretrained=True)", "api_arguments": {"repo_or_dir": "snakers4/silero-models", "model": "silero_stt", "language": ["en", "de", "es"], "device": "device"}, "python_environment_requirements": ["pip install -q torchaudio omegaconf soundfile"], "example_code": ["import torch", "import zipfile", "import torchaudio", "from glob import glob", "device = torch.device('cpu')", "model, decoder, utils = torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_stt', language='en', device=device)", "(read_batch, split_into_batches, read_audio, prepare_model_input) = utils", "torch.hub.download_url_to_file('https://opus-codec.org/static/examples/samples/speech_orig.wav', dst ='speech_orig.wav', progress=True)", "test_files = glob('speech_orig.wav')", "batches = split_into_batches(test_files, batch_size=10)", "input = prepare_model_input(read_batch(batches[0]), device=device)", "output = model(input)", "for example in output:", " print(decoder(example.cpu()))"], "performance": {"dataset": "Open-STT", "accuracy": "See the wiki for quality and performance benchmarks"}, "description": "Silero Speech-To-Text models provide enterprise grade STT in a compact form-factor for several commonly spoken languages. The models are robust to a variety of dialects, codecs, domains, noises, and lower sampling rates. They consume a normalized audio in the form of samples and output frames with token probabilities. A decoder utility is provided for simplicity."}}
{"code": "{'Instruction': I want to count the number of people in a crowd using an image. Provide me with an API that can do this., 'Output': {'domain': 'Object Detection', 'api_call': 'model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained YOLOv5s model from PyTorch Hub, which can detect and localize people in an image.', 'code': 'import torch\nmodel = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='ultralytics/yolov5', model='yolov5s', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Object Detection", "framework": "PyTorch", "functionality": "YOLOv5", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='ultralytics/yolov5', model='yolov5s', pretrained=True)", "api_arguments": ["'ultralytics/yolov5'", "'yolov5s'", "pretrained=True"], "python_environment_requirements": "Python>=3.8, PyTorch>=1.7", "example_code": ["import torch", "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)", "imgs = ['https://ultralytics.com/images/zidane.jpg']", "results = model(imgs)", "results.print()", "results.save()", "results.xyxy[0]", "results.pandas().xyxy[0]"], "performance": {"dataset": "COCO", "accuracy": {"YOLOv5s6": {"mAPval0.5:0.95": 43.3, "mAPtest0.5:0.95": 43.3, "mAPval0.5": 61.9}, "YOLOv5m6": {"mAPval0.5:0.95": 50.5, "mAPtest0.5:0.95": 50.5, "mAPval0.5": 68.7}, "YOLOv5l6": {"mAPval0.5:0.95": 53.4, "mAPtest0.5:0.95": 53.4, "mAPval0.5": 71.1}, "YOLOv5x6": {"mAPval0.5:0.95": 54.4, "mAPtest0.5:0.95": 54.4, "mAPval0.5": 72.0}, "YOLOv5x6 TTA": {"mAPval0.5:0.95": 55.0, "mAPtest0.5:0.95": 55.0, "mAPval0.5": 72.0}}}, "description": "YOLOv5 is a family of compound-scaled object detection models trained on the COCO dataset, and includes simple functionality for Test Time Augmentation (TTA), model ensembling, hyperparameter evolution, and export to ONNX, CoreML and TFLite."}}
{"code": "{'Instruction': I want to create some new clothing designs using AI. Which API should I use to generate diverse clothing designs?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'DCGAN', pretrained=True, useGPU=use_gpu)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained DCGAN model from PyTorch Hub, which can generate diverse clothing designs using generative adversarial networks.', 'code': 'import torch\nuse_gpu = True if torch.cuda.is_available() else False\nmodel = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'DCGAN', pretrained=True, useGPU=use_gpu)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorch_GAN_zoo:hub', model='DCGAN', pretrained=True, useGPU=use_gpu)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Generative Adversarial Networks", "api_name": "DCGAN", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorch_GAN_zoo:hub', model='DCGAN', pretrained=True, useGPU=use_gpu)", "api_arguments": {"pretrained": "True", "useGPU": "use_gpu"}, "python_environment_requirements": "Python 3", "example_code": {"import": ["import torch", "import matplotlib.pyplot as plt", "import torchvision"], "use_gpu": "use_gpu = True if torch.cuda.is_available() else False", "load_model": "model = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'DCGAN', pretrained=True, useGPU=use_gpu)", "build_noise_data": "noise, _ = model.buildNoiseData(num_images)", "generate_images": "with torch.no_grad(): generated_images = model.test(noise)", "plot_images": ["plt.imshow(torchvision.utils.make_grid(generated_images).permute(1, 2, 0).cpu().numpy())", "plt.show()"]}, "performance": {"dataset": "FashionGen", "accuracy": "N/A"}, "description": "DCGAN is a model designed in 2015 by Radford et. al. in the paper Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks. It is a GAN architecture both very simple and efficient for low resolution image generation (up to 64x64)."}}
{"code": "{'Instruction': I want to implement an image classification model for an app, can you suggest an API that I can use and give me the code snippet to load the model?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x8d_wsl')\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNext WSL model for image classification from PyTorch Hub to classify images with high accuracy.', 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x8d_wsl')\nmodel.eval()'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x8d_wsl', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNext WSL", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x8d_wsl', pretrained=True)", "api_arguments": [{"name": "resnext101_32x8d_wsl", "type": "str", "description": "ResNeXt-101 32x8d WSL model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x8d_wsl')", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(output[0])", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeXt-101 32x8d": {"Top-1 Acc.": "82.2", "Top-5 Acc.": "96.4"}}}, "description": "The provided ResNeXt models are pre-trained in weakly-supervised fashion on 940 million public images with 1.5K hashtags matching with 1000 ImageNet1K synsets, followed by fine-tuning on ImageNet1K dataset. The models significantly improve the training accuracy on ImageNet compared to training from scratch. They achieve state-of-the-art accuracy of 85.4% on ImageNet with the ResNext-101 32x48d model."}}
{"code": "{'Instruction': Tell me an API that can accurately classify a wide range of images into different categories., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x48d_wsl', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNext WSL model from PyTorch Hub for accurate image classification across a wide range of categories.', 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x48d_wsl', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x48d_wsl', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNext WSL", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x48d_wsl', pretrained=True)", "api_arguments": [{"name": "resnext101_32x48d_wsl", "type": "str", "description": "ResNeXt-101 32x48d WSL model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x48d_wsl')", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(output[0])", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeXt-101 32x48d": {"Top-1 Acc.": "85.4", "Top-5 Acc.": "97.6"}}}, "description": "The provided ResNeXt models are pre-trained in weakly-supervised fashion on 940 million public images with 1.5K hashtags matching with 1000 ImageNet1K synsets, followed by fine-tuning on ImageNet1K dataset. The models significantly improve the training accuracy on ImageNet compared to training from scratch. They achieve state-of-the-art accuracy of 85.4% on ImageNet with the ResNext-101 32x48d model."}}
{"code": "{'Instruction': A researcher needs to categorize real world objects in images using machine learning. Provide a suitable API for image classification., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x32d_wsl', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNext WSL model from PyTorch Hub for image classification. This model achieves state-of-the-art accuracy and works well for categorizing objects in images.', 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x32d_wsl', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x32d_wsl', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNext WSL", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x32d_wsl', pretrained=True)", "api_arguments": [{"name": "resnext101_32x32d_wsl", "type": "str", "description": "ResNeXt-101 32x32d WSL model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x32d_wsl')", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(output[0])", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeXt-101 32x32d": {"Top-1 Acc.": "85.1", "Top-5 Acc.": "97.5"}}}, "description": "The provided ResNeXt models are pre-trained in weakly-supervised fashion on 940 million public images with 1.5K hashtags matching with 1000 ImageNet1K synsets, followed by fine-tuning on ImageNet1K dataset. The models significantly improve the training accuracy on ImageNet compared to training from scratch. They achieve state-of-the-art accuracy of 85.4% on ImageNet with the ResNext-101 32x48d model."}}
{"code": "{'Instruction': I am developing a visual content recognition system for an e-commerce website to categorize images. Recommend an API that can help me with image classification., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Use the pretrained ResNext WSL model from PyTorch Hub for image classification. This model provides state-of-the-art performance and is suitable for e-commerce image categorization.', 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x16d_wsl', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNext WSL", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x16d_wsl', pretrained=True)", "api_arguments": [{"name": "resnext101_32x16d_wsl", "type": "str", "description": "ResNeXt-101 32x16d WSL model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl')", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(output[0])", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeXt-101 32x16d": {"Top-1 Acc.": "84.2", "Top-5 Acc.": "97.2"}}}, "description": "The provided ResNeXt models are pre-trained in weakly-supervised fashion on 940 million public images with 1.5K hashtags matching with 1000 ImageNet1K synsets, followed by fine-tuning on ImageNet1K dataset. The models significantly improve the training accuracy on ImageNet compared to training from scratch. They achieve state-of-the-art accuracy of 85.4% on ImageNet with the ResNext-101 32x48d model."}}
{"code": "{'Instruction': Discover an API suitable for segmenting abnormal regions in brain MRI scans., 'Output': {'domain': 'Semantic Segmentation', 'api_call': \"model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pre-trained U-Net model for biomedical image segmentation, specifically designed for abnormality segmentation in brain MRI scans.', 'code': 'import torch\nmodel = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='mateuszbuda/brain-segmentation-pytorch', model='unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "Biomedical Image Segmentation", "api_name": "U-Net for brain MRI", "api_call": "torch.hub.load(repo_or_dir='mateuszbuda/brain-segmentation-pytorch', model='unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)", "api_arguments": {"in_channels": 3, "out_channels": 1, "init_features": 32, "pretrained": true}, "python_environment_requirements": ["torch", "torchvision", "numpy", "PIL"], "example_code": ["import torch", "model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)", "import numpy as np", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "m, s = np.mean(input_image, axis=(0, 1)), np.std(input_image, axis=(0, 1))", "preprocess = transforms.Compose([transforms.ToTensor(),transforms.Normalize(mean=m, std=s)])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model = model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(torch.round(output[0]))"], "performance": {"dataset": "kaggle.com/mateuszbuda/lgg-mri-segmentation"}, "description": "U-Net with batch normalization for biomedical image segmentation with pretrained weights for abnormality segmentation in brain MRI. The model comprises four levels of blocks containing two convolutional layers with batch normalization and ReLU activation function, and one max pooling layer in the encoding part and up-convolutional layers instead in the decoding part. The number of convolutional filters in each block is 32, 64, 128, and 256. The bottleneck layer has 512 convolutional filters. From the encoding layers, skip connections are used to the corresponding layers in the decoding part. Input image is a 3-channel brain MRI slice from pre-contrast, FLAIR, and post-contrast sequences, respectively. Output is a one-channel probability map of abnormality regions with the same size as the input image."}}
{"code": "{'Instruction': I need an API that I can use to create a project for a Tech event about classifying several bird species for local naturalists., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resnet50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained ResNet50 v1.5 model from PyTorch Hub and fine-tune it for bird species classification tasks to suit local naturalists' needs.\", 'code': 'import torch\nmodel = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resnet50', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_resnet50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNet50", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_resnet50', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["pip install validators matplotlib"], "example_code": ["import torch", "from PIL import Image", "import torchvision.transforms as transforms", "import numpy as np", "import json", "import requests", "import matplotlib.pyplot as plt", "import warnings", "warnings.filterwarnings('ignore')", "%matplotlib inline", "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')", "print(f'Using {device} for inference')", "resnet50 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resnet50', pretrained=True)", "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_convnets_processing_utils')", "resnet50.eval().to(device)", "uris = [...]", "batch = torch.cat([utils.prepare_input_from_uri(uri) for uri in uris]).to(device)", "with torch.no_grad():", " output = torch.nn.functional.softmax(resnet50(batch), dim=1)", "results = utils.pick_n_best(predictions=output, n=5)", "for uri, result in zip(uris, results):", " img = Image.open(requests.get(uri, stream=True).raw)", " img.thumbnail((256,256), Image.ANTIALIAS)", " plt.imshow(img)", " plt.show()", " print(result)"], "performance": {"dataset": "ImageNet", "accuracy": "~0.5% top1 improvement over ResNet50 v1"}, "description": "The ResNet50 v1.5 model is a modified version of the original ResNet50 v1 model. The difference between v1 and v1.5 is that, in the bottleneck blocks which requires downsampling, v1 has stride = 2 in the first 1x1 convolution, whereas v1.5 has stride = 2 in the 3x3 convolution. This difference makes ResNet50 v1.5 slightly more accurate (~0.5% top1) than v1, but comes with a small performance drawback (~5% imgs/sec). The model is initialized as described in Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification. This model is trained with mixed precision using Tensor Cores on Volta, Turing, and the NVIDIA Ampere GPU architectures."}}
{"code": "{'Instruction': Can you provide a GAN architecture that can generate high-resolution images progressively?, 'Output': {'domain': 'Image Generation', 'api_call': \"model = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'PGAN', model_name='celebAHQ-512', pretrained=True, useGPU=use_gpu)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained Progressive Growing of GANs (PGAN) model from PyTorch Hub. This method generates high-resolution images by progressively increasing the resolution throughout the training process, resulting in improved quality, stability, and variation.\", 'code': \"import torch\nuse_gpu = True if torch.cuda.is_available() else False\nmodel = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'PGAN', model_name='celebAHQ-512', pretrained=True, useGPU=use_gpu)\"}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorch_GAN_zoo:hub', model='PGAN', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Generative Adversarial Networks (GANs)", "api_name": "PGAN", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorch_GAN_zoo:hub', model='PGAN', pretrained=True)", "api_arguments": {"repo_or_dir": "facebookresearch/pytorch_GAN_zoo:hub", "model": "PGAN", "model_name": "celebAHQ-512", "pretrained": "True", "useGPU": "use_gpu"}, "python_environment_requirements": "Python 3", "example_code": {"import": "import torch", "use_gpu": "use_gpu = True if torch.cuda.is_available() else False", "load_model": "model = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'PGAN', model_name='celebAHQ-512', pretrained=True, useGPU=use_gpu)", "build_noise_data": "noise, _ = model.buildNoiseData(num_images)", "test": "generated_images = model.test(noise)", "plot_images": {"import_matplotlib": "import matplotlib.pyplot as plt", "import_torchvision": "import torchvision", "make_grid": "grid = torchvision.utils.make_grid(generated_images.clamp(min=-1, max=1), scale_each=True, normalize=True)", "imshow": "plt.imshow(grid.permute(1, 2, 0).cpu().numpy())", "show": "plt.show()"}}, "performance": {"dataset": "celebA", "accuracy": "High-quality celebrity faces"}, "description": "Progressive Growing of GANs (PGAN) is a method for generating high-resolution images using generative adversarial networks. The model is trained progressively, starting with low-resolution images and gradually increasing the resolution until the desired output is achieved. This implementation is based on the paper by Tero Karras et al., 'Progressive Growing of GANs for Improved Quality, Stability, and Variation'."}}
{"code": "{'Instruction': \"I'm building an app for tourists to identify famous landmarks based on their photos. Suggest an API.\", 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'alexnet', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained AlexNet model from PyTorch Hub and fine-tune it for landmark recognition to efficiently identify famous landmarks in tourists' photos.\", 'code': \"import torch\nmodel = torch.hub.load('pytorch/vision', 'alexnet', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='alexnet', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "AlexNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='alexnet', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": ">=1.9.0", "torchvision": ">=0.10.0"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"top-1_error": 43.45, "top-5_error": 20.91}}, "description": "AlexNet is a deep convolutional neural network that achieved a top-5 error of 15.3% in the 2012 ImageNet Large Scale Visual Recognition Challenge. The main contribution of the original paper was the depth of the model, which was computationally expensive but made feasible through the use of GPUs during training. The pretrained AlexNet model in PyTorch can be used for image classification tasks."}}
{"code": "{'Instruction': Tell me an API that can efficiently classify thousands of real-world objects from an image., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_se_resnext101_32x4d', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Use the SE-ResNeXt101-32x4d model from PyTorch Hub. It is a pre-trained model, designed to identify thousands of real-world objects within an image efficiently.\", 'code': 'import torch\nmodel = torch.hub.load(\"NVIDIA/DeepLearningExamples:torchhub\", \"nvidia_se_resnext101_32x4d\", pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_se_resnext101_32x4d', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SE-ResNeXt101", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_se_resnext101_32x4d', pretrained=True)", "api_arguments": "N/A", "python_environment_requirements": "validators, matplotlib", "example_code": "import torch\nfrom PIL import Image\nimport torchvision.transforms as transforms\nimport numpy as np\nimport json\nimport requests\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nprint(f'Using {device} for inference')\nresneXt = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_se_resnext101_32x4d')\nutils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_convnets_processing_utils')\nresneXt.eval().to(device)\nuris = ['http://images.cocodataset.org/test-stuff2017/000000024309.jpg','http://images.cocodataset.org/test-stuff2017/000000028117.jpg','http://images.cocodataset.org/test-stuff2017/000000006149.jpg','http://images.cocodataset.org/test-stuff2017/000000004954.jpg']\nbatch = torch.cat([utils.prepare_input_from_uri(uri) for uri in uris]).to(device)\nwith torch.no_grad():\n output = torch.nn.functional.softmax(resneXt(batch), dim=1)\nresults = utils.pick_n_best(predictions=output, n=5)\nfor uri, result in zip(uris, results):\n img = Image.open(requests.get(uri, stream=True).raw)\n img.thumbnail((256,256), Image.ANTIALIAS)\n plt.imshow(img)\n plt.show()\n print(result)", "performance": {"dataset": "IMAGENET", "accuracy": "N/A"}, "description": "The SE-ResNeXt101-32x4d is a ResNeXt101-32x4d model with added Squeeze-and-Excitation module. This model is trained with mixed precision using Tensor Cores on Volta, Turing, and the NVIDIA Ampere GPU architectures, which allows researchers to get results 3x faster than training without Tensor Cores while experiencing the benefits of mixed precision training."}}
{"code": "{'Instruction': I have the URL of an image and I want to classify it in real-time. Provide me an API that can classify the image and output 5 top predictions., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resneXt', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNeXt101-32x4d model provided by NVIDIA and DeepLearningExamples from PyTorch Hub. This model can classify the input image and return the top 5 predictions.', 'code': 'import torch\nmodel = torch.hub.load(\"NVIDIA/DeepLearningExamples:torchhub\", \"nvidia_resneXt\", pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_resneXt', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNeXt101-32x4d", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_resneXt', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "validators", "matplotlib"], "example_code": ["import torch", "from PIL import Image", "import torchvision.transforms as transforms", "import numpy as np", "import json", "import requests", "import matplotlib.pyplot as plt", "import warnings", "warnings.filterwarnings('ignore')", "%matplotlib inline", "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")", "print(f'Using {device} for inference')", "resneXt = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resneXt')", "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_convnets_processing_utils')", "resneXt.eval().to(device)", "uris = [", " 'http://images.cocodataset.org/test-stuff2017/000000024309.jpg',", " 'http://images.cocodataset.org/test-stuff2017/000000028117.jpg',", " 'http://images.cocodataset.org/test-stuff2017/000000006149.jpg',", " 'http://images.cocodataset.org/test-stuff2017/000000004954.jpg',", "]", "batch = torch.cat(", " [utils.prepare_input_from_uri(uri) for uri in uris]", ").to(device)", "with torch.no_grad():", " output = torch.nn.functional.softmax(resneXt(batch), dim=1)", "results = utils.pick_n_best(predictions=output, n=5)", "for uri, result in zip(uris, results):", " img = Image.open(requests.get(uri, stream=True).raw)", " img.thumbnail((256,256), Image.ANTIALIAS)", " plt.imshow(img)", " plt.show()", " print(result)"], "performance": {"dataset": "IMAGENET", "accuracy": "Not provided"}, "description": "ResNeXt101-32x4d is a model introduced in the Aggregated Residual Transformations for Deep Neural Networks paper. It is based on regular ResNet model, substituting 3x3 convolutions inside the bottleneck block for 3x3 grouped convolutions. This model is trained with mixed precision using Tensor Cores on Volta, Turing, and the NVIDIA Ampere GPU architectures. It can be deployed for inference on the NVIDIA Triton Inference Server using TorchScript, ONNX Runtime or TensorRT as an execution backend."}}
{"code": "{'Instruction': Recommend an API capable of converting given text to natural-sounding speech., 'Output': {'domain': 'Text-to-Speech', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_waveglow', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained WaveGlow model from NVIDIA for speech synthesis. Combine Tacotron 2 and WaveGlow models to create a text-to-speech system that synthesizes natural-sounding speech from raw text.', 'code': \"import torch\nmodel = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_waveglow', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_waveglow', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Text-to-Speech", "framework": "PyTorch", "functionality": "Speech Synthesis", "api_name": "WaveGlow", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_waveglow', pretrained=True)", "api_arguments": {"repo_or_dir": "NVIDIA/DeepLearningExamples:torchhub", "model": "nvidia_waveglow", "model_math": "fp32"}, "python_environment_requirements": ["numpy", "scipy", "librosa", "unidecode", "inflect", "libsndfile1"], "example_code": {"load_waveglow_model": "waveglow = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_waveglow', model_math='fp32')", "prepare_waveglow_model": ["waveglow = waveglow.remove_weightnorm(waveglow)", "waveglow = waveglow.to('cuda')", "waveglow.eval()"], "load_tacotron2_model": "tacotron2 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tacotron2', model_math='fp32')", "prepare_tacotron2_model": ["tacotron2 = tacotron2.to('cuda')", "tacotron2.eval()"], "synthesize_speech": ["text = \"hello world, I missed you so much\"", "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tts_utils')", "sequences, lengths = utils.prepare_input_sequence([text])", "with torch.no_grad():", " mel, _, _ = tacotron2.infer(sequences, lengths)", " audio = waveglow.infer(mel)", "audio_numpy = audio[0].data.cpu().numpy()", "rate = 22050"], "save_audio": "write(\"audio.wav\", rate, audio_numpy)", "play_audio": "Audio(audio_numpy, rate=rate)"}, "performance": {"dataset": "LJ Speech", "accuracy": null}, "description": "The Tacotron 2 and WaveGlow model form a text-to-speech system that enables users to synthesize natural-sounding speech from raw transcripts without any additional prosody information. The Tacotron 2 model produces mel spectrograms from input text using encoder-decoder architecture. WaveGlow is a flow-based model that consumes the mel spectrograms to generate speech."}}
{"code": "{'Instruction': I want to train a sentiment analysis model to classify Yelp reviews. Suggest an API for a good base model that I can fine-tune., 'Output': {'domain': 'Text Classification', 'api_call': \"model = torch.hub.load('pytorch/fairseq', 'roberta.large', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained RoBERTa-large model from PyTorch Hub, which can be fine-tuned for sentiment analysis on Yelp reviews.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/fairseq', 'roberta.large', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/fairseq', model='roberta.large', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "RoBERTa", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='pytorch/fairseq', model='roberta.large', pretrained=True)", "api_arguments": ["'pytorch/fairseq'", "'roberta.large'"], "python_environment_requirements": ["regex", "requests", "hydra-core", "omegaconf"], "example_code": ["import torch", "roberta = torch.hub.load('pytorch/fairseq', 'roberta.large')", "roberta.eval()", "tokens = roberta.encode('Hello world!')", "last_layer_features = roberta.extract_features(tokens)", "all_layers = roberta.extract_features(tokens, return_all_hiddens=True)", "roberta = torch.hub.load('pytorch/fairseq', 'roberta.large.mnli')", "roberta.eval()", "tokens = roberta.encode('Roberta is a heavily optimized version of BERT.', 'Roberta is not very optimized.')", "prediction = roberta.predict('mnli', tokens).argmax().item()", "tokens = roberta.encode('Roberta is a heavily optimized version of BERT.', 'Roberta is based on BERT.')", "prediction = roberta.predict('mnli', tokens).argmax().item()", "roberta.register_classification_head('new_task', num_classes=3)", "logprobs = roberta.predict('new_task', tokens)"], "performance": {"dataset": "MNLI", "accuracy": "N/A"}, "description": "RoBERTa is a robustly optimized version of BERT, a revolutionary self-supervised pretraining technique that learns to predict intentionally hidden (masked) sections of text. RoBERTa builds on BERT's language masking strategy and modifies key hyperparameters, including removing BERT's next-sentence pretraining objective, and training with much larger mini-batches and learning rates. RoBERTa was also trained on an order of magnitude more data than BERT, for a longer amount of time, allowing it to generalize even better to downstream tasks."}}
{"code": "{'Instruction': I am developing an image classification application. What API can I use to perform classification using a Dense Convolutional Network?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'densenet121', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained DenseNet-121 model from PyTorch Hub for image classification using a Dense Convolutional Network.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'densenet121', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet121', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Dense Convolutional Network", "api_name": "Densenet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet121', pretrained=True)", "api_arguments": [{"name": "densenet121", "type": "str", "description": "Densenet-121 model"}], "python_environment_requirements": {"torch": "latest", "torchvision": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet121', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"densenet121": {"Top-1 error": 25.35, "Top-5 error": 7.83}}}, "description": "Dense Convolutional Network (DenseNet) connects each layer to every other layer in a feed-forward fashion. It alleviates the vanishing-gradient problem, strengthens feature propagation, encourages feature reuse, and substantially reduces the number of parameters."}}
{"code": "{'Instruction': Please recommend an API suitable for image classification that can identify objects contained in arbitrary photographs., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'densenet201', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the Densenet-201 model from PyTorch Hub with pre-trained weights for image classification, capable of identifying objects in various images.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'densenet201', pretrained=True)\nmodel.eval()'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet201', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Dense Convolutional Network", "api_name": "Densenet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet201', pretrained=True)", "api_arguments": [{"name": "densenet201", "type": "str", "description": "Densenet-201 model"}], "python_environment_requirements": {"torch": "latest", "torchvision": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet201', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"densenet201": {"Top-1 error": 22.8, "Top-5 error": 6.43}}}, "description": "Dense Convolutional Network (DenseNet) connects each layer to every other layer in a feed-forward fashion. It alleviates the vanishing-gradient problem, strengthens feature propagation, encourages feature reuse, and substantially reduces the number of parameters."}}
{"code": "{'Instruction': A zoo automated their entrance control system, and they need a program that recognizes animal types to monitor statistics. Please provide me with an API to classify animals based on their images., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'densenet169', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained DenseNet-169 model from PyTorch Hub, which can be fine-tuned for animal classification given images of different animals.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'densenet169', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet169', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Dense Convolutional Network", "api_name": "Densenet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet169', pretrained=True)", "api_arguments": [{"name": "densenet169", "type": "str", "description": "Densenet-169 model"}], "python_environment_requirements": {"torch": "latest", "torchvision": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet169', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"densenet169": {"Top-1 error": 24.0, "Top-5 error": 7.0}}}, "description": "Dense Convolutional Network (DenseNet) connects each layer to every other layer in a feed-forward fashion. It alleviates the vanishing-gradient problem, strengthens feature propagation, encourages feature reuse, and substantially reduces the number of parameters."}}
{"code": "{'Instruction': I am a software developer working on an autonomous car project. I need an API to segment an image so I can identify different objects in the area., 'Output': {'domain': 'Semantic Segmentation', 'api_call': \"model = torch.hub.load('pytorch/vision', 'fcn_resnet101', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the FCN-ResNet101 model with pretrained weights from the PyTorch hub to semantically segment input images.', 'code': 'import torch\nmodel = torch.hub.load(\"pytorch/vision\", \"fcn_resnet101\", pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='fcn_resnet101', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "Fully-Convolutional Network", "api_name": "fcn_resnet101", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='fcn_resnet101', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "boolean", "description": "If True, returns a model pre-trained on COCO train2017"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "matplotlib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'fcn_resnet101', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "input_image = input_image.convert(\"RGB\")", "preprocess = transforms.Compose([", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)['out'][0]", "output_predictions = output.argmax(0)"], "performance": {"dataset": "COCO val2017", "accuracy": {"fcn_resnet101": {"Mean IOU": 63.7, "Global Pixelwise Accuracy": 91.9}}}, "description": "FCN-ResNet is a Fully-Convolutional Network model using a ResNet-50 or a ResNet-101 backbone. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset."}}
{"code": "{'Instruction': I have a project where I am asked to perform semantic segmentation on cityscape images. Recommend me an API that would be useful for this., 'Output': {'domain': 'Semantic Segmentation', 'api_call': \"model = torch.hub.load('pytorch/vision', 'fcn_resnet50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Use the Fully-Convolutional Network (FCN-ResNet50) API from the PyTorch Vision library, pretrained for semantic segmentation.', 'code': \"import torch\nmodel = torch.hub.load('pytorch/vision', 'fcn_resnet50', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='fcn_resnet50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "Fully-Convolutional Network", "api_name": "fcn_resnet50", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='fcn_resnet50', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "boolean", "description": "If True, returns a model pre-trained on COCO train2017"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "matplotlib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'fcn_resnet50', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "input_image = input_image.convert(\"RGB\")", "preprocess = transforms.Compose([", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)['out'][0]", "output_predictions = output.argmax(0)"], "performance": {"dataset": "COCO val2017", "accuracy": {"fcn_resnet50": {"Mean IOU": 60.5, "Global Pixelwise Accuracy": 91.4}}}, "description": "FCN-ResNet is a Fully-Convolutional Network model using a ResNet-50 or a ResNet-101 backbone. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset."}}
{"code": "{'Instruction': I am a software engineer developing a mobile application capable of classifying images. Please provide me with an API for image classification in PyTorch that has low memory requirements and high efficiency., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet39ds', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained HarDNet-39DS model from PyTorch Hub, which is fast, efficient, and designed for low memory traffic CNN applications.', 'code': 'import torch\nmodel = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet39ds', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet39ds', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "HarDNet", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet39ds', pretrained=True)", "api_arguments": [{"name": "hardnet39ds", "type": "str", "description": "HarDNet-39DS model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet39ds', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"hardnet39ds": {"Top-1 error": 27.92, "Top-5 error": 9.57}}}, "description": "Harmonic DenseNet (HarDNet) is a low memory traffic CNN model, which is fast and efficient. The basic concept is to minimize both computational cost and memory access cost at the same time, such that the HarDNet models are 35% faster than ResNet running on GPU comparing to models with the same accuracy (except the two DS models that were designed for comparing with MobileNet)."}}
{"code": "{'Instruction': Recommend an API that can classify objects in images efficiently, suitable to run on smartphones., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet68ds', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained HarDNet-68DS model from PyTorch Hub, which is a low memory traffic CNN optimized for smartphones and other resource-limited devices.', 'code': 'import torch\nmodel = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet68ds', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet68ds', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "HarDNet", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet68ds', pretrained=True)", "api_arguments": [{"name": "hardnet68ds", "type": "str", "description": "HarDNet-68DS model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet68ds', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"hardnet68ds": {"Top-1 error": 25.71, "Top-5 error": 8.13}}}, "description": "Harmonic DenseNet (HarDNet) is a low memory traffic CNN model, which is fast and efficient. The basic concept is to minimize both computational cost and memory access cost at the same time, such that the HarDNet models are 35% faster than ResNet running on GPU comparing to models with the same accuracy (except the two DS models that were designed for comparing with MobileNet)."}}
{"code": "{'Instruction': I work at Google Photos and my team needs an API that can efficiently classify photos into different categories. What do you suggest?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet85', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained HarDNet-85 model for image classification from PyTorch Hub to efficiently classify photos into different categories.', 'code': 'import torch\nmodel = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet85', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet85', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "HarDNet", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet85', pretrained=True)", "api_arguments": [{"name": "hardnet85", "type": "str", "description": "HarDNet-85 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet85', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"hardnet85": {"Top-1 error": 21.96, "Top-5 error": 6.11}}}, "description": "Harmonic DenseNet (HarDNet) is a low memory traffic CNN model, which is fast and efficient. The basic concept is to minimize both computational cost and memory access cost at the same time, such that the HarDNet models are 35% faster than ResNet running on GPU comparing to models with the same accuracy (except the two DS models that were designed for comparing with MobileNet)."}}
{"code": "{'Instruction': Identify an API that can classify objects in images quickly and requires low memory., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet68', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained HarDNet-68 model from PyTorch Hub, which is fast, efficient, and has low memory requirements, for image classification tasks.', 'code': 'import torch\nmodel = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet68', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet68', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "HarDNet", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet68', pretrained=True)", "api_arguments": [{"name": "hardnet68", "type": "str", "description": "HarDNet-68 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet68', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"hardnet68": {"Top-1 error": 23.52, "Top-5 error": 6.99}}}, "description": "Harmonic DenseNet (HarDNet) is a low memory trafficCNN model, which is fast and efficient. The basic concept is to minimize both computational cost and memory access cost at the same time, such that the HarDNet models are 35% faster than ResNet running on GPU comparing to models with the same accuracy (except the two DS models that were designed for comparing with MobileNet)."}}
{"code": "{'Instruction': A teenager wants their child to play with his new toy cars, but they want the phone to recognize the cars using machine vision. Give me an API that is computationally cheap and could recognize toy cars., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('huawei-noah/ghostnet', 'ghostnet_1x', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the GhostNet model from PyTorch Hub, which is designed for efficiency and can be fine-tuned to recognize toy cars using machine vision.', 'code': 'import torch\nmodel = torch.hub.load('huawei-noah/ghostnet', 'ghostnet_1x', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/ghostnet', model='ghostnet_1x', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Efficient networks by generating more features from cheap operations", "api_name": "GhostNet", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/ghostnet', model='ghostnet_1x', pretrained=True)", "api_arguments": ["pretrained"], "python_environment_requirements": ["torch", "torchvision", "PIL"], "example_code": ["import torch", "model = torch.hub.load('huawei-noah/ghostnet', 'ghostnet_1x', pretrained=True)", "model.eval()", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)", "print(probabilities)"], "performance": {"dataset": "ImageNet", "accuracy": {"Top-1 acc": "73.98", "Top-5 acc": "91.46"}}, "description": "The GhostNet architecture is based on an Ghost module structure which generates more features from cheap operations. Based on a set of intrinsic feature maps, a series of cheap operations are applied to generate many ghost feature maps that could fully reveal information underlying intrinsic features. Experiments conducted on benchmarks demonstrate the superiority of GhostNet in terms of speed and accuracy tradeoff."}}
{"code": "{'Instruction': Invent a smart camera system for a library that is capable of finding the book section from an image. Recommend an API capable of doing this., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'googlenet', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained GoogLeNet model from PyTorch Hub, which can be fine-tuned for classifying book sections in a library based on images.\", 'code': \"import torch\nmodel = torch.hub.load('pytorch/vision', 'googlenet', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='googlenet', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "GoogLeNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='googlenet', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": ">=1.0.0", "torchvision": ">=0.2.2"}, "example_code": {"import": ["import torch", "import urllib", "from PIL import Image", "from torchvision import transforms"], "load_model": "model = torch.hub.load('pytorch/vision:v0.10.0', 'googlenet', pretrained=True)", "model_eval": "model.eval()", "image_preprocessing": ["input_image = Image.open(filename)", "preprocess = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)"], "model_execution": ["if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)"], "output_processing": ["probabilities = torch.nn.functional.softmax(output[0], dim=0)", "top5_prob, top5_catid = torch.topk(probabilities, 5)"]}, "performance": {"dataset": "ImageNet", "accuracy": {"Top-1 error": "30.22", "Top-5 error": "10.47"}}, "description": "GoogLeNet is based on a deep convolutional neural network architecture codenamed 'Inception', which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014)."}}
{"code": "{'Instruction': Our team needs an image classification model for cross-domain tasks. Recommend a pre-trained model we can use in a project., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('XingangPan/IBN-Net', 'resnet101_ibn_a', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNet-101-IBN-a model from PyTorch Hub that has domain/appearance invariance, making it suitable for cross-domain tasks.', 'code': 'import torch\nmodel = torch.hub.load(\"XingangPan/IBN-Net\", \"resnet101_ibn_a\", pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnet101_ibn_a', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "IBN-Net", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnet101_ibn_a', pretrained=True)", "api_arguments": [{"name": "resnet101_ibn_a", "type": "str", "description": "ResNet-101-IBN-a model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('XingangPan/IBN-Net', 'resnet101_ibn_a', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"resnet101_ibn_a": {"Top-1 acc": 78.61, "Top-5 acc": 94.41}}}, "description": "IBN-Net is a CNN model with domain/appearance invariance. Motivated by style transfer works, IBN-Net carefully unifies instance normalization and batch normalization in a single deep network. It provides a simple way to increase both modeling and generalization capacities without adding model complexity. IBN-Net is especially suitable for cross domain or person/vehicle re-identification tasks."}}
{"code": "{'Instruction': A fashion E-commerce platform needs to predict clothing types from images. Suggest me a machine learning API that can be used to classify clothing items., 'Output': {'domain': 'Image Classification', 'api_call': 'model = torch.hub.load('XingangPan/IBN-Net', 'resnet50_ibn_a', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNet-50-IBN-a (IBN-Net) model from PyTorch Hub. This high-performance model can be fine-tuned for classifying clothing types from images provided by a fashion e-commerce platform.', 'code': 'import torch\nmodel = torch.hub.load('XingangPan/IBN-Net', 'resnet50_ibn_a', pretrained=True)\nmodel.eval()' }}", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnet50_ibn_a', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "IBN-Net", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnet50_ibn_a', pretrained=True)", "api_arguments": [{"name": "resnet50_ibn_a", "type": "str", "description": "ResNet-50-IBN-a model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('XingangPan/IBN-Net', 'resnet50_ibn_a', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"resnet50_ibn_a": {"Top-1 acc": 77.46, "Top-5 acc": 93.68}}}, "description": "IBN-Net is a CNN model with domain/appearance invariance. Motivated by style transfer works, IBN-Net carefully unifies instance normalization and batch normalization in a single deep network. It provides a simple way to increase both modeling and generalization capacities without adding model complexity. IBN-Net is especially suitable for cross domain or person/vehicle re-identification tasks."}}
{"code": "{'Instruction': I need a model that can classify various objects in images while being capable of distinguishing between a wide range of domains and appearances. Can you suggest an API for this?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('XingangPan/IBN-Net', 'se_resnet101_ibn_a', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the IBN-Net model from PyTorch Hub, which unifies instance normalization and batch normalization for cross-domain object classification.', 'code': 'import torch\nmodel = torch.hub.load('XingangPan/IBN-Net', 'se_resnet101_ibn_a', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='se_resnet101_ibn_a', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "IBN-Net", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='se_resnet101_ibn_a', pretrained=True)", "api_arguments": [{"name": "se_resnet101_ibn_a", "type": "str", "description": "SE-ResNet-101-IBN-a model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('XingangPan/IBN-Net', 'se_resnet101_ibn_a', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"se_resnet101_ibn_a": {"Top-1 acc": 78.75, "Top-5 acc": 94.49}}}, "description": "IBN-Net is a CNN model with domain/appearance invariance. Motivated by style transfer works, IBN-Net carefully unifies instance normalization and batch normalization in a single deep network. It provides a simple way to increase both modeling and generalization capacities without adding model complexity. IBN-Net is especially suitable for cross domain or person/vehicle re-identification tasks."}}
{"code": "{'Instruction': Suggest a suitable API for cross-domain classification tasks like person re-identification., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('XingangPan/IBN-Net', 'resnext101_ibn_a', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained IBN-Net (ResNeXt-101-IBN-a) model from PyTorch Hub, which is specifically designed for cross-domain classification tasks and provides domain/appearance invariance.', 'code': 'import torch\nmodel = torch.hub.load('XingangPan/IBN-Net', 'resnext101_ibn_a', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnext101_ibn_a', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "IBN-Net", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnext101_ibn_a', pretrained=True)", "api_arguments": [{"name": "resnext101_ibn_a", "type": "str", "description": "ResNeXt-101-IBN-a model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('XingangPan/IBN-Net', 'resnext101_ibn_a', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"resnext101_ibn_a": {"Top-1 acc": 79.12, "Top-5 acc": 94.58}}}, "description": "IBN-Net is a CNN model with domain/appearance invariance. Motivated by style transfer works, IBN-Net carefully unifies instance normalization and batch normalization in a single deep network. It provides a simple way to increase both modeling and generalization capacities without adding model complexity. IBN-Net is especially suitable for cross domain or person/vehicle re-identification tasks."}}
{"code": "{'Instruction': Can you give me an API capable of identifying animal species from a provided image?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNeSt50 model from PyTorch Hub, which can be fine-tuned for animal species identification from an image.', 'code': 'import torch\nmodel = torch.hub.load('zhanghang1989/ResNeSt', 'resnest50', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNeSt", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest50', pretrained=True)", "api_arguments": "resnest50", "python_environment_requirements": {"torch": "1.0.0", "torchvision": "0.2.2"}, "example_code": ["import torch", "model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest50', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeSt-50": 81.03}}, "description": "ResNeSt models are from the ResNeSt: Split-Attention Networks paper. They are a new ResNet variant that enables attention across feature-map groups. By stacking Split-Attention blocks ResNet-style, ResNeSt models outperform other networks with similar model complexities, and also help downstream tasks including object detection, instance segmentation, and semantic segmentation."}}
{"code": "{'Instruction': I have images of various objects and I need to classify them into different categories. Can you suggest an API that can help me with this?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest200', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained ResNeSt model for image classification from PyTorch Hub to classify images into different categories.\", 'code': 'import torch\nmodel = torch.hub.load('zhanghang1989/ResNeSt', 'resnest200', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest200', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNeSt", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest200', pretrained=True)", "api_arguments": "resnest200", "python_environment_requirements": {"torch": "1.0.0", "torchvision": "0.2.2"}, "example_code": ["import torch", "model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest200', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeSt-200": 83.84}}, "description": "ResNeSt models are from the ResNeSt: Split-Attention Networks paper. They are a new ResNet variant that enables attention across feature-map groups. By stacking Split-Attention blocks ResNet-style, ResNeSt models outperform other networks with similar model complexities, and also help downstream tasks including object detection, instance segmentation, and semantic segmentation."}}
{"code": "{'Instruction': Create an instance of an API that takes an image of a bird and predicts what type of bird it is., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest101', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNeSt-101 model from PyTorch Hub, which may be fine-tuned for bird species identification by training on a dataset of bird images.', 'code': 'import torch\nmodel = torch.hub.load('zhanghang1989/ResNeSt', 'resnest101', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest101', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNeSt", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest101', pretrained=True)", "api_arguments": "resnest101", "python_environment_requirements": {"torch": "1.0.0", "torchvision": "0.2.2"}, "example_code": ["import torch", "model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest101', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeSt-101": 82.83}}, "description": "ResNeSt models are from the ResNeSt: Split-Attention Networks paper. They are a new ResNet variant that enables attention across feature-map groups. By stacking Split-Attention blocks ResNet-style, ResNeSt models outperform other networks with similar model complexities, and also help downstream tasks including object detection, instance segmentation, and semantic segmentation."}}
{"code": "{'Instruction': Find me an efficient API for image classification that's optimized for better performance on CPUs., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_cpu', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained ProxylessNAS model from PyTorch Hub, which is specifically optimized for better performance on CPUs while maintaining high accuracy for image classification tasks.\", 'code': 'import torch\nmodel = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_cpu', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_cpu', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "ProxylessNAS", "api_name": "mit-han-lab/ProxylessNAS", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_cpu', pretrained=True)", "api_arguments": [{"name": "proxylessnas_cpu", "type": "str", "description": "ProxylessNAS optimized for CPU"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "target_platform = 'proxyless_cpu'", "model = torch.hub.load('mit-han-lab/ProxylessNAS', target_platform, pretrained=True)", "model.eval()"], "performance": {"dataset": [{"model_structure": "proxylessnas_cpu", "accuracy": 75.3}]}, "description": "ProxylessNAS models are from the ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware paper. They specialize CNN architectures for different hardware platforms, offering free yet significant performance boost on all three platforms (CPU, GPU, and Mobile) with similar accuracy."}}
{"code": "{'Instruction': I need to classify objects using an efficient model optimized for GPU. Provide me an API for that purpose., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_gpu', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained ProxylessNAS model from PyTorch Hub, which is optimized for GPU and offers efficient performance for object classification tasks.\", 'code': \"import torch\nmodel = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_gpu', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_gpu', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "ProxylessNAS", "api_name": "mit-han-lab/ProxylessNAS", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_gpu', pretrained=True)", "api_arguments": [{"name": "proxylessnas_gpu", "type": "str", "description": "ProxylessNAS optimized for GPU"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "target_platform = 'proxyless_gpu'", "model = torch.hub.load('mit-han-lab/ProxylessNAS', target_platform, pretrained=True)", "model.eval()"], "performance": {"dataset": [{"model_structure": "proxylessnas_gpu", "accuracy": 75.1}]}, "description": "ProxylessNAS models are from the ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware paper. They specialize CNN architectures for different hardware platforms, offering free yet significant performance boost on all three platforms (CPU, GPU, and Mobile) with similar accuracy."}}
{"code": "{'Instruction': We need to develop an object detection app for mobile devices. Suggest an API for a neural network based on ProxylessNAS that is optimized for mobile devices., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_mobile', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ProxylessNAS Mobile model from PyTorch Hub, which is specialized for object detection on mobile devices.', 'code': \"import torch\nmodel = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_mobile', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_mobile', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "ProxylessNAS", "api_name": "mit-han-lab/ProxylessNAS", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_mobile', pretrained=True)", "api_arguments": [{"name": "proxylessnas_mobile", "type": "str", "description": "ProxylessNAS optimized for Mobile"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "target_platform = 'proxyless_mobile'", "model = torch.hub.load('mit-han-lab/ProxylessNAS', target_platform, pretrained=True)", "model.eval()"], "performance": {"dataset": [{"model_structure": "proxylessnas_mobile", "accuracy": 74.6}]}, "description": "ProxylessNAS models are from the ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware paper. They specialize CNN architectures for different hardware platforms, offering free yet significant performance boost on all three platforms (CPU, GPU, and Mobile) with similar accuracy."}}
{"code": "{'Instruction': I want to create an app that identifies dog breeds from user uploaded images. Suggest an API that could recognize different dog breeds from images., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'mobilenet_v2', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MobileNet v2 model from PyTorch Hub, which can be fine-tuned for dog breed recognition in images.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'mobilenet_v2', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='mobilenet_v2', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MobileNet v2", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='mobilenet_v2', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision', 'mobilenet_v2', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)"], "performance": {"dataset": "ImageNet", "accuracy": {"top-1_error": 28.12, "top-5_error": 9.71}}, "description": "The MobileNet v2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input. MobileNet v2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, non-linearities in the narrow layers were removed in order to maintain representational power."}}
{"code": "{'Instruction': I have a dataset of images and want to classify them into different categories. Suggest an API that can do this task with high accuracy., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'resnet50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNet50 model from PyTorch Hub, which offers high accuracy for classifying images into different categories.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'resnet50', pretrained=True)\nmodel.eval()'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Deep Residual Networks", "api_name": "ResNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet50', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "bool", "default": "False", "description": "If True, returns a model pre-trained on ImageNet"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "resnet50", "top-1_error": 23.85, "top-5_error": 7.13}}, "description": "ResNet models are deep residual networks pre-trained on ImageNet. They were proposed in the paper 'Deep Residual Learning for Image Recognition'. Available model variants include ResNet18, ResNet34, ResNet50, ResNet101, and ResNet152."}}
{"code": "{'Instruction': I am working on a project that requires me to identify objects in an image. Find me an API that can do the task with pretrained models., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'resnet34', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pre-trained ResNet34 model from PyTorch Hub for object identification in images.\", 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'resnet34', pretrained=True)\nmodel.eval()'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet34', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Deep Residual Networks", "api_name": "ResNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet34', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "bool", "default": "False", "description": "If True, returns a model pre-trained on ImageNet"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet34', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "resnet34", "top-1_error": 26.7, "top-5_error": 8.58}}, "description": "ResNet models are deep residual networks pre-trained on ImageNet. They were proposed in the paper 'Deep Residual Learning for Image Recognition'. Available model variants include ResNet18, ResNet34, ResNet50, ResNet101, and ResNet152."}}
{"code": "{'Instruction': Can you assist me in selecting an API that automatically sorts through images of different kinds of animals?, 'Output': {'domain': 'Classification', 'api_call': 'model = torch.hub.load('pytorch/vision', 'resnet152', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNet152 model from PyTorch Hub, which can be used for sorting images of different animals.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'resnet152', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet152', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Deep Residual Networks", "api_name": "ResNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet152', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "bool", "default": "False", "description": "If True, returns a model pre-trained on ImageNet"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet152', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "resnet152", "top-1_error": 21.69, "top-5_error": 5.94}}, "description": "ResNet models are deep residual networks pre-trained on ImageNet. They were proposed in the paper 'Deep Residual Learning for Image Recognition'. Available model variants include ResNet18, ResNet34, ResNet50, ResNet101, and ResNet152."}}
{"code": "{'Instruction': Develop a program to categorize animals' species from a set of images with the help of an API., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'resnext50_32x4d', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNext50_32x4d model from PyTorch Hub and fine-tune it for animal species classification. The model can then be used to categorize animals from a set of images.', 'code': \"import torch\nmodel = torch.hub.load('pytorch/vision', 'resnext50_32x4d', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnext50_32x4d', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNext", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnext50_32x4d', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "PIL"], "example_code": ["import torch", "from PIL import Image", "from torchvision import transforms", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnext50_32x4d', pretrained=True)", "model.eval()", "input_image = Image.open('dog.jpg')", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)"], "performance": {"dataset": "ImageNet", "accuracy": {"resnext50_32x4d": {"top-1": 22.38, "top-5": 6.3}}}, "description": "ResNext is a next-generation ResNet architecture for image classification. It is more efficient and accurate than the original ResNet. This implementation includes two versions of the model, resnext50_32x4d and resnext101_32x8d, with 50 and 101 layers respectively."}}
{"code": "{'Instruction': A museum wants to build an image classifier for paintings. Recommend me an appropriate API that can be fine-tuned for painting classification., 'Output': {'domain': 'Classification', 'api_call': 'model = torch.hub.load(repo_or_dir=\"pytorch/vision\", model=\"resnext101_32x4d\", pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNext model from PyTorch Hub, which can be fine-tuned for painting classification.', 'code': \"import torch\nfrom torchvision import models\nmodel = torch.hub.load('pytorch/vision', 'resnext101_32x4d', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnext101_32x4d', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNext", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnext101_32x4d', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "PIL"], "example_code": ["import torch", "from PIL import Image", "from torchvision import transforms", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnext101_32x4d', pretrained=True)", "model.eval()", "input_image = Image.open('dog.jpg')", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)"], "performance": {"dataset": "ImageNet", "resnext101_32x8d": {"top-1": 20.69, "top-5": 5.47}}, "description": "ResNext is a next-generation ResNet architecture for image classification. It is more efficient and accurate than the original ResNet. This implementation includes two versions of the model, resnext50_32x4d and resnext101_32x8d, with 50 and 101 layers respectively."}}
{"code": "{'Instruction': A startup is in need of an efficient image classification API. Tell me about an API that can classify images effectively., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_s', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the SNNMLP Small model from PyTorch Hub for efficient and accurate image classification.', 'code': 'import torch\nmodel = torch.hub.load(\"huawei-noah/Efficient-AI-Backbones\", \"snnmlp_s\", pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_s', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SNNMLP", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_s', pretrained=True)", "api_arguments": [{"name": "snnmlp_s", "type": "str", "description": "SNNMLP Small model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_s', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "SNNMLP Small", "top-1": 83.3}}, "description": "SNNMLP incorporates the mechanism of LIF neurons into the MLP models, to achieve better accuracy without extra FLOPs. We propose a full-precision LIF operation to communicate between patches, including horizontal LIF and vertical LIF in different directions. We also propose to use group LIF to extract better local features. With LIF modules, our SNNMLP model achieves 81.9%, 83.3% and 83.6% top-1 accuracy on ImageNet dataset with only 4.4G, 8.5G and 15.2G FLOPs, respectively."}}
{"code": "{'Instruction': Find me an API that can recognize dog breeds in a given image., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'squeezenet1_0', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained SqueezeNet model from PyTorch Hub, which can be fine-tuned to identify dog breeds in a given image.\", 'code': \"import torch\nmodel = torch.hub.load('pytorch/vision', 'squeezenet1_0', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='squeezenet1_0', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SqueezeNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='squeezenet1_0', pretrained=True)", "api_arguments": {"version": "v0.10.0", "model": ["squeezenet1_0"], "pretrained": "True"}, "python_environment_requirements": {"torch": ">=1.9.0", "torchvision": ">=0.10.0"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'squeezenet1_0', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)", "print(probabilities)"], "performance": {"dataset": "ImageNet", "accuracy": {"squeezenet1_0": {"Top-1 error": 41.9, "Top-5 error": 19.58}}}, "description": "SqueezeNet is an image classification model that achieves AlexNet-level accuracy with 50x fewer parameters. It has two versions: squeezenet1_0 and squeezenet1_1, with squeezenet1_1 having 2.4x less computation and slightly fewer parameters than squeezenet1_0, without sacrificing accuracy."}}
{"code": "{'Instruction': What are the operational concepts for replacing an old neural network mechanism facilitating an MLP model with better accuracy?, 'Output': {'domain': 'Neural Network Improvement', 'solution': {'mechanism': 'Leaky Integrate-and-Fire (LIF) neurons', 'api_call': \"torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_t', pretrained=True)\", 'principle': 'SNNMLP utilizes the LIF mechanism to enhance the MLP model accuracy without additional FLOPs'}, 'explanation': 'LIF neurons are implemented in the SNNMLP model through full-precision LIF operations for communication between patches, horizontal and vertical LIF neurons in different directions, and group LIF for better local feature extraction. The resulting model achieves significantly higher top-1 accuracy on the ImageNet dataset with the same FLOPs.'}}", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_t', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SNNMLP", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_t', pretrained=True)", "api_arguments": [{"name": "snnmlp_t", "type": "str", "description": "SNNMLP Tiny model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_t', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "SNNMLP Tiny", "top-1": 81.88}}, "description": "SNNMLP incorporates the mechanism of LIF neurons into the MLP models, to achieve better accuracy without extra FLOPs. We propose a full-precision LIF operation to communicate between patches, including horizontal LIF and vertical LIF in different directions. We also propose to use group LIF to extract better local features. With LIF modules, our SNNMLP model achieves 81.9%, 83.3% and 83.6% top-1 accuracy on ImageNet dataset with only 4.4G, 8.5G and 15.2G FLOPs, respectively."}}
{"code": "{'Instruction': We are looking to classify the make and model of a car from visible images. Can you provide an API to accomplish this?, 'Output': {'domain': 'Classification', 'api_call': 'model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_b', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained SNNMLP model from PyTorch Hub, which can be fine-tuned to classify car make and model from images.', 'code': 'import torch\nmodel = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_b', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_b', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SNNMLP", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_b', pretrained=True)", "api_arguments": [{"name": "snnmlp_b", "type": "str", "description": "SNNMLP Base model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_b', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "SNNMLP Base", "top-1": 85.59}}, "description": "SNNMLP incorporates the mechanism of LIF neurons into the MLP models, to achieve better accuracy without extra FLOPs. We propose a full-precision LIF operation to communicate between patches, including horizontal LIF and vertical LIF in different directions. We also propose to use group LIF to extract better local features. With LIF modules, our SNNMLP model achieves 81.9%, 83.3% and 83.6% top-1 accuracy on ImageNet dataset with only 4.4G, 8.5G and 15.2G FLOPs, respectively."}}
{"code": "{'Instruction': Design an image classification system for a smart doorbell that aims to classify different types of animals like cats, dogs, and birds. Propose a machine learning API suitable for this purpose., 'Output': {'domain': 'Image Classification', 'api_call': 'model = torch.hub.load('pytorch/vision', 'squeezenet1_1', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the SqueezeNet model from PyTorch Hub, which is an image classification model suitable for classifying animals like cats, dogs, and birds.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'squeezenet1_1', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='squeezenet1_1', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SqueezeNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='squeezenet1_1', pretrained=True)", "api_arguments": {"version": "v0.10.0", "model": ["squeezenet1_1"], "pretrained": "True"}, "python_environment_requirements": {"torch": ">=1.9.0", "torchvision": ">=0.10.0"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'squeezenet1_1', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)", "print(probabilities)"], "performance": {"dataset": "ImageNet", "accuracy": {"squeezenet1_1": {"Top-1 error": 41.81, "Top-5 error": 19.38}}}, "description": "SqueezeNet is an image classification model that achieves AlexNet-level accuracy with 50x fewer parameters. It has two versions: squeezenet1_0 and squeezenet1_1, with squeezenet1_1 having 2.4x less computation and slightly fewer parameters than squeezenet1_0, without sacrificing accuracy."}}
{"code": "{'Instruction': A startup is building a robot to classify street objects. Tell me an API that can detect objects in a street view photograph and provide efficient inference., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'shufflenet_v2_x1_0', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ShuffleNet v2 model from PyTorch Hub, which is optimized for speed and memory, making it suitable for detecting objects in street view images.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'shufflenet_v2_x1_0', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='shufflenet_v2_x1_0', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ShuffleNet v2", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='shufflenet_v2_x1_0', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": "torch", "torchvision": "torchvision", "PIL": "Image", "urllib": "urllib"}, "example_code": {"import_libraries": ["import torch", "from PIL import Image", "from torchvision import transforms", "import urllib"], "load_model": ["model = torch.hub.load('pytorch/vision:v0.10.0', 'shufflenet_v2_x1_0', pretrained=True)", "model.eval()"], "load_image": ["url, filename = ('https://github.com/pytorch/hub/raw/master/images/dog.jpg', 'dog.jpg')", "try: urllib.URLopener().retrieve(url, filename)", "except: urllib.request.urlretrieve(url, filename)", "input_image = Image.open(filename)"], "preprocess_image": ["preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)"], "run_inference": ["if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)"], "get_probabilities": ["probabilities = torch.nn.functional.softmax(output[0], dim=0)"], "top_categories": ["top5_prob, top5_catid = torch.topk(probabilities, 5)", "for i in range(top5_prob.size(0)):", " print(categories[top5_catid[i]], top5_prob[i].item())"]}, "performance": {"dataset": "Imagenet", "accuracy": {"top-1_error": 30.64, "top-5_error": 11.68}}, "description": "ShuffleNet V2 is an efficient ConvNet optimized for speed and memory, pre-trained on Imagenet. It is designed based on practical guidelines for efficient network design, including speed and accuracy tradeoff."}}
{"code": "{'Instruction': Can you suggest an API I can use to classify various types of plants from their images?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg13', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained VGG13 model from PyTorch Hub, which can be fine-tuned for plant classification from images.', 'code': 'import torch\nmodel = torch.hub.load(\"pytorch/vision\", \"vgg13\", pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg13', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg13', pretrained=True)", "api_arguments": [{"name": "vgg13", "type": "str", "description": "VGG13 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg13', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg13": {"Top-1 error": 30.07, "Top-5 error": 10.75}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn,vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': Help me to identify what objects are in an image. Can you suggest an API that is capable of doing this?, 'Output': {'domain': 'Image Recognition', 'api_call': 'model = torch.hub.load('pytorch/vision', 'vgg11', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained VGG11 model from PyTorch Hub, which is capable of recognizing objects within an image.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg11', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg11', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg11', pretrained=True)", "api_arguments": [{"name": "vgg11", "type": "str", "description": "VGG11 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg11', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg11": {"Top-1 error": 30.98, "Top-5 error": 11.37}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': What is a pre-trained model I can use to recognize objects in a picture?, 'Output': {'domain': 'Image Recognition', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg13_bn', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained VGG13 model with batch normalization from PyTorch Hub for image recognition and object detection.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg13_bn', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg13_bn', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg13_bn', pretrained=True)", "api_arguments": [{"name": "vgg13_bn", "type": "str", "description": "VGG13 model with batch normalization"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg13_bn', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg13_bn": {"Top-1 error": 28.45, "Top-5 error": 9.63}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': I am building a software for an AI art gallery, and it needs to recognize artistic characteristics of different famous painting images. Identify a pre-trained model which can help me in this task., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg11_bn', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load a pretrained VGG11 with batch normalization model from the PyTorch Hub which can be fine-tuned for recognizing artistic characteristics in famous paintings.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg11_bn', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg11_bn', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg11_bn', pretrained=True)", "api_arguments": [{"name": "vgg11_bn", "type": "str", "description": "VGG11 model with batch normalization"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg11_bn', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg11_bn": {"Top-1 error": 26.7, "Top-5 error": 8.58}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': I have a bunch of pictures of plants, and I need an API that can identify the species for me., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg16', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained VGG16 model from PyTorch Hub, which can be fine-tuned to recognize and classify plant species accurately.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg16', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg16', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg16', pretrained=True)", "api_arguments": [{"name": "vgg16", "type": "str", "description": "VGG16 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg16": {"Top-1 error": 28.41, "Top-5 error": 9.62}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': An app developer needs a machine learning API for recognizing images taken by users to suggest appropriate captions for those images, recommend an API for that., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg19', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained VGG-19 model from PyTorch Hub, which can be used for image recognition to suggest suitable captions based on the recognized objects in the user's images.\", 'code': 'import torch\nmodel = torch.hub.load(\\'pytorch/vision\\', \\'vgg19\\', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg19', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg19', pretrained=True)", "api_arguments": [{"name": "vgg19", "type": "str", "description": "VGG19 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg19', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg19": {"Top-1 error": 27.62, "Top-5 error": 9.12}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': Recommend an API that will help me classify animals and birds in high quality images., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'wide_resnet101_2', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Wide ResNet101_2 model from PyTorch Hub for classifying animals and birds in high-quality images.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'wide_resnet101_2', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='wide_resnet101_2', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Wide Residual Networks", "api_name": "wide_resnet101_2", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='wide_resnet101_2', pretrained=True)", "api_arguments": "pretrained", "python_environment_requirements": "torch, torchvision", "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'wide_resnet101_2', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"wide_resnet101_2": {"Top-1 error": 21.16, "Top-5 error": 5.72}}}, "description": "Wide Residual networks simply have increased number of channels compared to ResNet. Otherwise the architecture is the same. Deeper ImageNet models with bottleneck block have increased number of channels in the inner 3x3 convolution. The wide_resnet50_2 and wide_resnet101_2 models were trained in FP16 with mixed precision training using SGD with warm restarts. Checkpoints have weights in half precision (except batch norm) for smaller size, and can be used in FP32 models too."}}
{"code": "{'Instruction': I need an image recognition API. Can you please suggest a model?, 'Output': {'domain': 'Image Recognition', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg19_bn', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained VGG19 model with batch normalization from PyTorch Hub for image recognition tasks.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg19_bn', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg19_bn', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg19_bn', pretrained=True)", "api_arguments": [{"name": "vgg19_bn", "type": "str", "description": "VGG19 model with batch normalization"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg19_bn', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg19_bn": {"Top-1 error": 25.76, "Top-5 error": 8.15}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': I am a content moderator for a video streaming platform, and I need a pretrained API to identify category of actions performed in a clip. What do you recommend?, 'Output': {'domain': 'Video Classification', 'api_call': \"model = torch.hub.load('facebookresearch/pytorchvideo', 'x3d_s', pretrained=True)\", 'api_provider': 'PyTorchVideo', 'explanation': \"Load the pretrained X3D-Small model from Facebook's PyTorchVideo for efficient video clip classification into various action categories.\", 'code': 'import torch\nmodel = torch.hub.load(\"facebookresearch/pytorchvideo\", \"x3d_s\", pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='x3d_s', pretrained=True)", "provider": "PyTorchVideo", "api_data": {"domain": "Video Classification", "framework": "PyTorchVideo", "functionality": "X3D Networks", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='x3d_s', pretrained=True)", "api_arguments": {"repository": "facebookresearch/pytorchvideo", "model": "x3d_s", "pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "pytorchvideo"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/pytorchvideo', 'x3d_s', pretrained=True)", "device = 'cpu'", "model = model.eval()", "model = model.to(device)"], "performance": {"dataset": "Kinetics 400", "accuracy": {"top1": 73.33, "top5": 91.27}, "flops": 2.96, "params": 3.79}, "description": "X3D model architectures are based on the paper 'X3D: Expanding Architectures for Efficient Video Recognition' by Christoph Feichtenhofer. They are pretrained on the Kinetics 400 dataset. This model is capable of classifying video clips into different action categories. It is provided by the FAIR PyTorchVideo library."}}
{"code": "{'Instruction': I built an AI personal trainer app that uses images from users' workouts to give live feedback. Recommend a model for classifying these workout images., 'Output': {'domain': 'Image Classification', 'api_call': 'model = torch.hub.load('pytorch/vision', 'wide_resnet50_2', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load a pretrained Wide ResNet-50-2 model from PyTorch Hub, which is robust and suitable for classifying images of users\\' workouts.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'wide_resnet50_2', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='wide_resnet50_2', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Wide Residual Networks", "api_name": "wide_resnet50_2", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='wide_resnet50_2', pretrained=True)", "api_arguments": "pretrained", "python_environment_requirements": "torch, torchvision", "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'wide_resnet50_2', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"wide_resnet50_2": {"Top-1 error": 21.49, "Top-5 error": 5.91}}}, "description": "Wide Residual networks simply have increased number of channels compared to ResNet. Otherwise the architecture is the same. Deeper ImageNet models with bottleneck block have increased number of channels in the inner 3x3 convolution. The wide_resnet50_2 and wide_resnet101_2 models were trained in FP16 with mixed precision training using SGD with warm restarts. Checkpoints have weights in half precision (except batch norm) for smaller size, and can be used in FP32 models too."}}
{"code": "{'Instruction': Tell me an API that can be used to classify different actions happening in a video clip., 'Output': {'domain': 'Video Classification', 'api_call': \"model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)\", 'api_provider': 'PyTorchVideo', 'explanation': \"Load the pretrained SlowFast Networks model from PyTorchVideo Hub, which is capable of classifying video clips into different action categories based on the Kinetics 400 dataset.\", 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='slowfast_r50', pretrained=True)", "provider": "PyTorchVideo", "api_data": {"domain": "Video Classification", "framework": "PyTorchVideo", "functionality": "SlowFast Networks", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='slowfast_r50', pretrained=True)", "api_arguments": {"repository": "facebookresearch/pytorchvideo", "model": "slowfast_r50", "pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "pytorchvideo"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)", "device = 'cpu'", "model = model.eval()", "model = model.to(device)"], "performance": {"dataset": "Kinetics 400", "accuracy": {"top1": 76.94, "top5": 92.69}, "flops": 65.71, "params": 34.57}, "description": "Slow Fast model architectures are based on the paper 'SlowFast Networks for Video Recognition' by Christoph Feichtenhofer et al. They are pretrained on the Kinetics 400 dataset using the 8x8 setting. This model is capable of classifying video clips into different action categories. It is provided by the FAIR PyTorchVideo library."}}
{"code": "{'Instruction': Recommend an image classification API capable of working efficiently on NVIDIA GPUs., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_gpunet', pretrained=True, model_type=model_type, model_math=precision)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained GPUNet model from NVIDIA Deep Learning Examples library in PyTorch Hub. GPUNet is optimized for NVIDIA GPU and TensorRT performance, offering efficient image classification.', 'code': \"import torch\nmodel_type = 'GPUNet-0'\nprecision = 'fp32'\ngpunet = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_gpunet', pretrained=True, model_type=model_type, model_math=precision)\"}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_gpunet', pretrained=True, model_type=model_type, model_math=precision)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "GPUNet Networks", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_gpunet', pretrained=True, model_type=model_type, model_math=precision)", "api_arguments": {"repository": "NVIDIA/DeepLearningExamples:torchhub", "model": "nvidia_gpunet", "pretrained": "True", "model_type": "GPUNet-0", "model_math": "fp32"}, "python_environment_requirements": ["torch", "validators", "matplotlib", "timm==0.5.4"], "example_code": ["import torch", "model_type = 'GPUNet-0'", "precision = 'fp32'", "gpunet = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_gpunet', pretrained=True, model_type=model_type, model_math=precision)", "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')", "gpunet.to(device)", "gpunet.eval()"], "performance": {"dataset": "IMAGENET", "description": "GPUNet demonstrates state-of-the-art inference performance up to 2x faster than EfficientNet-X and FBNet-V3."}, "description": "GPUNet is a family of Convolutional Neural Networks designed by NVIDIA using novel Neural Architecture Search (NAS) methods. They are optimized for NVIDIA GPU and TensorRT performance. GPUNet models are pretrained on the IMAGENET dataset and are capable of classifying images into different categories. The models are provided by the NVIDIA Deep Learning Examples library."}}
{"code": "{'Instruction': We need to compare the performance of an NLP model across different frameworks, can you provide an API to load a pretrained transformer model for sequence classification?, 'Output': {'domain': 'Natural Language Processing', 'api_call': \"model = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-cased')\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained BERT model from PyTorch-Transformers library, which can be used for sequence classification tasks.', 'code': \"import torch\nmodel = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-cased')\"}}", "api_call": "torch.hub.load(repo_or_dir='huggingface/pytorch-transformers')", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Natural Language Processing", "api_name": "PyTorch-Transformers", "api_call": "torch.hub.load(repo_or_dir='huggingface/pytorch-transformers')", "api_arguments": ["pretrained_model_or_path", "output_attention", "output_hidden_states", "config", "from_tf"], "python_environment_requirements": ["tqdm", "boto3", "requests", "regex", "sentencepiece", "sacremoses"], "example_code": "import torch\ntokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-cased')\n\ntext_1 = \"Jim Henson was a puppeteer\"\ntext_2 = \"Who was Jim Henson ?\"\n\nindexed_tokens = tokenizer.encode(text_1, text_2, add_special_tokens=True)\n\nmodel = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-cased')\n\nwith torch.no_grad():\n encoded_layers, _ = model(tokens_tensor, token_type_ids=segments_tensors)", "performance": {"dataset": [{"name": "MRPC", "accuracy": "Not provided"}]}, "description": "PyTorch-Transformers is a library of state-of-the-art pre-trained models for Natural Language Processing (NLP) including BERT, GPT, GPT-2, Transformer-XL, XLNet, XLM, RoBERTa, and DistilBERT. The library provides functionality for tokenization, configuration, and various model architectures for different tasks such as causal language modeling, sequence classification, question answering, and masked language modeling."}}
{"code": "{'Instruction': I want to quickly and efficiently classify images, I need an API for Once-for-all Networks., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load(repo_or_dir='mit-han-lab/once-for-all', model='ofa_supernet_mbv3_w10', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Once-for-all (OFA) Networks model from PyTorch Hub to efficiently classify images across different devices with varying resource constraints.', 'code': 'import torch\nmodel = torch.hub.load('mit-han-lab/once-for-all', 'ofa_supernet_mbv3_w10', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/once-for-all', model='ofa_supernet_mbv3_w10', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Once-for-all (OFA) Networks", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/once-for-all', model='ofa_supernet_mbv3_w10', pretrained=True)", "api_arguments": {"repository": "mit-han-lab/once-for-all", "model": "ofa_supernet_mbv3_w10", "pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "super_net_name = 'ofa_supernet_mbv3_w10'", "super_net = torch.hub.load('mit-han-lab/once-for-all', super_net_name, pretrained=True).eval()"], "performance": {"description": "OFA networks outperform state-of-the-art NAS methods (up to 4.0% ImageNet top1 accuracy improvement over MobileNetV3, or same accuracy but 1.5x faster than MobileNetV3, 2.6x faster than EfficientNet w.r.t measured latency) while reducing many orders of magnitude GPU hours and CO2 emission."}, "description": "Once-for-all (OFA) networks are a family of neural networks designed by MIT Han Lab. They decouple training and search, achieving efficient inference across various edge devices and resource constraints. OFA networks are pretrained on the IMAGENET dataset and are capable of classifying images into different categories."}}
{"code": "{'Instruction': A startup company is working on a new app where customers can upload photos and have them automatically categorized into objects. Suggest an API that can recognize objects in images., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models', 'resnet18_swsl', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Use the pretrained ResNet-18 model from Facebook Research, which is trained on semi-supervised and semi-weakly supervised ImageNet datasets for object recognition in images.', 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models', 'resnet18_swsl', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/semi-supervised-ImageNet1K-models', model='resnet18_swsl', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Semi-supervised and semi-weakly supervised ImageNet Models", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/semi-supervised-ImageNet1K-models', model='resnet18_swsl', pretrained=True)", "api_arguments": {"repository": "facebookresearch/semi-supervised-ImageNet1K-models", "model": "resnet18_swsl", "pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models', 'resnet18_swsl', pretrained=True)", "model.eval()"], "performance": {"description": "Semi-supervised and semi-weakly supervised ImageNet models achieve state-of-the-art accuracy of 81.2% on ImageNet for the widely used/adopted ResNet-50 model architecture."}, "description": "Semi-supervised and semi-weakly supervised ImageNet Models are introduced in the 'Billion scale semi-supervised learning for image classification' paper. These models are pretrained on a subset of unlabeled YFCC100M public image dataset and fine-tuned with the ImageNet1K training dataset. They are capable of classifying images into different categories and are provided by the Facebook Research library."}}
{"code": "{'Instruction': I am building a mobile app that classifies pictures of cats and dogs. Recommend an API for image classification., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_efficientnet_b0', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained EfficientNet-B0 model for image classification from PyTorch Hub to classify images of cats and dogs with high efficiency and low computational cost.', 'code': \"import torch\nmodel = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_efficientnet_b0', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_efficientnet_b0', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "EfficientNet", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_efficientnet_b0', pretrained=True)", "api_arguments": ["model_name", "pretrained"], "python_environment_requirements": ["validators", "matplotlib"], "example_code": "import torch\n\nefficientnet = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_efficientnet_b0', pretrained=True)\nutils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_convnets_processing_utils')\n\nefficientnet.eval().to(device)\n\nbatch = torch.cat([utils.prepare_input_from_uri(uri) for uri in uris]).to(device)\n\nwith torch.no_grad():\n output = torch.nn.functional.softmax(efficientnet(batch), dim=1)\n \nresults = utils.pick_n_best(predictions=output, n=5)", "performance": {"dataset": {"name": "IMAGENET", "accuracy": "Not provided"}}, "description": "EfficientNet is a family of image classification models that achieve state-of-the-art accuracy while being smaller and faster. The models are trained with mixed precision using Tensor Cores on the NVIDIA Volta and Ampere GPU architectures. The EfficientNet models include EfficientNet-B0, EfficientNet-B4, EfficientNet-WideSE-B0, and EfficientNet-WideSE-B4. The WideSE models use wider Squeeze-and-Excitation layers than the original EfficientNet models, resulting in slightly better accuracy."}}
{"code": "{'Instruction': Which API can I use to build a powerful English to French translation system?, 'Output': {'domain': 'Semantic Segmentation', 'api_call': \"model = torch.hub.load('pytorch/fairseq', 'transformer.wmt14.en-fr', tokenizer='moses', bpe='subword_nmt')\", 'api_provider': 'PyTorch', 'explanation': 'Use the Transformer (NMT) model from PyTorch Hub for building an English to French translation system based on state-of-the-art sequence-to-sequence modeling.', 'code': 'import torch\nmodel = torch.hub.load(\"pytorch/fairseq\", \"transformer.wmt14.en-fr\", tokenizer=\"moses\", bpe=\"subword_nmt\")'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/fairseq')", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "Neural Machine Translation", "api_name": "Transformer (NMT)", "api_call": "torch.hub.load(repo_or_dir='pytorch/fairseq')", "api_arguments": ["model_name", "tokenizer", "bpe", "beam", "sampling", "sampling_topk"], "python_environment_requirements": ["bitarray", "fastBPE", "hydra-core", "omegaconf", "regex", "requests", "sacremoses", "subword_nmt"], "example_code": "import torch\n\nen2fr = torch.hub.load('pytorch/fairseq', 'transformer.wmt14.en-fr', tokenizer='moses', bpe='subword_nmt')\n\nen2fr.cuda()\n\nfr = en2fr.translate('Hello world!', beam=5)\nassert fr == 'Bonjour \u00e0 tous !'", "performance": {"dataset": [{"name": "WMT'14", "accuracy": "Not provided"}, {"name": "WMT'18", "accuracy": "Not provided"}, {"name": "WMT'19", "accuracy": "Not provided"}]}, "description": "Transformer (NMT) is a powerful sequence-to-sequence modeling architecture that produces state-of-the-art neural machine translation systems. It is based on the paper 'Attention Is All You Need' and has been improved using techniques such as large-scale semi-supervised training, back-translation, and noisy-channel reranking. It supports English-French and English-German translation as well as round-trip translation for paraphrasing."}}
{"code": "{'Instruction': Tell me a model to use if I want to identify different objects in images without having my own dataset., 'Output': {'domain': 'Object Detection', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Single Shot MultiBox Detector (SSD) model from PyTorch Hub, which is trained on the COCO dataset and can detect objects in images accurately and efficiently.', 'code': 'import torch\nmodel = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_ssd', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Object Detection", "framework": "PyTorch", "functionality": "Single Shot MultiBox Detector", "api_name": "SSD", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_ssd', pretrained=True)", "api_arguments": ["model_name"], "python_environment_requirements": ["numpy", "scipy", "scikit-image", "matplotlib"], "example_code": "import torch\n\nssd_model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd')\nutils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd_processing_utils')\n\nssd_model.to('cuda')\nssd_model.eval()\n\ninputs = [utils.prepare_input(uri) for uri in uris]\ntensor = utils.prepare_tensor(inputs)\n\nwith torch.no_grad():\n detections_batch = ssd_model(tensor)\n\nresults_per_input = utils.decode_results(detections_batch)\nbest_results_per_input = [utils.pick_best(results, 0.40) for results in results_per_input]", "performance": {"dataset": {"name": "COCO", "accuracy": "Not provided"}}, "description": "The SSD (Single Shot MultiBox Detector) model is an object detection model based on the paper 'SSD: Single Shot MultiBox Detector'. It uses a deep neural network for detecting objects in images. This implementation replaces the obsolete VGG model backbone with the more modern ResNet-50 model. The SSD model is trained on the COCO dataset and can be used to detect objects in images with high accuracy and efficiency."}}
{"code": "{'Instruction': Let me know about an API that can convert English text into speech., 'Output': {'domain': 'Speech Synthesis', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tacotron2', model_math='fp16')\", 'api_provider': 'PyTorch', 'explanation': 'Load NVIDIA Tacotron 2 from PyTorch Hub. It is a neural network model that generates natural-sounding speech from English text.', 'code': \"import torch\nmodel = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tacotron2', model_math='fp16')\"}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_tacotron2', model_math='fp16')", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Text-to-Speech", "api_name": "Tacotron 2", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_tacotron2', model_math='fp16')", "api_arguments": {"model_math": "fp16"}, "python_environment_requirements": ["numpy", "scipy", "librosa", "unidecode", "inflect", "libsndfile1"], "example_code": ["import torch", "tacotron2 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tacotron2', model_math='fp16')", "tacotron2 = tacotron2.to('cuda')", "tacotron2.eval()", "text = 'Hello world, I missed you so much.'", "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tts_utils')", "sequences, lengths = utils.prepare_input_sequence([text])", "with torch.no_grad():", " mel, _, _ = tacotron2.infer(sequences, lengths)", " audio = waveglow.infer(mel)", "audio_numpy = audio[0].data.cpu().numpy()", "rate = 22050"], "performance": {"dataset": "LJ Speech", "accuracy": "Not specified"}, "description": "The Tacotron 2 model generates mel spectrograms from input text using an encoder-decoder architecture, and it is designed for generating natural-sounding speech from raw transcripts without any additional prosody information. This implementation uses Dropout instead of Zoneout to regularize the LSTM layers. The WaveGlow model (also available via torch.hub) is a flow-based model that consumes the mel spectrograms to generate speech."}}
{"code": "{'Instruction': I want to build an Instagram-like app that can tag dogs based on their breed. What API can help me with this?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MEAL_V2 model with ResNet-50 from PyTorch Hub, which can be fine-tuned for dog breed classification.', 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50', pretrained=True)", "api_arguments": {"model_name": "mealv2_resnest50"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ ResNet50", "resolution": "224", "parameters": "25.6M", "top1": "80.67", "top5": "95.09"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': Propose a way to classify random images with an efficiency of around 80% without any tricks., 'Output': {'domain': 'Image Classification', 'api_call': 'model = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MEAL-V2 model based on ensemble knowledge distillation from PyTorch Hub. MEAL-V2 models achieve state-of-the-art results (~80% Top-1 accuracy on ImageNet) without relying on common tricks and enhancements.', 'code': \"import torch\\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V1', model='mealv1_resnest50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V1', model='mealv1_resnest50', pretrained=True)", "api_arguments": {"model_name": "mealv1_resnest50"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V1 w/ ResNet50", "resolution": "224", "parameters": "25.6M", "top1": "78.21", "top5": "94.01"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': I need an API to help me identify different breeds of dogs in images. Can you find me one?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50_380x380', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained MEAL V2 model from PyTorch Hub, which can be fine-tuned for dog breed identification in images.\", 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50_380x380', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50_380x380', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50_380x380', pretrained=True)", "api_arguments": {"model_name": "mealv2_resnest50_380x380"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ ResNet50", "resolution": "380", "parameters": "25.6M", "top1": "81.72", "top5": "95.81"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': I am developing an app that can recognize different dog breeds from a photo. Can you recommend an API for this task?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', 'mealv2_mobilenetv3_small_075', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MEAL V2 model with MobileNet V3-Small 0.75 architecture from PyTorch Hub for image classification. This model can be fine-tuned for dog breed identification.', 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', 'mealv2_mobilenetv3_small_075', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenetv3_small_075', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenetv3_small_075', pretrained=True)", "api_arguments": {"model_name": "mealv2_mobilenetv3_small_075"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ MobileNet V3-Small 0.75", "resolution": "224", "parameters": "2.04M", "top1": "67.60", "top5": "87.23"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': I have an image of an object, and I need an API to classify it. Ensure the API provides a high accuracy without tricks like architecture modification or mixup/cutmix training., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained MEAL V2 model from PyTorch Hub for image classification. This model achieves high accuracy without tricks like architecture modification, mixup/cutmix training, or additional outside training data.\", 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50_cutmix', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50_cutmix', pretrained=True)", "api_arguments": {"model_name": "mealv2_resnest50_cutmix"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 + CutMix w/ ResNet50", "resolution": "224", "parameters": "25.6M", "top1": "80.98", "top5": "95.35"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': I need an API that can identify the type of fruit in an image., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenetv3_small_100', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MEAL-V2 model from PyTorch Hub, which can be fine-tuned for fruit classification.', 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenetv3_small_100', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenetv3_small_100', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenetv3_small_100', pretrained=True)", "api_arguments": {"model_name": "mealv2_mobilenetv3_small_100"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ MobileNet V3-Small 1.0", "resolution": "224", "parameters": "2.54M", "top1": "69.65", "top5": "88.71"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': We want to build an application that recognize objects in an image. What API can classify these images for us?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', 'mealv2_mobilenet_v3_large_100', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained MEAL V2 model with MobileNet V3-Large architecture from PyTorch Hub to classify objects in images.\", 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', 'mealv2_mobilenet_v3_large_100', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenet_v3_large_100', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenet_v3_large_100', pretrained=True)", "api_arguments": {"model_name": "mealv2_mobilenet_v3_large_100"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ MobileNet V3-Large 1.0", "resolution": "224", "parameters": "5.48M", "top1": "76.92", "top5": "93.32"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': Recommend an API to analyze workout videos and classify them into different exercise categories., 'Output': {'domain': 'Video Classification', 'api_call': \"model = torch.hub.load('facebookresearch/pytorchvideo', 'slow_r50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Use the pretrained 3D ResNet (slow_r50) model from PyTorch Hub to classify workout videos into different exercise categories.', 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/pytorchvideo', 'slow_r50', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='slow_r50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Video Classification", "framework": "PyTorch", "functionality": "3D ResNet", "api_name": "slow_r50", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='slow_r50', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["torch", "json", "urllib", "pytorchvideo", "torchvision", "torchaudio", "torchtext", "torcharrow", "TorchData", "TorchRec", "TorchServe", "PyTorch on XLA Devices"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/pytorchvideo', 'slow_r50', pretrained=True)", "device = 'cpu'", "model = model.eval()", "model = model.to(device)"], "performance": {"dataset": "Kinetics 400", "accuracy": {"top_1": 74.58, "top_5": 91.63}, "Flops (G)": 54.52, "Params (M)": 32.45}, "description": "The 3D ResNet model is a Resnet-style video classification network pretrained on the Kinetics 400 dataset. It is based on the architecture from the paper 'SlowFast Networks for Video Recognition' by Christoph Feichtenhofer et al."}}
{"code": "{'Instruction': Identify an API that can detect vehicles, drivable areas, and lanes from images of roads., 'Output': {'domain': 'Object Detection', 'api_call': \"model = torch.hub.load(repo_or_dir='datvuthanh/hybridnets', model='hybridnets', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained HybridNets model from PyTorch Hub, which can detect vehicles, drivable areas, and lanes from road images.', 'code': \"import torch\nmodel = torch.hub.load('datvuthanh/hybridnets', 'hybridnets', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='datvuthanh/hybridnets', model='hybridnets', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Object Detection", "framework": "PyTorch", "functionality": "Traffic Object Detection, Drivable Area Segmentation, Lane Detection", "api_name": "HybridNets", "api_call": "torch.hub.load(repo_or_dir='datvuthanh/hybridnets', model='hybridnets', pretrained=True)", "api_arguments": "pretrained", "python_environment_requirements": "Python>=3.7, PyTorch>=1.10", "example_code": "import torch\nmodel = torch.hub.load('datvuthanh/hybridnets', 'hybridnets', pretrained=True)\nimg = torch.randn(1,3,640,384)\nfeatures, regression, classification, anchors, segmentation = model(img)", "performance": {"dataset": [{"name": "BDD100K", "accuracy": {"Traffic Object Detection": {"Recall (%)": 92.8, "mAP@0.5 (%)": 77.3}, "Drivable Area Segmentation": {"Drivable mIoU (%)": 90.5}, "Lane Line Detection": {"Accuracy (%)": 85.4, "Lane Line IoU (%)": 31.6}}}]}, "description": "HybridNets is an end2end perception network for multi-tasks. Our work focused on traffic object detection, drivable area segmentation and lane detection. HybridNets can run real-time on embedded systems, and obtains SOTA Object Detection, Lane Detection on BDD100K Dataset."}}
{"code": "{'Instruction': Describe an API suitable for recognizing objects in an image with at least 80% accuracy on the ImageNet dataset., 'Output': {'domain': 'Image Classification', 'api_call': 'model = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_efficientnet_b0', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained MEAL V2 model with EfficientNet-B0 from PyTorch Hub, which achieves 80%+ Top-1 accuracy on the ImageNet dataset for object recognition tasks.\", 'code': 'import torch\nmodel = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_efficientnet_b0', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_efficientnet_b0', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_efficientnet_b0', pretrained=True)", "api_arguments": {"model_name": "mealv2_efficientnet_b0"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ EfficientNet-B0", "resolution": "224", "parameters": "5.29M", "top1": "78.29", "top5": "93.95"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': Find me an API that can compute depth information from a single image of a scene., 'Output': {'domain': 'Image Depth Estimation', 'api_call': \"model = torch.hub.load('intel-isl/MiDaS', 'DPT_Large', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MiDaS model from PyTorch Hub to compute depth information from a single image.', 'code': \"import torch\nmodel = torch.hub.load('intel-isl/MiDaS', 'DPT_Large', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='intel-isl/MiDaS', model='DPT_Large', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Computing relative depth from a single image", "api_name": "MiDaS", "api_call": "torch.hub.load(repo_or_dir='intel-isl/MiDaS', model='DPT_Large', pretrained=True)", "api_arguments": {"repo_or_dir": "intel-isl/MiDaS", "model": "model_type"}, "python_environment_requirements": "pip install timm", "example_code": ["import cv2", "import torch", "import urllib.request", "import matplotlib.pyplot as plt", "url, filename = ('https://github.com/pytorch/hub/raw/master/images/dog.jpg', 'dog.jpg')", "urllib.request.urlretrieve(url, filename)", "model_type = 'DPT_Large'", "midas = torch.hub.load('intel-isl/MiDaS', 'DPT_Large')", "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')", "midas.to(device)", "midas.eval()", "midas_transforms = torch.hub.load('intel-isl/MiDaS', 'transforms')", "if model_type == 'DPT_Large' or model_type == 'DPT_Hybrid':", " transform = midas_transforms.dpt_transform", "else:", " transform = midas_transforms.small_transform", "img = cv2.imread(filename)", "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)", "input_batch = transform(img).to(device)", "with torch.no_grad():", " prediction = midas(input_batch)", "prediction = torch.nn.functional.interpolate(", " prediction.unsqueeze(1),", " size=img.shape[:2],", " mode='bicubic',", " align_corners=False,", ").squeeze()", "output = prediction.cpu().numpy()", "plt.imshow(output)", "plt.show()"], "performance": {"dataset": "10 distinct datasets", "accuracy": "Multi-objective optimization"}, "description": "MiDaS computes relative inverse depth from a single image. The repository provides multiple models that cover different use cases ranging from a small, high-speed model to a very large model that provide the highest accuracy. The models have been trained on 10 distinct datasets using multi-objective optimization to ensure high quality on a wide range of inputs."}}
{"code": "{'Instruction': A friend is building a robot that can detect distance of object from camera. Provide an ML API that computes relative depth using a single image input., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('intel-isl/MiDaS', 'DPT_Hybrid', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MiDaS model for computing relative depth from a single image in PyTorch Hub. It can be used to estimate object distances given a camera input.', 'code': 'import torch\nmodel = torch.hub.load('intel-isl/MiDaS', 'DPT_Hybrid', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='intel-isl/MiDaS', model='DPT_Hybrid', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Computing relative depth from a single image", "api_name": "MiDaS", "api_call": "torch.hub.load(repo_or_dir='intel-isl/MiDaS', model='DPT_Hybrid', pretrained=True)", "api_arguments": {"repo_or_dir": "intel-isl/MiDaS", "model": "model_type"}, "python_environment_requirements": "pip install timm", "example_code": ["import cv2", "import torch", "import urllib.request", "import matplotlib.pyplot as plt", "url, filename = ('https://github.com/pytorch/hub/raw/master/images/dog.jpg', 'dog.jpg')", "urllib.request.urlretrieve(url, filename)", "model_type = 'DPT_Large'", "midas = torch.hub.load('intel-isl/MiDaS', 'DPT_Hybrid')", "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')", "midas.to(device)", "midas.eval()", "midas_transforms = torch.hub.load('intel-isl/MiDaS', 'transforms')", "if model_type == 'DPT_Large' or model_type == 'DPT_Hybrid':", " transform = midas_transforms.dpt_transform", "else:", " transform = midas_transforms.small_transform", "img = cv2.imread(filename)", "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)", "input_batch = transform(img).to(device)", "with torch.no_grad():", " prediction = midas(input_batch)", "prediction = torch.nn.functional.interpolate(", " prediction.unsqueeze(1),", " size=img.shape[:2],", " mode='bicubic',", " align_corners=False,", ").squeeze()", "output = prediction.cpu().numpy()", "plt.imshow(output)", "plt.show()"], "performance": {"dataset": "10 distinct datasets", "accuracy": "Multi-objective optimization"}, "description": "MiDaS computes relative inverse depth from a single image. The repository provides multiple models that cover different use cases ranging from a small, high-speed model to a very large model that provide the highest accuracy. The models have been trained on 10 distinct datasets using multi-objective optimization to ensure high quality on a wide range of inputs."}}
{"code": "{'Instruction': Tell me about an API that can estimate depth for single images, suitable for real-time scenarios like autonomous vehicles., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('intel-isl/MiDaS', 'MiDaS_small', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the MiDaS_small model from PyTorch Hub, which can compute relative depth from a single image and is suitable for real-time scenarios such as autonomous vehicles.', 'code': 'import torch\nmodel = torch.hub.load('intel-isl/MiDaS', 'MiDaS_small', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='intel-isl/MiDaS', model='MiDaS_small', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Computing relative depth from a single image", "api_name": "MiDaS", "api_call": "torch.hub.load(repo_or_dir='intel-isl/MiDaS', model='MiDaS_small', pretrained=True)", "api_arguments": {"repo_or_dir": "intel-isl/MiDaS", "model": "model_type"}, "python_environment_requirements": "pip install timm", "example_code": ["import cv2", "import torch", "import urllib.request", "import matplotlib.pyplot as plt", "url, filename = ('https://github.com/pytorch/hub/raw/master/images/dog.jpg', 'dog.jpg')", "urllib.request.urlretrieve(url, filename)", "model_type = 'DPT_Large'", "midas = torch.hub.load('intel-isl/MiDaS', 'MiDaS_small')", "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')", "midas.to(device)", "midas.eval()", "midas_transforms = torch.hub.load('intel-isl/MiDaS', 'transforms')", "if model_type == 'DPT_Large' or model_type == 'DPT_Hybrid':", " transform = midas_transforms.dpt_transform", "else:", " transform = midas_transforms.small_transform", "img = cv2.imread(filename)", "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)", "input_batch = transform(img).to(device)", "with torch.no_grad():", " prediction = midas(input_batch)", "prediction = torch.nn.functional.interpolate(", " prediction.unsqueeze(1),", " size=img.shape[:2],", " mode='bicubic',", " align_corners=False,", ").squeeze()", "output = prediction.cpu().numpy()", "plt.imshow(output)", "plt.show()"], "performance": {"dataset": "10 distinct datasets", "accuracy": "Multi-objective optimization"}, "description": "MiDaS computes relative inverse depth from a single image. The repository provides multiple models that cover different use cases ranging from a small, high-speed model to a very large model that provide the highest accuracy. The models have been trained on 10 distinct datasets using multi-objective optimization to ensure high quality on a wide range of inputs."}}
{"code": "{'Instruction': I need to separate the vocals from a piece of music for a DJ performance. Find me an API that can do this., 'Output': {'domain': 'Audio Separation', 'api_call': \"separator = torch.hub.load('sigsep/open-unmix-pytorch', 'umxhq', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Open-Unmix model from PyTorch Hub to separate vocals, drums, bass, and other instruments in a pop music track. This model provides accurate music source separation tailored for DJ performances.', 'code': 'import torch\nseparator = torch.hub.load('sigsep/open-unmix-pytorch', 'umxhq', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='sigsep/open-unmix-pytorch', model='umxhq', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Audio Separation", "framework": "PyTorch", "functionality": "Music Source Separation", "api_name": "Open-Unmix", "api_call": "torch.hub.load(repo_or_dir='sigsep/open-unmix-pytorch', model='umxhq', pretrained=True)", "api_arguments": ["model_name"], "python_environment_requirements": ["PyTorch >=1.6.0", "torchaudio"], "example_code": ["import torch", "separator = torch.hub.load('sigsep/open-unmix-pytorch', 'umxhq')", "audio = torch.rand((1, 2, 100000))", "original_sample_rate = separator.sample_rate", "estimates = separator(audio)"], "performance": {"dataset": "MUSDB18", "accuracy": "N/A"}, "description": "Open-Unmix provides ready-to-use models that allow users to separate pop music into four stems: vocals, drums, bass and the remaining other instruments. The models were pre-trained on the freely available MUSDB18 dataset."}}
{"code": "{'Instruction': Can you suggest an API that can be used to convert a recorded speech to text? I have recordings from meetings I want to be transcribed., 'Output': {'domain': 'Speech-To-Text', 'api_call': \"model = torch.hub.load('snakers4/silero-models', 'silero_stt', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Silero Speech-To-Text model from PyTorch Hub, which can transcribe recordings from meetings into text.', 'code': 'import torch\nmodel = torch.hub.load('snakers4/silero-models', 'silero_stt', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_stt', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Speech-To-Text", "api_name": "Silero Models", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_stt', pretrained=True)", "api_arguments": {"repo_or_dir": "snakers4/silero-models", "model": "silero_stt", "language": ["en", "de", "es"], "device": "device"}, "python_environment_requirements": ["pip install -q torchaudio omegaconf soundfile"], "example_code": ["import torch", "import zipfile", "import torchaudio", "from glob import glob", "device = torch.device('cpu')", "model, decoder, utils = torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_stt', language='en', device=device)", "(read_batch, split_into_batches, read_audio, prepare_model_input) = utils", "torch.hub.download_url_to_file('https://opus-codec.org/static/examples/samples/speech_orig.wav', dst ='speech_orig.wav', progress=True)", "test_files = glob('speech_orig.wav')", "batches = split_into_batches(test_files, batch_size=10)", "input = prepare_model_input(read_batch(batches[0]), device=device)", "output = model(input)", "for example in output:", " print(decoder(example.cpu()))"], "performance": {"dataset": "Open-STT", "accuracy": "See the wiki for quality and performance benchmarks"}, "description": "Silero Speech-To-Text models provide enterprise grade STT in a compact form-factor for several commonly spoken languages. The models are robust to a variety of dialects, codecs, domains, noises, and lower sampling rates. They consume a normalized audio in the form of samples and output frames with token probabilities. A decoder utility is provided for simplicity."}}
{"code": "{'Instruction': Recommend an API to analyze audio streams and detect when someone is speaking., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the Silero Voice Activity Detector (VAD) model from PyTorch Hub, which can analyze audio streams and detect speech segments.', 'code': 'import torch\nmodel, utils = torch.hub.load(repo_or_dir=\"snakers4/silero-vad\", model=\"silero_vad\", force_reload=True)'}}", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Voice Activity Detection", "api_name": "Silero Voice Activity Detector", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=True)", "api_arguments": {"repo_or_dir": "snakers4/silero-vad", "model": "silero_vad", "force_reload": "True"}, "python_environment_requirements": {"torchaudio": "pip install -q torchaudio"}, "example_code": {"import": ["import torch", "torch.set_num_threads(1)", "from IPython.display import Audio", "from pprint import pprint"], "download_example": "torch.hub.download_url_to_file('https://models.silero.ai/vad_models/en.wav', 'en_example.wav')", "load_model": "model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=True)", "load_utils": "(get_speech_timestamps, _, read_audio, _) = utils", "set_sampling_rate": "sampling_rate = 16000", "read_audio": "wav = read_audio('en_example.wav', sampling_rate=sampling_rate)", "get_speech_timestamps": "speech_timestamps = get_speech_timestamps(wav, model, sampling_rate=sampling_rate)", "print_speech_timestamps": "pprint(speech_timestamps)"}, "performance": {"dataset": "", "accuracy": ""}, "description": "Silero VAD is a pre-trained enterprise-grade Voice Activity Detector (VAD) that aims to provide a high-quality and modern alternative to the WebRTC Voice Activity Detector. The model is optimized for performance on 1 CPU thread and is quantized."}}
{"code": "{'Instruction': Identify an API for recognizing objects and dividing them into different regions in given images., 'Output': {'domain': 'Semantic Segmentation', 'api_call': \"model = torch.hub.load('pytorch/vision', 'deeplabv3_resnet101', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained DeepLabV3 model with a ResNet101 backbone from PyTorch Hub, which can recognize objects and divide them into different regions in images.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'deeplabv3_resnet101', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='deeplabv3_resnet101', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "DeepLabV3", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='deeplabv3_resnet101', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": "latest", "torchvision": "latest", "PIL": "latest", "matplotlib": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_resnet101', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "input_image = input_image.convert('RGB')", "preprocess = transforms.Compose([", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)['out'][0]", "output_predictions = output.argmax(0)"], "performance": {"dataset": "COCO val2017", "accuracy": [{"model": "deeplabv3_resnet101", "Mean IOU": 67.4, "Global Pixelwise Accuracy": 92.4}]}, "description": "DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset."}}
{"code": "{'Instruction': Help me find an API for segmenting objects in images using a pre-trained model for semantic segmentation., 'Output': {'domain': 'Semantic Segmentation', 'api_call': \"model = torch.hub.load('pytorch/vision', 'deeplabv3_resnet50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained DeepLabV3 model with a ResNet-50 backbone from PyTorch Hub for semantic segmentation of objects in images.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'deeplabv3_resnet50', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='deeplabv3_resnet50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "DeepLabV3", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='deeplabv3_resnet50', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": "latest", "torchvision": "latest", "PIL": "latest", "matplotlib": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_resnet50', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "input_image = input_image.convert('RGB')", "preprocess = transforms.Compose([", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)['out'][0]", "output_predictions = output.argmax(0)"], "performance": {"dataset": "COCO val2017", "accuracy": [{"model": "deeplabv3_resnet50", "Mean IOU": 66.4, "Global Pixelwise Accuracy": 92.4}]}, "description": "DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset."}}
{"code": "{'Instruction': An architect wishes to semantically segment different objects in an urban image. What API can they use for this purpose?, 'Output': {'domain': 'Semantic Segmentation', 'api_call': \"model = torch.hub.load('pytorch/vision', 'deeplabv3_mobilenet_v3_large', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Use the DeepLabV3 model with MobileNet-V3 backbone from PyTorch Hub, pretrained on a subset of COCO and Pascal VOC datasets, to perform semantic segmentation on urban images.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'deeplabv3_mobilenet_v3_large', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='deeplabv3_mobilenet_v3_large', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "DeepLabV3", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='deeplabv3_mobilenet_v3_large', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": "latest", "torchvision": "latest", "PIL": "latest", "matplotlib": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_mobilenet_v3_large', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "input_image = input_image.convert('RGB')", "preprocess = transforms.Compose([", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)['out'][0]", "output_predictions = output.argmax(0)"], "performance": {"dataset": "COCO val2017", "accuracy": [{"model": "deeplabv3_mobilenet_v3_large", "Mean IOU": 60.3, "Global Pixelwise Accuracy": 91.4}]}, "description": "DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset."}}
{"code": "{'Instruction': ABC news needs a text-to-speech service for their articles. Offer them an API., 'Output': {'domain': 'Text-To-Speech', 'api_call': 'model, symbols, sample_rate, example_text, apply_tts = torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_tts', language=language, speaker=speaker)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Silero Text-to-Speech model from PyTorch Hub to convert ABC news articles into natural-sounding speech in a single line of code.', 'code': 'import torch\nlanguage = 'en'\nspeaker = 'lj_16khz'\ndevice = torch.device('cpu')\nmodel, symbols, sample_rate, example_text, apply_tts = torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_tts', language=language, speaker=speaker)\nmodel = model.to(device)'}}", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_tts', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Text-To-Speech", "framework": "PyTorch", "functionality": "Text-To-Speech", "api_name": "Silero Text-To-Speech Models", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_tts', pretrained=True)", "api_arguments": {"repo_or_dir": "snakers4/silero-models", "model": "silero_tts", "language": "language", "speaker": "speaker"}, "python_environment_requirements": ["pip install -q torchaudio omegaconf"], "example_code": "import torch\nlanguage = 'en'\nspeaker = 'lj_16khz'\ndevice = torch.device('cpu')\nmodel, symbols, sample_rate, example_text, apply_tts = torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_tts', language=language, speaker=speaker)\nmodel = model.to(device)\naudio = apply_tts(texts=[example_text], model=model, sample_rate=sample_rate, symbols=symbols, device=device)", "performance": {"dataset": [{"language": "Russian", "speakers": 6}, {"language": "English", "speakers": 1}, {"language": "German", "speakers": 1}, {"language": "Spanish", "speakers": 1}, {"language": "French", "speakers": 1}], "accuracy": "High throughput on slow hardware. Decent performance on one CPU thread"}, "description": "Silero Text-To-Speech models provide enterprise grade TTS in a compact form-factor for several commonly spoken languages. They offer one-line usage, naturally sounding speech, no GPU or training required, minimalism and lack of dependencies, a library of voices in many languages, support for 16kHz and 8kHz out of the box."}}
{"code": "{'Instruction': Design a neural network that efficiently generates low-resolution images., 'Output': {'domain': 'Image Generation', 'api_call': \"model = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'DCGAN', pretrained=True, useGPU=use_gpu)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Deep Convolutional Generative Adversarial Networks (DCGAN) model from PyTorch Hub, designed for efficient generation of low-resolution images.', 'code': 'import torch\nuse_gpu = True if torch.cuda.is_available() else False\nmodel = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'DCGAN', pretrained=True, useGPU=use_gpu)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorch_GAN_zoo:hub', model='DCGAN', pretrained=True, useGPU=use_gpu)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Generative Adversarial Networks", "api_name": "DCGAN", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorch_GAN_zoo:hub', model='DCGAN', pretrained=True, useGPU=use_gpu)", "api_arguments": {"pretrained": "True", "useGPU": "use_gpu"}, "python_environment_requirements": "Python 3", "example_code": {"import": ["import torch", "import matplotlib.pyplot as plt", "import torchvision"], "use_gpu": "use_gpu = True if torch.cuda.is_available() else False", "load_model": "model = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'DCGAN', pretrained=True, useGPU=use_gpu)", "build_noise_data": "noise, _ = model.buildNoiseData(num_images)", "generate_images": "with torch.no_grad(): generated_images = model.test(noise)", "plot_images": ["plt.imshow(torchvision.utils.make_grid(generated_images).permute(1, 2, 0).cpu().numpy())", "plt.show()"]}, "performance": {"dataset": "FashionGen", "accuracy": "N/A"}, "description": "DCGAN is a model designed in 2015 by Radford et. al. in the paper Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks. It is a GAN architecture both very simple and efficient for low resolution image generation (up to 64x64)."}}
{"code": "{'Instruction': Recommend an API suitable for classifying images of different dog breeds., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x8d_wsl', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained ResNext-101 32x8d WSL model from PyTorch Hub for classifying images of different dog breeds with high accuracy.\", 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x8d_wsl', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x8d_wsl', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNext WSL", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x8d_wsl', pretrained=True)", "api_arguments": [{"name": "resnext101_32x8d_wsl", "type": "str", "description": "ResNeXt-101 32x8d WSL model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x8d_wsl')", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(output[0])", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeXt-101 32x8d": {"Top-1 Acc.": "82.2", "Top-5 Acc.": "96.4"}}}, "description": "The provided ResNeXt models are pre-trained in weakly-supervised fashion on 940 million public images with 1.5K hashtags matching with 1000 ImageNet1K synsets, followed by fine-tuning on ImageNet1K dataset. The models significantly improve the training accuracy on ImageNet compared to training from scratch. They achieve state-of-the-art accuracy of 85.4% on ImageNet with the ResNext-101 32x48d model."}}
{"code": "{'Instruction': A researcher is looking for an image classification API that has been trained on a large dataset like ImageNet. Provide the API details for the ResNext WSL model., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNext WSL model from PyTorch Hub, which has been trained on ImageNet for image classification tasks.', 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x16d_wsl', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNext WSL", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x16d_wsl', pretrained=True)", "api_arguments": [{"name": "resnext101_32x16d_wsl", "type": "str", "description": "ResNeXt-101 32x16d WSL model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl')", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(output[0])", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeXt-101 32x16d": {"Top-1 Acc.": "84.2", "Top-5 Acc.": "97.2"}}}, "description": "The provided ResNeXt models are pre-trained in weakly-supervised fashion on 940 million public images with 1.5K hashtags matching with 1000 ImageNet1K synsets, followed by fine-tuning on ImageNet1K dataset. The models significantly improve the training accuracy on ImageNet compared to training from scratch. They achieve state-of-the-art accuracy of 85.4% on ImageNet with the ResNext-101 32x48d model."}}
{"code": "{'Instruction': Find me an pretrained model for image classification that has scored 85% or more Top-1 accuracy on ImageNet., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x32d_wsl', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNeXt WSL model from PyTorch Hub, which has achieved a state-of-the-art Top-1 accuracy of 85.1% on ImageNet.', 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x32d_wsl', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x32d_wsl', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNext WSL", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x32d_wsl', pretrained=True)", "api_arguments": [{"name": "resnext101_32x32d_wsl", "type": "str", "description": "ResNeXt-101 32x32d WSL model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x32d_wsl')", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(output[0])", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeXt-101 32x32d": {"Top-1 Acc.": "85.1", "Top-5 Acc.": "97.5"}}}, "description": "The provided ResNeXt models are pre-trained in weakly-supervised fashion on 940 million public images with 1.5K hashtags matching with 1000 ImageNet1K synsets, followed by fine-tuning on ImageNet1K dataset. The models significantly improve the training accuracy on ImageNet compared to training from scratch. They achieve state-of-the-art accuracy of 85.4% on ImageNet with the ResNext-101 32x48d model."}}
{"code": "{'Instruction': I want an API to classify birds into different species using images. Can you provide me a code snippet to load a pre-trained model that can perform this task?, 'Output': {'domain': 'Classification', 'api_call': 'model = torch.hub.load(\"facebookresearch/WSL-Images\", \"resnext101_32x48d_wsl\", pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNext-101 32x48d WSL model from PyTorch Hub, which can be fine-tuned for bird species classification based on given images.', 'code': 'import torch\nmodel = torch.hub.load(\"facebookresearch/WSL-Images\", \"resnext101_32x48d_wsl\", pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x48d_wsl', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNext WSL", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x48d_wsl', pretrained=True)", "api_arguments": [{"name": "resnext101_32x48d_wsl", "type": "str", "description": "ResNeXt-101 32x48d WSL model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x48d_wsl')", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(output[0])", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeXt-101 32x48d": {"Top-1 Acc.": "85.4", "Top-5 Acc.": "97.6"}}}, "description": "The provided ResNeXt models are pre-trained in weakly-supervised fashion on 940 million public images with 1.5K hashtags matching with 1000 ImageNet1K synsets, followed by fine-tuning on ImageNet1K dataset. The models significantly improve the training accuracy on ImageNet compared to training from scratch. They achieve state-of-the-art accuracy of 85.4% on ImageNet with the ResNext-101 32x48d model."}}
{"code": "{'Instruction': I need an API that can classify images into 1000 different classes with high accuracy. How can create this?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resnet50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNet50 v1.5 model from PyTorch Hub, which comes with optimized performance and high accuracy for classifying images into 1000 different classes.', 'code': 'import torch\nmodel = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resnet50', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_resnet50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNet50", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_resnet50', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["pip install validators matplotlib"], "example_code": ["import torch", "from PIL import Image", "import torchvision.transforms as transforms", "import numpy as np", "import json", "import requests", "import matplotlib.pyplot as plt", "import warnings", "warnings.filterwarnings('ignore')", "%matplotlib inline", "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')", "print(f'Using {device} for inference')", "resnet50 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resnet50', pretrained=True)", "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_convnets_processing_utils')", "resnet50.eval().to(device)", "uris = [...]", "batch = torch.cat([utils.prepare_input_from_uri(uri) for uri in uris]).to(device)", "with torch.no_grad():", " output = torch.nn.functional.softmax(resnet50(batch), dim=1)", "results = utils.pick_n_best(predictions=output, n=5)", "for uri, result in zip(uris, results):", " img = Image.open(requests.get(uri, stream=True).raw)", " img.thumbnail((256,256), Image.ANTIALIAS)", " plt.imshow(img)", " plt.show()", " print(result)"], "performance": {"dataset": "ImageNet", "accuracy": "~0.5% top1 improvement over ResNet50 v1"}, "description": "The ResNet50 v1.5 model is a modified version of the original ResNet50 v1 model. The difference between v1 and v1.5 is that, in the bottleneck blocks which requires downsampling, v1 has stride = 2 in the first 1x1 convolution, whereas v1.5 has stride = 2 in the 3x3 convolution. This difference makes ResNet50 v1.5 slightly more accurate (~0.5% top1) than v1, but comes with a small performance drawback (~5% imgs/sec). The model is initialized as described in Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification. This model is trained with mixed precision using Tensor Cores on Volta, Turing, and the NVIDIA Ampere GPU architectures."}}
{"code": "{'Instruction': A designer wants to create an AI model that would generate thousands unique celebrity faces for his digital galleries. Present an API that can accomplish this., 'Output': {'domain': 'Generative Adversarial Networks (GANs)', 'api_call': \"model = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'PGAN', model_name='celebAHQ-512', pretrained=True, useGPU=use_gpu)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained Progressive Growing of GANs (PGAN) model from PyTorch Hub, which can generate high-quality celebrity faces.\", 'code': \"import torch\nuse_gpu = True if torch.cuda.is_available() else False\nmodel = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'PGAN', model_name='celebAHQ-512', pretrained=True, useGPU=use_gpu)\"}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorch_GAN_zoo:hub', model='PGAN', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Generative Adversarial Networks (GANs)", "api_name": "PGAN", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorch_GAN_zoo:hub', model='PGAN', pretrained=True)", "api_arguments": {"repo_or_dir": "facebookresearch/pytorch_GAN_zoo:hub", "model": "PGAN", "model_name": "celebAHQ-512", "pretrained": "True", "useGPU": "use_gpu"}, "python_environment_requirements": "Python 3", "example_code": {"import": "import torch", "use_gpu": "use_gpu = True if torch.cuda.is_available() else False", "load_model": "model = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'PGAN', model_name='celebAHQ-512', pretrained=True, useGPU=use_gpu)", "build_noise_data": "noise, _ = model.buildNoiseData(num_images)", "test": "generated_images = model.test(noise)", "plot_images": {"import_matplotlib": "import matplotlib.pyplot as plt", "import_torchvision": "import torchvision", "make_grid": "grid = torchvision.utils.make_grid(generated_images.clamp(min=-1, max=1), scale_each=True, normalize=True)", "imshow": "plt.imshow(grid.permute(1, 2, 0).cpu().numpy())", "show": "plt.show()"}}, "performance": {"dataset": "celebA", "accuracy": "High-quality celebrity faces"}, "description": "Progressive Growing of GANs (PGAN) is a method for generating high-resolution images using generative adversarial networks. The model is trained progressively, starting with low-resolution images and gradually increasing the resolution until the desired output is achieved. This implementation is based on the paper by Tero Karras et al., 'Progressive Growing of GANs for Improved Quality, Stability, and Variation'."}}
{"code": "{'Instruction': Sandra, a radiologist, needs to find abnormal regions in a brain MRI image quickly. Recommend an API that can automatically identify abnormal regions in a 3-channel brain MRI slice., 'Output': {'domain': 'Semantic Segmentation', 'api_call': \"model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained U-Net model for brain MRI segmentation from PyTorch Hub. This model can identify abnormal regions in a 3-channel brain MRI slice from pre-contrast, FLAIR, and post-contrast sequences.', 'code': 'import torch\nmodel = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='mateuszbuda/brain-segmentation-pytorch', model='unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "Biomedical Image Segmentation", "api_name": "U-Net for brain MRI", "api_call": "torch.hub.load(repo_or_dir='mateuszbuda/brain-segmentation-pytorch', model='unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)", "api_arguments": {"in_channels": 3, "out_channels": 1, "init_features": 32, "pretrained": true}, "python_environment_requirements": ["torch", "torchvision", "numpy", "PIL"], "example_code": ["import torch", "model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)", "import numpy as np", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "m, s = np.mean(input_image, axis=(0, 1)), np.std(input_image, axis=(0, 1))", "preprocess = transforms.Compose([transforms.ToTensor(),transforms.Normalize(mean=m, std=s)])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model = model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(torch.round(output[0]))"], "performance": {"dataset": "kaggle.com/mateuszbuda/lgg-mri-segmentation"}, "description": "U-Net with batch normalization for biomedical image segmentation with pretrained weights for abnormality segmentation in brain MRI. The model comprises four levels of blocks containing two convolutional layers with batch normalization and ReLU activation function, and one max pooling layer in the encoding part and up-convolutional layers instead in the decoding part. The number of convolutional filters in each block is 32, 64, 128, and 256. The bottleneck layer has 512 convolutional filters. From the encoding layers, skip connections are used to the corresponding layers in the decoding part. Input image is a 3-channel brain MRI slice from pre-contrast, FLAIR, and post-contrast sequences, respectively. Output is a one-channel probability map of abnormality regions with the same size as the input image."}}
{"code": "{'Instruction': Suggest me an API for classifying dog breeds from an image of a dog and output the top three likely breeds., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resneXt', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNeXt101-32x4d model from PyTorch Hub, which you can fine-tune for dog breed classification. After fine-tuning, the model can output the top three likely dog breeds when given an image of a dog.', 'code': 'import torch\nmodel = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resneXt', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_resneXt', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNeXt101-32x4d", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_resneXt', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "validators", "matplotlib"], "example_code": ["import torch", "from PIL import Image", "import torchvision.transforms as transforms", "import numpy as np", "import json", "import requests", "import matplotlib.pyplot as plt", "import warnings", "warnings.filterwarnings('ignore')", "%matplotlib inline", "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")", "print(f'Using {device} for inference')", "resneXt = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resneXt')", "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_convnets_processing_utils')", "resneXt.eval().to(device)", "uris = [", " 'http://images.cocodataset.org/test-stuff2017/000000024309.jpg',", " 'http://images.cocodataset.org/test-stuff2017/000000028117.jpg',", " 'http://images.cocodataset.org/test-stuff2017/000000006149.jpg',", " 'http://images.cocodataset.org/test-stuff2017/000000004954.jpg',", "]", "batch = torch.cat(", " [utils.prepare_input_from_uri(uri) for uri in uris]", ").to(device)", "with torch.no_grad():", " output = torch.nn.functional.softmax(resneXt(batch), dim=1)", "results = utils.pick_n_best(predictions=output, n=5)", "for uri, result in zip(uris, results):", " img = Image.open(requests.get(uri, stream=True).raw)", " img.thumbnail((256,256), Image.ANTIALIAS)", " plt.imshow(img)", " plt.show()", " print(result)"], "performance": {"dataset": "IMAGENET", "accuracy": "Not provided"}, "description": "ResNeXt101-32x4d is a model introduced in the Aggregated Residual Transformations for Deep Neural Networks paper. It is based on regular ResNet model, substituting 3x3 convolutions inside the bottleneck block for 3x3 grouped convolutions. This model is trained with mixed precision using Tensor Cores on Volta, Turing, and the NVIDIA Ampere GPU architectures. It can be deployed for inference on the NVIDIA Triton Inference Server using TorchScript, ONNX Runtime or TensorRT as an execution backend."}}
{"code": "{'Instruction': Recommend an API to classify objects in an image taken from the camera feed of a drone., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_se_resnext101_32x4d', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Use the pretrained SE-ResNeXt101-32x4d model from PyTorch Hub for classifying objects in an image taken from a drone camera feed.', 'code': \"import torch\nmodel = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_se_resnext101_32x4d', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_se_resnext101_32x4d', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SE-ResNeXt101", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_se_resnext101_32x4d', pretrained=True)", "api_arguments": "N/A", "python_environment_requirements": "validators, matplotlib", "example_code": "import torch\nfrom PIL import Image\nimport torchvision.transforms as transforms\nimport numpy as np\nimport json\nimport requests\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nprint(f'Using {device} for inference')\nresneXt = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_se_resnext101_32x4d')\nutils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_convnets_processing_utils')\nresneXt.eval().to(device)\nuris = ['http://images.cocodataset.org/test-stuff2017/000000024309.jpg','http://images.cocodataset.org/test-stuff2017/000000028117.jpg','http://images.cocodataset.org/test-stuff2017/000000006149.jpg','http://images.cocodataset.org/test-stuff2017/000000004954.jpg']\nbatch = torch.cat([utils.prepare_input_from_uri(uri) for uri in uris]).to(device)\nwith torch.no_grad():\n output = torch.nn.functional.softmax(resneXt(batch), dim=1)\nresults = utils.pick_n_best(predictions=output, n=5)\nfor uri, result in zip(uris, results):\n img = Image.open(requests.get(uri, stream=True).raw)\n img.thumbnail((256,256), Image.ANTIALIAS)\n plt.imshow(img)\n plt.show()\n print(result)", "performance": {"dataset": "IMAGENET", "accuracy": "N/A"}, "description": "The SE-ResNeXt101-32x4d is a ResNeXt101-32x4d model with added Squeeze-and-Excitation module. This model is trained with mixed precision using Tensor Cores on Volta, Turing, and the NVIDIA Ampere GPU architectures, which allows researchers to get results 3x faster than training without Tensor Cores while experiencing the benefits of mixed precision training."}}
{"code": "{'Instruction': I need to classify objects in images with high accuracy. Can you recommend an API to use for this purpose?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'densenet169', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained DenseNet-169 model from PyTorch Hub to classify objects in images with high accuracy.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'densenet169', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet169', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Dense Convolutional Network", "api_name": "Densenet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet169', pretrained=True)", "api_arguments": [{"name": "densenet169", "type": "str", "description": "Densenet-169 model"}], "python_environment_requirements": {"torch": "latest", "torchvision": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet169', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"densenet169": {"Top-1 error": 24.0, "Top-5 error": 7.0}}}, "description": "Dense Convolutional Network (DenseNet) connects each layer to every other layer in a feed-forward fashion. It alleviates the vanishing-gradient problem, strengthens feature propagation, encourages feature reuse, and substantially reduces the number of parameters."}}
{"code": "{'Instruction': Snapchat is working on a new feature to enable their users to tag their pictures with predefined categories. Suggest a machine learning model that can classify an image into different categories., 'Output': {'domain': 'Image Classification', 'api_call': 'model = torch.hub.load('pytorch/vision', 'alexnet', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained AlexNet model from PyTorch Hub for classifying images into different categories. This deep convolutional neural network can be easily applied to a variety of image classification tasks.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'alexnet', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='alexnet', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "AlexNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='alexnet', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": ">=1.9.0", "torchvision": ">=0.10.0"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"top-1_error": 43.45, "top-5_error": 20.91}}, "description": "AlexNet is a deep convolutional neural network that achieved a top-5 error of 15.3% in the 2012 ImageNet Large Scale Visual Recognition Challenge. The main contribution of the original paper was the depth of the model, which was computationally expensive but made feasible through the use of GPUs during training. The pretrained AlexNet model in PyTorch can be used for image classification tasks."}}
{"code": "{'Instruction': My application requires turning text into natural-sounding speech. What API can I use to achieve this?, 'Output': {'domain': 'Text-to-Speech', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_waveglow', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'You can use the pretrained WaveGlow model from NVIDIA via PyTorch Hub. It is a text-to-speech system that synthesizes natural-sounding speech from raw text.', 'code': 'import torch\nmodel = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_waveglow', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_waveglow', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Text-to-Speech", "framework": "PyTorch", "functionality": "Speech Synthesis", "api_name": "WaveGlow", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_waveglow', pretrained=True)", "api_arguments": {"repo_or_dir": "NVIDIA/DeepLearningExamples:torchhub", "model": "nvidia_waveglow", "model_math": "fp32"}, "python_environment_requirements": ["numpy", "scipy", "librosa", "unidecode", "inflect", "libsndfile1"], "example_code": {"load_waveglow_model": "waveglow = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_waveglow', model_math='fp32')", "prepare_waveglow_model": ["waveglow = waveglow.remove_weightnorm(waveglow)", "waveglow = waveglow.to('cuda')", "waveglow.eval()"], "load_tacotron2_model": "tacotron2 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tacotron2', model_math='fp32')", "prepare_tacotron2_model": ["tacotron2 = tacotron2.to('cuda')", "tacotron2.eval()"], "synthesize_speech": ["text = \"hello world, I missed you so much\"", "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tts_utils')", "sequences, lengths = utils.prepare_input_sequence([text])", "with torch.no_grad():", " mel, _, _ = tacotron2.infer(sequences, lengths)", " audio = waveglow.infer(mel)", "audio_numpy = audio[0].data.cpu().numpy()", "rate = 22050"], "save_audio": "write(\"audio.wav\", rate, audio_numpy)", "play_audio": "Audio(audio_numpy, rate=rate)"}, "performance": {"dataset": "LJ Speech", "accuracy": null}, "description": "The Tacotron 2 and WaveGlow model form a text-to-speech system that enables users to synthesize natural-sounding speech from raw transcripts without any additional prosody information. The Tacotron 2 model produces mel spectrograms from input text using encoder-decoder architecture. WaveGlow is a flow-based model that consumes the mel spectrograms to generate speech."}}
{"code": "{'Instruction': I need to classify a picture of a cat among 1000 classes in ImageNet. Give me an API that can classify my cat picture., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'densenet121', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the DenseNet-121 model from PyTorch Hub with pre-trained weights, which can classify images into 1000 classes, including cats.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'densenet121', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet121', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Dense Convolutional Network", "api_name": "Densenet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet121', pretrained=True)", "api_arguments": [{"name": "densenet121", "type": "str", "description": "Densenet-121 model"}], "python_environment_requirements": {"torch": "latest", "torchvision": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet121', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"densenet121": {"Top-1 error": 25.35, "Top-5 error": 7.83}}}, "description": "Dense Convolutional Network (DenseNet) connects each layer to every other layer in a feed-forward fashion. It alleviates the vanishing-gradient problem, strengthens feature propagation, encourages feature reuse, and substantially reduces the number of parameters."}}
{"code": "{'Instruction': I am running a sentiment analysis on public opinion about a new social media app, could you recommend an API to classify the sentiments given a piece of text?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/fairseq', 'roberta.large', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained RoBERTa model from PyTorch Hub, which can be fine-tuned for sentiment analysis of text data to understand public opinion about a new social media app.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/fairseq', 'roberta.large', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/fairseq', model='roberta.large', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "RoBERTa", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='pytorch/fairseq', model='roberta.large', pretrained=True)", "api_arguments": ["'pytorch/fairseq'", "'roberta.large'"], "python_environment_requirements": ["regex", "requests", "hydra-core", "omegaconf"], "example_code": ["import torch", "roberta = torch.hub.load('pytorch/fairseq', 'roberta.large')", "roberta.eval()", "tokens = roberta.encode('Hello world!')", "last_layer_features = roberta.extract_features(tokens)", "all_layers = roberta.extract_features(tokens, return_all_hiddens=True)", "roberta = torch.hub.load('pytorch/fairseq', 'roberta.large.mnli')", "roberta.eval()", "tokens = roberta.encode('Roberta is a heavily optimized version of BERT.', 'Roberta is not very optimized.')", "prediction = roberta.predict('mnli', tokens).argmax().item()", "tokens = roberta.encode('Roberta is a heavily optimized version of BERT.', 'Roberta is based on BERT.')", "prediction = roberta.predict('mnli', tokens).argmax().item()", "roberta.register_classification_head('new_task', num_classes=3)", "logprobs = roberta.predict('new_task', tokens)"], "performance": {"dataset": "MNLI", "accuracy": "N/A"}, "description": "RoBERTa is a robustly optimized version of BERT, a revolutionary self-supervised pretraining technique that learns to predict intentionally hidden (masked) sections of text. RoBERTa builds on BERT's language masking strategy and modifies key hyperparameters, including removing BERT's next-sentence pretraining objective, and training with much larger mini-batches and learning rates. RoBERTa was also trained on an order of magnitude more data than BERT, for a longer amount of time, allowing it to generalize even better to downstream tasks."}}
{"code": "{'Instruction': Give me an API that can output the category of a car accident given a car accident dashcam footage., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet39ds', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained HarDNet model from PyTorch Hub, which can be fine-tuned for car accident classification given a dashcam footage of the accident.', 'code': 'import torch\nmodel = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet39ds', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet39ds', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "HarDNet", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet39ds', pretrained=True)", "api_arguments": [{"name": "hardnet39ds", "type": "str", "description": "HarDNet-39DS model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet39ds', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"hardnet39ds": {"Top-1 error": 27.92, "Top-5 error": 9.57}}}, "description": "Harmonic DenseNet (HarDNet) is a low memory traffic CNN model, which is fast and efficient. The basic concept is to minimize both computational cost and memory access cost at the same time, such that the HarDNet models are 35% faster than ResNet running on GPU comparing to models with the same accuracy (except the two DS models that were designed for comparing with MobileNet)."}}
{"code": "{'Instruction': Explain an API for street scene semantic segmentation using an RGB image and find the names of objects from the segmentation model., 'Output': {'domain': 'Semantic Segmentation', 'api_call': \"model = torch.hub.load('pytorch/vision', 'fcn_resnet50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained FCN-ResNet50 model from PyTorch Hub for street scene semantic segmentation using RGB images. The model provides object names from the segmentation results.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'fcn_resnet50', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='fcn_resnet50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "Fully-Convolutional Network", "api_name": "fcn_resnet50", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='fcn_resnet50', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "boolean", "description": "If True, returns a model pre-trained on COCO train2017"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "matplotlib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'fcn_resnet50', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "input_image = input_image.convert(\"RGB\")", "preprocess = transforms.Compose([", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)['out'][0]", "output_predictions = output.argmax(0)"], "performance": {"dataset": "COCO val2017", "accuracy": {"fcn_resnet50": {"Mean IOU": 60.5, "Global Pixelwise Accuracy": 91.4}}}, "description": "FCN-ResNet is a Fully-Convolutional Network model using a ResNet-50 or a ResNet-101 backbone. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset."}}
{"code": "{'Instruction': We would like to do semantic segmentation on a given image. Provide an API that can perform this functionality using the Fully Convolutional Network with ResNet101 backbone., 'Output': {'domain': 'Semantic Segmentation', 'api_call': 'model = torch.hub.load('pytorch/vision', 'fcn_resnet101', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained FCN-ResNet101 model from PyTorch Hub for semantic segmentation.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'fcn_resnet101', pretrained=True)\nmodel.eval()'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='fcn_resnet101', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "Fully-Convolutional Network", "api_name": "fcn_resnet101", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='fcn_resnet101', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "boolean", "description": "If True, returns a model pre-trained on COCO train2017"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "matplotlib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'fcn_resnet101', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "input_image = input_image.convert(\"RGB\")", "preprocess = transforms.Compose([", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)['out'][0]", "output_predictions = output.argmax(0)"], "performance": {"dataset": "COCO val2017", "accuracy": {"fcn_resnet101": {"Mean IOU": 63.7, "Global Pixelwise Accuracy": 91.9}}}, "description": "FCN-ResNet is a Fully-Convolutional Network model using a ResNet-50 or a ResNet-101 backbone. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset."}}
{"code": "{'Instruction': A photographer needs a tool for tagging various types of images. Recommend an API that would be suitable for classifying images into different categories., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet68ds', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Use the pretrained HarDNet model for Image Classification from PyTorch Hub to efficiently classify images into various categories by minimizing computational and memory access costs.', 'code': 'import torch\nmodel = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet68ds', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet68ds', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "HarDNet", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet68ds', pretrained=True)", "api_arguments": [{"name": "hardnet68ds", "type": "str", "description": "HarDNet-68DS model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet68ds', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"hardnet68ds": {"Top-1 error": 25.71, "Top-5 error": 8.13}}}, "description": "Harmonic DenseNet (HarDNet) is a low memory traffic CNN model, which is fast and efficient. The basic concept is to minimize both computational cost and memory access cost at the same time, such that the HarDNet models are 35% faster than ResNet running on GPU comparing to models with the same accuracy (except the two DS models that were designed for comparing with MobileNet)."}}
{"code": "{'Instruction': I work at a Tesla factory, and I need to find an AI that can classify whether the assembly of the Model 3 is of good quality or not. What should I use?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'densenet161', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained DenseNet-161 model from PyTorch Hub, which can be fine-tuned to classify the assembly quality of Tesla Model 3.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'densenet161', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet161', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Dense Convolutional Network", "api_name": "Densenet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet161', pretrained=True)", "api_arguments": [{"name": "densenet161", "type": "str", "description": "Densenet-161 model"}], "python_environment_requirements": {"torch": "latest", "torchvision": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet161', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"densenet161": {"Top-1 error": 22.35, "Top-5 error": 6.2}}}, "description": "Dense Convolutional Network (DenseNet) connects each layer to every other layer in a feed-forward fashion. It alleviates the vanishing-gradient problem, strengthens feature propagation, encourages feature reuse, and substantially reduces the number of parameters."}}
{"code": "{'Instruction': I am developing a mobile app that identifies objects within images. Suggest an API that is efficient in terms of memory and computation for image classification., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet85', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the HarDNet-85 model from PyTorch Hub, which is a low memory traffic and computationally efficient model designed for image classification.', 'code': 'import torch\nmodel = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet85', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet85', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "HarDNet", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet85', pretrained=True)", "api_arguments": [{"name": "hardnet85", "type": "str", "description": "HarDNet-85 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet85', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"hardnet85": {"Top-1 error": 21.96, "Top-5 error": 6.11}}}, "description": "Harmonic DenseNet (HarDNet) is a low memory traffic CNN model, which is fast and efficient. The basic concept is to minimize both computational cost and memory access cost at the same time, such that the HarDNet models are 35% faster than ResNet running on GPU comparing to models with the same accuracy (except the two DS models that were designed for comparing with MobileNet)."}}
{"code": "{'Instruction': I am building a recommendation system for clothing and need an image classification API for detecting types of clothing. Can you suggest one?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet68', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained HarDNet-68 model from PyTorch Hub, which can be fine-tuned for clothing type classification based on images.', 'code': 'import torch\nmodel = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet68', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet68', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "HarDNet", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet68', pretrained=True)", "api_arguments": [{"name": "hardnet68", "type": "str", "description": "HarDNet-68 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet68', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"hardnet68": {"Top-1 error": 23.52, "Top-5 error": 6.99}}}, "description": "Harmonic DenseNet (HarDNet) is a low memory trafficCNN model, which is fast and efficient. The basic concept is to minimize both computational cost and memory access cost at the same time, such that the HarDNet models are 35% faster than ResNet running on GPU comparing to models with the same accuracy (except the two DS models that were designed for comparing with MobileNet)."}}
{"code": "{'Instruction': I want to build an application for image categorization using minimum resources such as memory and computational speed. Suggest me an API that serves my purpose., 'Output': {'domain': 'Classification', 'api_call': 'model = torch.hub.load(\\'huawei-noah/ghostnet\\', \\'ghostnet_1x\\', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Use the pretrained GhostNet model from PyTorch Hub which is designed for efficient image classification using minimal resources like computational speed and memory.', 'code': 'import torch\nmodel = torch.hub.load(\\'huawei-noah/ghostnet\\', \\'ghostnet_1x\\', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/ghostnet', model='ghostnet_1x', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Efficient networks by generating more features from cheap operations", "api_name": "GhostNet", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/ghostnet', model='ghostnet_1x', pretrained=True)", "api_arguments": ["pretrained"], "python_environment_requirements": ["torch", "torchvision", "PIL"], "example_code": ["import torch", "model = torch.hub.load('huawei-noah/ghostnet', 'ghostnet_1x', pretrained=True)", "model.eval()", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)", "print(probabilities)"], "performance": {"dataset": "ImageNet", "accuracy": {"Top-1 acc": "73.98", "Top-5 acc": "91.46"}}, "description": "The GhostNet architecture is based on an Ghost module structure which generates more features from cheap operations. Based on a set of intrinsic feature maps, a series of cheap operations are applied to generate many ghost feature maps that could fully reveal information underlying intrinsic features. Experiments conducted on benchmarks demonstrate the superiority of GhostNet in terms of speed and accuracy tradeoff."}}
{"code": "{'Instruction': A content curator wants to automatically classify images to better organize their media library. Recommend an API that can help classify images into different categories., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'googlenet', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained GoogLeNet model from PyTorch Hub to classify images into different categories.', 'code': \"import torch\nmodel = torch.hub.load('pytorch/vision', 'googlenet', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='googlenet', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "GoogLeNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='googlenet', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": ">=1.0.0", "torchvision": ">=0.2.2"}, "example_code": {"import": ["import torch", "import urllib", "from PIL import Image", "from torchvision import transforms"], "load_model": "model = torch.hub.load('pytorch/vision:v0.10.0', 'googlenet', pretrained=True)", "model_eval": "model.eval()", "image_preprocessing": ["input_image = Image.open(filename)", "preprocess = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)"], "model_execution": ["if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)"], "output_processing": ["probabilities = torch.nn.functional.softmax(output[0], dim=0)", "top5_prob, top5_catid = torch.topk(probabilities, 5)"]}, "performance": {"dataset": "ImageNet", "accuracy": {"Top-1 error": "30.22", "Top-5 error": "10.47"}}, "description": "GoogLeNet is based on a deep convolutional neural network architecture codenamed 'Inception', which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014)."}}
{"code": "{'Instruction': I am working on a project that requires classifying thousands of images into different categories. Suggest an API that can identify objects within the images for my task., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('XingangPan/IBN-Net', 'resnext101_ibn_a', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained IBN-Net model for object classification from PyTorch Hub to classify images into different categories.', 'code': 'import torch\nmodel = torch.hub.load('XingangPan/IBN-Net', 'resnext101_ibn_a', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnext101_ibn_a', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "IBN-Net", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnext101_ibn_a', pretrained=True)", "api_arguments": [{"name": "resnext101_ibn_a", "type": "str", "description": "ResNeXt-101-IBN-a model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('XingangPan/IBN-Net', 'resnext101_ibn_a', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"resnext101_ibn_a": {"Top-1 acc": 79.12, "Top-5 acc": 94.58}}}, "description": "IBN-Net is a CNN model with domain/appearance invariance. Motivated by style transfer works, IBN-Net carefully unifies instance normalization and batch normalization in a single deep network. It provides a simple way to increase both modeling and generalization capacities without adding model complexity. IBN-Net is especially suitable for cross domain or person/vehicle re-identification tasks."}}
{"code": "{'Instruction': I am conducting research on cross-domain image classification. Can you suggest an API that offers a pretrained model that considers domain/appearance invariance?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('XingangPan/IBN-Net', 'resnet101_ibn_a', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNet-101-IBN-a model from PyTorch Hub to perform cross-domain image classification. This model is designed with domain/appearance invariance.', 'code': 'import torch\nmodel = torch.hub.load('XingangPan/IBN-Net', 'resnet101_ibn_a', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnet101_ibn_a', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "IBN-Net", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnet101_ibn_a', pretrained=True)", "api_arguments": [{"name": "resnet101_ibn_a", "type": "str", "description": "ResNet-101-IBN-a model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('XingangPan/IBN-Net', 'resnet101_ibn_a', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"resnet101_ibn_a": {"Top-1 acc": 78.61, "Top-5 acc": 94.41}}}, "description": "IBN-Net is a CNN model with domain/appearance invariance. Motivated by style transfer works, IBN-Net carefully unifies instance normalization and batch normalization in a single deep network. It provides a simple way to increase both modeling and generalization capacities without adding model complexity. IBN-Net is especially suitable for cross domain or person/vehicle re-identification tasks."}}
{"code": "{'Instruction': I work for a camera security company, and I need a deep learning model to interpret and recognize person/vehicle in live streaming CCTV. Which API should I use?, 'Output': {'domain': 'Classification', 'api_call': 'model = torch.hub.load('XingangPan/IBN-Net', 'se_resnet101_ibn_a', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pre-trained IBN-Net model from PyTorch Hub, specifically the SE-ResNet-101-IBN-a variant, which is suitable for person/vehicle re-identification tasks.', 'code': 'import torch\nmodel = torch.hub.load('XingangPan/IBN-Net', 'se_resnet101_ibn_a', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='se_resnet101_ibn_a', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "IBN-Net", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='se_resnet101_ibn_a', pretrained=True)", "api_arguments": [{"name": "se_resnet101_ibn_a", "type": "str", "description": "SE-ResNet-101-IBN-a model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('XingangPan/IBN-Net', 'se_resnet101_ibn_a', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"se_resnet101_ibn_a": {"Top-1 acc": 78.75, "Top-5 acc": 94.49}}}, "description": "IBN-Net is a CNN model with domain/appearance invariance. Motivated by style transfer works, IBN-Net carefully unifies instance normalization and batch normalization in a single deep network. It provides a simple way to increase both modeling and generalization capacities without adding model complexity. IBN-Net is especially suitable for cross domain or person/vehicle re-identification tasks."}}
{"code": "{'Instruction': I need to classify photos of different human faces taken from different places. Can you provide me an API that can help with that?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('XingangPan/IBN-Net', 'resnet50_ibn_a', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNet-50-IBN-a model from PyTorch Hub, which is designed for domain/appearance invariance and is suitable for classifying human faces taken from different locations.', 'code': 'import torch\nmodel = torch.hub.load('XingangPan/IBN-Net', 'resnet50_ibn_a', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnet50_ibn_a', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "IBN-Net", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnet50_ibn_a', pretrained=True)", "api_arguments": [{"name": "resnet50_ibn_a", "type": "str", "description": "ResNet-50-IBN-a model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('XingangPan/IBN-Net', 'resnet50_ibn_a', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"resnet50_ibn_a": {"Top-1 acc": 77.46, "Top-5 acc": 93.68}}}, "description": "IBN-Net is a CNN model with domain/appearance invariance. Motivated by style transfer works, IBN-Net carefully unifies instance normalization and batch normalization in a single deep network. It provides a simple way to increase both modeling and generalization capacities without adding model complexity. IBN-Net is especially suitable for cross domain or person/vehicle re-identification tasks."}}
{"code": "{'Instruction': The AI team at Snapchat is looking for a model to classify objects using images. Recommend a suitable API for this task., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'inception_v3', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pre-trained Inception v3 model from PyTorch Hub. This model is great for image classification and has achieved outstanding results on the ImageNet dataset.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'inception_v3', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='inception_v3', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "Inception_v3", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='inception_v3', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": "1.9.0", "torchvision": "0.10.0"}, "example_code": {"import_libraries": "import torch", "load_model": "model = torch.hub.load('pytorch/vision:v0.10.0', 'inception_v3', pretrained=True)", "model_evaluation": "model.eval()"}, "performance": {"dataset": "imagenet", "accuracy": {"top-1_error": 22.55, "top-5_error": 6.44}}, "description": "Inception v3, also called GoogleNetv3, is a famous Convolutional Neural Network trained on the ImageNet dataset from 2015. It is based on the exploration of ways to scale up networks to utilize the added computation as efficiently as possible by using suitably factorized convolutions and aggressive regularization. The model achieves a top-1 error of 22.55% and a top-5 error of 6.44% on the ImageNet dataset."}}
{"code": "{'Instruction': Prepare an API to classify objects in an image using a model with split-attention networks., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest101', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNeSt101 model from PyTorch Hub, a ResNet variant with split-attention networks, for image classification.', 'code': 'import torch\nmodel = torch.hub.load('zhanghang1989/ResNeSt', 'resnest101', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest101', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNeSt", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest101', pretrained=True)", "api_arguments": "resnest101", "python_environment_requirements": {"torch": "1.0.0", "torchvision": "0.2.2"}, "example_code": ["import torch", "model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest101', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeSt-101": 82.83}}, "description": "ResNeSt models are from the ResNeSt: Split-Attention Networks paper. They are a new ResNet variant that enables attention across feature-map groups. By stacking Split-Attention blocks ResNet-style, ResNeSt models outperform other networks with similar model complexities, and also help downstream tasks including object detection, instance segmentation, and semantic segmentation."}}
{"code": "{'Instruction': I need an image classifier based on a new ResNet variant to classify images according to the trained categories. Which API should I use?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNeSt50 model from PyTorch Hub for image classification. This is a ResNet variant with Split-Attention Networks.', 'code': 'import torch\nmodel = torch.hub.load('zhanghang1989/ResNeSt', 'resnest50', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNeSt", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest50', pretrained=True)", "api_arguments": "resnest50", "python_environment_requirements": {"torch": "1.0.0", "torchvision": "0.2.2"}, "example_code": ["import torch", "model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest50', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeSt-50": 81.03}}, "description": "ResNeSt models are from the ResNeSt: Split-Attention Networks paper. They are a new ResNet variant that enables attention across feature-map groups. By stacking Split-Attention blocks ResNet-style, ResNeSt models outperform other networks with similar model complexities, and also help downstream tasks including object detection, instance segmentation, and semantic segmentation."}}
{"code": "{'Instruction': A real-estate company is classifying homes using images taken by a drone. Suggest an API for classifying images of homes., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest269', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNeSt-269 model from PyTorch Hub to classify images of homes for the real-estate company.', 'code': 'import torch\nmodel = torch.hub.load('zhanghang1989/ResNeSt', 'resnest269', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest269', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNeSt", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest269', pretrained=True)", "api_arguments": "resnest269", "python_environment_requirements": {"torch": "1.0.0", "torchvision": "0.2.2"}, "example_code": ["import torch", "model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest269', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeSt-269": 84.54}}, "description": "ResNeSt models are from the ResNeSt: Split-Attention Networks paper. They are a new ResNet variant that enables attention across feature-map groups. By stacking Split-Attention blocks ResNet-style, ResNeSt models outperform other networks with similar model complexities, and also help downstream tasks including object detection, instance segmentation, and semantic segmentation."}}
{"code": "{'Instruction': Identify a deep learning model that can efficiently be used for image classification on a mobile device., 'Output': {'domain': 'Image Classification', 'api_call': 'model = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_mobile', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the ProxylessNAS Mobile model from PyTorch Hub, which is optimized for mobile devices with similar accuracy as other deep learning models.', 'code': 'import torch\nmodel = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_mobile', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_cpu', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "ProxylessNAS", "api_name": "mit-han-lab/ProxylessNAS", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_cpu', pretrained=True)", "api_arguments": [{"name": "proxylessnas_cpu", "type": "str", "description": "ProxylessNAS optimized for CPU"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "target_platform = 'proxyless_cpu'", "model = torch.hub.load('mit-han-lab/ProxylessNAS', target_platform, pretrained=True)", "model.eval()"], "performance": {"dataset": [{"model_structure": "proxylessnas_cpu", "accuracy": 75.3}]}, "description": "ProxylessNAS models are from the ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware paper. They specialize CNN architectures for different hardware platforms, offering free yet significant performance boost on all three platforms (CPU, GPU, and Mobile) with similar accuracy."}}
{"code": "{'Instruction': A wildlife photographer wants to classify animal species in his photos. Please recommend a machine learning API for this task., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'mobilenet_v2', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MobileNet v2 model from PyTorch Hub, which is suitable for image classification tasks including identifying animal species in photographs.', 'code': 'import torch\nmodel = torch.hub.load(\"pytorch/vision\", \"mobilenet_v2\", pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='mobilenet_v2', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MobileNet v2", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='mobilenet_v2', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision', 'mobilenet_v2', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)"], "performance": {"dataset": "ImageNet", "accuracy": {"top-1_error": 28.12, "top-5_error": 9.71}}, "description": "The MobileNet v2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input. MobileNet v2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, non-linearities in the narrow layers were removed in order to maintain representational power."}}
{"code": "{'Instruction': I need to classify the objects in the image using an API that is optimized for the GPU. Can you suggest one?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_gpu', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the ProxylessNAS model with pre-trained weights from PyTorch Hub, optimized for GPU. This neural architecture search directly optimizes the task and offers a significant performance boost while maintaining accuracy.', 'code': 'import torch\nmodel = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_gpu', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_gpu', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "ProxylessNAS", "api_name": "mit-han-lab/ProxylessNAS", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_gpu', pretrained=True)", "api_arguments": [{"name": "proxylessnas_gpu", "type": "str", "description": "ProxylessNAS optimized for GPU"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "target_platform = 'proxyless_gpu'", "model = torch.hub.load('mit-han-lab/ProxylessNAS', target_platform, pretrained=True)", "model.eval()"], "performance": {"dataset": [{"model_structure": "proxylessnas_gpu", "accuracy": 75.1}]}, "description": "ProxylessNAS models are from the ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware paper. They specialize CNN architectures for different hardware platforms, offering free yet significant performance boost on all three platforms (CPU, GPU, and Mobile) with similar accuracy."}}
{"code": "{'Instruction': I want to create an app that can classify images taken by phone cameras. Recommend me an API that is optimized for mobile devices., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_mobile', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained ProxylessNAS Mobile model from PyTorch Hub, optimized for mobile devices to perform image classification tasks on phone cameras.\", 'code': 'import torch\nmodel = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_mobile', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_mobile', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "ProxylessNAS", "api_name": "mit-han-lab/ProxylessNAS", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_mobile', pretrained=True)", "api_arguments": [{"name": "proxylessnas_mobile", "type": "str", "description": "ProxylessNAS optimized for Mobile"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "target_platform = 'proxyless_mobile'", "model = torch.hub.load('mit-han-lab/ProxylessNAS', target_platform, pretrained=True)", "model.eval()"], "performance": {"dataset": [{"model_structure": "proxylessnas_mobile", "accuracy": 74.6}]}, "description": "ProxylessNAS models are from the ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware paper. They specialize CNN architectures for different hardware platforms, offering free yet significant performance boost on all three platforms (CPU, GPU, and Mobile) with similar accuracy."}}
{"code": "{'Instruction': I want to classify images of dogs into different breeds using a pre-trained deep learning model. Which API can I use?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'resnet18', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Use the pre-trained ResNet model from PyTorch Hub, which can be fine-tuned for dog breed classification.\", 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'resnet18', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet18', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Deep Residual Networks", "api_name": "ResNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet18', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "bool", "default": "False", "description": "If True, returns a model pre-trained on ImageNet"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "resnet18", "top-1_error": 30.24, "top-5_error": 10.92}}, "description": "ResNet models are deep residual networks pre-trained on ImageNet. They were proposed in the paper 'Deep Residual Learning for Image Recognition'. Available model variants include ResNet18, ResNet34, ResNet50, ResNet101, and ResNet152."}}
{"code": "{'Instruction': Tell me how to create a code for identifying whether an image contains a dog or a cat., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'resnet50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNet50 model from PyTorch Hub, which can be fine-tuned for binary classification to identify if an image contains a dog or a cat.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'resnet50', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Deep Residual Networks", "api_name": "ResNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet50', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "bool", "default": "False", "description": "If True, returns a model pre-trained on ImageNet"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "resnet50", "top-1_error": 23.85, "top-5_error": 7.13}}, "description": "ResNet models are deep residual networks pre-trained on ImageNet. They were proposed in the paper 'Deep Residual Learning for Image Recognition'. Available model variants include ResNet18, ResNet34, ResNet50, ResNet101, and ResNet152."}}
{"code": "{'Instruction': I need to categorize images into 1000 classes. Can you recommend an API for image classification?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'resnet101', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNet-101 model from PyTorch Hub for image classification into over 1000 classes from ImageNet.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'resnet101', pretrained=True)\nmodel.eval()'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet101', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Deep Residual Networks", "api_name": "ResNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet101', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "bool", "default": "False", "description": "If True, returns a model pre-trained on ImageNet"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet101', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "resnet101", "top-1_error": 22.63, "top-5_error": 6.44}}, "description": "ResNet models are deep residual networks pre-trained on ImageNet. They were proposed in the paper 'Deep Residual Learning for Image Recognition'. Available model variants include ResNet18, ResNet34, ResNet50, ResNet101, and ResNet152."}}
{"code": "{'Instruction': I am developing an app to label dog breeds found in images. Can you suggest an API that can be used to classify dog breeds?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'resnet152', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load a pre-trained ResNet model for image classification from PyTorch Hub. Fine-tune this model on a dataset of dog breed images to classify the breeds.\", 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'resnet152', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet152', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Deep Residual Networks", "api_name": "ResNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet152', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "bool", "default": "False", "description": "If True, returns a model pre-trained on ImageNet"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet152', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "resnet152", "top-1_error": 21.69, "top-5_error": 5.94}}, "description": "ResNet models are deep residual networks pre-trained on ImageNet. They were proposed in the paper 'Deep Residual Learning for Image Recognition'. Available model variants include ResNet18, ResNet34, ResNet50, ResNet101, and ResNet152."}}
{"code": "{'Instruction': A mobile application developer wants to build a plant species identifier. Recommend an API that can easily classify plant species from images., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'resnext50_32x4d', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNext50 model from PyTorch Hub, which can be fine-tuned for plant species classification from images.', 'code': 'import torch\nmodel = torch.hub.load(\"pytorch/vision\", \"resnext50_32x4d\", pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnext50_32x4d', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNext", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnext50_32x4d', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "PIL"], "example_code": ["import torch", "from PIL import Image", "from torchvision import transforms", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnext50_32x4d', pretrained=True)", "model.eval()", "input_image = Image.open('dog.jpg')", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)"], "performance": {"dataset": "ImageNet", "accuracy": {"resnext50_32x4d": {"top-1": 22.38, "top-5": 6.3}}}, "description": "ResNext is a next-generation ResNet architecture for image classification. It is more efficient and accurate than the original ResNet. This implementation includes two versions of the model, resnext50_32x4d and resnext101_32x8d, with 50 and 101 layers respectively."}}
{"code": "{'Instruction': Discuss an API that can classify images using lateral inhibition-inspired mechanisms and efficient backbones., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_t', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained SNNMLP model from PyTorch Hub, which incorporates lateral inhibition-inspired mechanisms and efficient backbones for improved image classification accuracy.\", 'code': 'import torch\nmodel = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_t', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_t', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SNNMLP", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_t', pretrained=True)", "api_arguments": [{"name": "snnmlp_t", "type": "str", "description": "SNNMLP Tiny model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_t', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "SNNMLP Tiny", "top-1": 81.88}}, "description": "SNNMLP incorporates the mechanism of LIF neurons into the MLP models, to achieve better accuracy without extra FLOPs. We propose a full-precision LIF operation to communicate between patches, including horizontal LIF and vertical LIF in different directions. We also propose to use group LIF to extract better local features. With LIF modules, our SNNMLP model achieves 81.9%, 83.3% and 83.6% top-1 accuracy on ImageNet dataset with only 4.4G, 8.5G and 15.2G FLOPs, respectively."}}
{"code": "{'Instruction': I want an API that can effectively distinguish between breeds of dogs using deep learning., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_b', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained SNNMLP base model from PyTorch Hub, which can be fine-tuned for dog breed classification.', 'code': 'import torch\nmodel = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_b', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_b', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SNNMLP", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_b', pretrained=True)", "api_arguments": [{"name": "snnmlp_b", "type": "str", "description": "SNNMLP Base model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_b', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "SNNMLP Base", "top-1": 85.59}}, "description": "SNNMLP incorporates the mechanism of LIF neurons into the MLP models, to achieve better accuracy without extra FLOPs. We propose a full-precision LIF operation to communicate between patches, including horizontal LIF and vertical LIF in different directions. We also propose to use group LIF to extract better local features. With LIF modules, our SNNMLP model achieves 81.9%, 83.3% and 83.6% top-1 accuracy on ImageNet dataset with only 4.4G, 8.5G and 15.2G FLOPs, respectively."}}
{"code": "{'Instruction': Can you suggest an API that can classify images with the least number of parameters possible without losing much accuracy?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'squeezenet1_0', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained SqueezeNet model from PyTorch Hub, which can classify images with 50x fewer parameters than AlexNet and maintains a comparable level of accuracy.\", 'code': \"import torch\nmodel = torch.hub.load('pytorch/vision', 'squeezenet1_0', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='squeezenet1_0', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SqueezeNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='squeezenet1_0', pretrained=True)", "api_arguments": {"version": "v0.10.0", "model": ["squeezenet1_0"], "pretrained": "True"}, "python_environment_requirements": {"torch": ">=1.9.0", "torchvision": ">=0.10.0"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'squeezenet1_0', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)", "print(probabilities)"], "performance": {"dataset": "ImageNet", "accuracy": {"squeezenet1_0": {"Top-1 error": 41.9, "Top-5 error": 19.58}}}, "description": "SqueezeNet is an image classification model that achieves AlexNet-level accuracy with 50x fewer parameters. It has two versions: squeezenet1_0 and squeezenet1_1, with squeezenet1_1 having 2.4x less computation and slightly fewer parameters than squeezenet1_0, without sacrificing accuracy."}}
{"code": "{'Instruction': I am building an image recognition software for my school project, Can you give me an API that will help me in recognizing the images I upload?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'shufflenet_v2_x1_0', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pre-trained ShuffleNet v2 model from PyTorch Hub, suitable for efficient image classification on the images you upload.\", 'code': \"import torch\nmodel = torch.hub.load('pytorch/vision', 'shufflenet_v2_x1_0', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='shufflenet_v2_x1_0', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ShuffleNet v2", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='shufflenet_v2_x1_0', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": "torch", "torchvision": "torchvision", "PIL": "Image", "urllib": "urllib"}, "example_code": {"import_libraries": ["import torch", "from PIL import Image", "from torchvision import transforms", "import urllib"], "load_model": ["model = torch.hub.load('pytorch/vision:v0.10.0', 'shufflenet_v2_x1_0', pretrained=True)", "model.eval()"], "load_image": ["url, filename = ('https://github.com/pytorch/hub/raw/master/images/dog.jpg', 'dog.jpg')", "try: urllib.URLopener().retrieve(url, filename)", "except: urllib.request.urlretrieve(url, filename)", "input_image = Image.open(filename)"], "preprocess_image": ["preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)"], "run_inference": ["if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)"], "get_probabilities": ["probabilities = torch.nn.functional.softmax(output[0], dim=0)"], "top_categories": ["top5_prob, top5_catid = torch.topk(probabilities, 5)", "for i in range(top5_prob.size(0)):", " print(categories[top5_catid[i]], top5_prob[i].item())"]}, "performance": {"dataset": "Imagenet", "accuracy": {"top-1_error": 30.64, "top-5_error": 11.68}}, "description": "ShuffleNet V2 is an efficient ConvNet optimized for speed and memory, pre-trained on Imagenet. It is designed based on practical guidelines for efficient network design, including speed and accuracy tradeoff."}}
{"code": "{'Instruction': I am working on an app that can identify plants from images. Can you provide a good API for this task?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'squeezenet1_1', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Use the SqueezeNet model provided by PyTorch Hub for image classification. The model can be fine-tuned with a dataset containing plant images to identify plants.', 'code': \"import torch\nmodel = torch.hub.load('pytorch/vision', 'squeezenet1_1', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='squeezenet1_1', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SqueezeNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='squeezenet1_1', pretrained=True)", "api_arguments": {"version": "v0.10.0", "model": ["squeezenet1_1"], "pretrained": "True"}, "python_environment_requirements": {"torch": ">=1.9.0", "torchvision": ">=0.10.0"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'squeezenet1_1', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)", "print(probabilities)"], "performance": {"dataset": "ImageNet", "accuracy": {"squeezenet1_1": {"Top-1 error": 41.81, "Top-5 error": 19.38}}}, "description": "SqueezeNet is an image classification model that achieves AlexNet-level accuracy with 50x fewer parameters. It has two versions: squeezenet1_0 and squeezenet1_1, with squeezenet1_1 having 2.4x less computation and slightly fewer parameters than squeezenet1_0, without sacrificing accuracy."}}
{"code": "{'Instruction': I want to recognize objects in images. Recommend an API for image recognition., 'Output': {'domain': 'Image Recognition', 'api_call': 'model = torch.hub.load('pytorch/vision', 'vgg11', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained VGG11 model from PyTorch Hub for large-scale image recognition tasks.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg11', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg11', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg11', pretrained=True)", "api_arguments": [{"name": "vgg11", "type": "str", "description": "VGG11 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg11', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg11": {"Top-1 error": 30.98, "Top-5 error": 11.37}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': What is a suitable API that utilizes deep learning to detect and recognize hundreds of object categories in images?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg11_bn', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained VGG11_bn model from PyTorch Hub, which can detect and recognize hundreds of object categories in images using deep learning.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg11_bn', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg11_bn', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg11_bn', pretrained=True)", "api_arguments": [{"name": "vgg11_bn", "type": "str", "description": "VGG11 model with batch normalization"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg11_bn', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg11_bn": {"Top-1 error": 26.7, "Top-5 error": 8.58}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': I have a bunch of images and I need a pre-trained deep learning model for image classification. Suggest an API for me to use., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg13', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pre-trained VGG13 model from PyTorch Hub, which is an effective model for large-scale image recognition tasks.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg13', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg13', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg13', pretrained=True)", "api_arguments": [{"name": "vgg13", "type": "str", "description": "VGG13 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg13', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg13": {"Top-1 error": 30.07, "Top-5 error": 10.75}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn,vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': I have an image of an object and I want to classify the object in the image. Can you suggest an API that can help me with this task?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg16', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained VGG16 model from PyTorch Hub for use in classifying the object in the image.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg16', pretrained=True)\nmodel.eval()'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg16', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg16', pretrained=True)", "api_arguments": [{"name": "vgg16", "type": "str", "description": "VGG16 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg16": {"Top-1 error": 28.41, "Top-5 error": 9.62}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': I want an API to recognize different animals in a series of photos. Can you give me an example?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg16_bn', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained VGG16 model with batch normalization from PyTorch Hub, which can be used to recognize animals in images.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg16_bn', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg16_bn', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg16_bn', pretrained=True)", "api_arguments": [{"name": "vgg16_bn", "type": "str", "description": "VGG16 model with batch normalization"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16_bn', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg16_bn": {"Top-1 error": 26.63, "Top-5 error": 8.5}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': I want to create an API that recognizes objects in an image. Recommend a pre-trained model that will be suitable., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg19_bn', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained VGG19 model with batch normalization from PyTorch Hub for image recognition.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg19_bn', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg19_bn', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg19_bn', pretrained=True)", "api_arguments": [{"name": "vgg19_bn", "type": "str", "description": "VGG19 model with batch normalization"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg19_bn', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg19_bn": {"Top-1 error": 25.76, "Top-5 error": 8.15}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': You are building a Chatbot for a car company that helps identify the model just by looking a picture. Provide an API that can help you recognize car models from a given image., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load(repo_or_dir='pytorch/vision', model='vgg19', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained VGG19 model from PyTorch Hub, which can be fine-tuned for car model recognition from a given image.', 'code': 'import torch\nmodel = torch.hub.load(\"pytorch/vision\", \"vgg19\", pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg19', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg19', pretrained=True)", "api_arguments": [{"name": "vgg19", "type": "str", "description": "VGG19 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg19', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg19": {"Top-1 error": 27.62, "Top-5 error": 9.12}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': I need to create an image classifier implementing Wide Residual Networks. Will you provide me an API call to accomplish this task using pre-trained weights?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'wide_resnet50_2', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the Wide ResNet-50 model from PyTorch Hub with pre-trained weights for image classification.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'wide_resnet50_2', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='wide_resnet50_2', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Wide Residual Networks", "api_name": "wide_resnet50_2", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='wide_resnet50_2', pretrained=True)", "api_arguments": "pretrained", "python_environment_requirements": "torch, torchvision", "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'wide_resnet50_2', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"wide_resnet50_2": {"Top-1 error": 21.49, "Top-5 error": 5.91}}}, "description": "Wide Residual networks simply have increased number of channels compared to ResNet. Otherwise the architecture is the same. Deeper ImageNet models with bottleneck block have increased number of channels in the inner 3x3 convolution. The wide_resnet50_2 and wide_resnet101_2 models were trained in FP16 with mixed precision training using SGD with warm restarts. Checkpoints have weights in half precision (except batch norm) for smaller size, and can be used in FP32 models too."}}
{"code": "{'Instruction': A start-up company plans to develop media dashboard to monitor and categorize the latest YouTube trends. Which API should they use for video classification?, 'Output': {'domain': 'Video Classification', 'api_call': 'model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained SlowFast model for video classification from PyTorch Hub to analyze and classify the action categories of YouTube videos.', 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='slowfast_r50', pretrained=True)", "provider": "PyTorchVideo", "api_data": {"domain": "Video Classification", "framework": "PyTorchVideo", "functionality": "SlowFast Networks", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='slowfast_r50', pretrained=True)", "api_arguments": {"repository": "facebookresearch/pytorchvideo", "model": "slowfast_r50", "pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "pytorchvideo"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)", "device = 'cpu'", "model = model.eval()", "model = model.to(device)"], "performance": {"dataset": "Kinetics 400", "accuracy": {"top1": 76.94, "top5": 92.69}, "flops": 65.71, "params": 34.57}, "description": "Slow Fast model architectures are based on the paper 'SlowFast Networks for Video Recognition' by Christoph Feichtenhofer et al. They are pretrained on the Kinetics 400 dataset using the 8x8 setting. This model is capable of classifying video clips into different action categories. It is provided by the FAIR PyTorchVideo library."}}
{"code": "{'Instruction': Determine an API that can identify plant species from images., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'wide_resnet101_2', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Wide ResNet101_2 model from PyTorch Hub, which can be fine-tuned to classify plant species from images.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'wide_resnet101_2', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='wide_resnet101_2', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Wide Residual Networks", "api_name": "wide_resnet101_2", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='wide_resnet101_2', pretrained=True)", "api_arguments": "pretrained", "python_environment_requirements": "torch, torchvision", "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'wide_resnet101_2', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"wide_resnet101_2": {"Top-1 error": 21.16, "Top-5 error": 5.72}}}, "description": "Wide Residual networks simply have increased number of channels compared to ResNet. Otherwise the architecture is the same. Deeper ImageNet models with bottleneck block have increased number of channels in the inner 3x3 convolution. The wide_resnet50_2 and wide_resnet101_2 models were trained in FP16 with mixed precision training using SGD with warm restarts. Checkpoints have weights in half precision (except batch norm) for smaller size, and can be used in FP32 models too."}}
{"code": "{'Instruction': Find me an API to use for video analysis in a project where we need to analyze football games., 'Output': {'domain': 'Video Classification', 'api_call': 'model = torch.hub.load('facebookresearch/pytorchvideo', 'x3d_s', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained X3D model for video classification from PyTorch Hub to analyze football games and classify various actions performed by the players.', 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/pytorchvideo', 'x3d_s', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='x3d_s', pretrained=True)", "provider": "PyTorchVideo", "api_data": {"domain": "Video Classification", "framework": "PyTorchVideo", "functionality": "X3D Networks", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='x3d_s', pretrained=True)", "api_arguments": {"repository": "facebookresearch/pytorchvideo", "model": "x3d_s", "pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "pytorchvideo"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/pytorchvideo', 'x3d_s', pretrained=True)", "device = 'cpu'", "model = model.eval()", "model = model.to(device)"], "performance": {"dataset": "Kinetics 400", "accuracy": {"top1": 73.33, "top5": 91.27}, "flops": 2.96, "params": 3.79}, "description": "X3D model architectures are based on the paper 'X3D: Expanding Architectures for Efficient Video Recognition' by Christoph Feichtenhofer. They are pretrained on the Kinetics 400 dataset. This model is capable of classifying video clips into different action categories. It is provided by the FAIR PyTorchVideo library."}}
{"code": "{'Instruction': Share with me a model to classify furniture pictures posted on an online store., 'Output': {'domain': 'Image Classification', 'api_call': 'model = torch.hub.load('mit-han-lab/once-for-all', 'ofa_supernet_mbv3_w10', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Use the pretrained OFA model from the MIT Han Lab for image classification. It has great performance across a wide range of devices and can be fine-tuned for furniture classification.', 'code': 'import torch\nmodel = torch.hub.load('mit-han-lab/once-for-all', 'ofa_supernet_mbv3_w10', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/once-for-all', model='ofa_supernet_mbv3_w10', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Once-for-all (OFA) Networks", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/once-for-all', model='ofa_supernet_mbv3_w10', pretrained=True)", "api_arguments": {"repository": "mit-han-lab/once-for-all", "model": "ofa_supernet_mbv3_w10", "pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "super_net_name = 'ofa_supernet_mbv3_w10'", "super_net = torch.hub.load('mit-han-lab/once-for-all', super_net_name, pretrained=True).eval()"], "performance": {"description": "OFA networks outperform state-of-the-art NAS methods (up to 4.0% ImageNet top1 accuracy improvement over MobileNetV3, or same accuracy but 1.5x faster than MobileNetV3, 2.6x faster than EfficientNet w.r.t measured latency) while reducing many orders of magnitude GPU hours and CO2 emission."}, "description": "Once-for-all (OFA) networks are a family of neural networks designed by MIT Han Lab. They decouple training and search, achieving efficient inference across various edge devices and resource constraints. OFA networks are pretrained on the IMAGENET dataset and are capable of classifying images into different categories."}}
{"code": "{'Instruction': Imagine a new photo album app that sorts photos by its theme. Find me an API that can classify images according to their theme., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_gpunet', model_type=model_type, pretrained=True, model_math=precision)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained GPUNet model optimized for NVIDIA GPU performance from PyTorch Hub. This model can classify images into different categories based on their theme.', 'code': 'import torch\nmodel_type = 'GPUNet-0'\nprecision = 'fp32'\nmodel = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_gpunet', model_type=model_type, pretrained=True, model_math=precision)'}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_gpunet', pretrained=True, model_type=model_type, model_math=precision)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "GPUNet Networks", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_gpunet', pretrained=True, model_type=model_type, model_math=precision)", "api_arguments": {"repository": "NVIDIA/DeepLearningExamples:torchhub", "model": "nvidia_gpunet", "pretrained": "True", "model_type": "GPUNet-0", "model_math": "fp32"}, "python_environment_requirements": ["torch", "validators", "matplotlib", "timm==0.5.4"], "example_code": ["import torch", "model_type = 'GPUNet-0'", "precision = 'fp32'", "gpunet = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_gpunet', pretrained=True, model_type=model_type, model_math=precision)", "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')", "gpunet.to(device)", "gpunet.eval()"], "performance": {"dataset": "IMAGENET", "description": "GPUNet demonstrates state-of-the-art inference performance up to 2x faster than EfficientNet-X and FBNet-V3."}, "description": "GPUNet is a family of Convolutional Neural Networks designed by NVIDIA using novel Neural Architecture Search (NAS) methods. They are optimized for NVIDIA GPU and TensorRT performance. GPUNet models are pretrained on the IMAGENET dataset and are capable of classifying images into different categories. The models are provided by the NVIDIA Deep Learning Examples library."}}
{"code": "{'Instruction': Identify an API that can analyze customer reviews and classify them as positive or negative., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-cased')\", 'api_provider': 'PyTorch', 'explanation': 'Load a pretrained BERT model from Hugging Face PyTorch-Transformers for sentiment analysis of customer reviews by classifying them as positive or negative.', 'code': 'import torch\nmodel = torch.hub.load(\\'huggingface/pytorch-transformers\\', \\'model\\', \\'bert-base-cased\\')'}}", "api_call": "torch.hub.load(repo_or_dir='huggingface/pytorch-transformers')", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Natural Language Processing", "api_name": "PyTorch-Transformers", "api_call": "torch.hub.load(repo_or_dir='huggingface/pytorch-transformers')", "api_arguments": ["pretrained_model_or_path", "output_attention", "output_hidden_states", "config", "from_tf"], "python_environment_requirements": ["tqdm", "boto3", "requests", "regex", "sentencepiece", "sacremoses"], "example_code": "import torch\ntokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-cased')\n\ntext_1 = \"Jim Henson was a puppeteer\"\ntext_2 = \"Who was Jim Henson ?\"\n\nindexed_tokens = tokenizer.encode(text_1, text_2, add_special_tokens=True)\n\nmodel = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-cased')\n\nwith torch.no_grad():\n encoded_layers, _ = model(tokens_tensor, token_type_ids=segments_tensors)", "performance": {"dataset": [{"name": "MRPC", "accuracy": "Not provided"}]}, "description": "PyTorch-Transformers is a library of state-of-the-art pre-trained models for Natural Language Processing (NLP) including BERT, GPT, GPT-2, Transformer-XL, XLNet, XLM, RoBERTa, and DistilBERT. The library provides functionality for tokenization, configuration, and various model architectures for different tasks such as causal language modeling, sequence classification, question answering, and masked language modeling."}}
{"code": "{'Instruction': Design an API and example code that can classify optical, cable, and copper wires by images with accuracy., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models', 'resnet18_swsl', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained ResNet-18 model from Facebook Research's semi-supervised ImageNet models to classify images of optical, cable, and copper wires with high accuracy.\", 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models', 'resnet18_swsl', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/semi-supervised-ImageNet1K-models', model='resnet18_swsl', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Semi-supervised and semi-weakly supervised ImageNet Models", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/semi-supervised-ImageNet1K-models', model='resnet18_swsl', pretrained=True)", "api_arguments": {"repository": "facebookresearch/semi-supervised-ImageNet1K-models", "model": "resnet18_swsl", "pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models', 'resnet18_swsl', pretrained=True)", "model.eval()"], "performance": {"description": "Semi-supervised and semi-weakly supervised ImageNet models achieve state-of-the-art accuracy of 81.2% on ImageNet for the widely used/adopted ResNet-50 model architecture."}, "description": "Semi-supervised and semi-weakly supervised ImageNet Models are introduced in the 'Billion scale semi-supervised learning for image classification' paper. These models are pretrained on a subset of unlabeled YFCC100M public image dataset and fine-tuned with the ImageNet1K training dataset. They are capable of classifying images into different categories and are provided by the Facebook Research library."}}
{"code": "{'Instruction': Find me an API for neural machine translation that can translate English text to French., 'Output': {'domain': 'Translation', 'api_call': \"model = torch.hub.load('pytorch/fairseq', 'transformer.wmt14.en-fr', tokenizer='moses', bpe='subword_nmt')\", 'api_provider': 'PyTorch', 'explanation': 'Load the Transformer model for neural machine translation from PyTorch Hub to translate English text to French.', 'code': \"import torch\nmodel = torch.hub.load('pytorch/fairseq', 'transformer.wmt14.en-fr', tokenizer='moses', bpe='subword_nmt')\"}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/fairseq')", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "Neural Machine Translation", "api_name": "Transformer (NMT)", "api_call": "torch.hub.load(repo_or_dir='pytorch/fairseq')", "api_arguments": ["model_name", "tokenizer", "bpe", "beam", "sampling", "sampling_topk"], "python_environment_requirements": ["bitarray", "fastBPE", "hydra-core", "omegaconf", "regex", "requests", "sacremoses", "subword_nmt"], "example_code": "import torch\n\nen2fr = torch.hub.load('pytorch/fairseq', 'transformer.wmt14.en-fr', tokenizer='moses', bpe='subword_nmt')\n\nen2fr.cuda()\n\nfr = en2fr.translate('Hello world!', beam=5)\nassert fr == 'Bonjour \u00e0 tous !'", "performance": {"dataset": [{"name": "WMT'14", "accuracy": "Not provided"}, {"name": "WMT'18", "accuracy": "Not provided"}, {"name": "WMT'19", "accuracy": "Not provided"}]}, "description": "Transformer (NMT) is a powerful sequence-to-sequence modeling architecture that produces state-of-the-art neural machine translation systems. It is based on the paper 'Attention Is All You Need' and has been improved using techniques such as large-scale semi-supervised training, back-translation, and noisy-channel reranking. It supports English-French and English-German translation as well as round-trip translation for paraphrasing."}}
{"code": "{'Instruction': Design an API to identify objects in an image quickly and efficiently., 'Output': {'domain': 'Object Detection', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Single Shot MultiBox Detector (SSD) model from PyTorch Hub for efficient and high-accuracy object detection in images.', 'code': \"import torch\nmodel = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_ssd', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Object Detection", "framework": "PyTorch", "functionality": "Single Shot MultiBox Detector", "api_name": "SSD", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_ssd', pretrained=True)", "api_arguments": ["model_name"], "python_environment_requirements": ["numpy", "scipy", "scikit-image", "matplotlib"], "example_code": "import torch\n\nssd_model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd')\nutils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd_processing_utils')\n\nssd_model.to('cuda')\nssd_model.eval()\n\ninputs = [utils.prepare_input(uri) for uri in uris]\ntensor = utils.prepare_tensor(inputs)\n\nwith torch.no_grad():\n detections_batch = ssd_model(tensor)\n\nresults_per_input = utils.decode_results(detections_batch)\nbest_results_per_input = [utils.pick_best(results, 0.40) for results in results_per_input]", "performance": {"dataset": {"name": "COCO", "accuracy": "Not provided"}}, "description": "The SSD (Single Shot MultiBox Detector) model is an object detection model based on the paper 'SSD: Single Shot MultiBox Detector'. It uses a deep neural network for detecting objects in images. This implementation replaces the obsolete VGG model backbone with the more modern ResNet-50 model. The SSD model is trained on the COCO dataset and can be used to detect objects in images with high accuracy and efficiency."}}
{"code": "{'Instruction': Come up with an API for our IVR system that generates natural-sounding spoken responses from provided text., 'Output': {'domain': 'Text-to-Speech', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tacotron2', model_math='fp16')\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Tacotron 2 model from PyTorch Hub, which generates natural-sounding speech from text input, suitable for an IVR system.', 'code': 'import torch\nmodel = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tacotron2', model_math=\"fp16\")'}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_tacotron2', model_math='fp16')", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Text-to-Speech", "api_name": "Tacotron 2", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_tacotron2', model_math='fp16')", "api_arguments": {"model_math": "fp16"}, "python_environment_requirements": ["numpy", "scipy", "librosa", "unidecode", "inflect", "libsndfile1"], "example_code": ["import torch", "tacotron2 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tacotron2', model_math='fp16')", "tacotron2 = tacotron2.to('cuda')", "tacotron2.eval()", "text = 'Hello world, I missed you so much.'", "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tts_utils')", "sequences, lengths = utils.prepare_input_sequence([text])", "with torch.no_grad():", " mel, _, _ = tacotron2.infer(sequences, lengths)", " audio = waveglow.infer(mel)", "audio_numpy = audio[0].data.cpu().numpy()", "rate = 22050"], "performance": {"dataset": "LJ Speech", "accuracy": "Not specified"}, "description": "The Tacotron 2 model generates mel spectrograms from input text using an encoder-decoder architecture, and it is designed for generating natural-sounding speech from raw transcripts without any additional prosody information. This implementation uses Dropout instead of Zoneout to regularize the LSTM layers. The WaveGlow model (also available via torch.hub) is a flow-based model that consumes the mel spectrograms to generate speech."}}
{"code": "{'Instruction': I want to extract features from a given image using a pre-trained deep learning model. Can you provide me an API for this?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V1','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MEAL_V2 model from PyTorch Hub, which can be used for feature extraction and image classification.', 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V1','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V1', model='mealv1_resnest50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V1', model='mealv1_resnest50', pretrained=True)", "api_arguments": {"model_name": "mealv1_resnest50"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V1 w/ ResNet50", "resolution": "224", "parameters": "25.6M", "top1": "78.21", "top5": "94.01"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': An artist wants to claim their work in Reddit. Give an API that can identify their art from an uploaded picture., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MEAL V2 model from PyTorch Hub, which can be fine-tuned to identify an artist's artwork from an uploaded picture on Reddit.', 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50', pretrained=True)", "api_arguments": {"model_name": "mealv2_resnest50"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ ResNet50", "resolution": "224", "parameters": "25.6M", "top1": "80.67", "top5": "95.09"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': I want an API for classifying images into 1000 classes. What do you suggest?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenetv3_small_075', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MEAL V2 model with MobileNet V3-Small 0.75 architecture from PyTorch Hub for classifying images into 1000 classes.', 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenetv3_small_075', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenetv3_small_075', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenetv3_small_075', pretrained=True)", "api_arguments": {"model_name": "mealv2_mobilenetv3_small_075"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ MobileNet V3-Small 0.75", "resolution": "224", "parameters": "2.04M", "top1": "67.60", "top5": "87.23"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': An app developer wants to classify images of pets to make a pet-themed camera app. Suggest an API that can classify images of different pets., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', 'mealv2_resnest50_380x380', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained MEAL-V2 model for image classification from PyTorch Hub to classify images of different pets in a pet-themed camera app.\", 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', 'mealv2_resnest50_380x380', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50_380x380', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50_380x380', pretrained=True)", "api_arguments": {"model_name": "mealv2_resnest50_380x380"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ ResNet50", "resolution": "380", "parameters": "25.6M", "top1": "81.72", "top5": "95.81"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': Please provide an API that can classify images of healthy foods to identify if it's vegan or not based on its ingredients., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenet_v3_large_100', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MEAL_V2 model from PyTorch Hub. This model can be fine-tuned for classifying food images into vegan or non-vegan based on the ingredients present in the image.', 'code': 'import torch\nmodel = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenet_v3_large_100', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenet_v3_large_100', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenet_v3_large_100', pretrained=True)", "api_arguments": {"model_name": "mealv2_mobilenet_v3_large_100"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ MobileNet V3-Large 1.0", "resolution": "224", "parameters": "5.48M", "top1": "76.92", "top5": "93.32"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': What is an API that can classify images into categories without using common tricks such as mixup, cutmix, or autoaugmentation?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenetv3_small_100', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained MEAL V2 model from PyTorch Hub for image classification. It achieves state-of-the-art performance without using common tricks such as architecture modification, outside training data, autoaugmentation, cosine learning rate, mixup/cutmix, or label smoothing.\", 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenetv3_small_100', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenetv3_small_100', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenetv3_small_100', pretrained=True)", "api_arguments": {"model_name": "mealv2_mobilenetv3_small_100"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ MobileNet V3-Small 1.0", "resolution": "224", "parameters": "2.54M", "top1": "69.65", "top5": "88.71"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': Help me to classify images in a Python project. What API should I use to achieve more than 80% top-1 accuracy on ImageNet without using any tricks?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_efficientnet_b0', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pre-trained MEAL V2 model with more than 80% top-1 accuracy on ImageNet using the PyTorch framework. The MEAL V2 technique boosts classification accuracy without any tricks.\", 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_efficientnet_b0', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_efficientnet_b0', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_efficientnet_b0', pretrained=True)", "api_arguments": {"model_name": "mealv2_efficientnet_b0"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ EfficientNet-B0", "resolution": "224", "parameters": "5.29M", "top1": "78.29", "top5": "93.95"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': Suggest me an AI model to classify animals given an image of a dog., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MEAL_V2 image classification model from PyTorch Hub to classify animals from given images.', 'code': \"import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50_cutmix', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50_cutmix', pretrained=True)", "api_arguments": {"model_name": "mealv2_resnest50_cutmix"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 + CutMix w/ ResNet50", "resolution": "224", "parameters": "25.6M", "top1": "80.98", "top5": "95.35"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': I want an object detection API capable of handling object detection, drivable area segmentation, and lane detection for autonomous driving tasks., 'Output': {'domain': 'Object Detection', 'api_call': \"model = torch.hub.load('hustvl/yolop', 'yolop', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained YOLOP model from PyTorch Hub, which is specifically designed for autonomous driving tasks such as object detection, drivable area segmentation, and lane detection.', 'code': \"import torch\nmodel = torch.hub.load('hustvl/yolop', 'yolop', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='hustvl/yolop', model='yolop', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Object Detection", "framework": "PyTorch", "functionality": "Object Detection, Drivable Area Segmentation, Lane Detection", "api_name": "YOLOP", "api_call": "torch.hub.load(repo_or_dir='hustvl/yolop', model='yolop', pretrained=True)", "api_arguments": "pretrained", "python_environment_requirements": "pip install -qr https://github.com/hustvl/YOLOP/blob/main/requirements.txt", "example_code": "import torch\nmodel = torch.hub.load('hustvl/yolop', 'yolop', pretrained=True)\nimg = torch.randn(1,3,640,640)\ndet_out, da_seg_out,ll_seg_out = model(img)", "performance": {"dataset": "BDD100K", "accuracy": {"Object Detection": {"Recall(%)": 89.2, "mAP50(%)": 76.5, "Speed(fps)": 41}, "Drivable Area Segmentation": {"mIOU(%)": 91.5, "Speed(fps)": 41}, "Lane Detection": {"mIOU(%)": 70.5, "IOU(%)": 26.2}}}, "description": "YOLOP is an efficient multi-task network that can jointly handle three crucial tasks in autonomous driving: object detection, drivable area segmentation and lane detection. And it is also the first to reach real-time on embedded devices while maintaining state-of-the-art level performance on the BDD100K dataset."}}
{"code": "{'Instruction': Design an autonomous vehicle algorithm that can detect other vehicles, pedestrians, and the drivable area on the road. Suggest an appropriate API for this purpose., 'Output': {'domain': 'Object Detection', 'api_call': \"model = torch.hub.load('datvuthanh/hybridnets', 'hybridnets', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the HybridNets model from PyTorch Hub with pre-trained weights, which can detect traffic objects, drivable area segmentation, and lane detection for autonomous vehicle applications.', 'code': \"import torch\nmodel = torch.hub.load('datvuthanh/hybridnets', 'hybridnets', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='datvuthanh/hybridnets', model='hybridnets', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Object Detection", "framework": "PyTorch", "functionality": "Traffic Object Detection, Drivable Area Segmentation, Lane Detection", "api_name": "HybridNets", "api_call": "torch.hub.load(repo_or_dir='datvuthanh/hybridnets', model='hybridnets', pretrained=True)", "api_arguments": "pretrained", "python_environment_requirements": "Python>=3.7, PyTorch>=1.10", "example_code": "import torch\nmodel = torch.hub.load('datvuthanh/hybridnets', 'hybridnets', pretrained=True)\nimg = torch.randn(1,3,640,384)\nfeatures, regression, classification, anchors, segmentation = model(img)", "performance": {"dataset": [{"name": "BDD100K", "accuracy": {"Traffic Object Detection": {"Recall (%)": 92.8, "mAP@0.5 (%)": 77.3}, "Drivable Area Segmentation": {"Drivable mIoU (%)": 90.5}, "Lane Line Detection": {"Accuracy (%)": 85.4, "Lane Line IoU (%)": 31.6}}}]}, "description": "HybridNets is an end2end perception network for multi-tasks. Our work focused on traffic object detection, drivable area segmentation and lane detection. HybridNets can run real-time on embedded systems, and obtains SOTA Object Detection, Lane Detection on BDD100K Dataset."}}
{"code": "{'Instruction': Calculate the relative inverse depth from a single input image. Is there an API that can do this?, 'Output': {'domain': 'Single Image Depth Estimation', 'api_call': \"model = torch.hub.load('intel-isl/MiDaS', 'DPT_Large', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MiDaS model from the PyTorch Hub to compute the relative inverse depth from a single input image.', 'code': \"import torch\nmodel = torch.hub.load('intel-isl/MiDaS', 'DPT_Large', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='intel-isl/MiDaS', model='DPT_Large', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Computing relative depth from a single image", "api_name": "MiDaS", "api_call": "torch.hub.load(repo_or_dir='intel-isl/MiDaS', model='DPT_Large', pretrained=True)", "api_arguments": {"repo_or_dir": "intel-isl/MiDaS", "model": "model_type"}, "python_environment_requirements": "pip install timm", "example_code": ["import cv2", "import torch", "import urllib.request", "import matplotlib.pyplot as plt", "url, filename = ('https://github.com/pytorch/hub/raw/master/images/dog.jpg', 'dog.jpg')", "urllib.request.urlretrieve(url, filename)", "model_type = 'DPT_Large'", "midas = torch.hub.load('intel-isl/MiDaS', 'DPT_Large')", "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')", "midas.to(device)", "midas.eval()", "midas_transforms = torch.hub.load('intel-isl/MiDaS', 'transforms')", "if model_type == 'DPT_Large' or model_type == 'DPT_Hybrid':", " transform = midas_transforms.dpt_transform", "else:", " transform = midas_transforms.small_transform", "img = cv2.imread(filename)", "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)", "input_batch = transform(img).to(device)", "with torch.no_grad():", " prediction = midas(input_batch)", "prediction = torch.nn.functional.interpolate(", " prediction.unsqueeze(1),", " size=img.shape[:2],", " mode='bicubic',", " align_corners=False,", ").squeeze()", "output = prediction.cpu().numpy()", "plt.imshow(output)", "plt.show()"], "performance": {"dataset": "10 distinct datasets", "accuracy": "Multi-objective optimization"}, "description": "MiDaS computes relative inverse depth from a single image. The repository provides multiple models that cover different use cases ranging from a small, high-speed model to a very large model that provide the highest accuracy. The models have been trained on 10 distinct datasets using multi-objective optimization to ensure high quality on a wide range of inputs."}}
{"code": "{'Instruction': How can I estimate depth from a single image? Can you suggest an API for it?, 'Output': {'domain': 'Depth Estimation', 'api_call': \"model = torch.hub.load('intel-isl/MiDaS', 'DPT_Hybrid', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Use the MiDaS model from PyTorch Hub to estimate depth from a single image. Use the 'DPT_Hybrid' model for high-quality depth estimation.\", 'code': \"import torch\nmodel = torch.hub.load('intel-isl/MiDaS', 'DPT_Hybrid', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='intel-isl/MiDaS', model='DPT_Hybrid', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Computing relative depth from a single image", "api_name": "MiDaS", "api_call": "torch.hub.load(repo_or_dir='intel-isl/MiDaS', model='DPT_Hybrid', pretrained=True)", "api_arguments": {"repo_or_dir": "intel-isl/MiDaS", "model": "model_type"}, "python_environment_requirements": "pip install timm", "example_code": ["import cv2", "import torch", "import urllib.request", "import matplotlib.pyplot as plt", "url, filename = ('https://github.com/pytorch/hub/raw/master/images/dog.jpg', 'dog.jpg')", "urllib.request.urlretrieve(url, filename)", "model_type = 'DPT_Large'", "midas = torch.hub.load('intel-isl/MiDaS', 'DPT_Hybrid')", "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')", "midas.to(device)", "midas.eval()", "midas_transforms = torch.hub.load('intel-isl/MiDaS', 'transforms')", "if model_type == 'DPT_Large' or model_type == 'DPT_Hybrid':", " transform = midas_transforms.dpt_transform", "else:", " transform = midas_transforms.small_transform", "img = cv2.imread(filename)", "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)", "input_batch = transform(img).to(device)", "with torch.no_grad():", " prediction = midas(input_batch)", "prediction = torch.nn.functional.interpolate(", " prediction.unsqueeze(1),", " size=img.shape[:2],", " mode='bicubic',", " align_corners=False,", ").squeeze()", "output = prediction.cpu().numpy()", "plt.imshow(output)", "plt.show()"], "performance": {"dataset": "10 distinct datasets", "accuracy": "Multi-objective optimization"}, "description": "MiDaS computes relative inverse depth from a single image. The repository provides multiple models that cover different use cases ranging from a small, high-speed model to a very large model that provide the highest accuracy. The models have been trained on 10 distinct datasets using multi-objective optimization to ensure high quality on a wide range of inputs."}}
{"code": "{'Instruction': A DJ wants to separate a song into four stems: vocals, drums, bass, and other instruments. What API can be used to achieve this?, 'Output': {'domain': 'Audio Separation', 'api_call': \"model = torch.hub.load('sigsep/open-unmix-pytorch', 'umxhq')\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained Open-Unmix model from PyTorch Hub to separate pop music into vocals, drums, bass, and other instruments.\", 'code': 'import torch\nmodel = torch.hub.load('sigsep/open-unmix-pytorch', 'umxhq')'}}", "api_call": "torch.hub.load(repo_or_dir='sigsep/open-unmix-pytorch', model='umxhq', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Audio Separation", "framework": "PyTorch", "functionality": "Music Source Separation", "api_name": "Open-Unmix", "api_call": "torch.hub.load(repo_or_dir='sigsep/open-unmix-pytorch', model='umxhq', pretrained=True)", "api_arguments": ["model_name"], "python_environment_requirements": ["PyTorch >=1.6.0", "torchaudio"], "example_code": ["import torch", "separator = torch.hub.load('sigsep/open-unmix-pytorch', 'umxhq')", "audio = torch.rand((1, 2, 100000))", "original_sample_rate = separator.sample_rate", "estimates = separator(audio)"], "performance": {"dataset": "MUSDB18", "accuracy": "N/A"}, "description": "Open-Unmix provides ready-to-use models that allow users to separate pop music into four stems: vocals, drums, bass and the remaining other instruments. The models were pre-trained on the freely available MUSDB18 dataset."}}
{"code": "{'Instruction': Recommend an API that can convert a given text into speech with minimal dependencies., 'Output': {'domain': 'Text-To-Speech', 'api_call': \"model = torch.hub.load('snakers4/silero-models', 'silero_tts', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained Silero Text-To-Speech model from PyTorch Hub, which can convert a given text into natural-sounding speech with minimal dependencies.\", 'code': \"import torch\nmodel, symbols, sample_rate, example_text, apply_tts = torch.hub.load('snakers4/silero-models', 'silero_tts', language='en', speaker='lj_16khz')\"}}", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_tts', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Text-To-Speech", "framework": "PyTorch", "functionality": "Text-To-Speech", "api_name": "Silero Text-To-Speech Models", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_tts', pretrained=True)", "api_arguments": {"repo_or_dir": "snakers4/silero-models", "model": "silero_tts", "language": "language", "speaker": "speaker"}, "python_environment_requirements": ["pip install -q torchaudio omegaconf"], "example_code": "import torch\nlanguage = 'en'\nspeaker = 'lj_16khz'\ndevice = torch.device('cpu')\nmodel, symbols, sample_rate, example_text, apply_tts = torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_tts', language=language, speaker=speaker)\nmodel = model.to(device)\naudio = apply_tts(texts=[example_text], model=model, sample_rate=sample_rate, symbols=symbols, device=device)", "performance": {"dataset": [{"language": "Russian", "speakers": 6}, {"language": "English", "speakers": 1}, {"language": "German", "speakers": 1}, {"language": "Spanish", "speakers": 1}, {"language": "French", "speakers": 1}], "accuracy": "High throughput on slow hardware. Decent performance on one CPU thread"}, "description": "Silero Text-To-Speech models provide enterprise grade TTS in a compact form-factor for several commonly spoken languages. They offer one-line usage, naturally sounding speech, no GPU or training required, minimalism and lack of dependencies, a library of voices in many languages, support for 16kHz and 8kHz out of the box."}}
{"code": "{'Instruction': My application requires voice activity detection for processing phone calls. Give me an API that detects speech segments in an audio file., 'Output': {'domain': 'Audio Classification', 'api_call': \"model = torch.hub.load('snakers4/silero-vad', 'silero_vad', force_reload=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the Silero Voice Activity Detector (VAD) model from PyTorch Hub to detect speech segments in an audio file.', 'code': \"import torch\nmodel, utils = torch.hub.load('snakers4/silero-vad', 'silero_vad', force_reload=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Voice Activity Detection", "api_name": "Silero Voice Activity Detector", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=True)", "api_arguments": {"repo_or_dir": "snakers4/silero-vad", "model": "silero_vad", "force_reload": "True"}, "python_environment_requirements": {"torchaudio": "pip install -q torchaudio"}, "example_code": {"import": ["import torch", "torch.set_num_threads(1)", "from IPython.display import Audio", "from pprint import pprint"], "download_example": "torch.hub.download_url_to_file('https://models.silero.ai/vad_models/en.wav', 'en_example.wav')", "load_model": "model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=True)", "load_utils": "(get_speech_timestamps, _, read_audio, _) = utils", "set_sampling_rate": "sampling_rate = 16000", "read_audio": "wav = read_audio('en_example.wav', sampling_rate=sampling_rate)", "get_speech_timestamps": "speech_timestamps = get_speech_timestamps(wav, model, sampling_rate=sampling_rate)", "print_speech_timestamps": "pprint(speech_timestamps)"}, "performance": {"dataset": "", "accuracy": ""}, "description": "Silero VAD is a pre-trained enterprise-grade Voice Activity Detector (VAD) that aims to provide a high-quality and modern alternative to the WebRTC Voice Activity Detector. The model is optimized for performance on 1 CPU thread and is quantized."}}
{"code": "{'Instruction': A drone startup requires an API to analyze aerial images to recognize roads, buildings, and trees., 'Output': {'domain': 'Semantic Segmentation', 'api_call': \"model = torch.hub.load('pytorch/vision', 'deeplabv3_resnet50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained DeepLabV3 model with a ResNet-50 backbone to perform semantic segmentation on aerial images, allowing the recognition of roads, buildings, and trees.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'deeplabv3_resnet50', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='deeplabv3_resnet50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "DeepLabV3", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='deeplabv3_resnet50', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": "latest", "torchvision": "latest", "PIL": "latest", "matplotlib": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_resnet50', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "input_image = input_image.convert('RGB')", "preprocess = transforms.Compose([", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)['out'][0]", "output_predictions = output.argmax(0)"], "performance": {"dataset": "COCO val2017", "accuracy": [{"model": "deeplabv3_resnet50", "Mean IOU": 66.4, "Global Pixelwise Accuracy": 92.4}]}, "description": "DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset."}}
{"code": "{'Instruction': How can I transcribe a recorded sales pitch from English speech to text using an API?, 'Output': {'domain': 'Classification', 'api_call': \"model, decoder, utils = torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_stt', language='en', device=device)\", 'api_provider': 'PyTorch', 'explanation': 'Load the Silero Speech-To-Text model from PyTorch Hub, which can transcribe a sales pitch from English speech to text.', 'code': 'import torch\ndevice = torch.device('cpu')\nmodel, decoder, utils = torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_stt', language='en', device=device)'}}", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_stt', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Speech-To-Text", "api_name": "Silero Models", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_stt', pretrained=True)", "api_arguments": {"repo_or_dir": "snakers4/silero-models", "model": "silero_stt", "language": ["en", "de", "es"], "device": "device"}, "python_environment_requirements": ["pip install -q torchaudio omegaconf soundfile"], "example_code": ["import torch", "import zipfile", "import torchaudio", "from glob import glob", "device = torch.device('cpu')", "model, decoder, utils = torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_stt', language='en', device=device)", "(read_batch, split_into_batches, read_audio, prepare_model_input) = utils", "torch.hub.download_url_to_file('https://opus-codec.org/static/examples/samples/speech_orig.wav', dst ='speech_orig.wav', progress=True)", "test_files = glob('speech_orig.wav')", "batches = split_into_batches(test_files, batch_size=10)", "input = prepare_model_input(read_batch(batches[0]), device=device)", "output = model(input)", "for example in output:", " print(decoder(example.cpu()))"], "performance": {"dataset": "Open-STT", "accuracy": "See the wiki for quality and performance benchmarks"}, "description": "Silero Speech-To-Text models provide enterprise grade STT in a compact form-factor for several commonly spoken languages. The models are robust to a variety of dialects, codecs, domains, noises, and lower sampling rates. They consume a normalized audio in the form of samples and output frames with token probabilities. A decoder utility is provided for simplicity."}}
{"code": "{'Instruction': I am working on a project that involves identifying bird species in pictures. Find me an API that can classify the bird species in an image., 'Output': {'domain': 'Image classification', 'api_call': \"model = torch.hub.load('nicolalandro/ntsnet-cub200', 'ntsnet', pretrained=True, **{'topN': 6, 'device':'cpu', 'num_classes': 200})\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained NTS-Net model from PyTorch Hub, which is trained on the CUB-200 2011 dataset, to classify bird species in an image.\", 'code': \"import torch\nmodel = torch.hub.load('nicolalandro/ntsnet-cub200', 'ntsnet', pretrained=True, **{'topN': 6, 'device':'cpu', 'num_classes': 200})\"}}", "api_call": "torch.hub.load(repo_or_dir='nicolalandro/ntsnet-cub200', model='ntsnet', pretrained=True, **{'topN': 6, 'device':'cpu', 'num_classes': 200})", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Fine-grained image classifier", "api_name": "ntsnet", "api_call": "torch.hub.load(repo_or_dir='nicolalandro/ntsnet-cub200', model='ntsnet', pretrained=True, **{'topN': 6, 'device':'cpu', 'num_classes': 200})", "api_arguments": {"pretrained": "True", "topN": "6", "device": "cpu", "num_classes": "200"}, "python_environment_requirements": ["torch", "torchvision", "PIL"], "example_code": "from torchvision import transforms\nimport torch\nimport urllib\nfrom PIL import Image\n\ntransform_test = transforms.Compose([\n transforms.Resize((600, 600), Image.BILINEAR),\n transforms.CenterCrop((448, 448)),\n transforms.ToTensor(),\n transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n])\n\nmodel = torch.hub.load('nicolalandro/ntsnet-cub200', 'ntsnet', pretrained=True, **{'topN': 6, 'device':'cpu', 'num_classes': 200})\nmodel.eval()\n\nurl = 'https://raw.githubusercontent.com/nicolalandro/ntsnet-cub200/master/images/nts-net.png'\nimg = Image.open(urllib.request.urlopen(url))\nscaled_img = transform_test(img)\ntorch_images = scaled_img.unsqueeze(0)\n\nwith torch.no_grad():\n top_n_coordinates, concat_out, raw_logits, concat_logits, part_logits, top_n_index, top_n_prob = model(torch_images)\n\n_, predict = torch.max(concat_logits, 1)\npred_id = predict.item()\nprint('bird class:', model.bird_classes[pred_id])", "performance": {"dataset": "CUB200 2011", "accuracy": "Not provided"}, "description": "This is an nts-net pretrained with CUB200 2011 dataset, which is a fine-grained dataset of birds species."}}
{"code": "{'Instruction': A drone is flying over the neighborhood and is recording video. How can I process the video and perform semantic segmentation on each frame?, 'Output': {'domain': 'Semantic Segmentation', 'api_call': \"model = torch.hub.load('pytorch/vision', 'deeplabv3_resnet101', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained DeepLabV3 model for semantic segmentation from PyTorch Hub. Then, iterate over individual frames of the video to perform semantic segmentation on each frame.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'deeplabv3_resnet101', pretrained=True)\nmodel.eval()' }}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='deeplabv3_resnet101', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "DeepLabV3", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='deeplabv3_resnet101', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": "latest", "torchvision": "latest", "PIL": "latest", "matplotlib": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_resnet101', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "input_image = input_image.convert('RGB')", "preprocess = transforms.Compose([", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)['out'][0]", "output_predictions = output.argmax(0)"], "performance": {"dataset": "COCO val2017", "accuracy": [{"model": "deeplabv3_resnet101", "Mean IOU": 67.4, "Global Pixelwise Accuracy": 92.4}]}, "description": "DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset."}}
{"code": "{'Instruction': I need to detect objects in a given image from a security camera. Can you provide me an API that can perform this task?, 'Output': {'domain': 'Object Detection', 'api_call': \"model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Use the pretrained YOLOv5 model from PyTorch Hub to perform object detection in a given security camera image.', 'code': 'import torch\nmodel = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='ultralytics/yolov5', model='yolov5s', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Object Detection", "framework": "PyTorch", "functionality": "YOLOv5", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='ultralytics/yolov5', model='yolov5s', pretrained=True)", "api_arguments": ["'ultralytics/yolov5'", "'yolov5s'", "pretrained=True"], "python_environment_requirements": "Python>=3.8, PyTorch>=1.7", "example_code": ["import torch", "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)", "imgs = ['https://ultralytics.com/images/zidane.jpg']", "results = model(imgs)", "results.print()", "results.save()", "results.xyxy[0]", "results.pandas().xyxy[0]"], "performance": {"dataset": "COCO", "accuracy": {"YOLOv5s6": {"mAPval0.5:0.95": 43.3, "mAPtest0.5:0.95": 43.3, "mAPval0.5": 61.9}, "YOLOv5m6": {"mAPval0.5:0.95": 50.5, "mAPtest0.5:0.95": 50.5, "mAPval0.5": 68.7}, "YOLOv5l6": {"mAPval0.5:0.95": 53.4, "mAPtest0.5:0.95": 53.4, "mAPval0.5": 71.1}, "YOLOv5x6": {"mAPval0.5:0.95": 54.4, "mAPtest0.5:0.95": 54.4, "mAPval0.5": 72.0}, "YOLOv5x6 TTA": {"mAPval0.5:0.95": 55.0, "mAPtest0.5:0.95": 55.0, "mAPval0.5": 72.0}}}, "description": "YOLOv5 is a family of compound-scaled object detection models trained on the COCO dataset, and includes simple functionality for Test Time Augmentation (TTA), model ensembling, hyperparameter evolution, and export to ONNX, CoreML and TFLite."}}
{"code": "{'Instruction': Recommend an API that can generate images of fashionable clothes and accessories automatically., 'Output': {'domain': 'Image Generation', 'api_call': \"model = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'DCGAN', pretrained=True, useGPU=use_gpu)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Deep Convolutional Generative Adversarial Networks (DCGAN) model from PyTorch Hub to generate images of fashionable clothes and accessories.', 'code': 'import torch\nuse_gpu = True if torch.cuda.is_available() else False\nmodel = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'DCGAN', pretrained=True, useGPU=use_gpu)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorch_GAN_zoo:hub', model='DCGAN', pretrained=True, useGPU=use_gpu)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Generative Adversarial Networks", "api_name": "DCGAN", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorch_GAN_zoo:hub', model='DCGAN', pretrained=True, useGPU=use_gpu)", "api_arguments": {"pretrained": "True", "useGPU": "use_gpu"}, "python_environment_requirements": "Python 3", "example_code": {"import": ["import torch", "import matplotlib.pyplot as plt", "import torchvision"], "use_gpu": "use_gpu = True if torch.cuda.is_available() else False", "load_model": "model = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'DCGAN', pretrained=True, useGPU=use_gpu)", "build_noise_data": "noise, _ = model.buildNoiseData(num_images)", "generate_images": "with torch.no_grad(): generated_images = model.test(noise)", "plot_images": ["plt.imshow(torchvision.utils.make_grid(generated_images).permute(1, 2, 0).cpu().numpy())", "plt.show()"]}, "performance": {"dataset": "FashionGen", "accuracy": "N/A"}, "description": "DCGAN is a model designed in 2015 by Radford et. al. in the paper Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks. It is a GAN architecture both very simple and efficient for low resolution image generation (up to 64x64)."}}
{"code": "{'Instruction': Buzzfeed would like to classify images of their new articles into categories. Suggest an appropriate API to accomplish this task., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x8d_wsl', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained ResNext WSL model for image classification from PyTorch Hub to classify images related to Buzzfeed's articles into different categories.\", 'code': 'import torch\nmodel = torch.hub.load(\"facebookresearch/WSL-Images\", \"resnext101_32x8d_wsl\", pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x8d_wsl', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNext WSL", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x8d_wsl', pretrained=True)", "api_arguments": [{"name": "resnext101_32x8d_wsl", "type": "str", "description": "ResNeXt-101 32x8d WSL model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x8d_wsl')", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(output[0])", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeXt-101 32x8d": {"Top-1 Acc.": "82.2", "Top-5 Acc.": "96.4"}}}, "description": "The provided ResNeXt models are pre-trained in weakly-supervised fashion on 940 million public images with 1.5K hashtags matching with 1000 ImageNet1K synsets, followed by fine-tuning on ImageNet1K dataset. The models significantly improve the training accuracy on ImageNet compared to training from scratch. They achieve state-of-the-art accuracy of 85.4% on ImageNet with the ResNext-101 32x48d model."}}
{"code": "{'Instruction': Help me label the objects in images so they can be used for an e-commerce platform. What AI algorithm can do that?, 'Output': {'domain': 'Image Classification', 'api_call': 'model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNeXt WSL model from PyTorch Hub for image classification, which can identify objects in images for categorization in an e-commerce platform.', 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x16d_wsl', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNext WSL", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x16d_wsl', pretrained=True)", "api_arguments": [{"name": "resnext101_32x16d_wsl", "type": "str", "description": "ResNeXt-101 32x16d WSL model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl')", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(output[0])", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeXt-101 32x16d": {"Top-1 Acc.": "84.2", "Top-5 Acc.": "97.2"}}}, "description": "The provided ResNeXt models are pre-trained in weakly-supervised fashion on 940 million public images with 1.5K hashtags matching with 1000 ImageNet1K synsets, followed by fine-tuning on ImageNet1K dataset. The models significantly improve the training accuracy on ImageNet compared to training from scratch. They achieve state-of-the-art accuracy of 85.4% on ImageNet with the ResNext-101 32x48d model."}}
{"code": "{'Instruction': Help a startup to develop a recommendation system to identify plants in their garden that visitors can take pictures of, suggest an API that can be used for plant identification from a picture., 'Output': {'domain': 'Classification', 'api_call': 'model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x32d_wsl', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNext WSL model from PyTorch Hub, which can be fine-tuned for plant identification from images.', 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x32d_wsl', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x32d_wsl', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNext WSL", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x32d_wsl', pretrained=True)", "api_arguments": [{"name": "resnext101_32x32d_wsl", "type": "str", "description": "ResNeXt-101 32x32d WSL model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x32d_wsl')", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(output[0])", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeXt-101 32x32d": {"Top-1 Acc.": "85.1", "Top-5 Acc.": "97.5"}}}, "description": "The provided ResNeXt models are pre-trained in weakly-supervised fashion on 940 million public images with 1.5K hashtags matching with 1000 ImageNet1K synsets, followed by fine-tuning on ImageNet1K dataset. The models significantly improve the training accuracy on ImageNet compared to training from scratch. They achieve state-of-the-art accuracy of 85.4% on ImageNet with the ResNext-101 32x48d model."}}
{"code": "{'Instruction': I am a developer at an online store, trying to classify the objects in the images provided for each product. Suggest an AI API which can be utilized for this purpose., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x48d_wsl', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pre-trained ResNeXt-101 32x48d WSL model from PyTorch Hub, which is trained on a large dataset for object classification and achieves state-of-the-art accuracy (85.4% top-1) on ImageNet.\", 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x48d_wsl', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x48d_wsl', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNext WSL", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x48d_wsl', pretrained=True)", "api_arguments": [{"name": "resnext101_32x48d_wsl", "type": "str", "description": "ResNeXt-101 32x48d WSL model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x48d_wsl')", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(output[0])", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeXt-101 32x48d": {"Top-1 Acc.": "85.4", "Top-5 Acc.": "97.6"}}}, "description": "The provided ResNeXt models are pre-trained in weakly-supervised fashion on 940 million public images with 1.5K hashtags matching with 1000 ImageNet1K synsets, followed by fine-tuning on ImageNet1K dataset. The models significantly improve the training accuracy on ImageNet compared to training from scratch. They achieve state-of-the-art accuracy of 85.4% on ImageNet with the ResNext-101 32x48d model."}}
{"code": "{'Instruction': Recommend me an API to classify images into different categories quickly., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resneXt', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Use the pretrained ResNeXt101-32x4d model from NVIDIA PyTorch Hub to classify images into different categories efficiently.', 'code': 'import torch\nmodel = torch.hub.load(\"NVIDIA/DeepLearningExamples:torchhub\", \"nvidia_resneXt\", pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_resneXt', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNeXt101-32x4d", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_resneXt', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "validators", "matplotlib"], "example_code": ["import torch", "from PIL import Image", "import torchvision.transforms as transforms", "import numpy as np", "import json", "import requests", "import matplotlib.pyplot as plt", "import warnings", "warnings.filterwarnings('ignore')", "%matplotlib inline", "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")", "print(f'Using {device} for inference')", "resneXt = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resneXt')", "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_convnets_processing_utils')", "resneXt.eval().to(device)", "uris = [", " 'http://images.cocodataset.org/test-stuff2017/000000024309.jpg',", " 'http://images.cocodataset.org/test-stuff2017/000000028117.jpg',", " 'http://images.cocodataset.org/test-stuff2017/000000006149.jpg',", " 'http://images.cocodataset.org/test-stuff2017/000000004954.jpg',", "]", "batch = torch.cat(", " [utils.prepare_input_from_uri(uri) for uri in uris]", ").to(device)", "with torch.no_grad():", " output = torch.nn.functional.softmax(resneXt(batch), dim=1)", "results = utils.pick_n_best(predictions=output, n=5)", "for uri, result in zip(uris, results):", " img = Image.open(requests.get(uri, stream=True).raw)", " img.thumbnail((256,256), Image.ANTIALIAS)", " plt.imshow(img)", " plt.show()", " print(result)"], "performance": {"dataset": "IMAGENET", "accuracy": "Not provided"}, "description": "ResNeXt101-32x4d is a model introduced in the Aggregated Residual Transformations for Deep Neural Networks paper. It is based on regular ResNet model, substituting 3x3 convolutions inside the bottleneck block for 3x3 grouped convolutions. This model is trained with mixed precision using Tensor Cores on Volta, Turing, and the NVIDIA Ampere GPU architectures. It can be deployed for inference on the NVIDIA Triton Inference Server using TorchScript, ONNX Runtime or TensorRT as an execution backend."}}
{"code": "{'Instruction': Develop a solution to generate realistic high-quality faces. You can use a pre-trained GAN model as a starting point., 'Output': {'domain': 'Image Generation', 'api_call': \"model = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'PGAN', model_name='celebAHQ-512', pretrained=True, useGPU=use_gpu)\", 'api_provider': 'PyTorch', 'explanation': 'Load the Progressive Growing of GANs (PGAN) model with pre-trained weights on the CelebA dataset to generate high-quality celebrity faces.', 'code': 'import torch\nuse_gpu = True if torch.cuda.is_available() else False\nmodel = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'PGAN', model_name='celebAHQ-512', pretrained=True, useGPU=use_gpu)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorch_GAN_zoo:hub', model='PGAN', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Generative Adversarial Networks (GANs)", "api_name": "PGAN", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorch_GAN_zoo:hub', model='PGAN', pretrained=True)", "api_arguments": {"repo_or_dir": "facebookresearch/pytorch_GAN_zoo:hub", "model": "PGAN", "model_name": "celebAHQ-512", "pretrained": "True", "useGPU": "use_gpu"}, "python_environment_requirements": "Python 3", "example_code": {"import": "import torch", "use_gpu": "use_gpu = True if torch.cuda.is_available() else False", "load_model": "model = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'PGAN', model_name='celebAHQ-512', pretrained=True, useGPU=use_gpu)", "build_noise_data": "noise, _ = model.buildNoiseData(num_images)", "test": "generated_images = model.test(noise)", "plot_images": {"import_matplotlib": "import matplotlib.pyplot as plt", "import_torchvision": "import torchvision", "make_grid": "grid = torchvision.utils.make_grid(generated_images.clamp(min=-1, max=1), scale_each=True, normalize=True)", "imshow": "plt.imshow(grid.permute(1, 2, 0).cpu().numpy())", "show": "plt.show()"}}, "performance": {"dataset": "celebA", "accuracy": "High-quality celebrity faces"}, "description": "Progressive Growing of GANs (PGAN) is a method for generating high-resolution images using generative adversarial networks. The model is trained progressively, starting with low-resolution images and gradually increasing the resolution until the desired output is achieved. This implementation is based on the paper by Tero Karras et al., 'Progressive Growing of GANs for Improved Quality, Stability, and Variation'."}}
{"code": "{'Instruction': I have brain MRI scans and I want to segment the abnormal regions. Recommend an API that can perform this task., 'Output': {'domain': 'Semantic Segmentation', 'api_call': \"model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained U-Net model for brain MRI segmentation from PyTorch Hub to segment abnormal regions in brain MRI scans.', 'code': 'import torch\nmodel = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='mateuszbuda/brain-segmentation-pytorch', model='unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "Biomedical Image Segmentation", "api_name": "U-Net for brain MRI", "api_call": "torch.hub.load(repo_or_dir='mateuszbuda/brain-segmentation-pytorch', model='unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)", "api_arguments": {"in_channels": 3, "out_channels": 1, "init_features": 32, "pretrained": true}, "python_environment_requirements": ["torch", "torchvision", "numpy", "PIL"], "example_code": ["import torch", "model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)", "import numpy as np", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "m, s = np.mean(input_image, axis=(0, 1)), np.std(input_image, axis=(0, 1))", "preprocess = transforms.Compose([transforms.ToTensor(),transforms.Normalize(mean=m, std=s)])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model = model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(torch.round(output[0]))"], "performance": {"dataset": "kaggle.com/mateuszbuda/lgg-mri-segmentation"}, "description": "U-Net with batch normalization for biomedical image segmentation with pretrained weights for abnormality segmentation in brain MRI. The model comprises four levels of blocks containing two convolutional layers with batch normalization and ReLU activation function, and one max pooling layer in the encoding part and up-convolutional layers instead in the decoding part. The number of convolutional filters in each block is 32, 64, 128, and 256. The bottleneck layer has 512 convolutional filters. From the encoding layers, skip connections are used to the corresponding layers in the decoding part. Input image is a 3-channel brain MRI slice from pre-contrast, FLAIR, and post-contrast sequences, respectively. Output is a one-channel probability map of abnormality regions with the same size as the input image."}}
{"code": "{'Instruction': Detect if a given pair of sentences about artificial intelligence has any contradiction., 'Input': 'Roberta is a heavily optimized version of BERT. Roberta is not very optimized.', 'Output': 'contradiction'}", "api_call": "torch.hub.load(repo_or_dir='pytorch/fairseq', model='roberta.large', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "RoBERTa", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='pytorch/fairseq', model='roberta.large', pretrained=True)", "api_arguments": ["'pytorch/fairseq'", "'roberta.large'"], "python_environment_requirements": ["regex", "requests", "hydra-core", "omegaconf"], "example_code": ["import torch", "roberta = torch.hub.load('pytorch/fairseq', 'roberta.large')", "roberta.eval()", "tokens = roberta.encode('Hello world!')", "last_layer_features = roberta.extract_features(tokens)", "all_layers = roberta.extract_features(tokens, return_all_hiddens=True)", "roberta = torch.hub.load('pytorch/fairseq', 'roberta.large.mnli')", "roberta.eval()", "tokens = roberta.encode('Roberta is a heavily optimized version of BERT.', 'Roberta is not very optimized.')", "prediction = roberta.predict('mnli', tokens).argmax().item()", "tokens = roberta.encode('Roberta is a heavily optimized version of BERT.', 'Roberta is based on BERT.')", "prediction = roberta.predict('mnli', tokens).argmax().item()", "roberta.register_classification_head('new_task', num_classes=3)", "logprobs = roberta.predict('new_task', tokens)"], "performance": {"dataset": "MNLI", "accuracy": "N/A"}, "description": "RoBERTa is a robustly optimized version of BERT, a revolutionary self-supervised pretraining technique that learns to predict intentionally hidden (masked) sections of text. RoBERTa builds on BERT's language masking strategy and modifies key hyperparameters, including removing BERT's next-sentence pretraining objective, and training with much larger mini-batches and learning rates. RoBERTa was also trained on an order of magnitude more data than BERT, for a longer amount of time, allowing it to generalize even better to downstream tasks."}}
{"code": "{'Instruction': Recommend an API to classify car models from images, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'alexnet', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained AlexNet model from PyTorch Hub, which can be fine-tuned for classifying car models from images.', 'code': \"import torch\nmodel = torch.hub.load('pytorch/vision', 'alexnet', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='alexnet', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "AlexNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='alexnet', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": ">=1.9.0", "torchvision": ">=0.10.0"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"top-1_error": 43.45, "top-5_error": 20.91}}, "description": "AlexNet is a deep convolutional neural network that achieved a top-5 error of 15.3% in the 2012 ImageNet Large Scale Visual Recognition Challenge. The main contribution of the original paper was the depth of the model, which was computationally expensive but made feasible through the use of GPUs during training. The pretrained AlexNet model in PyTorch can be used for image classification tasks."}}
{"code": "{'Instruction': An automotive company is developing their own autonomous vehicle and they need a pretrained model to classify road signs. Propose an API that can be used for this task., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resnet50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNet50 v1.5 model from PyTorch Hub. This model can be fine-tuned to classify road signs for autonomous vehicles.', 'code': 'import torch\nmodel = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resnet50', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_resnet50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNet50", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_resnet50', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["pip install validators matplotlib"], "example_code": ["import torch", "from PIL import Image", "import torchvision.transforms as transforms", "import numpy as np", "import json", "import requests", "import matplotlib.pyplot as plt", "import warnings", "warnings.filterwarnings('ignore')", "%matplotlib inline", "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')", "print(f'Using {device} for inference')", "resnet50 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resnet50', pretrained=True)", "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_convnets_processing_utils')", "resnet50.eval().to(device)", "uris = [...]", "batch = torch.cat([utils.prepare_input_from_uri(uri) for uri in uris]).to(device)", "with torch.no_grad():", " output = torch.nn.functional.softmax(resnet50(batch), dim=1)", "results = utils.pick_n_best(predictions=output, n=5)", "for uri, result in zip(uris, results):", " img = Image.open(requests.get(uri, stream=True).raw)", " img.thumbnail((256,256), Image.ANTIALIAS)", " plt.imshow(img)", " plt.show()", " print(result)"], "performance": {"dataset": "ImageNet", "accuracy": "~0.5% top1 improvement over ResNet50 v1"}, "description": "The ResNet50 v1.5 model is a modified version of the original ResNet50 v1 model. The difference between v1 and v1.5 is that, in the bottleneck blocks which requires downsampling, v1 has stride = 2 in the first 1x1 convolution, whereas v1.5 has stride = 2 in the 3x3 convolution. This difference makes ResNet50 v1.5 slightly more accurate (~0.5% top1) than v1, but comes with a small performance drawback (~5% imgs/sec). The model is initialized as described in Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification. This model is trained with mixed precision using Tensor Cores on Volta, Turing, and the NVIDIA Ampere GPU architectures."}}
{"code": "{'Instruction': We are looking for an image classification model for our large datasets. Suggest an API that can classify images efficiently and accurately., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_se_resnext101_32x4d', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained SE-ResNeXt101-32x4d model from NVIDIA Deep Learning Examples PyTorch Hub. This model offers efficient and accurate image classification and is trained using mixed precision, providing faster results using Tensor Cores on Volta, Turing, and NVIDIA Ampere GPU architectures.', 'code': \"import torch\nmodel = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_se_resnext101_32x4d', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_se_resnext101_32x4d', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SE-ResNeXt101", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_se_resnext101_32x4d', pretrained=True)", "api_arguments": "N/A", "python_environment_requirements": "validators, matplotlib", "example_code": "import torch\nfrom PIL import Image\nimport torchvision.transforms as transforms\nimport numpy as np\nimport json\nimport requests\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nprint(f'Using {device} for inference')\nresneXt = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_se_resnext101_32x4d')\nutils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_convnets_processing_utils')\nresneXt.eval().to(device)\nuris = ['http://images.cocodataset.org/test-stuff2017/000000024309.jpg','http://images.cocodataset.org/test-stuff2017/000000028117.jpg','http://images.cocodataset.org/test-stuff2017/000000006149.jpg','http://images.cocodataset.org/test-stuff2017/000000004954.jpg']\nbatch = torch.cat([utils.prepare_input_from_uri(uri) for uri in uris]).to(device)\nwith torch.no_grad():\n output = torch.nn.functional.softmax(resneXt(batch), dim=1)\nresults = utils.pick_n_best(predictions=output, n=5)\nfor uri, result in zip(uris, results):\n img = Image.open(requests.get(uri, stream=True).raw)\n img.thumbnail((256,256), Image.ANTIALIAS)\n plt.imshow(img)\n plt.show()\n print(result)", "performance": {"dataset": "IMAGENET", "accuracy": "N/A"}, "description": "The SE-ResNeXt101-32x4d is a ResNeXt101-32x4d model with added Squeeze-and-Excitation module. This model is trained with mixed precision using Tensor Cores on Volta, Turing, and the NVIDIA Ampere GPU architectures, which allows researchers to get results 3x faster than training without Tensor Cores while experiencing the benefits of mixed precision training."}}
{"code": "{'Instruction': My app needs a realistic human speech from text, point me to an API that helps achieve this., 'Output': {'domain': 'Text-to-Speech', 'api_call': 'waveglow = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_waveglow', model_math='fp32')', 'api_provider': 'PyTorch', 'explanation': 'Load the WaveGlow model from PyTorch Hub, which, in combination with Tacotron 2, can synthesize natural-sounding speech from input text.', 'code': 'import torch\nwaveglow = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_waveglow', model_math='fp32')'}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_waveglow', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Text-to-Speech", "framework": "PyTorch", "functionality": "Speech Synthesis", "api_name": "WaveGlow", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_waveglow', pretrained=True)", "api_arguments": {"repo_or_dir": "NVIDIA/DeepLearningExamples:torchhub", "model": "nvidia_waveglow", "model_math": "fp32"}, "python_environment_requirements": ["numpy", "scipy", "librosa", "unidecode", "inflect", "libsndfile1"], "example_code": {"load_waveglow_model": "waveglow = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_waveglow', model_math='fp32')", "prepare_waveglow_model": ["waveglow = waveglow.remove_weightnorm(waveglow)", "waveglow = waveglow.to('cuda')", "waveglow.eval()"], "load_tacotron2_model": "tacotron2 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tacotron2', model_math='fp32')", "prepare_tacotron2_model": ["tacotron2 = tacotron2.to('cuda')", "tacotron2.eval()"], "synthesize_speech": ["text = \"hello world, I missed you so much\"", "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tts_utils')", "sequences, lengths = utils.prepare_input_sequence([text])", "with torch.no_grad():", " mel, _, _ = tacotron2.infer(sequences, lengths)", " audio = waveglow.infer(mel)", "audio_numpy = audio[0].data.cpu().numpy()", "rate = 22050"], "save_audio": "write(\"audio.wav\", rate, audio_numpy)", "play_audio": "Audio(audio_numpy, rate=rate)"}, "performance": {"dataset": "LJ Speech", "accuracy": null}, "description": "The Tacotron 2 and WaveGlow model form a text-to-speech system that enables users to synthesize natural-sounding speech from raw transcripts without any additional prosody information. The Tacotron 2 model produces mel spectrograms from input text using encoder-decoder architecture. WaveGlow is a flow-based model that consumes the mel spectrograms to generate speech."}}
{"code": "{'Instruction': I am creating a mobile app that identifies objects in images. Recommend an API for image classification., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'densenet121', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained DenseNet-121 model from PyTorch Hub for image classification with a dense convolutional network.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'densenet121', pretrained=True)\nmodel.eval()'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet121', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Dense Convolutional Network", "api_name": "Densenet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet121', pretrained=True)", "api_arguments": [{"name": "densenet121", "type": "str", "description": "Densenet-121 model"}], "python_environment_requirements": {"torch": "latest", "torchvision": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet121', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"densenet121": {"Top-1 error": 25.35, "Top-5 error": 7.83}}}, "description": "Dense Convolutional Network (DenseNet) connects each layer to every other layer in a feed-forward fashion. It alleviates the vanishing-gradient problem, strengthens feature propagation, encourages feature reuse, and substantially reduces the number of parameters."}}
{"code": "{'Instruction': I need an API for categorizing images of flowers into their respective classes. Propose an API that can help me., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'densenet201', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Densenet-201 model from PyTorch Hub, which can then be fine-tuned for flower classification tasks.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'densenet201', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet201', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Dense Convolutional Network", "api_name": "Densenet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet201', pretrained=True)", "api_arguments": [{"name": "densenet201", "type": "str", "description": "Densenet-201 model"}], "python_environment_requirements": {"torch": "latest", "torchvision": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet201', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"densenet201": {"Top-1 error": 22.8, "Top-5 error": 6.43}}}, "description": "Dense Convolutional Network (DenseNet) connects each layer to every other layer in a feed-forward fashion. It alleviates the vanishing-gradient problem, strengthens feature propagation, encourages feature reuse, and substantially reduces the number of parameters."}}
{"code": "{'Instruction': I want to classify images of different animals, give me an API that can help right now., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'densenet169', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained DenseNet-169 model from PyTorch Hub to classify images of different animals.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'densenet169', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet169', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Dense Convolutional Network", "api_name": "Densenet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet169', pretrained=True)", "api_arguments": [{"name": "densenet169", "type": "str", "description": "Densenet-169 model"}], "python_environment_requirements": {"torch": "latest", "torchvision": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet169', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"densenet169": {"Top-1 error": 24.0, "Top-5 error": 7.0}}}, "description": "Dense Convolutional Network (DenseNet) connects each layer to every other layer in a feed-forward fashion. It alleviates the vanishing-gradient problem, strengthens feature propagation, encourages feature reuse, and substantially reduces the number of parameters."}}
{"code": "{'Instruction': Suggest a deep learning API capable of categorizing images through an efficient neural network architecture., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'densenet161', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the DenseNet-161 model from PyTorch Hub, known for its efficient neural network architecture, suitable for image classification tasks.\", 'code': \"import torch\nmodel = torch.hub.load('pytorch/vision', 'densenet161', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet161', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Dense Convolutional Network", "api_name": "Densenet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet161', pretrained=True)", "api_arguments": [{"name": "densenet161", "type": "str", "description": "Densenet-161 model"}], "python_environment_requirements": {"torch": "latest", "torchvision": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet161', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"densenet161": {"Top-1 error": 22.35, "Top-5 error": 6.2}}}, "description": "Dense Convolutional Network (DenseNet) connects each layer to every other layer in a feed-forward fashion. It alleviates the vanishing-gradient problem, strengthens feature propagation, encourages feature reuse, and substantially reduces the number of parameters."}}
{"code": "{'Instruction': Suggest a suitable API to segment an image into semantically meaningful regions, such as people, cars, and buildings., 'Output': {'domain': 'Semantic Segmentation', 'api_call': \"model = torch.hub.load('pytorch/vision', 'fcn_resnet50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Use PyTorch's FCN-ResNet50 model for semantic segmentation, which can separate an image into semantically meaningful regions like people, cars, and buildings.\", 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'fcn_resnet50', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='fcn_resnet50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "Fully-Convolutional Network", "api_name": "fcn_resnet50", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='fcn_resnet50', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "boolean", "description": "If True, returns a model pre-trained on COCO train2017"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "matplotlib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'fcn_resnet50', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "input_image = input_image.convert(\"RGB\")", "preprocess = transforms.Compose([", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)['out'][0]", "output_predictions = output.argmax(0)"], "performance": {"dataset": "COCO val2017", "accuracy": {"fcn_resnet50": {"Mean IOU": 60.5, "Global Pixelwise Accuracy": 91.4}}}, "description": "FCN-ResNet is a Fully-Convolutional Network model using a ResNet-50 or a ResNet-101 backbone. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset."}}
{"code": "{'Instruction': For a city development project, I need to segment roads, buildings, and trees from satellite images. Propose an API that can help me with this task., 'Output': {'domain': 'Semantic Segmentation', 'api_call': \"model = torch.hub.load('pytorch/vision', 'fcn_resnet101', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained FCN-ResNet101 model from PyTorch Hub for semantic segmentation. This model can be fine-tuned to segment roads, buildings, and trees from satellite images.', 'code': 'import torch\nmodel = torch.hub.load(\"pytorch/vision\", \"fcn_resnet101\", pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='fcn_resnet101', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "Fully-Convolutional Network", "api_name": "fcn_resnet101", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='fcn_resnet101', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "boolean", "description": "If True, returns a model pre-trained on COCO train2017"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "matplotlib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'fcn_resnet101', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "input_image = input_image.convert(\"RGB\")", "preprocess = transforms.Compose([", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)['out'][0]", "output_predictions = output.argmax(0)"], "performance": {"dataset": "COCO val2017", "accuracy": {"fcn_resnet101": {"Mean IOU": 63.7, "Global Pixelwise Accuracy": 91.9}}}, "description": "FCN-ResNet is a Fully-Convolutional Network model using a ResNet-50 or a ResNet-101 backbone. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset."}}
{"code": "{'Instruction': AppRocket wants a recommendation for a lightweight API to classify images for their mobile app. What is lightweight API that can use for this purpose?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet39ds', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the lightweight HarDNet-39DS model from PyTorch Hub for image classification in your mobile app.', 'code': 'import torch\nmodel = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet39ds', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet39ds', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "HarDNet", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet39ds', pretrained=True)", "api_arguments": [{"name": "hardnet39ds", "type": "str", "description": "HarDNet-39DS model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet39ds', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"hardnet39ds": {"Top-1 error": 27.92, "Top-5 error": 9.57}}}, "description": "Harmonic DenseNet (HarDNet) is a low memory traffic CNN model, which is fast and efficient. The basic concept is to minimize both computational cost and memory access cost at the same time, such that the HarDNet models are 35% faster than ResNet running on GPU comparing to models with the same accuracy (except the two DS models that were designed for comparing with MobileNet)."}}
{"code": "{'Instruction': I am building an application that can recognize objects in images. Provide me with an API that can classify objects in an image., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet85', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained HarDNet-85 model from PyTorch Hub for image classification, which can recognize objects in images.', 'code': 'import torch\nmodel = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet85', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet85', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "HarDNet", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet85', pretrained=True)", "api_arguments": [{"name": "hardnet85", "type": "str", "description": "HarDNet-85 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet85', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"hardnet85": {"Top-1 error": 21.96, "Top-5 error": 6.11}}}, "description": "Harmonic DenseNet (HarDNet) is a low memory traffic CNN model, which is fast and efficient. The basic concept is to minimize both computational cost and memory access cost at the same time, such that the HarDNet models are 35% faster than ResNet running on GPU comparing to models with the same accuracy (except the two DS models that were designed for comparing with MobileNet)."}}
{"code": "{'Instruction': Find an API that can help a mobile gallery app categorize images based on their content while having a low memory footprint., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet68ds', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained HarDNet model, which is designed for low memory and computational footprint, suitable for mobile gallery app image categorization.', 'code': 'import torch\nmodel = torch.hub.load(\"PingoLH/Pytorch-HarDNet\", \"hardnet68ds\", pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet68ds', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "HarDNet", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet68ds', pretrained=True)", "api_arguments": [{"name": "hardnet68ds", "type": "str", "description": "HarDNet-68DS model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet68ds', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"hardnet68ds": {"Top-1 error": 25.71, "Top-5 error": 8.13}}}, "description": "Harmonic DenseNet (HarDNet) is a low memory traffic CNN model, which is fast and efficient. The basic concept is to minimize both computational cost and memory access cost at the same time, such that the HarDNet models are 35% faster than ResNet running on GPU comparing to models with the same accuracy (except the two DS models that were designed for comparing with MobileNet)."}}
{"code": "{'Instruction': Obtain an API that can classify images using a lightweight neural network architecture., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('huawei-noah/ghostnet', 'ghostnet_1x', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained GhostNet model from PyTorch Hub to classify images. GhostNet is efficient and lightweight, designed based on ghost modules that generate more features from cheap operations.\", 'code': \"import torch\nmodel = torch.hub.load('huawei-noah/ghostnet', 'ghostnet_1x', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/ghostnet', model='ghostnet_1x', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Efficient networks by generating more features from cheap operations", "api_name": "GhostNet", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/ghostnet', model='ghostnet_1x', pretrained=True)", "api_arguments": ["pretrained"], "python_environment_requirements": ["torch", "torchvision", "PIL"], "example_code": ["import torch", "model = torch.hub.load('huawei-noah/ghostnet', 'ghostnet_1x', pretrained=True)", "model.eval()", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)", "print(probabilities)"], "performance": {"dataset": "ImageNet", "accuracy": {"Top-1 acc": "73.98", "Top-5 acc": "91.46"}}, "description": "The GhostNet architecture is based on an Ghost module structure which generates more features from cheap operations. Based on a set of intrinsic feature maps, a series of cheap operations are applied to generate many ghost feature maps that could fully reveal information underlying intrinsic features. Experiments conducted on benchmarks demonstrate the superiority of GhostNet in terms of speed and accuracy tradeoff."}}
{"code": "{'Instruction': What API could be used to identify plants in a garden from taken image?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'googlenet', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained GoogLeNet model from PyTorch Hub, which can be fine-tuned to identify plants in a garden using images.\", 'code': \"import torch\nmodel = torch.hub.load('pytorch/vision', 'googlenet', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='googlenet', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "GoogLeNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='googlenet', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": ">=1.0.0", "torchvision": ">=0.2.2"}, "example_code": {"import": ["import torch", "import urllib", "from PIL import Image", "from torchvision import transforms"], "load_model": "model = torch.hub.load('pytorch/vision:v0.10.0', 'googlenet', pretrained=True)", "model_eval": "model.eval()", "image_preprocessing": ["input_image = Image.open(filename)", "preprocess = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)"], "model_execution": ["if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)"], "output_processing": ["probabilities = torch.nn.functional.softmax(output[0], dim=0)", "top5_prob, top5_catid = torch.topk(probabilities, 5)"]}, "performance": {"dataset": "ImageNet", "accuracy": {"Top-1 error": "30.22", "Top-5 error": "10.47"}}, "description": "GoogLeNet is based on a deep convolutional neural network architecture codenamed 'Inception', which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014)."}}
{"code": "{'Instruction': I am planning to build a fashion blog which sorts numerous fashion trends. Find a proper API that can classify fashion styles from an image., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('XingangPan/IBN-Net', 'resnet50_ibn_a', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained IBN-Net model from PyTorch Hub to classify fashion styles from images. The model can be fine-tuned to recognize specific fashion trends.', 'code': 'import torch\nmodel = torch.hub.load('XingangPan/IBN-Net', 'resnet50_ibn_a', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnet50_ibn_a', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "IBN-Net", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnet50_ibn_a', pretrained=True)", "api_arguments": [{"name": "resnet50_ibn_a", "type": "str", "description": "ResNet-50-IBN-a model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('XingangPan/IBN-Net', 'resnet50_ibn_a', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"resnet50_ibn_a": {"Top-1 acc": 77.46, "Top-5 acc": 93.68}}}, "description": "IBN-Net is a CNN model with domain/appearance invariance. Motivated by style transfer works, IBN-Net carefully unifies instance normalization and batch normalization in a single deep network. It provides a simple way to increase both modeling and generalization capacities without adding model complexity. IBN-Net is especially suitable for cross domain or person/vehicle re-identification tasks."}}
{"code": "{'Instruction': I am working on a cross-domain image classification project, and I need a pre-trained model that has strong domain invariance. Can you recommend an API for this?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('XingangPan/IBN-Net', 'resnet101_ibn_a', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNet-IBN model from PyTorch Hub, which is specifically designed for domain invariance in image classification tasks.', 'code': 'import torch\nmodel = torch.hub.load('XingangPan/IBN-Net', 'resnet101_ibn_a', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnet101_ibn_a', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "IBN-Net", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnet101_ibn_a', pretrained=True)", "api_arguments": [{"name": "resnet101_ibn_a", "type": "str", "description": "ResNet-101-IBN-a model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('XingangPan/IBN-Net', 'resnet101_ibn_a', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"resnet101_ibn_a": {"Top-1 acc": 78.61, "Top-5 acc": 94.41}}}, "description": "IBN-Net is a CNN model with domain/appearance invariance. Motivated by style transfer works, IBN-Net carefully unifies instance normalization and batch normalization in a single deep network. It provides a simple way to increase both modeling and generalization capacities without adding model complexity. IBN-Net is especially suitable for cross domain or person/vehicle re-identification tasks."}}
{"code": "{'Instruction': We are a project lead for a computer vision company and we are building a custom person re-identification system. Share with me an API that can be beneficial for cross-domain identification task., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('XingangPan/IBN-Net', 'se_resnet101_ibn_a', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pre-trained IBN-Net model from PyTorch Hub that unifies instance normalization and batch normalization for cross-domain identification tasks specifically for person re-identification.', 'code': 'import torch\nmodel = torch.hub.load('XingangPan/IBN-Net', 'se_resnet101_ibn_a', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='se_resnet101_ibn_a', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "IBN-Net", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='se_resnet101_ibn_a', pretrained=True)", "api_arguments": [{"name": "se_resnet101_ibn_a", "type": "str", "description": "SE-ResNet-101-IBN-a model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('XingangPan/IBN-Net', 'se_resnet101_ibn_a', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"se_resnet101_ibn_a": {"Top-1 acc": 78.75, "Top-5 acc": 94.49}}}, "description": "IBN-Net is a CNN model with domain/appearance invariance. Motivated by style transfer works, IBN-Net carefully unifies instance normalization and batch normalization in a single deep network. It provides a simple way to increase both modeling and generalization capacities without adding model complexity. IBN-Net is especially suitable for cross domain or person/vehicle re-identification tasks."}}
{"code": "{'Instruction': I am looking for an API that can classify animals in images. Could you recommend a suitable API?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNeSt-50 model from PyTorch Hub, which can be fine-tuned for animal classification in images.', 'code': 'import torch\nmodel = torch.hub.load('zhanghang1989/ResNeSt', 'resnest50', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNeSt", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest50', pretrained=True)", "api_arguments": "resnest50", "python_environment_requirements": {"torch": "1.0.0", "torchvision": "0.2.2"}, "example_code": ["import torch", "model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest50', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeSt-50": 81.03}}, "description": "ResNeSt models are from the ResNeSt: Split-Attention Networks paper. They are a new ResNet variant that enables attention across feature-map groups. By stacking Split-Attention blocks ResNet-style, ResNeSt models outperform other networks with similar model complexities, and also help downstream tasks including object detection, instance segmentation, and semantic segmentation."}}
{"code": "{'Instruction': I need to accurately identify cross domain objects. Give me an API that can handle object recognition tasks with improved performance on cross domain tasks., 'Output': {'domain': 'Object Recognition', 'api_call': 'model = torch.hub.load('XingangPan/IBN-Net', 'resnext101_ibn_a', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained IBN-Net model (ResNeXt-101-IBN-a) from PyTorch Hub, which is especially suitable for cross-domain object recognition tasks as it combines instance normalization and batch normalization.', 'code': 'import torch\nmodel = torch.hub.load('XingangPan/IBN-Net', 'resnext101_ibn_a', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnext101_ibn_a', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "IBN-Net", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnext101_ibn_a', pretrained=True)", "api_arguments": [{"name": "resnext101_ibn_a", "type": "str", "description": "ResNeXt-101-IBN-a model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('XingangPan/IBN-Net', 'resnext101_ibn_a', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"resnext101_ibn_a": {"Top-1 acc": 79.12, "Top-5 acc": 94.58}}}, "description": "IBN-Net is a CNN model with domain/appearance invariance. Motivated by style transfer works, IBN-Net carefully unifies instance normalization and batch normalization in a single deep network. It provides a simple way to increase both modeling and generalization capacities without adding model complexity. IBN-Net is especially suitable for cross domain or person/vehicle re-identification tasks."}}
{"code": "{'Instruction': I am maintaining a gallery and I am looking for a library that can classify images to different categories. Can you suggest an API?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'inception_v3', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Use the Inception v3 model available in the PyTorch Vision library for image classification into different categories.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'inception_v3', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='inception_v3', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "Inception_v3", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='inception_v3', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": "1.9.0", "torchvision": "0.10.0"}, "example_code": {"import_libraries": "import torch", "load_model": "model = torch.hub.load('pytorch/vision:v0.10.0', 'inception_v3', pretrained=True)", "model_evaluation": "model.eval()"}, "performance": {"dataset": "imagenet", "accuracy": {"top-1_error": 22.55, "top-5_error": 6.44}}, "description": "Inception v3, also called GoogleNetv3, is a famous Convolutional Neural Network trained on the ImageNet dataset from 2015. It is based on the exploration of ways to scale up networks to utilize the added computation as efficiently as possible by using suitably factorized convolutions and aggressive regularization. The model achieves a top-1 error of 22.55% and a top-5 error of 6.44% on the ImageNet dataset."}}
{"code": "{'Instruction': Find an API that can efficiently classify dog breeds from the given photographs., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest101', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNeSt101 model from PyTorch Hub, which can be fine-tuned for dog breed classification based on input images.', 'code': 'import torch\nmodel = torch.hub.load('zhanghang1989/ResNeSt', 'resnest101', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest101', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNeSt", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest101', pretrained=True)", "api_arguments": "resnest101", "python_environment_requirements": {"torch": "1.0.0", "torchvision": "0.2.2"}, "example_code": ["import torch", "model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest101', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeSt-101": 82.83}}, "description": "ResNeSt models are from the ResNeSt: Split-Attention Networks paper. They are a new ResNet variant that enables attention across feature-map groups. By stacking Split-Attention blocks ResNet-style, ResNeSt models outperform other networks with similar model complexities, and also help downstream tasks including object detection, instance segmentation, and semantic segmentation."}}
{"code": "{'Instruction': I want to classify images for my large-scale imaging project. Recommend an API to use for high accuracy image classification., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest269', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNeSt-269 model from PyTorch Hub for high accuracy image classification.', 'code': 'import torch\nmodel = torch.hub.load('zhanghang1989/ResNeSt', 'resnest269', pretrained=True)\nmodel.eval()'}}", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest269', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNeSt", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest269', pretrained=True)", "api_arguments": "resnest269", "python_environment_requirements": {"torch": "1.0.0", "torchvision": "0.2.2"}, "example_code": ["import torch", "model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest269', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeSt-269": 84.54}}, "description": "ResNeSt models are from the ResNeSt: Split-Attention Networks paper. They are a new ResNet variant that enables attention across feature-map groups. By stacking Split-Attention blocks ResNet-style, ResNeSt models outperform other networks with similar model complexities, and also help downstream tasks including object detection, instance segmentation, and semantic segmentation."}}
{"code": "{'Instruction': Recommend an API that can optimize neural network architecture for efficient object classification on mobile devices., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_cpu', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pre-trained ProxylessNAS model from PyTorch Hub, which is optimized for efficient object classification on mobile devices.', 'code': 'import torch\nmodel = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_cpu', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_cpu', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "ProxylessNAS", "api_name": "mit-han-lab/ProxylessNAS", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_cpu', pretrained=True)", "api_arguments": [{"name": "proxylessnas_cpu", "type": "str", "description": "ProxylessNAS optimized for CPU"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "target_platform = 'proxyless_cpu'", "model = torch.hub.load('mit-han-lab/ProxylessNAS', target_platform, pretrained=True)", "model.eval()"], "performance": {"dataset": [{"model_structure": "proxylessnas_cpu", "accuracy": 75.3}]}, "description": "ProxylessNAS models are from the ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware paper. They specialize CNN architectures for different hardware platforms, offering free yet significant performance boost on all three platforms (CPU, GPU, and Mobile) with similar accuracy."}}
{"code": "{'Instruction': Provide me with an API capable of classifying images from a news website into Sports, Entertainment, World News, and Technology categories., 'Output': {'domain': 'Image Classification', 'api_call': 'model = torch.hub.load('pytorch/vision', 'mobilenet_v2', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MobileNet v2 model for image classification from PyTorch Hub, and fine-tune it on a dataset with news categories such as Sports, Entertainment, World News, and Technology.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'mobilenet_v2', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='mobilenet_v2', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MobileNet v2", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='mobilenet_v2', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision', 'mobilenet_v2', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)"], "performance": {"dataset": "ImageNet", "accuracy": {"top-1_error": 28.12, "top-5_error": 9.71}}, "description": "The MobileNet v2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input. MobileNet v2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, non-linearities in the narrow layers were removed in order to maintain representational power."}}
{"code": "{'Instruction': We need a neural network for image classification and It should be specifically designed for different hardware platforms. Tell me an API that can help with this requirement., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_gpu', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ProxylessNAS model from PyTorch Hub, which specializes CNN architectures for different hardware platforms (CPU, GPU, Mobile) and provides a high performing image classification.', 'code': 'import torch\nmodel = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_gpu', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_gpu', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "ProxylessNAS", "api_name": "mit-han-lab/ProxylessNAS", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_gpu', pretrained=True)", "api_arguments": [{"name": "proxylessnas_gpu", "type": "str", "description": "ProxylessNAS optimized for GPU"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "target_platform = 'proxyless_gpu'", "model = torch.hub.load('mit-han-lab/ProxylessNAS', target_platform, pretrained=True)", "model.eval()"], "performance": {"dataset": [{"model_structure": "proxylessnas_gpu", "accuracy": 75.1}]}, "description": "ProxylessNAS models are from the ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware paper. They specialize CNN architectures for different hardware platforms, offering free yet significant performance boost on all three platforms (CPU, GPU, and Mobile) with similar accuracy."}}
{"code": "{'Instruction': I need to identify plant species from leaf images using a pretrained model. Recommend an appropriate API for this., 'Output': {'domain': 'Classification', 'api_call': 'model = torch.hub.load('pytorch/vision', 'resnet18', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Use the pretrained ResNet18 model from PyTorch Hub, which can be fine-tuned for plant species classification based on leaf images.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'resnet18', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet18', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Deep Residual Networks", "api_name": "ResNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet18', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "bool", "default": "False", "description": "If True, returns a model pre-trained on ImageNet"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "resnet18", "top-1_error": 30.24, "top-5_error": 10.92}}, "description": "ResNet models are deep residual networks pre-trained on ImageNet. They were proposed in the paper 'Deep Residual Learning for Image Recognition'. Available model variants include ResNet18, ResNet34, ResNet50, ResNet101, and ResNet152."}}
{"code": "{'Instruction': Identify an API that can automatically detect objects in an image and return the object labels, 'Output': {'domain': 'Object Detection', 'api_call': \"model = torch.hub.load('pytorch/vision', 'resnet34', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNet34 model from PyTorch Hub, which can be fine-tuned for object detection and return object labels given an input image.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'resnet34', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet34', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Deep Residual Networks", "api_name": "ResNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet34', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "bool", "default": "False", "description": "If True, returns a model pre-trained on ImageNet"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet34', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "resnet34", "top-1_error": 26.7, "top-5_error": 8.58}}, "description": "ResNet models are deep residual networks pre-trained on ImageNet. They were proposed in the paper 'Deep Residual Learning for Image Recognition'. Available model variants include ResNet18, ResNet34, ResNet50, ResNet101, and ResNet152."}}
{"code": "{'Instruction': A programmer needs to classify images into 1000 categories like cats, dogs, planes, etc. Offer them an API that can perform this task., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'resnet50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pre-trained ResNet50 model from PyTorch Hub, which is capable of classifying images into 1000 categories such as cats, dogs, planes, etc.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'resnet50', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Deep Residual Networks", "api_name": "ResNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet50', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "bool", "default": "False", "description": "If True, returns a model pre-trained on ImageNet"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "resnet50", "top-1_error": 23.85, "top-5_error": 7.13}}, "description": "ResNet models are deep residual networks pre-trained on ImageNet. They were proposed in the paper 'Deep Residual Learning for Image Recognition'. Available model variants include ResNet18, ResNet34, ResNet50, ResNet101, and ResNet152."}}
{"code": "{'Instruction': A startup needs to classify images using state of the art research. Suggest an image classification API to do this., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'resnet101', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNet101 model from PyTorch Hub to classify images using state-of-the-art deep residual networks.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'resnet101', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet101', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Deep Residual Networks", "api_name": "ResNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet101', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "bool", "default": "False", "description": "If True, returns a model pre-trained on ImageNet"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet101', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "resnet101", "top-1_error": 22.63, "top-5_error": 6.44}}, "description": "ResNet models are deep residual networks pre-trained on ImageNet. They were proposed in the paper 'Deep Residual Learning for Image Recognition'. Available model variants include ResNet18, ResNet34, ResNet50, ResNet101, and ResNet152."}}
{"code": "{'Instruction': A scientist wants to identify bird species by uploading photos. Propose an API that can classify bird species from an input image. , 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'resnet152', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained ResNet152 model from PyTorch Hub, which can be fine-tuned for bird species classification using input images.\", 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'resnet152', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet152', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Deep Residual Networks", "api_name": "ResNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet152', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "bool", "default": "False", "description": "If True, returns a model pre-trained on ImageNet"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet152', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "resnet152", "top-1_error": 21.69, "top-5_error": 5.94}}, "description": "ResNet models are deep residual networks pre-trained on ImageNet. They were proposed in the paper 'Deep Residual Learning for Image Recognition'. Available model variants include ResNet18, ResNet34, ResNet50, ResNet101, and ResNet152."}}
{"code": "{'Instruction': Suggest an API to develop a tool to classify plants based on their images., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'resnext50_32x4d', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNext model from PyTorch Hub, which can be fine-tuned for plant classification based on images.', 'code': \"import torch\nmodel = torch.hub.load('pytorch/vision', 'resnext50_32x4d', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnext50_32x4d', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNext", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnext50_32x4d', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "PIL"], "example_code": ["import torch", "from PIL import Image", "from torchvision import transforms", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnext50_32x4d', pretrained=True)", "model.eval()", "input_image = Image.open('dog.jpg')", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)"], "performance": {"dataset": "ImageNet", "accuracy": {"resnext50_32x4d": {"top-1": 22.38, "top-5": 6.3}}}, "description": "ResNext is a next-generation ResNet architecture for image classification. It is more efficient and accurate than the original ResNet. This implementation includes two versions of the model, resnext50_32x4d and resnext101_32x8d, with 50 and 101 layers respectively."}}
{"code": "{'Instruction': You want an API to classify an image in order to organize your photos. Recommend an API that can be used for this purpose., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'resnext101_32x4d', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNext101 model from PyTorch Hub for image classification, which can be used to categorize and organize your photos.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'resnext101_32x4d', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnext101_32x4d', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNext", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnext101_32x4d', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "PIL"], "example_code": ["import torch", "from PIL import Image", "from torchvision import transforms", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnext101_32x4d', pretrained=True)", "model.eval()", "input_image = Image.open('dog.jpg')", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)"], "performance": {"dataset": "ImageNet", "resnext101_32x8d": {"top-1": 20.69, "top-5": 5.47}}, "description": "ResNext is a next-generation ResNet architecture for image classification. It is more efficient and accurate than the original ResNet. This implementation includes two versions of the model, resnext50_32x4d and resnext101_32x8d, with 50 and 101 layers respectively."}}
{"code": "{'Instruction': Introduce an image classifier API that can predict the class of a given image for an app that tags photos according to their content., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_t', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained SNNMLP model for image classification from PyTorch Hub to predict the class of a given image.', 'code': \"import torch\nmodel = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_t', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_t', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SNNMLP", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_t', pretrained=True)", "api_arguments": [{"name": "snnmlp_t", "type": "str", "description": "SNNMLP Tiny model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_t', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "SNNMLP Tiny", "top-1": 81.88}}, "description": "SNNMLP incorporates the mechanism of LIF neurons into the MLP models, to achieve better accuracy without extra FLOPs. We propose a full-precision LIF operation to communicate between patches, including horizontal LIF and vertical LIF in different directions. We also propose to use group LIF to extract better local features. With LIF modules, our SNNMLP model achieves 81.9%, 83.3% and 83.6% top-1 accuracy on ImageNet dataset with only 4.4G, 8.5G and 15.2G FLOPs, respectively."}}
{"code": "{'Instruction': Find me an AI framework that can classify various animals' images when given a picture file with a high accuracy., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_b', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained SNNMLP from the PyTorch Hub, which is a high-accuracy model for image classification, and can be used for classifying various animal images.', 'code': 'import torch\nmodel = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_b', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_b', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SNNMLP", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_b', pretrained=True)", "api_arguments": [{"name": "snnmlp_b", "type": "str", "description": "SNNMLP Base model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_b', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "SNNMLP Base", "top-1": 85.59}}, "description": "SNNMLP incorporates the mechanism of LIF neurons into the MLP models, to achieve better accuracy without extra FLOPs. We propose a full-precision LIF operation to communicate between patches, including horizontal LIF and vertical LIF in different directions. We also propose to use group LIF to extract better local features. With LIF modules, our SNNMLP model achieves 81.9%, 83.3% and 83.6% top-1 accuracy on ImageNet dataset with only 4.4G, 8.5G and 15.2G FLOPs, respectively."}}
{"code": "{'Instruction': An app developer is searching for an AI solution to classify pets in photos. Recommend an API that can fulfill this need., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_s', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained SNNMLP model from PyTorch Hub. The SNNMLP model incorporates LIF neurons into an MLP-based architecture, with good accuracy and low computational requirements. This can be fine-tuned for pet image classification.', 'code': \"import torch\nmodel = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_s', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_s', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SNNMLP", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_s', pretrained=True)", "api_arguments": [{"name": "snnmlp_s", "type": "str", "description": "SNNMLP Small model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_s', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "SNNMLP Small", "top-1": 83.3}}, "description": "SNNMLP incorporates the mechanism of LIF neurons into the MLP models, to achieve better accuracy without extra FLOPs. We propose a full-precision LIF operation to communicate between patches, including horizontal LIF and vertical LIF in different directions. We also propose to use group LIF to extract better local features. With LIF modules, our SNNMLP model achieves 81.9%, 83.3% and 83.6% top-1 accuracy on ImageNet dataset with only 4.4G, 8.5G and 15.2G FLOPs, respectively."}}
{"code": "{'Instruction': Make a recommendation for a pretrained API for an image classification task., 'Output': {'domain': 'Image Classification', 'api_call': 'torch.hub.load('pytorch/vision', 'vgg11', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Pretrained VGG11 model from PyTorch Hub can be used for image classification tasks.', 'code': \"import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg11', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg11', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg11', pretrained=True)", "api_arguments": [{"name": "vgg11", "type": "str", "description": "VGG11 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg11', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg11": {"Top-1 error": 30.98, "Top-5 error": 11.37}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': I need a fast and lightweight image classification model for a mobile app that can recognize objects in images. What API do you suggest?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'squeezenet1_1', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained SqueezeNet model from PyTorch Hub for lightweight and fast image classification.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'squeezenet1_1', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='squeezenet1_1', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SqueezeNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='squeezenet1_1', pretrained=True)", "api_arguments": {"version": "v0.10.0", "model": ["squeezenet1_1"], "pretrained": "True"}, "python_environment_requirements": {"torch": ">=1.9.0", "torchvision": ">=0.10.0"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'squeezenet1_1', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)", "print(probabilities)"], "performance": {"dataset": "ImageNet", "accuracy": {"squeezenet1_1": {"Top-1 error": 41.81, "Top-5 error": 19.38}}}, "description": "SqueezeNet is an image classification model that achieves AlexNet-level accuracy with 50x fewer parameters. It has two versions: squeezenet1_0 and squeezenet1_1, with squeezenet1_1 having 2.4x less computation and slightly fewer parameters than squeezenet1_0, without sacrificing accuracy."}}
{"code": "{'Instruction': Create a model that can categorize images into categories given its pretrained models., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'squeezenet1_0', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained SqueezeNet model from PyTorch Hub for image classification. It achieves AlexNet-level accuracy with fewer parameters and can be fine-tuned for custom image categorization tasks.', 'code': 'import torch\nmodel = torch.hub.load(\"pytorch/vision\", \"squeezenet1_0\", pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='squeezenet1_0', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SqueezeNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='squeezenet1_0', pretrained=True)", "api_arguments": {"version": "v0.10.0", "model": ["squeezenet1_0"], "pretrained": "True"}, "python_environment_requirements": {"torch": ">=1.9.0", "torchvision": ">=0.10.0"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'squeezenet1_0', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)", "print(probabilities)"], "performance": {"dataset": "ImageNet", "accuracy": {"squeezenet1_0": {"Top-1 error": 41.9, "Top-5 error": 19.58}}}, "description": "SqueezeNet is an image classification model that achieves AlexNet-level accuracy with 50x fewer parameters. It has two versions: squeezenet1_0 and squeezenet1_1, with squeezenet1_1 having 2.4x less computation and slightly fewer parameters than squeezenet1_0, without sacrificing accuracy."}}
{"code": "{'Instruction': CCTV cameras need to classify opened doors from closed ones. Suggest an image recognition API for this purpose., 'Output': {'domain': 'Image Recognition', 'api_call': 'model = torch.hub.load('pytorch/vision', 'vgg13', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load a pre-trained VGG13 model from PyTorch Hub for image recognition to classify opened and closed doors.', 'code': 'import torch\nmodel = torch.h", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg13', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg13', pretrained=True)", "api_arguments": [{"name": "vgg13", "type": "str", "description": "VGG13 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg13', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg13": {"Top-1 error": 30.07, "Top-5 error": 10.75}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn,vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': What is an API that can be used to classify images of different categories using a computationally efficient model?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'shufflenet_v2_x1_0', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load ShuffleNet v2, a pretrained, computationally efficient image classification model from PyTorch Hub that can classify images into different categories.\", 'code': \"import torch\nmodel = torch.hub.load('pytorch/vision', 'shufflenet_v2_x1_0', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='shufflenet_v2_x1_0', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ShuffleNet v2", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='shufflenet_v2_x1_0', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": "torch", "torchvision": "torchvision", "PIL": "Image", "urllib": "urllib"}, "example_code": {"import_libraries": ["import torch", "from PIL import Image", "from torchvision import transforms", "import urllib"], "load_model": ["model = torch.hub.load('pytorch/vision:v0.10.0', 'shufflenet_v2_x1_0', pretrained=True)", "model.eval()"], "load_image": ["url, filename = ('https://github.com/pytorch/hub/raw/master/images/dog.jpg', 'dog.jpg')", "try: urllib.URLopener().retrieve(url, filename)", "except: urllib.request.urlretrieve(url, filename)", "input_image = Image.open(filename)"], "preprocess_image": ["preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)"], "run_inference": ["if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)"], "get_probabilities": ["probabilities = torch.nn.functional.softmax(output[0], dim=0)"], "top_categories": ["top5_prob, top5_catid = torch.topk(probabilities, 5)", "for i in range(top5_prob.size(0)):", " print(categories[top5_catid[i]], top5_prob[i].item())"]}, "performance": {"dataset": "Imagenet", "accuracy": {"top-1_error": 30.64, "top-5_error": 11.68}}, "description": "ShuffleNet V2 is an efficient ConvNet optimized for speed and memory, pre-trained on Imagenet. It is designed based on practical guidelines for efficient network design, including speed and accuracy tradeoff."}}
{"code": "{'Instruction': A student from MIT is working on a project that uses image recognition to identify the type of animal in an image. Give me an API that they can use to achieve this., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg11_bn', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained VGG11 model with batch normalization from PyTorch Hub, which can be used for animal image recognition.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg11_bn', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg11_bn', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg11_bn', pretrained=True)", "api_arguments": [{"name": "vgg11_bn", "type": "str", "description": "VGG11 model with batch normalization"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg11_bn', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg11_bn": {"Top-1 error": 26.7, "Top-5 error": 8.58}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': I want to get an API to recognize different types of animals in photographs, like lions, tigers, and bears., 'Output': {'domain': 'Image Recognition', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg13_bn', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained VGG13 model with batch normalization from PyTorch Hub, which can be fine-tuned for animal recognition tasks.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg13_bn', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg13_bn', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg13_bn', pretrained=True)", "api_arguments": [{"name": "vgg13_bn", "type": "str", "description": "VGG13 model with batch normalization"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg13_bn', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg13_bn": {"Top-1 error": 28.45, "Top-5 error": 9.63}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': Identify an API that allows for image recognition with a high accuracy rate., 'Output': {'domain': 'Image Recognition', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg16', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained VGG16 model from PyTorch Hub for image recognition with high accuracy.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg16', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg16', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg16', pretrained=True)", "api_arguments": [{"name": "vgg16", "type": "str", "description": "VGG16 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg16": {"Top-1 error": 28.41, "Top-5 error": 9.62}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': Recommend an API that can be employed to identify the breed of a dog from a given image, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg16_bn', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained VGG16 model with batch normalization from PyTorch Hub. You can fine-tune this model for dog breed identification.', 'code': \"import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg16_bn', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg16_bn', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg16_bn', pretrained=True)", "api_arguments": [{"name": "vgg16_bn", "type": "str", "description": "VGG16 model with batch normalization"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16_bn', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg16_bn": {"Top-1 error": 26.63, "Top-5 error": 8.5}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': Recommend an image recognition API that can identify the objects within a picture., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg19', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the VGG19 model from PyTorch Hub with pre-trained weights for image classification tasks. It can recognize objects within images.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg19', pretrained=True)\nmodel.eval()' }}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg19', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg19', pretrained=True)", "api_arguments": [{"name": "vgg19", "type": "str", "description": "VGG19 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg19', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg19": {"Top-1 error": 27.62, "Top-5 error": 9.12}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': I have a photo and I want to know the object type in it. Can you provide an API that can recognize the object?, 'Output': {'domain': 'Image Recognition', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg19_bn', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained VGG19 model with batch normalization from PyTorch Hub to recognize objects in a given photo.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg19_bn', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg19_bn', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg19_bn', pretrained=True)", "api_arguments": [{"name": "vgg19_bn", "type": "str", "description": "VGG19 model with batch normalization"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg19_bn', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg19_bn": {"Top-1 error": 25.76, "Top-5 error": 8.15}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': Help me choose an adequate API to classify satellite images into land use categories., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'wide_resnet101_2', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Wide ResNet-101-2 model from PyTorch Hub for satellite image classification into land use categories. The model can be fine-tuned for your specific task.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'wide_resnet101_2', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='wide_resnet101_2', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Wide Residual Networks", "api_name": "wide_resnet101_2", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='wide_resnet101_2', pretrained=True)", "api_arguments": "pretrained", "python_environment_requirements": "torch, torchvision", "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'wide_resnet101_2', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"wide_resnet101_2": {"Top-1 error": 21.16, "Top-5 error": 5.72}}}, "description": "Wide Residual networks simply have increased number of channels compared to ResNet. Otherwise the architecture is the same. Deeper ImageNet models with bottleneck block have increased number of channels in the inner 3x3 convolution. The wide_resnet50_2 and wide_resnet101_2 models were trained in FP16 with mixed precision training using SGD with warm restarts. Checkpoints have weights in half precision (except batch norm) for smaller size, and can be used in FP32 models too."}}
{"code": "{'Instruction': A start-up is building a dog breed detector and they need an API that can classify images into categories. Suggest an ideal API that can help them achieve this., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'wide_resnet50_2', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained Wide ResNet 50_2 model from PyTorch Hub, which can be fine-tuned for dog breed classification from images.\", 'code': \"import torch\nmodel = torch.hub.load('pytorch/vision', 'wide_resnet50_2', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='wide_resnet50_2', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Wide Residual Networks", "api_name": "wide_resnet50_2", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='wide_resnet50_2', pretrained=True)", "api_arguments": "pretrained", "python_environment_requirements": "torch, torchvision", "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'wide_resnet50_2', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"wide_resnet50_2": {"Top-1 error": 21.49, "Top-5 error": 5.91}}}, "description": "Wide Residual networks simply have increased number of channels compared to ResNet. Otherwise the architecture is the same. Deeper ImageNet models with bottleneck block have increased number of channels in the inner 3x3 convolution. The wide_resnet50_2 and wide_resnet101_2 models were trained in FP16 with mixed precision training using SGD with warm restarts. Checkpoints have weights in half precision (except batch norm) for smaller size, and can be used in FP32 models too."}}
{"code": "{'Instruction': I am developing a video indexing application and I want to classify actions in video clips. What API should I use?, 'Output': {'domain': 'Video Classification', 'api_call': \"model = torch.hub.load('facebookresearch/pytorchvideo', 'x3d_s', pretrained=True)\", 'api_provider': 'PyTorchVideo', 'explanation': 'Load the pretrained X3D model from PyTorchVideo library for classifying actions in video clips based on the Kinetics 400 dataset.', 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/pytorchvideo', 'x3d_s', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='x3d_s', pretrained=True)", "provider": "PyTorchVideo", "api_data": {"domain": "Video Classification", "framework": "PyTorchVideo", "functionality": "X3D Networks", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='x3d_s', pretrained=True)", "api_arguments": {"repository": "facebookresearch/pytorchvideo", "model": "x3d_s", "pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "pytorchvideo"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/pytorchvideo', 'x3d_s', pretrained=True)", "device = 'cpu'", "model = model.eval()", "model = model.to(device)"], "performance": {"dataset": "Kinetics 400", "accuracy": {"top1": 73.33, "top5": 91.27}, "flops": 2.96, "params": 3.79}, "description": "X3D model architectures are based on the paper 'X3D: Expanding Architectures for Efficient Video Recognition' by Christoph Feichtenhofer. They are pretrained on the Kinetics 400 dataset. This model is capable of classifying video clips into different action categories. It is provided by the FAIR PyTorchVideo library."}}
{"code": "{'Instruction': A racing company wants to optimize their code by using once-for-all networks. Suggest an API that can classify road traffic images with once-for-all networks., 'Output': {'domain': 'Classification', 'api_call': 'model = torch.hub.load('mit-han-lab/once-for-all', 'ofa_supernet_mbv3_w10', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Once-for-all (OFA) model from PyTorch Hub for efficient image classification of road traffic images.', 'code': 'import torch\nmodel = torch.hub.load('mit-han-lab/once-for-all', 'ofa_supernet_mbv3_w10', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/once-for-all', model='ofa_supernet_mbv3_w10', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Once-for-all (OFA) Networks", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/once-for-all', model='ofa_supernet_mbv3_w10', pretrained=True)", "api_arguments": {"repository": "mit-han-lab/once-for-all", "model": "ofa_supernet_mbv3_w10", "pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "super_net_name = 'ofa_supernet_mbv3_w10'", "super_net = torch.hub.load('mit-han-lab/once-for-all', super_net_name, pretrained=True).eval()"], "performance": {"description": "OFA networks outperform state-of-the-art NAS methods (up to 4.0% ImageNet top1 accuracy improvement over MobileNetV3, or same accuracy but 1.5x faster than MobileNetV3, 2.6x faster than EfficientNet w.r.t measured latency) while reducing many orders of magnitude GPU hours and CO2 emission."}, "description": "Once-for-all (OFA) networks are a family of neural networks designed by MIT Han Lab. They decouple training and search, achieving efficient inference across various edge devices and resource constraints. OFA networks are pretrained on the IMAGENET dataset and are capable of classifying images into different categories."}}
{"code": "{'Instruction': Recommend an API for a pet food company that wants to make an app to identify cat breeds in pictures., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models', 'resnet18_swsl', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNet-18 model from the Facebook Research PyTorch Hub for image classification. The model has been trained with semi-supervised learning using the ImageNet dataset and can be fine-tuned to classify different cat breeds.', 'code': \"import torch\nmodel = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models', 'resnet18_swsl', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/semi-supervised-ImageNet1K-models', model='resnet18_swsl', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Semi-supervised and semi-weakly supervised ImageNet Models", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/semi-supervised-ImageNet1K-models', model='resnet18_swsl', pretrained=True)", "api_arguments": {"repository": "facebookresearch/semi-supervised-ImageNet1K-models", "model": "resnet18_swsl", "pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models', 'resnet18_swsl', pretrained=True)", "model.eval()"], "performance": {"description": "Semi-supervised and semi-weakly supervised ImageNet models achieve state-of-the-art accuracy of 81.2% on ImageNet for the widely used/adopted ResNet-50 model architecture."}, "description": "Semi-supervised and semi-weakly supervised ImageNet Models are introduced in the 'Billion scale semi-supervised learning for image classification' paper. These models are pretrained on a subset of unlabeled YFCC100M public image dataset and fine-tuned with the ImageNet1K training dataset. They are capable of classifying images into different categories and are provided by the Facebook Research library."}}
{"code": "{'Instruction': A travel buddy application is helping the tourists communicate at many non-english countries. Provide me an API which can help tourists translate simple questions like \"where is the restroom?\" when given the destination language code., 'Output': {'domain': 'Semantic Segmentation', 'api_call': \"model = torch.hub.load('pytorch/fairseq', 'transformer.wmt14.en-fr')\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Transformer model for English-French translation from PyTorch Hub. This model can be used to translate simple questions from English to the target language, given the appropriate language code.', 'code': \"import torch\nmodel = torch.hub.load('pytorch/fairseq', 'transformer.wmt14.en-fr')\"}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/fairseq')", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "Neural Machine Translation", "api_name": "Transformer (NMT)", "api_call": "torch.hub.load(repo_or_dir='pytorch/fairseq')", "api_arguments": ["model_name", "tokenizer", "bpe", "beam", "sampling", "sampling_topk"], "python_environment_requirements": ["bitarray", "fastBPE", "hydra-core", "omegaconf", "regex", "requests", "sacremoses", "subword_nmt"], "example_code": "import torch\n\nen2fr = torch.hub.load('pytorch/fairseq', 'transformer.wmt14.en-fr', tokenizer='moses', bpe='subword_nmt')\n\nen2fr.cuda()\n\nfr = en2fr.translate('Hello world!', beam=5)\nassert fr == 'Bonjour \u00e0 tous !'", "performance": {"dataset": [{"name": "WMT'14", "accuracy": "Not provided"}, {"name": "WMT'18", "accuracy": "Not provided"}, {"name": "WMT'19", "accuracy": "Not provided"}]}, "description": "Transformer (NMT) is a powerful sequence-to-sequence modeling architecture that produces state-of-the-art neural machine translation systems. It is based on the paper 'Attention Is All You Need' and has been improved using techniques such as large-scale semi-supervised training, back-translation, and noisy-channel reranking. It supports English-French and English-German translation as well as round-trip translation for paraphrasing."}}
{"code": "{'Instruction': I am a product manager and I am building a recommendation system for my customers based on the pictures of the clothes they have purchased from our online store. Suggest an API that can classify the clothing type based on a given image., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_gpunet', pretrained=True, model_type='GPUNet-0', model_math='fp32')\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained GPUNet model from PyTorch Hub, optimized for NVIDIA GPU performance, to classify clothing items based on images.\", 'code': 'import torch\nmodel_type = \"GPUNet-0\"\nprecision = \"fp32\"\nmodel = torch.hub.load(\"NVIDIA/DeepLearningExamples:torchhub\", \"nvidia_gpunet\", pretrained=True, model_type=model_type, model_math=precision)'}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_gpunet', pretrained=True, model_type=model_type, model_math=precision)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "GPUNet Networks", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_gpunet', pretrained=True, model_type=model_type, model_math=precision)", "api_arguments": {"repository": "NVIDIA/DeepLearningExamples:torchhub", "model": "nvidia_gpunet", "pretrained": "True", "model_type": "GPUNet-0", "model_math": "fp32"}, "python_environment_requirements": ["torch", "validators", "matplotlib", "timm==0.5.4"], "example_code": ["import torch", "model_type = 'GPUNet-0'", "precision = 'fp32'", "gpunet = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_gpunet', pretrained=True, model_type=model_type, model_math=precision)", "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')", "gpunet.to(device)", "gpunet.eval()"], "performance": {"dataset": "IMAGENET", "description": "GPUNet demonstrates state-of-the-art inference performance up to 2x faster than EfficientNet-X and FBNet-V3."}, "description": "GPUNet is a family of Convolutional Neural Networks designed by NVIDIA using novel Neural Architecture Search (NAS) methods. They are optimized for NVIDIA GPU and TensorRT performance. GPUNet models are pretrained on the IMAGENET dataset and are capable of classifying images into different categories. The models are provided by the NVIDIA Deep Learning Examples library."}}
{"code": "{'Instruction': As a traffic analyst, I need an API that can detect vehicles from traffic camera images., 'Output': {'domain': 'Object Detection', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the SSD (Single Shot MultiBox Detector) model pre-trained on the COCO dataset from PyTorch Hub, which can detect vehicles from traffic camera images.', 'code': \"import torch\nmodel = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_ssd', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Object Detection", "framework": "PyTorch", "functionality": "Single Shot MultiBox Detector", "api_name": "SSD", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_ssd', pretrained=True)", "api_arguments": ["model_name"], "python_environment_requirements": ["numpy", "scipy", "scikit-image", "matplotlib"], "example_code": "import torch\n\nssd_model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd')\nutils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd_processing_utils')\n\nssd_model.to('cuda')\nssd_model.eval()\n\ninputs = [utils.prepare_input(uri) for uri in uris]\ntensor = utils.prepare_tensor(inputs)\n\nwith torch.no_grad():\n detections_batch = ssd_model(tensor)\n\nresults_per_input = utils.decode_results(detections_batch)\nbest_results_per_input = [utils.pick_best(results, 0.40) for results in results_per_input]", "performance": {"dataset": {"name": "COCO", "accuracy": "Not provided"}}, "description": "The SSD (Single Shot MultiBox Detector) model is an object detection model based on the paper 'SSD: Single Shot MultiBox Detector'. It uses a deep neural network for detecting objects in images. This implementation replaces the obsolete VGG model backbone with the more modern ResNet-50 model. The SSD model is trained on the COCO dataset and can be used to detect objects in images with high accuracy and efficiency."}}
{"code": "{'Instruction': Can you tell me an API that can be used for image classification of flowers with high accuracy and efficiency?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_efficientnet_b0', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained EfficientNet-B0 model from NVIDIA\\'s PyTorch Hub, which provides high accuracy and efficiency for image classification tasks, such as classifying flowers.', 'code': \"import torch\nefficientnet = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_efficientnet_b0', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_efficientnet_b0', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "EfficientNet", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_efficientnet_b0', pretrained=True)", "api_arguments": ["model_name", "pretrained"], "python_environment_requirements": ["validators", "matplotlib"], "example_code": "import torch\n\nefficientnet = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_efficientnet_b0', pretrained=True)\nutils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_convnets_processing_utils')\n\nefficientnet.eval().to(device)\n\nbatch = torch.cat([utils.prepare_input_from_uri(uri) for uri in uris]).to(device)\n\nwith torch.no_grad():\n output = torch.nn.functional.softmax(efficientnet(batch), dim=1)\n \nresults = utils.pick_n_best(predictions=output, n=5)", "performance": {"dataset": {"name": "IMAGENET", "accuracy": "Not provided"}}, "description": "EfficientNet is a family of image classification models that achieve state-of-the-art accuracy while being smaller and faster. The models are trained with mixed precision using Tensor Cores on the NVIDIA Volta and Ampere GPU architectures. The EfficientNet models include EfficientNet-B0, EfficientNet-B4, EfficientNet-WideSE-B0, and EfficientNet-WideSE-B4. The WideSE models use wider Squeeze-and-Excitation layers than the original EfficientNet models, resulting in slightly better accuracy."}}
{"code": "{'Instruction': I need a PyTorch API for image classification that has been pretrained on ImageNet and achieves at least 80% top-1 accuracy., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the MEAL_V2 model from PyTorch Hub, which has been pretrained on ImageNet and achieves over 80% top-1 accuracy.', 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50', pretrained=True)", "api_arguments": {"model_name": "mealv2_resnest50"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ ResNet50", "resolution": "224", "parameters": "25.6M", "top1": "80.67", "top5": "95.09"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': I need to classify an image using a state-of-the-art image classification model without any data augmentation techniques or additional tricks. What API should I use?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Use the MEAL V2 model from PyTorch Hub, which achieves state-of-the-art results without any data augmentation techniques or tricks.', 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50_cutmix', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50_cutmix', pretrained=True)", "api_arguments": {"model_name": "mealv2_resnest50_cutmix"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 + CutMix w/ ResNet50", "resolution": "224", "parameters": "25.6M", "top1": "80.98", "top5": "95.35"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': I am an engineer in the environmental sector and I am attempting to develop an AI model that can accurately classify images of animals taken with a trail camera. Find an API that will support this capability., 'Output': {'domain': 'Classification', 'api_call': 'model = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50_380x380', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MEAL V2 model for image classification from PyTorch Hub. This model can help in accurately classifying images of animals taken with a trail camera.', 'code': 'import torch\nmodel = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50_380x380', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50_380x380', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50_380x380', pretrained=True)", "api_arguments": {"model_name": "mealv2_resnest50_380x380"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ ResNet50", "resolution": "380", "parameters": "25.6M", "top1": "81.72", "top5": "95.81"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': I have an image classification task with limited computational resources. I need a lightweight image classification API recommendation for this task., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenetv3_small_075', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained MEAL V2 MobileNet V3 Small 0.75 model from PyTorch Hub for lightweight image classification with limited computational resources.\", 'code': 'import torch\nmodel = torch.hub.load(\"szq0214/MEAL-V2\", \"meal_v2\", model=\"mealv2_mobilenetv3_small_075\", pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenetv3_small_075', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenetv3_small_075', pretrained=True)", "api_arguments": {"model_name": "mealv2_mobilenetv3_small_075"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ MobileNet V3-Small 0.75", "resolution": "224", "parameters": "2.04M", "top1": "67.60", "top5": "87.23"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': Recommend an API that can analyze a video and classify different activities featured in the video., 'Output': {'domain': 'Video Classification', 'api_call': 'model = torch.hub.load('facebookresearch/pytorchvideo', 'slow_r50', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained 3D ResNet model from PyTorch Hub to classify activities within a video.', 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/pytorchvideo', 'slow_r50', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='slow_r50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Video Classification", "framework": "PyTorch", "functionality": "3D ResNet", "api_name": "slow_r50", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='slow_r50', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["torch", "json", "urllib", "pytorchvideo", "torchvision", "torchaudio", "torchtext", "torcharrow", "TorchData", "TorchRec", "TorchServe", "PyTorch on XLA Devices"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/pytorchvideo', 'slow_r50', pretrained=True)", "device = 'cpu'", "model = model.eval()", "model = model.to(device)"], "performance": {"dataset": "Kinetics 400", "accuracy": {"top_1": 74.58, "top_5": 91.63}, "Flops (G)": 54.52, "Params (M)": 32.45}, "description": "The 3D ResNet model is a Resnet-style video classification network pretrained on the Kinetics 400 dataset. It is based on the architecture from the paper 'SlowFast Networks for Video Recognition' by Christoph Feichtenhofer et al."}}
{"code": "{'Instruction': I want an API that can identify animals in a image from an ongoing research about endangered animals., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_efficientnet_b0', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained MEAL V2 model with EfficientNet-B0 for image classification. Fine-tune the model to identify animals in an ongoing research about endangered animals.\", 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_efficientnet_b0', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_efficientnet_b0', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_efficientnet_b0', pretrained=True)", "api_arguments": {"model_name": "mealv2_efficientnet_b0"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ EfficientNet-B0", "resolution": "224", "parameters": "5.29M", "top1": "78.29", "top5": "93.95"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': What API would help me estimate the depth of objects within a single image?, 'Output': {'domain': 'Depth Estimation', 'api_call': \"torch.hub.load('intel-isl/MiDaS', 'DPT_Large', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'The MiDaS API can be used to compute relative depth from single images. It provides different models suitable for various use cases, with its largest model, DPT_Large, offering the highest accuracy.', 'code': 'import torch\nmidas = torch.hub.load('intel-isl/MiDaS', 'DPT_Large', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='intel-isl/MiDaS', model='DPT_Large', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Computing relative depth from a single image", "api_name": "MiDaS", "api_call": "torch.hub.load(repo_or_dir='intel-isl/MiDaS', model='DPT_Large', pretrained=True)", "api_arguments": {"repo_or_dir": "intel-isl/MiDaS", "model": "model_type"}, "python_environment_requirements": "pip install timm", "example_code": ["import cv2", "import torch", "import urllib.request", "import matplotlib.pyplot as plt", "url, filename = ('https://github.com/pytorch/hub/raw/master/images/dog.jpg', 'dog.jpg')", "urllib.request.urlretrieve(url, filename)", "model_type = 'DPT_Large'", "midas = torch.hub.load('intel-isl/MiDaS', 'DPT_Large')", "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')", "midas.to(device)", "midas.eval()", "midas_transforms = torch.hub.load('intel-isl/MiDaS', 'transforms')", "if model_type == 'DPT_Large' or model_type == 'DPT_Hybrid':", " transform = midas_transforms.dpt_transform", "else:", " transform = midas_transforms.small_transform", "img = cv2.imread(filename)", "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)", "input_batch = transform(img).to(device)", "with torch.no_grad():", " prediction = midas(input_batch)", "prediction = torch.nn.functional.interpolate(", " prediction.unsqueeze(1),", " size=img.shape[:2],", " mode='bicubic',", " align_corners=False,", ").squeeze()", "output = prediction.cpu().numpy()", "plt.imshow(output)", "plt.show()"], "performance": {"dataset": "10 distinct datasets", "accuracy": "Multi-objective optimization"}, "description": "MiDaS computes relative inverse depth from a single image. The repository provides multiple models that cover different use cases ranging from a small, high-speed model to a very large model that provide the highest accuracy. The models have been trained on 10 distinct datasets using multi-objective optimization to ensure high quality on a wide range of inputs."}}
{"code": "{'Instruction': I want to build a self-driving car application that can detect objects, segment drivable areas, and detect lanes. Point me to an API that can do this., 'Output': {'domain': 'Object Detection', 'api_call': \"model = torch.hub.load('hustvl/yolop', 'yolop', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained YOLOP model from PyTorch Hub for object detection, drivable area segmentation, and lane detection that can be useful for self-driving car applications.', 'code': \"import torch\nmodel = torch.hub.load('hustvl/yolop', 'yolop', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='hustvl/yolop', model='yolop', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Object Detection", "framework": "PyTorch", "functionality": "Object Detection, Drivable Area Segmentation, Lane Detection", "api_name": "YOLOP", "api_call": "torch.hub.load(repo_or_dir='hustvl/yolop', model='yolop', pretrained=True)", "api_arguments": "pretrained", "python_environment_requirements": "pip install -qr https://github.com/hustvl/YOLOP/blob/main/requirements.txt", "example_code": "import torch\nmodel = torch.hub.load('hustvl/yolop', 'yolop', pretrained=True)\nimg = torch.randn(1,3,640,640)\ndet_out, da_seg_out,ll_seg_out = model(img)", "performance": {"dataset": "BDD100K", "accuracy": {"Object Detection": {"Recall(%)": 89.2, "mAP50(%)": 76.5, "Speed(fps)": 41}, "Drivable Area Segmentation": {"mIOU(%)": 91.5, "Speed(fps)": 41}, "Lane Detection": {"mIOU(%)": 70.5, "IOU(%)": 26.2}}}, "description": "YOLOP is an efficient multi-task network that can jointly handle three crucial tasks in autonomous driving: object detection, drivable area segmentation and lane detection. And it is also the first to reach real-time on embedded devices while maintaining state-of-the-art level performance on the BDD100K dataset."}}
{"code": "{'Instruction': I captured a photograph of a room and want to know the relative depth of the objects in it. Can you suggest an API which can help me with this?, 'Output': {'domain': 'Depth Estimation', 'api_call': \"model = torch.hub.load('intel-isl/MiDaS', 'MiDaS_small', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MiDaS model from PyTorch Hub, which can compute relative depth information from a single image of a room.', 'code': \"import torch\nmodel = torch.hub.load('intel-isl/MiDaS', 'MiDaS_small', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='intel-isl/MiDaS', model='MiDaS_small', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Computing relative depth from a single image", "api_name": "MiDaS", "api_call": "torch.hub.load(repo_or_dir='intel-isl/MiDaS', model='MiDaS_small', pretrained=True)", "api_arguments": {"repo_or_dir": "intel-isl/MiDaS", "model": "model_type"}, "python_environment_requirements": "pip install timm", "example_code": ["import cv2", "import torch", "import urllib.request", "import matplotlib.pyplot as plt", "url, filename = ('https://github.com/pytorch/hub/raw/master/images/dog.jpg', 'dog.jpg')", "urllib.request.urlretrieve(url, filename)", "model_type = 'DPT_Large'", "midas = torch.hub.load('intel-isl/MiDaS', 'MiDaS_small')", "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')", "midas.to(device)", "midas.eval()", "midas_transforms = torch.hub.load('intel-isl/MiDaS', 'transforms')", "if model_type == 'DPT_Large' or model_type == 'DPT_Hybrid':", " transform = midas_transforms.dpt_transform", "else:", " transform = midas_transforms.small_transform", "img = cv2.imread(filename)", "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)", "input_batch = transform(img).to(device)", "with torch.no_grad():", " prediction = midas(input_batch)", "prediction = torch.nn.functional.interpolate(", " prediction.unsqueeze(1),", " size=img.shape[:2],", " mode='bicubic',", " align_corners=False,", ").squeeze()", "output = prediction.cpu().numpy()", "plt.imshow(output)", "plt.show()"], "performance": {"dataset": "10 distinct datasets", "accuracy": "Multi-objective optimization"}, "description": "MiDaS computes relative inverse depth from a single image. The repository provides multiple models that cover different use cases ranging from a small, high-speed model to a very large model that provide the highest accuracy. The models have been trained on 10 distinct datasets using multi-objective optimization to ensure high quality on a wide range of inputs."}}
{"code": "{'Instruction': I have a pop music soundtrack and I want to extract drums, vocals, and bass from it. Give me an API that can perform this music separation., 'Output': {'domain': 'Audio Separation', 'api_call': \"model = torch.hub.load('sigsep/open-unmix-pytorch', 'umxhq', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Open-Unmix model from PyTorch Hub, which is specifically designed for music source separation, allowing you to extract drums, vocals, bass, and other instruments from the soundtrack.', 'code': 'import torch\nmodel = torch.hub.load('sigsep/open-unmix-pytorch', 'umxhq', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='sigsep/open-unmix-pytorch', model='umxhq', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Audio Separation", "framework": "PyTorch", "functionality": "Music Source Separation", "api_name": "Open-Unmix", "api_call": "torch.hub.load(repo_or_dir='sigsep/open-unmix-pytorch', model='umxhq', pretrained=True)", "api_arguments": ["model_name"], "python_environment_requirements": ["PyTorch >=1.6.0", "torchaudio"], "example_code": ["import torch", "separator = torch.hub.load('sigsep/open-unmix-pytorch', 'umxhq')", "audio = torch.rand((1, 2, 100000))", "original_sample_rate = separator.sample_rate", "estimates = separator(audio)"], "performance": {"dataset": "MUSDB18", "accuracy": "N/A"}, "description": "Open-Unmix provides ready-to-use models that allow users to separate pop music into four stems: vocals, drums, bass and the remaining other instruments. The models were pre-trained on the freely available MUSDB18 dataset."}}
{"code": "{'Instruction': What is a good API for transcribing spoken language within a podcast into readable text in multiple languages?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('snakers4/silero-models', 'silero_stt', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pre-trained Silero Speech-To-Text model from PyTorch Hub, which supports multiple languages and can transcribe spoken language in a podcast into readable text.', 'code': 'import torch\nmodel, decoder, utils = torch.hub.load(\"snakers4/silero-models\", \"silero_stt\", language=\"en\", device=torch.device(\"cpu\"))'}}", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_stt', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Speech-To-Text", "api_name": "Silero Models", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_stt', pretrained=True)", "api_arguments": {"repo_or_dir": "snakers4/silero-models", "model": "silero_stt", "language": ["en", "de", "es"], "device": "device"}, "python_environment_requirements": ["pip install -q torchaudio omegaconf soundfile"], "example_code": ["import torch", "import zipfile", "import torchaudio", "from glob import glob", "device = torch.device('cpu')", "model, decoder, utils = torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_stt', language='en', device=device)", "(read_batch, split_into_batches, read_audio, prepare_model_input) = utils", "torch.hub.download_url_to_file('https://opus-codec.org/static/examples/samples/speech_orig.wav', dst ='speech_orig.wav', progress=True)", "test_files = glob('speech_orig.wav')", "batches = split_into_batches(test_files, batch_size=10)", "input = prepare_model_input(read_batch(batches[0]), device=device)", "output = model(input)", "for example in output:", " print(decoder(example.cpu()))"], "performance": {"dataset": "Open-STT", "accuracy": "See the wiki for quality and performance benchmarks"}, "description": "Silero Speech-To-Text models provide enterprise grade STT in a compact form-factor for several commonly spoken languages. The models are robust to a variety of dialects, codecs, domains, noises, and lower sampling rates. They consume a normalized audio in the form of samples and output frames with token probabilities. A decoder utility is provided for simplicity."}}
{"code": "{'Instruction': What API should we use to perform semantic segmentation on aerial images for identifying buildings, roads, and vegetation?, 'Output': {'domain': 'Semantic Segmentation', 'api_call': \"model = torch.hub.load('pytorch/vision', 'deeplabv3_resnet50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained DeepLabV3 model with ResNet-50 backbone from PyTorch Hub for semantic segmentation tasks, such as identifying buildings, roads, and vegetation in aerial images.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'deeplabv3_resnet50', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='deeplabv3_resnet50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "DeepLabV3", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='deeplabv3_resnet50', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": "latest", "torchvision": "latest", "PIL": "latest", "matplotlib": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_resnet50', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "input_image = input_image.convert('RGB')", "preprocess = transforms.Compose([", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)['out'][0]", "output_predictions = output.argmax(0)"], "performance": {"dataset": "COCO val2017", "accuracy": [{"model": "deeplabv3_resnet50", "Mean IOU": 66.4, "Global Pixelwise Accuracy": 92.4}]}, "description": "DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset."}}
{"code": "{'Instruction': I need an API that can detect voice activity from an audio file. Propose an API for this task., 'Output': {'domain': 'Voice Activity Detection', 'api_call': \"model = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained Silero Voice Activity Detector from PyTorch Hub to detect voice activity in an audio file.\", 'code': 'import torch\nmodel, utils = torch.hub.load(repo_or_dir=\"snakers4/silero-vad\", model=\"silero_vad\", force_reload=True)'}}", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Voice Activity Detection", "api_name": "Silero Voice Activity Detector", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=True)", "api_arguments": {"repo_or_dir": "snakers4/silero-vad", "model": "silero_vad", "force_reload": "True"}, "python_environment_requirements": {"torchaudio": "pip install -q torchaudio"}, "example_code": {"import": ["import torch", "torch.set_num_threads(1)", "from IPython.display import Audio", "from pprint import pprint"], "download_example": "torch.hub.download_url_to_file('https://models.silero.ai/vad_models/en.wav', 'en_example.wav')", "load_model": "model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=True)", "load_utils": "(get_speech_timestamps, _, read_audio, _) = utils", "set_sampling_rate": "sampling_rate = 16000", "read_audio": "wav = read_audio('en_example.wav', sampling_rate=sampling_rate)", "get_speech_timestamps": "speech_timestamps = get_speech_timestamps(wav, model, sampling_rate=sampling_rate)", "print_speech_timestamps": "pprint(speech_timestamps)"}, "performance": {"dataset": "", "accuracy": ""}, "description": "Silero VAD is a pre-trained enterprise-grade Voice Activity Detector (VAD) that aims to provide a high-quality and modern alternative to the WebRTC Voice Activity Detector. The model is optimized for performance on 1 CPU thread and is quantized."}}
{"code": "{'Instruction': Create an audio version of an English text using an API., 'Output': {'domain': 'Text-To-Speech', 'api_call': \"model, symbols, sample_rate, example_text, apply_tts = torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_tts', language='en', speaker='lj_16khz')\", 'api_provider': 'PyTorch', 'explanation': 'Load the Silero Text-To-Speech model from PyTorch Hub to generate audio from an English text.', 'code': \"import torch\nlanguage = 'en'\nspeaker = 'lj_16khz'\ndevice = torch.device('cpu')\nmodel, symbols, sample_rate, example_text, apply_tts = torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_tts', language=language, speaker=speaker)\nmodel = model.to(device)\naudio = apply_tts(texts=[example_text], model=model, sample_rate=sample_rate, symbols=symbols, device=device)\"}}", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_tts', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Text-To-Speech", "framework": "PyTorch", "functionality": "Text-To-Speech", "api_name": "Silero Text-To-Speech Models", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_tts', pretrained=True)", "api_arguments": {"repo_or_dir": "snakers4/silero-models", "model": "silero_tts", "language": "language", "speaker": "speaker"}, "python_environment_requirements": ["pip install -q torchaudio omegaconf"], "example_code": "import torch\nlanguage = 'en'\nspeaker = 'lj_16khz'\ndevice = torch.device('cpu')\nmodel, symbols, sample_rate, example_text, apply_tts = torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_tts', language=language, speaker=speaker)\nmodel = model.to(device)\naudio = apply_tts(texts=[example_text], model=model, sample_rate=sample_rate, symbols=symbols, device=device)", "performance": {"dataset": [{"language": "Russian", "speakers": 6}, {"language": "English", "speakers": 1}, {"language": "German", "speakers": 1}, {"language": "Spanish", "speakers": 1}, {"language": "French", "speakers": 1}], "accuracy": "High throughput on slow hardware. Decent performance on one CPU thread"}, "description": "Silero Text-To-Speech models provide enterprise grade TTS in a compact form-factor for several commonly spoken languages. They offer one-line usage, naturally sounding speech, no GPU or training required, minimalism and lack of dependencies, a library of voices in many languages, support for 16kHz and 8kHz out of the box."}}
{"code": "{'Instruction': Locate an API that could assist me in developing a street view application that highlights trees and buildings within the given images., 'Output': {'domain': 'Semantic Segmentation', 'api_call': \"model = torch.hub.load('pytorch/vision', 'deeplabv3_resnet101', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained DeepLabV3 with ResNet-101 model from PyTorch Hub, which can be used for semantic segmentation tasks, such as separating trees and buildings in street view images.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'deeplabv3_resnet101', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='deeplabv3_resnet101', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "DeepLabV3", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='deeplabv3_resnet101', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": "latest", "torchvision": "latest", "PIL": "latest", "matplotlib": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_resnet101', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "input_image = input_image.convert('RGB')", "preprocess = transforms.Compose([", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)['out'][0]", "output_predictions = output.argmax(0)"], "performance": {"dataset": "COCO val2017", "accuracy": [{"model": "deeplabv3_resnet101", "Mean IOU": 67.4, "Global Pixelwise Accuracy": 92.4}]}, "description": "DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset."}}
{"code": "{'Instruction': I want to segment an image of Orchard Parks Shopping Center. Suggest an API for semantic segmentation that can segment items in a shopping center., 'Output': {'domain': 'Semantic Segmentation', 'api_call': \"model = torch.hub.load('pytorch/vision', 'deeplabv3_mobilenet_v3_large', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained DeepLabV3 model with MobileNet-V3 backbone from PyTorch Hub for semantic segmentation tasks, which can be utilized to segment items in a shopping center.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'deeplabv3_mobilenet_v3_large', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='deeplabv3_mobilenet_v3_large', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "DeepLabV3", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='deeplabv3_mobilenet_v3_large', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": "latest", "torchvision": "latest", "PIL": "latest", "matplotlib": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_mobilenet_v3_large', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "input_image = input_image.convert('RGB')", "preprocess = transforms.Compose([", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)['out'][0]", "output_predictions = output.argmax(0)"], "performance": {"dataset": "COCO val2017", "accuracy": [{"model": "deeplabv3_mobilenet_v3_large", "Mean IOU": 60.3, "Global Pixelwise Accuracy": 91.4}]}, "description": "DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset."}}
{"code": "{'Instruction': Get me an API to generate new clothing designs based on a database of existing clothes., 'Output': {'domain': 'Generation', 'api_call': \"model = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'DCGAN', pretrained=True, useGPU=use_gpu)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Deep Convolutional Generative Adversarial Networks (DCGAN) model from PyTorch Hub, capable of generating new clothing designs based on existing data.', 'code': 'import torch\nuse_gpu = True if torch.cuda.is_available() else False\nmodel = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'DCGAN', pretrained=True, useGPU=use_gpu)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorch_GAN_zoo:hub', model='DCGAN', pretrained=True, useGPU=use_gpu)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Generative Adversarial Networks", "api_name": "DCGAN", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorch_GAN_zoo:hub', model='DCGAN', pretrained=True, useGPU=use_gpu)", "api_arguments": {"pretrained": "True", "useGPU": "use_gpu"}, "python_environment_requirements": "Python 3", "example_code": {"import": ["import torch", "import matplotlib.pyplot as plt", "import torchvision"], "use_gpu": "use_gpu = True if torch.cuda.is_available() else False", "load_model": "model = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'DCGAN', pretrained=True, useGPU=use_gpu)", "build_noise_data": "noise, _ = model.buildNoiseData(num_images)", "generate_images": "with torch.no_grad(): generated_images = model.test(noise)", "plot_images": ["plt.imshow(torchvision.utils.make_grid(generated_images).permute(1, 2, 0).cpu().numpy())", "plt.show()"]}, "performance": {"dataset": "FashionGen", "accuracy": "N/A"}, "description": "DCGAN is a model designed in 2015 by Radford et. al. in the paper Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks. It is a GAN architecture both very simple and efficient for low resolution image generation (up to 64x64)."}}
{"code": "{'Instruction': A security company wants to create a project that involves object detection from the live video feed captured by their CCTV cameras. Propose an API that provides object detection in real time., 'Output': {'domain': 'Object Detection', 'api_call': 'model = torch.hub.load(repo_or_dir='ultralytics/yolov5', model='yolov5s', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained YOLOv5s model from PyTorch Hub for real-time object detection from live video feeds.', 'code': 'import torch\nmodel = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='ultralytics/yolov5', model='yolov5s', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Object Detection", "framework": "PyTorch", "functionality": "YOLOv5", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='ultralytics/yolov5', model='yolov5s', pretrained=True)", "api_arguments": ["'ultralytics/yolov5'", "'yolov5s'", "pretrained=True"], "python_environment_requirements": "Python>=3.8, PyTorch>=1.7", "example_code": ["import torch", "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)", "imgs = ['https://ultralytics.com/images/zidane.jpg']", "results = model(imgs)", "results.print()", "results.save()", "results.xyxy[0]", "results.pandas().xyxy[0]"], "performance": {"dataset": "COCO", "accuracy": {"YOLOv5s6": {"mAPval0.5:0.95": 43.3, "mAPtest0.5:0.95": 43.3, "mAPval0.5": 61.9}, "YOLOv5m6": {"mAPval0.5:0.95": 50.5, "mAPtest0.5:0.95": 50.5, "mAPval0.5": 68.7}, "YOLOv5l6": {"mAPval0.5:0.95": 53.4, "mAPtest0.5:0.95": 53.4, "mAPval0.5": 71.1}, "YOLOv5x6": {"mAPval0.5:0.95": 54.4, "mAPtest0.5:0.95": 54.4, "mAPval0.5": 72.0}, "YOLOv5x6 TTA": {"mAPval0.5:0.95": 55.0, "mAPtest0.5:0.95": 55.0, "mAPval0.5": 72.0}}}, "description": "YOLOv5 is a family of compound-scaled object detection models trained on the COCO dataset, and includes simple functionality for Test Time Augmentation (TTA), model ensembling, hyperparameter evolution, and export to ONNX, CoreML and TFLite."}}
{"code": "{'Instruction': I want to build an app for social media that filters profiles that have sunglasses in the picture. Give me an API that can detect presence of sunglasses within a picture., 'Output': {'domain': 'Object Detection', 'api_call': 'model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNext WSL model from PyTorch Hub, which can be fine-tuned for detecting sunglasses within an image.', 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x16d_wsl', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNext WSL", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x16d_wsl', pretrained=True)", "api_arguments": [{"name": "resnext101_32x16d_wsl", "type": "str", "description": "ResNeXt-101 32x16d WSL model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl')", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(output[0])", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeXt-101 32x16d": {"Top-1 Acc.": "84.2", "Top-5 Acc.": "97.2"}}}, "description": "The provided ResNeXt models are pre-trained in weakly-supervised fashion on 940 million public images with 1.5K hashtags matching with 1000 ImageNet1K synsets, followed by fine-tuning on ImageNet1K dataset. The models significantly improve the training accuracy on ImageNet compared to training from scratch. They achieve state-of-the-art accuracy of 85.4% on ImageNet with the ResNext-101 32x48d model."}}
{"code": "{'Instruction': Suggest an API for automatically classifying images of road safety hazards such as potholes, damaged sidewalks, and obscured traffic signals for a road safety app., 'Output': {'domain': 'Image Classification', 'api_call': 'model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x8d_wsl', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNeXt-101 32x8d WSL model from PyTorch Hub, which can be fine-tuned for road safety hazard classification.', 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x8d_wsl', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x8d_wsl', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNext WSL", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x8d_wsl', pretrained=True)", "api_arguments": [{"name": "resnext101_32x8d_wsl", "type": "str", "description": "ResNeXt-101 32x8d WSL model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x8d_wsl')", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(output[0])", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeXt-101 32x8d": {"Top-1 Acc.": "82.2", "Top-5 Acc.": "96.4"}}}, "description": "The provided ResNeXt models are pre-trained in weakly-supervised fashion on 940 million public images with 1.5K hashtags matching with 1000 ImageNet1K synsets, followed by fine-tuning on ImageNet1K dataset. The models significantly improve the training accuracy on ImageNet compared to training from scratch. They achieve state-of-the-art accuracy of 85.4% on ImageNet with the ResNext-101 32x48d model."}}
{"code": "{'Instruction': Can you suggest an ML API capable of predicting whether a given image is a cat, dog or other animal with high accuracy?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x48d_wsl', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the ResNext WSL model from PyTorch Hub, pretrained on ImageNet, which can be fine-tuned to classify images of cats, dogs, and other animals with high accuracy.\", 'code': \"import torch\nmodel = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x48d_wsl', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x48d_wsl', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNext WSL", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x48d_wsl', pretrained=True)", "api_arguments": [{"name": "resnext101_32x48d_wsl", "type": "str", "description": "ResNeXt-101 32x48d WSL model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x48d_wsl')", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(output[0])", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeXt-101 32x48d": {"Top-1 Acc.": "85.4", "Top-5 Acc.": "97.6"}}}, "description": "The provided ResNeXt models are pre-trained in weakly-supervised fashion on 940 million public images with 1.5K hashtags matching with 1000 ImageNet1K synsets, followed by fine-tuning on ImageNet1K dataset. The models significantly improve the training accuracy on ImageNet compared to training from scratch. They achieve state-of-the-art accuracy of 85.4% on ImageNet with the ResNext-101 32x48d model."}}
{"code": "{'Instruction': Recommend an API suitable for an image recognition task to classify different species of animals., 'Output': {'domain': 'Classification', 'api_call': 'model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resneXt')', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNeXt101-32x4d model from NVIDIA PyTorch Hub, which can be fine-tuned for classifying different animal species.', 'code': 'import torch\nmodel = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resneXt')'}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_resneXt', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNeXt101-32x4d", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_resneXt', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "validators", "matplotlib"], "example_code": ["import torch", "from PIL import Image", "import torchvision.transforms as transforms", "import numpy as np", "import json", "import requests", "import matplotlib.pyplot as plt", "import warnings", "warnings.filterwarnings('ignore')", "%matplotlib inline", "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")", "print(f'Using {device} for inference')", "resneXt = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resneXt')", "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_convnets_processing_utils')", "resneXt.eval().to(device)", "uris = [", " 'http://images.cocodataset.org/test-stuff2017/000000024309.jpg',", " 'http://images.cocodataset.org/test-stuff2017/000000028117.jpg',", " 'http://images.cocodataset.org/test-stuff2017/000000006149.jpg',", " 'http://images.cocodataset.org/test-stuff2017/000000004954.jpg',", "]", "batch = torch.cat(", " [utils.prepare_input_from_uri(uri) for uri in uris]", ").to(device)", "with torch.no_grad():", " output = torch.nn.functional.softmax(resneXt(batch), dim=1)", "results = utils.pick_n_best(predictions=output, n=5)", "for uri, result in zip(uris, results):", " img = Image.open(requests.get(uri, stream=True).raw)", " img.thumbnail((256,256), Image.ANTIALIAS)", " plt.imshow(img)", " plt.show()", " print(result)"], "performance": {"dataset": "IMAGENET", "accuracy": "Not provided"}, "description": "ResNeXt101-32x4d is a model introduced in the Aggregated Residual Transformations for Deep Neural Networks paper. It is based on regular ResNet model, substituting 3x3 convolutions inside the bottleneck block for 3x3 grouped convolutions. This model is trained with mixed precision using Tensor Cores on Volta, Turing, and the NVIDIA Ampere GPU architectures. It can be deployed for inference on the NVIDIA Triton Inference Server using TorchScript, ONNX Runtime or TensorRT as an execution backend."}}
{"code": "{'Instruction': A machine learning model is needed that can identify abnormalities in brain MRI scans. Recommend an API that could be used to train a model for this purpose., 'Output': {'domain': 'Semantic Segmentation', 'api_call': \"model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained U-Net model for brain MRI segmentation from PyTorch Hub, which can be used to identify abnormalities in brain MRI scans.', 'code': 'import torch\nmodel = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='mateuszbuda/brain-segmentation-pytorch', model='unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "Biomedical Image Segmentation", "api_name": "U-Net for brain MRI", "api_call": "torch.hub.load(repo_or_dir='mateuszbuda/brain-segmentation-pytorch', model='unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)", "api_arguments": {"in_channels": 3, "out_channels": 1, "init_features": 32, "pretrained": true}, "python_environment_requirements": ["torch", "torchvision", "numpy", "PIL"], "example_code": ["import torch", "model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)", "import numpy as np", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "m, s = np.mean(input_image, axis=(0, 1)), np.std(input_image, axis=(0, 1))", "preprocess = transforms.Compose([transforms.ToTensor(),transforms.Normalize(mean=m, std=s)])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model = model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(torch.round(output[0]))"], "performance": {"dataset": "kaggle.com/mateuszbuda/lgg-mri-segmentation"}, "description": "U-Net with batch normalization for biomedical image segmentation with pretrained weights for abnormality segmentation in brain MRI. The model comprises four levels of blocks containing two convolutional layers with batch normalization and ReLU activation function, and one max pooling layer in the encoding part and up-convolutional layers instead in the decoding part. The number of convolutional filters in each block is 32, 64, 128, and 256. The bottleneck layer has 512 convolutional filters. From the encoding layers, skip connections are used to the corresponding layers in the decoding part. Input image is a 3-channel brain MRI slice from pre-contrast, FLAIR, and post-contrast sequences, respectively. Output is a one-channel probability map of abnormality regions with the same size as the input image."}}
{"code": "{'Instruction': I want to create an album of generated paintings of famous landscapes. Provide me with an API that can generate such images., 'Output': {'domain': 'Image Generation', 'api_call': \"model = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'PGAN', model_name='celebAHQ-512', pretrained=True, useGPU=use_gpu)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Progressive Growing of GANs (PGAN) model from PyTorch Hub and fine-tune it for generating high-quality, realistic images of famous landscapes. The example code provided uses the model named \"celebAHQ-512\", but you can change this to a model trained on landscape images.', 'code': 'import torch\nuse_gpu = True if torch.cuda.is_available() else False\nmodel = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'PGAN', model_name='celebAHQ-512', pretrained=True, useGPU=use_gpu)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorch_GAN_zoo:hub', model='PGAN', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Generative Adversarial Networks (GANs)", "api_name": "PGAN", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorch_GAN_zoo:hub', model='PGAN', pretrained=True)", "api_arguments": {"repo_or_dir": "facebookresearch/pytorch_GAN_zoo:hub", "model": "PGAN", "model_name": "celebAHQ-512", "pretrained": "True", "useGPU": "use_gpu"}, "python_environment_requirements": "Python 3", "example_code": {"import": "import torch", "use_gpu": "use_gpu = True if torch.cuda.is_available() else False", "load_model": "model = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'PGAN', model_name='celebAHQ-512', pretrained=True, useGPU=use_gpu)", "build_noise_data": "noise, _ = model.buildNoiseData(num_images)", "test": "generated_images = model.test(noise)", "plot_images": {"import_matplotlib": "import matplotlib.pyplot as plt", "import_torchvision": "import torchvision", "make_grid": "grid = torchvision.utils.make_grid(generated_images.clamp(min=-1, max=1), scale_each=True, normalize=True)", "imshow": "plt.imshow(grid.permute(1, 2, 0).cpu().numpy())", "show": "plt.show()"}}, "performance": {"dataset": "celebA", "accuracy": "High-quality celebrity faces"}, "description": "Progressive Growing of GANs (PGAN) is a method for generating high-resolution images using generative adversarial networks. The model is trained progressively, starting with low-resolution images and gradually increasing the resolution until the desired output is achieved. This implementation is based on the paper by Tero Karras et al., 'Progressive Growing of GANs for Improved Quality, Stability, and Variation'."}}
{"code": "{'Instruction': Suggest an API for classifying a large dataset of images into different categories., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'alexnet', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pre-trained AlexNet model from PyTorch Hub for image classification on a large dataset.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'alexnet', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='alexnet', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "AlexNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='alexnet', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": ">=1.9.0", "torchvision": ">=0.10.0"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"top-1_error": 43.45, "top-5_error": 20.91}}, "description": "AlexNet is a deep convolutional neural network that achieved a top-5 error of 15.3% in the 2012 ImageNet Large Scale Visual Recognition Challenge. The main contribution of the original paper was the depth of the model, which was computationally expensive but made feasible through the use of GPUs during training. The pretrained AlexNet model in PyTorch can be used for image classification tasks."}}
{"code": "{'Instruction': Find me an API that can classify the images given corresponding image URLs. It should be fast and efficient., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resnet50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNet50 v1.5 model from NVIDIA via PyTorch Hub to perform fast and efficient image classification on image URLs.', 'code': 'import torch\nmodel = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resnet50', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_resnet50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNet50", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_resnet50', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["pip install validators matplotlib"], "example_code": ["import torch", "from PIL import Image", "import torchvision.transforms as transforms", "import numpy as np", "import json", "import requests", "import matplotlib.pyplot as plt", "import warnings", "warnings.filterwarnings('ignore')", "%matplotlib inline", "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')", "print(f'Using {device} for inference')", "resnet50 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resnet50', pretrained=True)", "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_convnets_processing_utils')", "resnet50.eval().to(device)", "uris = [...]", "batch = torch.cat([utils.prepare_input_from_uri(uri) for uri in uris]).to(device)", "with torch.no_grad():", " output = torch.nn.functional.softmax(resnet50(batch), dim=1)", "results = utils.pick_n_best(predictions=output, n=5)", "for uri, result in zip(uris, results):", " img = Image.open(requests.get(uri, stream=True).raw)", " img.thumbnail((256,256), Image.ANTIALIAS)", " plt.imshow(img)", " plt.show()", " print(result)"], "performance": {"dataset": "ImageNet", "accuracy": "~0.5% top1 improvement over ResNet50 v1"}, "description": "The ResNet50 v1.5 model is a modified version of the original ResNet50 v1 model. The difference between v1 and v1.5 is that, in the bottleneck blocks which requires downsampling, v1 has stride = 2 in the first 1x1 convolution, whereas v1.5 has stride = 2 in the 3x3 convolution. This difference makes ResNet50 v1.5 slightly more accurate (~0.5% top1) than v1, but comes with a small performance drawback (~5% imgs/sec). The model is initialized as described in Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification. This model is trained with mixed precision using Tensor Cores on Volta, Turing, and the NVIDIA Ampere GPU architectures."}}
{"code": "{'Instruction': I need to identify objects from a photo taken during a traffic scene, please provide me with an API call that can help in achieving this., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_se_resnext101_32x4d', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Use the SE-ResNeXt101-32x4d model from PyTorch Hub for classifying objects in traffic scenes.', 'code': 'import torch\nmodel = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_se_resnext101_32x4d', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_se_resnext101_32x4d', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SE-ResNeXt101", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_se_resnext101_32x4d', pretrained=True)", "api_arguments": "N/A", "python_environment_requirements": "validators, matplotlib", "example_code": "import torch\nfrom PIL import Image\nimport torchvision.transforms as transforms\nimport numpy as np\nimport json\nimport requests\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nprint(f'Using {device} for inference')\nresneXt = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_se_resnext101_32x4d')\nutils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_convnets_processing_utils')\nresneXt.eval().to(device)\nuris = ['http://images.cocodataset.org/test-stuff2017/000000024309.jpg','http://images.cocodataset.org/test-stuff2017/000000028117.jpg','http://images.cocodataset.org/test-stuff2017/000000006149.jpg','http://images.cocodataset.org/test-stuff2017/000000004954.jpg']\nbatch = torch.cat([utils.prepare_input_from_uri(uri) for uri in uris]).to(device)\nwith torch.no_grad():\n output = torch.nn.functional.softmax(resneXt(batch), dim=1)\nresults = utils.pick_n_best(predictions=output, n=5)\nfor uri, result in zip(uris, results):\n img = Image.open(requests.get(uri, stream=True).raw)\n img.thumbnail((256,256), Image.ANTIALIAS)\n plt.imshow(img)\n plt.show()\n print(result)", "performance": {"dataset": "IMAGENET", "accuracy": "N/A"}, "description": "The SE-ResNeXt101-32x4d is a ResNeXt101-32x4d model with added Squeeze-and-Excitation module. This model is trained with mixed precision using Tensor Cores on Volta, Turing, and the NVIDIA Ampere GPU architectures, which allows researchers to get results 3x faster than training without Tensor Cores while experiencing the benefits of mixed precision training."}}
{"code": "{'Instruction': I'm working on an application that converts text into speech. Tell me an API that can serve my purpose., 'Output': {'domain': 'Text-to-Speech', 'api_call': 'waveglow = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_waveglow', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Use the pretrained WaveGlow model from PyTorch Hub, which works in conjunction with Tacotron 2 to synthesize natural-sounding speech from text.', 'code': 'import torch\nwaveglow = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_waveglow', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_waveglow', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Text-to-Speech", "framework": "PyTorch", "functionality": "Speech Synthesis", "api_name": "WaveGlow", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_waveglow', pretrained=True)", "api_arguments": {"repo_or_dir": "NVIDIA/DeepLearningExamples:torchhub", "model": "nvidia_waveglow", "model_math": "fp32"}, "python_environment_requirements": ["numpy", "scipy", "librosa", "unidecode", "inflect", "libsndfile1"], "example_code": {"load_waveglow_model": "waveglow = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_waveglow', model_math='fp32')", "prepare_waveglow_model": ["waveglow = waveglow.remove_weightnorm(waveglow)", "waveglow = waveglow.to('cuda')", "waveglow.eval()"], "load_tacotron2_model": "tacotron2 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tacotron2', model_math='fp32')", "prepare_tacotron2_model": ["tacotron2 = tacotron2.to('cuda')", "tacotron2.eval()"], "synthesize_speech": ["text = \"hello world, I missed you so much\"", "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tts_utils')", "sequences, lengths = utils.prepare_input_sequence([text])", "with torch.no_grad():", " mel, _, _ = tacotron2.infer(sequences, lengths)", " audio = waveglow.infer(mel)", "audio_numpy = audio[0].data.cpu().numpy()", "rate = 22050"], "save_audio": "write(\"audio.wav\", rate, audio_numpy)", "play_audio": "Audio(audio_numpy, rate=rate)"}, "performance": {"dataset": "LJ Speech", "accuracy": null}, "description": "The Tacotron 2 and WaveGlow model form a text-to-speech system that enables users to synthesize natural-sounding speech from raw transcripts without any additional prosody information. The Tacotron 2 model produces mel spectrograms from input text using encoder-decoder architecture. WaveGlow is a flow-based model that consumes the mel spectrograms to generate speech."}}
{"code": "{'Instruction': Which pre-trained model should I use for sentiment analysis of customer reviews on our website?, 'Output': {'domain': 'Sentiment Analysis', 'api_call': \"model = torch.hub.load('pytorch/fairseq', 'roberta.large', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'You can use the RoBERTa large model from PyTorch Hub, which can be fine-tuned for the sentiment analysis task of customer reviews.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/fairseq', 'roberta.large', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/fairseq', model='roberta.large', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "RoBERTa", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='pytorch/fairseq', model='roberta.large', pretrained=True)", "api_arguments": ["'pytorch/fairseq'", "'roberta.large'"], "python_environment_requirements": ["regex", "requests", "hydra-core", "omegaconf"], "example_code": ["import torch", "roberta = torch.hub.load('pytorch/fairseq', 'roberta.large')", "roberta.eval()", "tokens = roberta.encode('Hello world!')", "last_layer_features = roberta.extract_features(tokens)", "all_layers = roberta.extract_features(tokens, return_all_hiddens=True)", "roberta = torch.hub.load('pytorch/fairseq', 'roberta.large.mnli')", "roberta.eval()", "tokens = roberta.encode('Roberta is a heavily optimized version of BERT.', 'Roberta is not very optimized.')", "prediction = roberta.predict('mnli', tokens).argmax().item()", "tokens = roberta.encode('Roberta is a heavily optimized version of BERT.', 'Roberta is based on BERT.')", "prediction = roberta.predict('mnli', tokens).argmax().item()", "roberta.register_classification_head('new_task', num_classes=3)", "logprobs = roberta.predict('new_task', tokens)"], "performance": {"dataset": "MNLI", "accuracy": "N/A"}, "description": "RoBERTa is a robustly optimized version of BERT, a revolutionary self-supervised pretraining technique that learns to predict intentionally hidden (masked) sections of text. RoBERTa builds on BERT's language masking strategy and modifies key hyperparameters, including removing BERT's next-sentence pretraining objective, and training with much larger mini-batches and learning rates. RoBERTa was also trained on an order of magnitude more data than BERT, for a longer amount of time, allowing it to generalize even better to downstream tasks."}}
{"code": "{'Instruction': I need to classify various images into pre-defined categories. Suggest me an API that can be utilized for image classification., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'densenet121', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Dense Convolutional Network (DenseNet) model from PyTorch Hub for image classification tasks.', 'code': 'import torch\nmodel = torch.hub.load(\"pytorch/vision\", \"densenet121\", pretrained=True)\nmodel.eval()'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet121', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Dense Convolutional Network", "api_name": "Densenet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet121', pretrained=True)", "api_arguments": [{"name": "densenet121", "type": "str", "description": "Densenet-121 model"}], "python_environment_requirements": {"torch": "latest", "torchvision": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet121', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"densenet121": {"Top-1 error": 25.35, "Top-5 error": 7.83}}}, "description": "Dense Convolutional Network (DenseNet) connects each layer to every other layer in a feed-forward fashion. It alleviates the vanishing-gradient problem, strengthens feature propagation, encourages feature reuse, and substantially reduces the number of parameters."}}
{"code": "{'Instruction': I am studying about vegetative cover in my college. Give me an API that can output segmented vegetation areas from a satellite image of a region., 'Output': {'domain': 'Semantic Segmentation', 'api_call': 'model = torch.hub.load('pytorch/vision', 'fcn_resnet50', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained FCN-ResNet50 model from PyTorch Hub, which can be fine-tuned to segment vegetative cover from a satellite image.\", 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'fcn_resnet50', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='fcn_resnet50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "Fully-Convolutional Network", "api_name": "fcn_resnet50", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='fcn_resnet50', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "boolean", "description": "If True, returns a model pre-trained on COCO train2017"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "matplotlib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'fcn_resnet50', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "input_image = input_image.convert(\"RGB\")", "preprocess = transforms.Compose([", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)['out'][0]", "output_predictions = output.argmax(0)"], "performance": {"dataset": "COCO val2017", "accuracy": {"fcn_resnet50": {"Mean IOU": 60.5, "Global Pixelwise Accuracy": 91.4}}}, "description": "FCN-ResNet is a Fully-Convolutional Network model using a ResNet-50 or a ResNet-101 backbone. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset."}}
{"code": "{'Instruction': I want to implement image classification based on DenseNet. Suggest an API that can classify images using DenseNet., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'densenet161', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained DenseNet-161 model from PyTorch Hub to perform image classification tasks.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'densenet161', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet161', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Dense Convolutional Network", "api_name": "Densenet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet161', pretrained=True)", "api_arguments": [{"name": "densenet161", "type": "str", "description": "Densenet-161 model"}], "python_environment_requirements": {"torch": "latest", "torchvision": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet161', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"densenet161": {"Top-1 error": 22.35, "Top-5 error": 6.2}}}, "description": "Dense Convolutional Network (DenseNet) connects each layer to every other layer in a feed-forward fashion. It alleviates the vanishing-gradient problem, strengthens feature propagation, encourages feature reuse, and substantially reduces the number of parameters."}}
{"code": "{'Instruction': An e-commerce company is building a recommendation system that relies on object classification. Please suggest an API for this., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'densenet201', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Use PyTorch Hub to load the pretrained DenseNet-201 model for image classification, which would be suitable for an e-commerce recommendation system.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'densenet201', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet201', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Dense Convolutional Network", "api_name": "Densenet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet201', pretrained=True)", "api_arguments": [{"name": "densenet201", "type": "str", "description": "Densenet-201 model"}], "python_environment_requirements": {"torch": "latest", "torchvision": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet201', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"densenet201": {"Top-1 error": 22.8, "Top-5 error": 6.43}}}, "description": "Dense Convolutional Network (DenseNet) connects each layer to every other layer in a feed-forward fashion. It alleviates the vanishing-gradient problem, strengthens feature propagation, encourages feature reuse, and substantially reduces the number of parameters."}}
{"code": "{'Instruction': A machine learning engineer building an image recognition system that filters cats and dogs. Give me an API that can classify whether an image is a cat or dog., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'densenet169', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Densenet169 model from PyTorch Hub. Fine-tune the model on a dataset of cat and dog images to classify whether an image contains a cat or a dog.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'densenet169', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet169', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Dense Convolutional Network", "api_name": "Densenet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet169', pretrained=True)", "api_arguments": [{"name": "densenet169", "type": "str", "description": "Densenet-169 model"}], "python_environment_requirements": {"torch": "latest", "torchvision": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet169', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"densenet169": {"Top-1 error": 24.0, "Top-5 error": 7.0}}}, "description": "Dense Convolutional Network (DenseNet) connects each layer to every other layer in a feed-forward fashion. It alleviates the vanishing-gradient problem, strengthens feature propagation, encourages feature reuse, and substantially reduces the number of parameters."}}
{"code": "{'Instruction': I need an API to perform semantic segmentation on my images. Preferably a fully-convolutional network. What can you suggest?, 'Output': {'domain': 'Semantic Segmentation', 'api_call': \"model = torch.hub.load('pytorch/vision', 'fcn_resnet101', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained FCN-ResNet101 model from PyTorch Hub for semantic segmentation using a Fully-Convolutional Network with a ResNet-101 backbone.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'fcn_resnet101', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='fcn_resnet101', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "Fully-Convolutional Network", "api_name": "fcn_resnet101", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='fcn_resnet101', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "boolean", "description": "If True, returns a model pre-trained on COCO train2017"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "matplotlib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'fcn_resnet101', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "input_image = input_image.convert(\"RGB\")", "preprocess = transforms.Compose([", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)['out'][0]", "output_predictions = output.argmax(0)"], "performance": {"dataset": "COCO val2017", "accuracy": {"fcn_resnet101": {"Mean IOU": 63.7, "Global Pixelwise Accuracy": 91.9}}}, "description": "FCN-ResNet is a Fully-Convolutional Network model using a ResNet-50 or a ResNet-101 backbone. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset."}}
{"code": "{'Instruction': Recommend an API that is suitable for detecting the type of currency in an image., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'googlenet', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained GoogLeNet model from PyTorch Hub, which can be fine-tuned for currency classification using images.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'googlenet', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='googlenet', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "GoogLeNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='googlenet', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": ">=1.0.0", "torchvision": ">=0.2.2"}, "example_code": {"import": ["import torch", "import urllib", "from PIL import Image", "from torchvision import transforms"], "load_model": "model = torch.hub.load('pytorch/vision:v0.10.0', 'googlenet', pretrained=True)", "model_eval": "model.eval()", "image_preprocessing": ["input_image = Image.open(filename)", "preprocess = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)"], "model_execution": ["if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)"], "output_processing": ["probabilities = torch.nn.functional.softmax(output[0], dim=0)", "top5_prob, top5_catid = torch.topk(probabilities, 5)"]}, "performance": {"dataset": "ImageNet", "accuracy": {"Top-1 error": "30.22", "Top-5 error": "10.47"}}, "description": "GoogLeNet is based on a deep convolutional neural network architecture codenamed 'Inception', which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014)."}}
{"code": "{'Instruction': Recommend a lightweight, efficient image classification model that can classify retail products by category., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet68', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Use the HarDNet-68 model from PyTorch Hub, a lightweight and efficient image classification model that can be fine-tuned for retail product classification by category.', 'code': 'import torch\nmodel = torch.hub.load(\\'PingoLH/Pytorch-HarDNet\\', \\'hardnet68\\', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet68', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "HarDNet", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet68', pretrained=True)", "api_arguments": [{"name": "hardnet68", "type": "str", "description": "HarDNet-68 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet68', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"hardnet68": {"Top-1 error": 23.52, "Top-5 error": 6.99}}}, "description": "Harmonic DenseNet (HarDNet) is a low memory trafficCNN model, which is fast and efficient. The basic concept is to minimize both computational cost and memory access cost at the same time, such that the HarDNet models are 35% faster than ResNet running on GPU comparing to models with the same accuracy (except the two DS models that were designed for comparing with MobileNet)."}}
{"code": "{'Instruction': I want a classifier that can recognize clothing items in images, taking into account the images may have different styles., 'Output': {'domain': 'Classification', 'api_call': 'model = torch.hub.load('XingangPan/IBN-Net', 'resnet50_ibn_a', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained ResNet-50-IBN-a model from PyTorch Hub, which incorporates instance normalization for better invariance to style variations in images.\", 'code': 'import torch\nmodel = torch.hub.load('XingangPan/IBN-Net', 'resnet50_ibn_a', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnet50_ibn_a', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "IBN-Net", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnet50_ibn_a', pretrained=True)", "api_arguments": [{"name": "resnet50_ibn_a", "type": "str", "description": "ResNet-50-IBN-a model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('XingangPan/IBN-Net', 'resnet50_ibn_a', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"resnet50_ibn_a": {"Top-1 acc": 77.46, "Top-5 acc": 93.68}}}, "description": "IBN-Net is a CNN model with domain/appearance invariance. Motivated by style transfer works, IBN-Net carefully unifies instance normalization and batch normalization in a single deep network. It provides a simple way to increase both modeling and generalization capacities without adding model complexity. IBN-Net is especially suitable for cross domain or person/vehicle re-identification tasks."}}
{"code": "{'Instruction': Recommend an API that can classify different types of plants from their images., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet85', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained HarDNet-85 model from PyTorch Hub, which can be fine-tuned for plant classification on images.', 'code': 'import torch\nmodel = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet85', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet85', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "HarDNet", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet85', pretrained=True)", "api_arguments": [{"name": "hardnet85", "type": "str", "description": "HarDNet-85 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet85', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"hardnet85": {"Top-1 error": 21.96, "Top-5 error": 6.11}}}, "description": "Harmonic DenseNet (HarDNet) is a low memory traffic CNN model, which is fast and efficient. The basic concept is to minimize both computational cost and memory access cost at the same time, such that the HarDNet models are 35% faster than ResNet running on GPU comparing to models with the same accuracy (except the two DS models that were designed for comparing with MobileNet)."}}
{"code": "{'Instruction': We are building a phone app for Intel Corporation which can detect various objects in an image. Can you help me with an API which can classify objects into different categories for given image?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('huawei-noah/ghostnet', 'ghostnet_1x', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained GhostNet model from PyTorch Hub, which can classify objects in an image into different categories with high accuracy.', 'code': 'import torch\nmodel = torch.hub.load('huawei-noah/ghostnet', 'ghostnet_1x', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/ghostnet', model='ghostnet_1x', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Efficient networks by generating more features from cheap operations", "api_name": "GhostNet", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/ghostnet', model='ghostnet_1x', pretrained=True)", "api_arguments": ["pretrained"], "python_environment_requirements": ["torch", "torchvision", "PIL"], "example_code": ["import torch", "model = torch.hub.load('huawei-noah/ghostnet', 'ghostnet_1x', pretrained=True)", "model.eval()", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)", "print(probabilities)"], "performance": {"dataset": "ImageNet", "accuracy": {"Top-1 acc": "73.98", "Top-5 acc": "91.46"}}, "description": "The GhostNet architecture is based on an Ghost module structure which generates more features from cheap operations. Based on a set of intrinsic feature maps, a series of cheap operations are applied to generate many ghost feature maps that could fully reveal information underlying intrinsic features. Experiments conducted on benchmarks demonstrate the superiority of GhostNet in terms of speed and accuracy tradeoff."}}
{"code": "{'Instruction': An e-commerce company is looking for an API to identify products in images for better recommendations on their app. Can you recommend an API that can perform this task?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('XingangPan/IBN-Net', 'resnext101_ibn_a', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained IBN-Net model from PyTorch Hub, which can be fine-tuned for product image classification tasks on your e-commerce app.', 'code': 'import torch\nmodel = torch.hub.load('XingangPan/IBN-Net', 'resnext101_ibn_a', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnext101_ibn_a', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "IBN-Net", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnext101_ibn_a', pretrained=True)", "api_arguments": [{"name": "resnext101_ibn_a", "type": "str", "description": "ResNeXt-101-IBN-a model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('XingangPan/IBN-Net', 'resnext101_ibn_a', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"resnext101_ibn_a": {"Top-1 acc": 79.12, "Top-5 acc": 94.58}}}, "description": "IBN-Net is a CNN model with domain/appearance invariance. Motivated by style transfer works, IBN-Net carefully unifies instance normalization and batch normalization in a single deep network. It provides a simple way to increase both modeling and generalization capacities without adding model complexity. IBN-Net is especially suitable for cross domain or person/vehicle re-identification tasks."}}
{"code": "{'Instruction': Recommend an API best suited for an app that needs to recognize unknown persons in images, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('XingangPan/IBN-Net', 'se_resnet101_ibn_a', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained IBN-Net, which unifies instance normalization and batch normalization, suitable for cross-domain or person identification tasks.', 'code': \"import torch\nmodel = torch.hub.load('XingangPan/IBN-Net', 'se_resnet101_ibn_a', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='se_resnet101_ibn_a', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "IBN-Net", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='se_resnet101_ibn_a', pretrained=True)", "api_arguments": [{"name": "se_resnet101_ibn_a", "type": "str", "description": "SE-ResNet-101-IBN-a model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('XingangPan/IBN-Net', 'se_resnet101_ibn_a', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"se_resnet101_ibn_a": {"Top-1 acc": 78.75, "Top-5 acc": 94.49}}}, "description": "IBN-Net is a CNN model with domain/appearance invariance. Motivated by style transfer works, IBN-Net carefully unifies instance normalization and batch normalization in a single deep network. It provides a simple way to increase both modeling and generalization capacities without adding model complexity. IBN-Net is especially suitable for cross domain or person/vehicle re-identification tasks."}}
{"code": "{'Instruction': Design an image classifier for a phone manufacturer to identify inappropriate images from user gallery and flag them., 'Output': {'domain': 'Image Classification', 'api_call': 'model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest50', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNeSt-50 model from PyTorch Hub, which can be fine-tuned for inappropriate image classification using a custom dataset.', 'code': 'import torch\nmodel = torch.hub.load('zhanghang1989/ResNeSt', 'resnest50', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNeSt", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest50', pretrained=True)", "api_arguments": "resnest50", "python_environment_requirements": {"torch": "1.0.0", "torchvision": "0.2.2"}, "example_code": ["import torch", "model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest50', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeSt-50": 81.03}}, "description": "ResNeSt models are from the ResNeSt: Split-Attention Networks paper. They are a new ResNet variant that enables attention across feature-map groups. By stacking Split-Attention blocks ResNet-style, ResNeSt models outperform other networks with similar model complexities, and also help downstream tasks including object detection, instance segmentation, and semantic segmentation."}}
{"code": "{'Instruction': Recommend an image classification API that performs state-of-the-art performance for a mobile app., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest200', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the ResNeSt-200 model from PyTorch Hub to get a state-of-the-art image classification model for your mobile app.\", 'code': 'import torch\nmodel = torch.hub.load('zhanghang1989/ResNeSt', 'resnest200', pretrained=True)\nmodel.eval()'}}", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest200', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNeSt", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest200', pretrained=True)", "api_arguments": "resnest200", "python_environment_requirements": {"torch": "1.0.0", "torchvision": "0.2.2"}, "example_code": ["import torch", "model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest200', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeSt-200": 83.84}}, "description": "ResNeSt models are from the ResNeSt: Split-Attention Networks paper. They are a new ResNet variant that enables attention across feature-map groups. By stacking Split-Attention blocks ResNet-style, ResNeSt models outperform other networks with similar model complexities, and also help downstream tasks including object detection, instance segmentation, and semantic segmentation."}}
{"code": "{'Instruction': Provide an API for classifying images of fruits in a retail store., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest101', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNeSt101 model from PyTorch Hub, which can be fine-tuned for fruit classification in a retail store.', 'code': 'import torch\nmodel = torch.hub.load('zhanghang1989/ResNeSt', 'resnest101', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest101', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNeSt", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest101', pretrained=True)", "api_arguments": "resnest101", "python_environment_requirements": {"torch": "1.0.0", "torchvision": "0.2.2"}, "example_code": ["import torch", "model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest101', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeSt-101": 82.83}}, "description": "ResNeSt models are from the ResNeSt: Split-Attention Networks paper. They are a new ResNet variant that enables attention across feature-map groups. By stacking Split-Attention blocks ResNet-style, ResNeSt models outperform other networks with similar model complexities, and also help downstream tasks including object detection, instance segmentation, and semantic segmentation."}}
{"code": "{'Instruction': Let's say I've created an app for birdwatchers to recognize birds in the images they take. Recommend me a high-performance image recognition API that would be ideal to use for my app., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'inception_v3', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Use the pretrained Inception v3 model from PyTorch Hub, which can be fine-tuned for bird species recognition. The model is trained on the ImageNet dataset and achieves high accuracy.\", 'code': \"import torch\nmodel = torch.hub.load('pytorch/vision', 'inception_v3', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='inception_v3', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "Inception_v3", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='inception_v3', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": "1.9.0", "torchvision": "0.10.0"}, "example_code": {"import_libraries": "import torch", "load_model": "model = torch.hub.load('pytorch/vision:v0.10.0', 'inception_v3', pretrained=True)", "model_evaluation": "model.eval()"}, "performance": {"dataset": "imagenet", "accuracy": {"top-1_error": 22.55, "top-5_error": 6.44}}, "description": "Inception v3, also called GoogleNetv3, is a famous Convolutional Neural Network trained on the ImageNet dataset from 2015. It is based on the exploration of ways to scale up networks to utilize the added computation as efficiently as possible by using suitably factorized convolutions and aggressive regularization. The model achieves a top-1 error of 22.55% and a top-5 error of 6.44% on the ImageNet dataset."}}
{"code": "{'Instruction': I am a Machine Learning Engineer at Pinterest and I need an API to classify images. Suggest an efficient API., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest269', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNeSt model for image classification from PyTorch Hub. It performs efficient classification of images using advanced ResNet-style architecture with split-attention.', 'code': 'import torch\nmodel = torch.hub.load('zhanghang1989/ResNeSt', 'resnest269', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest269', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNeSt", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest269', pretrained=True)", "api_arguments": "resnest269", "python_environment_requirements": {"torch": "1.0.0", "torchvision": "0.2.2"}, "example_code": ["import torch", "model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest269', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeSt-269": 84.54}}, "description": "ResNeSt models are from the ResNeSt: Split-Attention Networks paper. They are a new ResNet variant that enables attention across feature-map groups. By stacking Split-Attention blocks ResNet-style, ResNeSt models outperform other networks with similar model complexities, and also help downstream tasks including object detection, instance segmentation, and semantic segmentation."}}
{"code": "{'Instruction': I have a limited computational budget for image classification tasks. Suggest an API optimized for CPU performance., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_cpu', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ProxylessNAS model optimized for CPU performance from PyTorch Hub to perform image classification tasks with limited computational budget.', 'code': 'import torch\nmodel = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_cpu', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_cpu', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "ProxylessNAS", "api_name": "mit-han-lab/ProxylessNAS", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_cpu', pretrained=True)", "api_arguments": [{"name": "proxylessnas_cpu", "type": "str", "description": "ProxylessNAS optimized for CPU"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "target_platform = 'proxyless_cpu'", "model = torch.hub.load('mit-han-lab/ProxylessNAS', target_platform, pretrained=True)", "model.eval()"], "performance": {"dataset": [{"model_structure": "proxylessnas_cpu", "accuracy": 75.3}]}, "description": "ProxylessNAS models are from the ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware paper. They specialize CNN architectures for different hardware platforms, offering free yet significant performance boost on all three platforms (CPU, GPU, and Mobile) with similar accuracy."}}
{"code": "{'Instruction': Can you recommend an API for a mobile app developer who wants lightweight and fast image classification on their app?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_mobile', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the ProxylessNAS Mobile model from PyTorch Hub, which is optimized for low-resource hardware platforms found on most mobile devices, ensuring fast image classification.\", 'code': 'import torch\nmodel = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_mobile', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_mobile', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "ProxylessNAS", "api_name": "mit-han-lab/ProxylessNAS", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_mobile', pretrained=True)", "api_arguments": [{"name": "proxylessnas_mobile", "type": "str", "description": "ProxylessNAS optimized for Mobile"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "target_platform = 'proxyless_mobile'", "model = torch.hub.load('mit-han-lab/ProxylessNAS', target_platform, pretrained=True)", "model.eval()"], "performance": {"dataset": [{"model_structure": "proxylessnas_mobile", "accuracy": 74.6}]}, "description": "ProxylessNAS models are from the ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware paper. They specialize CNN architectures for different hardware platforms, offering free yet significant performance boost on all three platforms (CPU, GPU, and Mobile) with similar accuracy."}}
{"code": "{'Instruction': Develop an image classifier that can identify objects in an image. Provide an API example to load a pretrained classifier model., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'resnet18', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNet-18 model from PyTorch Hub for image classification to identify objects in an image.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'resnet18', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet18', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Deep Residual Networks", "api_name": "ResNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet18', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "bool", "default": "False", "description": "If True, returns a model pre-trained on ImageNet"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "resnet18", "top-1_error": 30.24, "top-5_error": 10.92}}, "description": "ResNet models are deep residual networks pre-trained on ImageNet. They were proposed in the paper 'Deep Residual Learning for Image Recognition'. Available model variants include ResNet18, ResNet34, ResNet50, ResNet101, and ResNet152."}}
{"code": "{'Instruction': I want to analyze the objects within my images taken from my phone to use it in my machine learning dataset, suggest me an API that is best suited for this purpose., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'mobilenet_v2', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MobileNet v2 model from PyTorch for lightweight and efficient image classification, particularly useful for processing images taken from your phone.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'mobilenet_v2', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='mobilenet_v2', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MobileNet v2", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='mobilenet_v2', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision', 'mobilenet_v2', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)"], "performance": {"dataset": "ImageNet", "accuracy": {"top-1_error": 28.12, "top-5_error": 9.71}}, "description": "The MobileNet v2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input. MobileNet v2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, non-linearities in the narrow layers were removed in order to maintain representational power."}}
{"code": "{'Instruction': How can a botanist distinguish between different plant species using an API based on deep residual networks?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'resnet34', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNet34 model from PyTorch Hub, which can be fine-tuned for plant species classification using deep residual networks.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'resnet34', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet34', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Deep Residual Networks", "api_name": "ResNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet34', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "bool", "default": "False", "description": "If True, returns a model pre-trained on ImageNet"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet34', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "resnet34", "top-1_error": 26.7, "top-5_error": 8.58}}, "description": "ResNet models are deep residual networks pre-trained on ImageNet. They were proposed in the paper 'Deep Residual Learning for Image Recognition'. Available model variants include ResNet18, ResNet34, ResNet50, ResNet101, and ResNet152."}}
{"code": "{'Instruction': A new dating app wants to analyze its user's images to determine if the user is an authentic account or an impersonator. Give me an example of API that can help differentiate between real profile images and fake one., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_gpu', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ProxylessNAS model optimized for GPU from PyTorch Hub, which can be fine-tuned for determining the authenticity of user profile images.', 'code': 'import torch\nmodel = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_gpu', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_gpu', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "ProxylessNAS", "api_name": "mit-han-lab/ProxylessNAS", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_gpu', pretrained=True)", "api_arguments": [{"name": "proxylessnas_gpu", "type": "str", "description": "ProxylessNAS optimized for GPU"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "target_platform = 'proxyless_gpu'", "model = torch.hub.load('mit-han-lab/ProxylessNAS', target_platform, pretrained=True)", "model.eval()"], "performance": {"dataset": [{"model_structure": "proxylessnas_gpu", "accuracy": 75.1}]}, "description": "ProxylessNAS models are from the ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware paper. They specialize CNN architectures for different hardware platforms, offering free yet significant performance boost on all three platforms (CPU, GPU, and Mobile) with similar accuracy."}}
{"code": "{'Instruction': Many image web services need an efficient image recognition model. Can you provide an API that uses deep residual networks for image classification?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'resnet50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained ResNet50 model from PyTorch Hub to perform image classification using deep residual networks.\", 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'resnet50', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Deep Residual Networks", "api_name": "ResNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet50', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "bool", "default": "False", "description": "If True, returns a model pre-trained on ImageNet"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "resnet50", "top-1_error": 23.85, "top-5_error": 7.13}}, "description": "ResNet models are deep residual networks pre-trained on ImageNet. They were proposed in the paper 'Deep Residual Learning for Image Recognition'. Available model variants include ResNet18, ResNet34, ResNet50, ResNet101, and ResNet152."}}
{"code": "{'Instruction': I'm designing an automatic image moderation system for my social platform. Recommend an API for classifying images into different categories., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'resnet101', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained ResNet101 model from PyTorch Hub for image classification. It's a deep residual network trained on ImageNet and is capable of classifying images into different categories.\", 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'resnet101', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet101', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Deep Residual Networks", "api_name": "ResNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet101', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "bool", "default": "False", "description": "If True, returns a model pre-trained on ImageNet"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet101', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "resnet101", "top-1_error": 22.63, "top-5_error": 6.44}}, "description": "ResNet models are deep residual networks pre-trained on ImageNet. They were proposed in the paper 'Deep Residual Learning for Image Recognition'. Available model variants include ResNet18, ResNet34, ResNet50, ResNet101, and ResNet152."}}
{"code": "{'Instruction': I have a set of pictures of birds and I want to put them in categories based on their species. What API can I use to classify these images based on their content?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'resnext50_32x4d', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNext50 model from PyTorch for image classification, which can be fine-tuned to classify bird species from images.', 'code': 'import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('pytorch/vision', 'resnext50_32x4d', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnext50_32x4d', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNext", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnext50_32x4d', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "PIL"], "example_code": ["import torch", "from PIL import Image", "from torchvision import transforms", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnext50_32x4d', pretrained=True)", "model.eval()", "input_image = Image.open('dog.jpg')", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)"], "performance": {"dataset": "ImageNet", "accuracy": {"resnext50_32x4d": {"top-1": 22.38, "top-5": 6.3}}}, "description": "ResNext is a next-generation ResNet architecture for image classification. It is more efficient and accurate than the original ResNet. This implementation includes two versions of the model, resnext50_32x4d and resnext101_32x8d, with 50 and 101 layers respectively."}}
{"code": "{'Instruction': Can you tell me about an API that can give me powerful image classification capabilities for a mobile app?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_s', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained SNNMLP Small model from the PyTorch Hub, suitable for mobile applications due to its low computational cost and high accuracy.', 'code': 'import torch\nmodel = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_s', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_s', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SNNMLP", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_s', pretrained=True)", "api_arguments": [{"name": "snnmlp_s", "type": "str", "description": "SNNMLP Small model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_s', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "SNNMLP Small", "top-1": 83.3}}, "description": "SNNMLP incorporates the mechanism of LIF neurons into the MLP models, to achieve better accuracy without extra FLOPs. We propose a full-precision LIF operation to communicate between patches, including horizontal LIF and vertical LIF in different directions. We also propose to use group LIF to extract better local features. With LIF modules, our SNNMLP model achieves 81.9%, 83.3% and 83.6% top-1 accuracy on ImageNet dataset with only 4.4G, 8.5G and 15.2G FLOPs, respectively."}}
{"code": "{'Instruction': Let's use a neural network to classify a given image. Recommend an API for this purpose., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_t', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained SNNMLP Tiny model from PyTorch Hub for image classification, incorporating Leaky Integrate-and-Fire (LIF) neurons to achieve better accuracy with fewer FLOPs.', 'code': 'import torch\nmodel = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_t', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_t', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SNNMLP", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_t', pretrained=True)", "api_arguments": [{"name": "snnmlp_t", "type": "str", "description": "SNNMLP Tiny model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_t', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "SNNMLP Tiny", "top-1": 81.88}}, "description": "SNNMLP incorporates the mechanism of LIF neurons into the MLP models, to achieve better accuracy without extra FLOPs. We propose a full-precision LIF operation to communicate between patches, including horizontal LIF and vertical LIF in different directions. We also propose to use group LIF to extract better local features. With LIF modules, our SNNMLP model achieves 81.9%, 83.3% and 83.6% top-1 accuracy on ImageNet dataset with only 4.4G, 8.5G and 15.2G FLOPs, respectively."}}
{"code": "{'Instruction': I work at a camera manufacturing company, and we want an API that will help our camera to identify and classify images. Ideally, the API should be based on deep learning techniques., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_b', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the SNNMLP model from PyTorch Hub with pre-trained weights for image classification. The model is based on deep learning techniques, and can identify and classify images.', 'code': 'import torch\nmodel = torch.hub.load(\"huawei-noah/Efficient-AI-Backbones\", \"snnmlp_b\", pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_b', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SNNMLP", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_b', pretrained=True)", "api_arguments": [{"name": "snnmlp_b", "type": "str", "description": "SNNMLP Base model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_b', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "SNNMLP Base", "top-1": 85.59}}, "description": "SNNMLP incorporates the mechanism of LIF neurons into the MLP models, to achieve better accuracy without extra FLOPs. We propose a full-precision LIF operation to communicate between patches, including horizontal LIF and vertical LIF in different directions. We also propose to use group LIF to extract better local features. With LIF modules, our SNNMLP model achieves 81.9%, 83.3% and 83.6% top-1 accuracy on ImageNet dataset with only 4.4G, 8.5G and 15.2G FLOPs, respectively."}}
{"code": "{'Instruction': In the process of automating image classification for a product recognition tool, provide me with an API that can effectively sort categorized images., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'squeezenet1_0', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained SqueezeNet model from PyTorch Hub for efficient image classification, providing AlexNet-level accuracy with significantly fewer parameters to organize categorized images.\", 'code': 'import torch\nmodel = torch.hub.load(\\'pytorch/vision\\', 'squeezenet1_0', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='squeezenet1_0', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SqueezeNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='squeezenet1_0', pretrained=True)", "api_arguments": {"version": "v0.10.0", "model": ["squeezenet1_0"], "pretrained": "True"}, "python_environment_requirements": {"torch": ">=1.9.0", "torchvision": ">=0.10.0"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'squeezenet1_0', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)", "print(probabilities)"], "performance": {"dataset": "ImageNet", "accuracy": {"squeezenet1_0": {"Top-1 error": 41.9, "Top-5 error": 19.58}}}, "description": "SqueezeNet is an image classification model that achieves AlexNet-level accuracy with 50x fewer parameters. It has two versions: squeezenet1_0 and squeezenet1_1, with squeezenet1_1 having 2.4x less computation and slightly fewer parameters than squeezenet1_0, without sacrificing accuracy."}}
{"code": "{'Instruction': \"An image from my friend's vacation album needs to be categorized without any further information. Give me an API that can classify the content of the image.\", 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'squeezenet1_1', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained SqueezeNet1_1 model from PyTorch Hub to classify the content of an image with minimal input information.', 'code': \"import torch\nmodel = torch.hub.load('pytorch/vision', 'squeezenet1_1', pretrained=True)\nmodel.eval()\"}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='squeezenet1_1', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SqueezeNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='squeezenet1_1', pretrained=True)", "api_arguments": {"version": "v0.10.0", "model": ["squeezenet1_1"], "pretrained": "True"}, "python_environment_requirements": {"torch": ">=1.9.0", "torchvision": ">=0.10.0"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'squeezenet1_1', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)", "print(probabilities)"], "performance": {"dataset": "ImageNet", "accuracy": {"squeezenet1_1": {"Top-1 error": 41.81, "Top-5 error": 19.38}}}, "description": "SqueezeNet is an image classification model that achieves AlexNet-level accuracy with 50x fewer parameters. It has two versions: squeezenet1_0 and squeezenet1_1, with squeezenet1_1 having 2.4x less computation and slightly fewer parameters than squeezenet1_0, without sacrificing accuracy."}}
{"code": "{'Instruction': A smartphone company wants to implement image classification for their mobile camera app. Provide an API that can identify objects in a given image., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'shufflenet_v2_x1_0', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ShuffleNet v2 model from PyTorch Hub for fast and efficient image classification for identifying objects in a given image.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'shufflenet_v2_x1_0', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='shufflenet_v2_x1_0', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ShuffleNet v2", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='shufflenet_v2_x1_0', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": "torch", "torchvision": "torchvision", "PIL": "Image", "urllib": "urllib"}, "example_code": {"import_libraries": ["import torch", "from PIL import Image", "from torchvision import transforms", "import urllib"], "load_model": ["model = torch.hub.load('pytorch/vision:v0.10.0', 'shufflenet_v2_x1_0', pretrained=True)", "model.eval()"], "load_image": ["url, filename = ('https://github.com/pytorch/hub/raw/master/images/dog.jpg', 'dog.jpg')", "try: urllib.URLopener().retrieve(url, filename)", "except: urllib.request.urlretrieve(url, filename)", "input_image = Image.open(filename)"], "preprocess_image": ["preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)"], "run_inference": ["if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)"], "get_probabilities": ["probabilities = torch.nn.functional.softmax(output[0], dim=0)"], "top_categories": ["top5_prob, top5_catid = torch.topk(probabilities, 5)", "for i in range(top5_prob.size(0)):", " print(categories[top5_catid[i]], top5_prob[i].item())"]}, "performance": {"dataset": "Imagenet", "accuracy": {"top-1_error": 30.64, "top-5_error": 11.68}}, "description": "ShuffleNet V2 is an efficient ConvNet optimized for speed and memory, pre-trained on Imagenet. It is designed based on practical guidelines for efficient network design, including speed and accuracy tradeoff."}}
{"code": "{'Instruction': Develop an API for classifying images of cars into brands based on a pretrained model., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg11_bn', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained VGG11 model with batch normalization from PyTorch Hub, which can be fine-tuned to classify car images into different brands.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg11_bn', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg11_bn', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg11_bn', pretrained=True)", "api_arguments": [{"name": "vgg11_bn", "type": "str", "description": "VGG11 model with batch normalization"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg11_bn', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg11_bn": {"Top-1 error": 26.7, "Top-5 error": 8.58}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': Recommend an API that can help a wildlife photographer to identify animals from their photos., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg13', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained VGG13 model from PyTorch Hub to assist a wildlife photographer in identifying animals from their photos.\", 'code': 'import torch\nmodel = torch.hub.load(\"pytorch/vision\", \"vgg13\", pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg13', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg13', pretrained=True)", "api_arguments": [{"name": "vgg13", "type": "str", "description": "VGG13 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg13', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg13": {"Top-1 error": 30.07, "Top-5 error": 10.75}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn,vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': I am a programmer who is interested in building a photo application that recognizes plants using camera feed. Recommend me an API and some sample code to achieve this., 'Output': {'domain': 'Image Recognition', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg13_bn', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Use VGG13-BN model from PyTorch Hub with pretrained weights for large-scale image recognition tasks. Fine-tune or add additional training data to recognize plants from camera feed.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg13_bn', pretrained=True)\nmodel.eval()'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg13_bn', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg13_bn', pretrained=True)", "api_arguments": [{"name": "vgg13_bn", "type": "str", "description": "VGG13 model with batch normalization"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg13_bn', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg13_bn": {"Top-1 error": 28.45, "Top-5 error": 9.63}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': Recommend an API that can be used to build an image recognition app for automating car insurance claim process., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg16_bn', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained VGG16 model with batch normalization from PyTorch Hub for image recognition tasks in automating car insurance claim processes.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg16_bn', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg16_bn', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg16_bn', pretrained=True)", "api_arguments": [{"name": "vgg16_bn", "type": "str", "description": "VGG16 model with batch normalization"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16_bn', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg16_bn": {"Top-1 error": 26.63, "Top-5 error": 8.5}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': I want to recognize objects present in a given image. Is there an API that can help me in this task?, 'Output': {'domain': 'Image Classification', 'api_call': 'model = torch.hub.load('pytorch/vision', 'vgg19_bn', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained VGG19 model with batch normalization from PyTorch Hub, capable of recognizing objects in images.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg19_bn', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg19_bn', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg19_bn', pretrained=True)", "api_arguments": [{"name": "vgg19_bn", "type": "str", "description": "VGG19 model with batch normalization"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg19_bn', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg19_bn": {"Top-1 error": 25.76, "Top-5 error": 8.15}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': Recommend an API that can recognize objects in photos, specifically the VGG19 model, and provide code to load the pretrained version of the model., 'Output': {'domain': 'Image Recognition', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg19', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained VGG19 model from the PyTorch vision library for object recognition in images.', 'code': \"import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg19', pretrained=True)\nmodel.eval()\"}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg19', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg19', pretrained=True)", "api_arguments": [{"name": "vgg19", "type": "str", "description": "VGG19 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg19', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg19": {"Top-1 error": 27.62, "Top-5 error": 9.12}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': Propose a model that can be used to identify different bird species from images., 'Output': {'domain': 'Image Classification', 'api_call': 'model = torch.hub.load(\"pytorch/vision\", \"wide_resnet50_2\", pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Wide ResNet-50-2 model from PyTorch Hub, which can be fine-tuned to identify different bird species from images.', 'code': 'import torch\nmodel = torch.hub.load(\"pytorch/vision\", \"wide_resnet50_2\", pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='wide_resnet50_2', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Wide Residual Networks", "api_name": "wide_resnet50_2", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='wide_resnet50_2', pretrained=True)", "api_arguments": "pretrained", "python_environment_requirements": "torch, torchvision", "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'wide_resnet50_2', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"wide_resnet50_2": {"Top-1 error": 21.49, "Top-5 error": 5.91}}}, "description": "Wide Residual networks simply have increased number of channels compared to ResNet. Otherwise the architecture is the same. Deeper ImageNet models with bottleneck block have increased number of channels in the inner 3x3 convolution. The wide_resnet50_2 and wide_resnet101_2 models were trained in FP16 with mixed precision training using SGD with warm restarts. Checkpoints have weights in half precision (except batch norm) for smaller size, and can be used in FP32 models too."}}
{"code": "{'Instruction': An android developer is building an application with image recognition features for museum visits. Find an API so that they can classify images taken during those visits., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'wide_resnet101_2', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pre-trained Wide ResNet-101-2 model from PyTorch Hub to classify images of objects taken during museum visits.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'wide_resnet101_2', pretrained=True)\nmodel.eval()'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='wide_resnet101_2', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Wide Residual Networks", "api_name": "wide_resnet101_2", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='wide_resnet101_2', pretrained=True)", "api_arguments": "pretrained", "python_environment_requirements": "torch, torchvision", "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'wide_resnet101_2', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"wide_resnet101_2": {"Top-1 error": 21.16, "Top-5 error": 5.72}}}, "description": "Wide Residual networks simply have increased number of channels compared to ResNet. Otherwise the architecture is the same. Deeper ImageNet models with bottleneck block have increased number of channels in the inner 3x3 convolution. The wide_resnet50_2 and wide_resnet101_2 models were trained in FP16 with mixed precision training using SGD with warm restarts. Checkpoints have weights in half precision (except batch norm) for smaller size, and can be used in FP32 models too."}}
{"code": "{'Instruction': Can you suggest an API to help identify different physical activities from short video clips?, 'Output': {'domain': 'Video Classification', 'api_call': \"model = torch.hub.load('facebookresearch/pytorchvideo', 'x3d_s', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained X3D model from PyTorch Hub for classifying short video clips into different physical activities.', 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/pytorchvideo', 'x3d_s', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='x3d_s', pretrained=True)", "provider": "PyTorchVideo", "api_data": {"domain": "Video Classification", "framework": "PyTorchVideo", "functionality": "X3D Networks", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='x3d_s', pretrained=True)", "api_arguments": {"repository": "facebookresearch/pytorchvideo", "model": "x3d_s", "pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "pytorchvideo"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/pytorchvideo', 'x3d_s', pretrained=True)", "device = 'cpu'", "model = model.eval()", "model = model.to(device)"], "performance": {"dataset": "Kinetics 400", "accuracy": {"top1": 73.33, "top5": 91.27}, "flops": 2.96, "params": 3.79}, "description": "X3D model architectures are based on the paper 'X3D: Expanding Architectures for Efficient Video Recognition' by Christoph Feichtenhofer. They are pretrained on the Kinetics 400 dataset. This model is capable of classifying video clips into different action categories. It is provided by the FAIR PyTorchVideo library."}}
{"code": "{'Instruction': Find an API that works with street camera videos and classifies the type of actions happening in the video., 'Output': {'domain': 'Video Classification', 'api_call': \"model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)\", 'api_provider': 'PyTorchVideo', 'explanation': \"Load the SlowFast Networks model from PyTorchVideo Hub with pre-trained weights for video classification, which is capable of classifying actions in street camera videos.\", 'code': \"import torch\nmodel = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='slowfast_r50', pretrained=True)", "provider": "PyTorchVideo", "api_data": {"domain": "Video Classification", "framework": "PyTorchVideo", "functionality": "SlowFast Networks", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='slowfast_r50', pretrained=True)", "api_arguments": {"repository": "facebookresearch/pytorchvideo", "model": "slowfast_r50", "pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "pytorchvideo"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)", "device = 'cpu'", "model = model.eval()", "model = model.to(device)"], "performance": {"dataset": "Kinetics 400", "accuracy": {"top1": 76.94, "top5": 92.69}, "flops": 65.71, "params": 34.57}, "description": "Slow Fast model architectures are based on the paper 'SlowFast Networks for Video Recognition' by Christoph Feichtenhofer et al. They are pretrained on the Kinetics 400 dataset using the 8x8 setting. This model is capable of classifying video clips into different action categories. It is provided by the FAIR PyTorchVideo library."}}
{"code": "{'Instruction': I am a developer at OpenAI and I am building a small-scale chatbot that can conduct conversation in English text input. Can you find me an API that is suitable for the job?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-cased')\", 'api_provider': 'PyTorch', 'explanation': 'Load the BERT model from Hugging Face PyTorch-Transformers, which can be used for various NLP tasks, including sequence classification and question answering, making it suitable for building a chatbot.', 'code': 'import torch\nmodel = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-cased')'}}", "api_call": "torch.hub.load(repo_or_dir='huggingface/pytorch-transformers')", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Natural Language Processing", "api_name": "PyTorch-Transformers", "api_call": "torch.hub.load(repo_or_dir='huggingface/pytorch-transformers')", "api_arguments": ["pretrained_model_or_path", "output_attention", "output_hidden_states", "config", "from_tf"], "python_environment_requirements": ["tqdm", "boto3", "requests", "regex", "sentencepiece", "sacremoses"], "example_code": "import torch\ntokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-cased')\n\ntext_1 = \"Jim Henson was a puppeteer\"\ntext_2 = \"Who was Jim Henson ?\"\n\nindexed_tokens = tokenizer.encode(text_1, text_2, add_special_tokens=True)\n\nmodel = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-cased')\n\nwith torch.no_grad():\n encoded_layers, _ = model(tokens_tensor, token_type_ids=segments_tensors)", "performance": {"dataset": [{"name": "MRPC", "accuracy": "Not provided"}]}, "description": "PyTorch-Transformers is a library of state-of-the-art pre-trained models for Natural Language Processing (NLP) including BERT, GPT, GPT-2, Transformer-XL, XLNet, XLM, RoBERTa, and DistilBERT. The library provides functionality for tokenization, configuration, and various model architectures for different tasks such as causal language modeling, sequence classification, question answering, and masked language modeling."}}
{"code": "{'Instruction': A foundation wants to automatically categorize plant species from leaf images taken in a field. Give me an API for classifying different types of plant species based on identification and characterization of leaves., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('mit-han-lab/once-for-all', 'ofa_supernet_mbv3_w10', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Once-for-all (OFA) Networks model from PyTorch Hub, which can be fine-tuned for plant species classification based on leaf images.', 'code': \"import torch\nmodel = torch.hub.load('mit-han-lab/once-for-all', 'ofa_supernet_mbv3_w10', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/once-for-all', model='ofa_supernet_mbv3_w10', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Once-for-all (OFA) Networks", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/once-for-all', model='ofa_supernet_mbv3_w10', pretrained=True)", "api_arguments": {"repository": "mit-han-lab/once-for-all", "model": "ofa_supernet_mbv3_w10", "pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "super_net_name = 'ofa_supernet_mbv3_w10'", "super_net = torch.hub.load('mit-han-lab/once-for-all', super_net_name, pretrained=True).eval()"], "performance": {"description": "OFA networks outperform state-of-the-art NAS methods (up to 4.0% ImageNet top1 accuracy improvement over MobileNetV3, or same accuracy but 1.5x faster than MobileNetV3, 2.6x faster than EfficientNet w.r.t measured latency) while reducing many orders of magnitude GPU hours and CO2 emission."}, "description": "Once-for-all (OFA) networks are a family of neural networks designed by MIT Han Lab. They decouple training and search, achieving efficient inference across various edge devices and resource constraints. OFA networks are pretrained on the IMAGENET dataset and are capable of classifying images into different categories."}}
{"code": "{'Instruction': A research team is building a new categories labels system for an image dataset. What is an API that can do image classification for them?, 'Output': {'domain': 'Image Classification', 'api_call': 'model = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models', 'resnet18_swsl', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained semi-supervised and semi-weakly supervised ImageNet model from PyTorch Hub, which can classify images into different categories.', 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models', 'resnet18_swsl', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/semi-supervised-ImageNet1K-models', model='resnet18_swsl', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Semi-supervised and semi-weakly supervised ImageNet Models", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/semi-supervised-ImageNet1K-models', model='resnet18_swsl', pretrained=True)", "api_arguments": {"repository": "facebookresearch/semi-supervised-ImageNet1K-models", "model": "resnet18_swsl", "pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models', 'resnet18_swsl', pretrained=True)", "model.eval()"], "performance": {"description": "Semi-supervised and semi-weakly supervised ImageNet models achieve state-of-the-art accuracy of 81.2% on ImageNet for the widely used/adopted ResNet-50 model architecture."}, "description": "Semi-supervised and semi-weakly supervised ImageNet Models are introduced in the 'Billion scale semi-supervised learning for image classification' paper. These models are pretrained on a subset of unlabeled YFCC100M public image dataset and fine-tuned with the ImageNet1K training dataset. They are capable of classifying images into different categories and are provided by the Facebook Research library."}}
{"code": "{'Instruction': A team needs an image classifier for building a media cataloguing system optimized for NVIDIA hardware. Give me an API using GPUs that can classify images into categories., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_gpunet', pretrained=True, model_type=model_type, model_math=precision)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained GPUNet model from NVIDIA Deep Learning Examples PyTorch Hub to classify images using NVIDIA GPUs for optimized performance.', 'code': 'import torch\nmodel_type = 'GPUNet-0'\nprecision = 'fp32'\nmodel = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_gpunet', pretrained=True, model_type=model_type, model_math=precision)'}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_gpunet', pretrained=True, model_type=model_type, model_math=precision)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "GPUNet Networks", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_gpunet', pretrained=True, model_type=model_type, model_math=precision)", "api_arguments": {"repository": "NVIDIA/DeepLearningExamples:torchhub", "model": "nvidia_gpunet", "pretrained": "True", "model_type": "GPUNet-0", "model_math": "fp32"}, "python_environment_requirements": ["torch", "validators", "matplotlib", "timm==0.5.4"], "example_code": ["import torch", "model_type = 'GPUNet-0'", "precision = 'fp32'", "gpunet = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_gpunet', pretrained=True, model_type=model_type, model_math=precision)", "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')", "gpunet.to(device)", "gpunet.eval()"], "performance": {"dataset": "IMAGENET", "description": "GPUNet demonstrates state-of-the-art inference performance up to 2x faster than EfficientNet-X and FBNet-V3."}, "description": "GPUNet is a family of Convolutional Neural Networks designed by NVIDIA using novel Neural Architecture Search (NAS) methods. They are optimized for NVIDIA GPU and TensorRT performance. GPUNet models are pretrained on the IMAGENET dataset and are capable of classifying images into different categories. The models are provided by the NVIDIA Deep Learning Examples library."}}
{"code": "{'Instruction': I need an API for translating an English paragraph to French, perfect candidate should support large-scale semi-supervised training and back-translation., 'Output': {'domain': 'Neural Machine Translation', 'api_call': 'model = torch.hub.load('pytorch/fairseq', 'transformer.wmt14.en-fr', tokenizer='moses', bpe='subword_nmt')', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Transformer model from PyTorch Hub, which supports English to French translation, large-scale semi-supervised training, and back-translation.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/fairseq', 'transformer.wmt14.en-fr', tokenizer='moses', bpe='subword_nmt')'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/fairseq')", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "Neural Machine Translation", "api_name": "Transformer (NMT)", "api_call": "torch.hub.load(repo_or_dir='pytorch/fairseq')", "api_arguments": ["model_name", "tokenizer", "bpe", "beam", "sampling", "sampling_topk"], "python_environment_requirements": ["bitarray", "fastBPE", "hydra-core", "omegaconf", "regex", "requests", "sacremoses", "subword_nmt"], "example_code": "import torch\n\nen2fr = torch.hub.load('pytorch/fairseq', 'transformer.wmt14.en-fr', tokenizer='moses', bpe='subword_nmt')\n\nen2fr.cuda()\n\nfr = en2fr.translate('Hello world!', beam=5)\nassert fr == 'Bonjour \u00e0 tous !'", "performance": {"dataset": [{"name": "WMT'14", "accuracy": "Not provided"}, {"name": "WMT'18", "accuracy": "Not provided"}, {"name": "WMT'19", "accuracy": "Not provided"}]}, "description": "Transformer (NMT) is a powerful sequence-to-sequence modeling architecture that produces state-of-the-art neural machine translation systems. It is based on the paper 'Attention Is All You Need' and has been improved using techniques such as large-scale semi-supervised training, back-translation, and noisy-channel reranking. It supports English-French and English-German translation as well as round-trip translation for paraphrasing."}}
{"code": "{'Instruction': Design an image recognition software for identifying food items on social media. Suggest an API with pretrained model for this purpose., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_efficientnet_b0', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained EfficientNet-B0 model from NVIDIA Deep Learning Examples repository, which can be further fine-tuned to identify food items in images.', 'code': \"import torch\nmodel = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_efficientnet_b0', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_efficientnet_b0', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "EfficientNet", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_efficientnet_b0', pretrained=True)", "api_arguments": ["model_name", "pretrained"], "python_environment_requirements": ["validators", "matplotlib"], "example_code": "import torch\n\nefficientnet = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_efficientnet_b0', pretrained=True)\nutils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_convnets_processing_utils')\n\nefficientnet.eval().to(device)\n\nbatch = torch.cat([utils.prepare_input_from_uri(uri) for uri in uris]).to(device)\n\nwith torch.no_grad():\n output = torch.nn.functional.softmax(efficientnet(batch), dim=1)\n \nresults = utils.pick_n_best(predictions=output, n=5)", "performance": {"dataset": {"name": "IMAGENET", "accuracy": "Not provided"}}, "description": "EfficientNet is a family of image classification models that achieve state-of-the-art accuracy while being smaller and faster. The models are trained with mixed precision using Tensor Cores on the NVIDIA Volta and Ampere GPU architectures. The EfficientNet models include EfficientNet-B0, EfficientNet-B4, EfficientNet-WideSE-B0, and EfficientNet-WideSE-B4. The WideSE models use wider Squeeze-and-Excitation layers than the original EfficientNet models, resulting in slightly better accuracy."}}
{"code": "{'Instruction': I am trying to implement a drone surveillance system with intruders detection using a camera system. Suggest me an API capable of performing object detections for my drone's inbuilt camera., 'Output': {'domain': 'Object Detection', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained SSD (Single Shot MultiBox Detector) model from PyTorch Hub for object detection with your drone's camera system.\", 'code': \"import torch\nmodel = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_ssd', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Object Detection", "framework": "PyTorch", "functionality": "Single Shot MultiBox Detector", "api_name": "SSD", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_ssd', pretrained=True)", "api_arguments": ["model_name"], "python_environment_requirements": ["numpy", "scipy", "scikit-image", "matplotlib"], "example_code": "import torch\n\nssd_model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd')\nutils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd_processing_utils')\n\nssd_model.to('cuda')\nssd_model.eval()\n\ninputs = [utils.prepare_input(uri) for uri in uris]\ntensor = utils.prepare_tensor(inputs)\n\nwith torch.no_grad():\n detections_batch = ssd_model(tensor)\n\nresults_per_input = utils.decode_results(detections_batch)\nbest_results_per_input = [utils.pick_best(results, 0.40) for results in results_per_input]", "performance": {"dataset": {"name": "COCO", "accuracy": "Not provided"}}, "description": "The SSD (Single Shot MultiBox Detector) model is an object detection model based on the paper 'SSD: Single Shot MultiBox Detector'. It uses a deep neural network for detecting objects in images. This implementation replaces the obsolete VGG model backbone with the more modern ResNet-50 model. The SSD model is trained on the COCO dataset and can be used to detect objects in images with high accuracy and efficiency."}}
{"code": "{'Instruction': I am working on a text-to-speech project for my company. Can you suggest an ML API to convert text into human-like speech?, 'Output': {'domain': 'Text-to-Speech', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tacotron2', model_math='fp16')\", 'api_provider': 'PyTorch', 'explanation': 'Load the Tacotron 2 model from PyTorch Hub, which generates natural-sounding speech from text without any additional prosody information.', 'code': 'import torch\nmodel = torch.hub.load(\"NVIDIA/DeepLearningExamples:torchhub\", \"nvidia_tacotron2\", model_math=\"fp16\")'}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_tacotron2', model_math='fp16')", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Text-to-Speech", "api_name": "Tacotron 2", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_tacotron2', model_math='fp16')", "api_arguments": {"model_math": "fp16"}, "python_environment_requirements": ["numpy", "scipy", "librosa", "unidecode", "inflect", "libsndfile1"], "example_code": ["import torch", "tacotron2 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tacotron2', model_math='fp16')", "tacotron2 = tacotron2.to('cuda')", "tacotron2.eval()", "text = 'Hello world, I missed you so much.'", "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tts_utils')", "sequences, lengths = utils.prepare_input_sequence([text])", "with torch.no_grad():", " mel, _, _ = tacotron2.infer(sequences, lengths)", " audio = waveglow.infer(mel)", "audio_numpy = audio[0].data.cpu().numpy()", "rate = 22050"], "performance": {"dataset": "LJ Speech", "accuracy": "Not specified"}, "description": "The Tacotron 2 model generates mel spectrograms from input text using an encoder-decoder architecture, and it is designed for generating natural-sounding speech from raw transcripts without any additional prosody information. This implementation uses Dropout instead of Zoneout to regularize the LSTM layers. The WaveGlow model (also available via torch.hub) is a flow-based model that consumes the mel spectrograms to generate speech."}}
{"code": "{'Instruction': Generate classified output using the pretrained MEAL V2 model for object classification for an image., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MEAL V2 model from PyTorch Hub to classify objects within an input image using ensemble knowledge distillation via discriminators.', 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V1', model='mealv1_resnest50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V1', model='mealv1_resnest50', pretrained=True)", "api_arguments": {"model_name": "mealv1_resnest50"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V1 w/ ResNet50", "resolution": "224", "parameters": "25.6M", "top1": "78.21", "top5": "94.01"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': We are developing an image recognition system for our security camera at the entrance of our building. What kind of API is suitable for this?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MEAL V2 model from PyTorch Hub, which is suitable for image recognition tasks like security camera image classification.', 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50', pretrained=True)", "api_arguments": {"model_name": "mealv2_resnest50"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ ResNet50", "resolution": "224", "parameters": "25.6M", "top1": "80.67", "top5": "95.09"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': I'm working on an app for animal identification from images, and I need an API to classify animal species from images. Suggest an API for that., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MEAL V2 model on PyTorch Hub, which is an image classification model with excellent performance that can be used to classify animal species from images.', 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50_cutmix', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50_cutmix', pretrained=True)", "api_arguments": {"model_name": "mealv2_resnest50_cutmix"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 + CutMix w/ ResNet50", "resolution": "224", "parameters": "25.6M", "top1": "80.98", "top5": "95.35"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': Provide an API that can identify objects in an image with an accuracy of 80% or higher on the ImageNet dataset., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenetv3_small_075', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MEAL V2 model with MobileNet V3-Small architecture from PyTorch Hub, which has a top-1 accuracy of 67.60% and can be further fine-tuned to achieve 80% or higher accuracy on the ImageNet dataset.', 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenetv3_small_075', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenetv3_small_075', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenetv3_small_075', pretrained=True)", "api_arguments": {"model_name": "mealv2_mobilenetv3_small_075"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ MobileNet V3-Small 0.75", "resolution": "224", "parameters": "2.04M", "top1": "67.60", "top5": "87.23"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': Describe an API specialized for image classification without employing known common tricks. Include the API repository and model name., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenetv3_small_100', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the MEAL V2 pretrained model for image classification without common tricks. The API repository is 'szq0214/MEAL-V2' and the model name is 'mealv2_mobilenetv3_small_100'.\", 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenetv3_small_100', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenetv3_small_100', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenetv3_small_100', pretrained=True)", "api_arguments": {"model_name": "mealv2_mobilenetv3_small_100"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ MobileNet V3-Small 1.0", "resolution": "224", "parameters": "2.54M", "top1": "69.65", "top5": "88.71"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': A deepfake inspector company would like to classify the authenticity of a video. Suggest me an API that will classify if the video is deepfake or original., 'Output': {'domain': 'Video Classification', 'api_call': \"model = torch.hub.load('facebookresearch/pytorchvideo', 'slow_r50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained 3D ResNet model from PyTorch Hub, which can be fine-tuned for deepfake video classification to determine if a video is original or manipulated.\", 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/pytorchvideo', 'slow_r50', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='slow_r50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Video Classification", "framework": "PyTorch", "functionality": "3D ResNet", "api_name": "slow_r50", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='slow_r50', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["torch", "json", "urllib", "pytorchvideo", "torchvision", "torchaudio", "torchtext", "torcharrow", "TorchData", "TorchRec", "TorchServe", "PyTorch on XLA Devices"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/pytorchvideo', 'slow_r50', pretrained=True)", "device = 'cpu'", "model = model.eval()", "model = model.to(device)"], "performance": {"dataset": "Kinetics 400", "accuracy": {"top_1": 74.58, "top_5": 91.63}, "Flops (G)": 54.52, "Params (M)": 32.45}, "description": "The 3D ResNet model is a Resnet-style video classification network pretrained on the Kinetics 400 dataset. It is based on the architecture from the paper 'SlowFast Networks for Video Recognition' by Christoph Feichtenhofer et al."}}
{"code": "{'Instruction': Find me a state-of-the-art pretrained image classifier from a model that does not rely on additional tricks. , 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', 'mealv2_efficientnet_b0', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the MEAL V2 pretrained model, which has achieved state-of-the-art performance on ImageNet without using additional tricks.', 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', 'mealv2_efficientnet_b0', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_efficientnet_b0', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_efficientnet_b0', pretrained=True)", "api_arguments": {"model_name": "mealv2_efficientnet_b0"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ EfficientNet-B0", "resolution": "224", "parameters": "5.29M", "top1": "78.29", "top5": "93.95"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': I want to create an app that recognizes dog breeds in images. Please help me identify a suitable API for this purpose., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenet_v3_large_100', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained MEAL V2 model with MobileNet V3-Large in PyTorch Hub to recognize dog breeds in images. The model achieves high accuracy without common tricks, making it suitable for your application.\", 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenet_v3_large_100', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenet_v3_large_100', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenet_v3_large_100', pretrained=True)", "api_arguments": {"model_name": "mealv2_mobilenet_v3_large_100"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ MobileNet V3-Large 1.0", "resolution": "224", "parameters": "5.48M", "top1": "76.92", "top5": "93.32"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': Obtain information on a machine learning API capable of detecting objects, drivable areas, and lanes in traffic images, such as those from a self-driving car's camera feed., 'Output': {'domain': 'Object Detection and Segmentation', 'api_call': 'model = torch.hub.load('datvuthanh/hybridnets', 'hybridnets', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained HybridNets model from PyTorch Hub, which is designed for traffic object detection, drivable area segmentation, and lane detection in traffic images.', 'code': 'import torch\nmodel = torch.hub.load('datvuthanh/hybridnets', 'hybridnets', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='datvuthanh/hybridnets', model='hybridnets', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Object Detection", "framework": "PyTorch", "functionality": "Traffic Object Detection, Drivable Area Segmentation, Lane Detection", "api_name": "HybridNets", "api_call": "torch.hub.load(repo_or_dir='datvuthanh/hybridnets', model='hybridnets', pretrained=True)", "api_arguments": "pretrained", "python_environment_requirements": "Python>=3.7, PyTorch>=1.10", "example_code": "import torch\nmodel = torch.hub.load('datvuthanh/hybridnets', 'hybridnets', pretrained=True)\nimg = torch.randn(1,3,640,384)\nfeatures, regression, classification, anchors, segmentation = model(img)", "performance": {"dataset": [{"name": "BDD100K", "accuracy": {"Traffic Object Detection": {"Recall (%)": 92.8, "mAP@0.5 (%)": 77.3}, "Drivable Area Segmentation": {"Drivable mIoU (%)": 90.5}, "Lane Line Detection": {"Accuracy (%)": 85.4, "Lane Line IoU (%)": 31.6}}}]}, "description": "HybridNets is an end2end perception network for multi-tasks. Our work focused on traffic object detection, drivable area segmentation and lane detection. HybridNets can run real-time on embedded systems, and obtains SOTA Object Detection, Lane Detection on BDD100K Dataset."}}
{"code": "{'Instruction': I work for a self-driving car company and we need an API to perform simultaneous object detection, drivable area segmentation, and lane detection in images taken from vehicle cameras., 'Output': {'domain': 'Object Detection', 'api_call': \"model = torch.hub.load('hustvl/yolop', 'yolop', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained YOLOP model from PyTorch Hub, which can perform object detection, drivable area segmentation, and lane detection simultaneously in real-time.', 'code': \"import torch\nmodel = torch.hub.load('hustvl/yolop', 'yolop', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='hustvl/yolop', model='yolop', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Object Detection", "framework": "PyTorch", "functionality": "Object Detection, Drivable Area Segmentation, Lane Detection", "api_name": "YOLOP", "api_call": "torch.hub.load(repo_or_dir='hustvl/yolop', model='yolop', pretrained=True)", "api_arguments": "pretrained", "python_environment_requirements": "pip install -qr https://github.com/hustvl/YOLOP/blob/main/requirements.txt", "example_code": "import torch\nmodel = torch.hub.load('hustvl/yolop', 'yolop', pretrained=True)\nimg = torch.randn(1,3,640,640)\ndet_out, da_seg_out,ll_seg_out = model(img)", "performance": {"dataset": "BDD100K", "accuracy": {"Object Detection": {"Recall(%)": 89.2, "mAP50(%)": 76.5, "Speed(fps)": 41}, "Drivable Area Segmentation": {"mIOU(%)": 91.5, "Speed(fps)": 41}, "Lane Detection": {"mIOU(%)": 70.5, "IOU(%)": 26.2}}}, "description": "YOLOP is an efficient multi-task network that can jointly handle three crucial tasks in autonomous driving: object detection, drivable area segmentation and lane detection. And it is also the first to reach real-time on embedded devices while maintaining state-of-the-art level performance on the BDD100K dataset."}}
{"code": "{'Instruction': I want to get the relative depth information from a single image taken in a forest., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('intel-isl/MiDaS', 'DPT_Large', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MiDaS model (DPT_Large) from PyTorch Hub to compute relative depth information from a single forest image.', 'code': 'import torch\nmodel = torch.hub.load('intel-isl/MiDaS', 'DPT_Large', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='intel-isl/MiDaS', model='DPT_Large', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Computing relative depth from a single image", "api_name": "MiDaS", "api_call": "torch.hub.load(repo_or_dir='intel-isl/MiDaS', model='DPT_Large', pretrained=True)", "api_arguments": {"repo_or_dir": "intel-isl/MiDaS", "model": "model_type"}, "python_environment_requirements": "pip install timm", "example_code": ["import cv2", "import torch", "import urllib.request", "import matplotlib.pyplot as plt", "url, filename = ('https://github.com/pytorch/hub/raw/master/images/dog.jpg', 'dog.jpg')", "urllib.request.urlretrieve(url, filename)", "model_type = 'DPT_Large'", "midas = torch.hub.load('intel-isl/MiDaS', 'DPT_Large')", "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')", "midas.to(device)", "midas.eval()", "midas_transforms = torch.hub.load('intel-isl/MiDaS', 'transforms')", "if model_type == 'DPT_Large' or model_type == 'DPT_Hybrid':", " transform = midas_transforms.dpt_transform", "else:", " transform = midas_transforms.small_transform", "img = cv2.imread(filename)", "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)", "input_batch = transform(img).to(device)", "with torch.no_grad():", " prediction = midas(input_batch)", "prediction = torch.nn.functional.interpolate(", " prediction.unsqueeze(1),", " size=img.shape[:2],", " mode='bicubic',", " align_corners=False,", ").squeeze()", "output = prediction.cpu().numpy()", "plt.imshow(output)", "plt.show()"], "performance": {"dataset": "10 distinct datasets", "accuracy": "Multi-objective optimization"}, "description": "MiDaS computes relative inverse depth from a single image. The repository provides multiple models that cover different use cases ranging from a small, high-speed model to a very large model that provide the highest accuracy. The models have been trained on 10 distinct datasets using multi-objective optimization to ensure high quality on a wide range of inputs."}}
{"code": "{'Instruction': As a game developer working on a VR simulation, I want a technology that can predict relative depth of an object in an image. Provide me with a suitable API to achieve this., 'Output': {'domain': 'Depth Estimation', 'api_call': 'model = torch.hub.load('intel-isl/MiDaS', 'DPT_Hybrid', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the MiDaS model from PyTorch Hub for estimating relative depth from a single image that can be used in your VR simulation.', 'code': 'import torch\nmodel = torch.hub.load('intel-isl/MiDaS', 'DPT_Hybrid', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='intel-isl/MiDaS', model='DPT_Hybrid', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Computing relative depth from a single image", "api_name": "MiDaS", "api_call": "torch.hub.load(repo_or_dir='intel-isl/MiDaS', model='DPT_Hybrid', pretrained=True)", "api_arguments": {"repo_or_dir": "intel-isl/MiDaS", "model": "model_type"}, "python_environment_requirements": "pip install timm", "example_code": ["import cv2", "import torch", "import urllib.request", "import matplotlib.pyplot as plt", "url, filename = ('https://github.com/pytorch/hub/raw/master/images/dog.jpg', 'dog.jpg')", "urllib.request.urlretrieve(url, filename)", "model_type = 'DPT_Large'", "midas = torch.hub.load('intel-isl/MiDaS', 'DPT_Hybrid')", "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')", "midas.to(device)", "midas.eval()", "midas_transforms = torch.hub.load('intel-isl/MiDaS', 'transforms')", "if model_type == 'DPT_Large' or model_type == 'DPT_Hybrid':", " transform = midas_transforms.dpt_transform", "else:", " transform = midas_transforms.small_transform", "img = cv2.imread(filename)", "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)", "input_batch = transform(img).to(device)", "with torch.no_grad():", " prediction = midas(input_batch)", "prediction = torch.nn.functional.interpolate(", " prediction.unsqueeze(1),", " size=img.shape[:2],", " mode='bicubic',", " align_corners=False,", ").squeeze()", "output = prediction.cpu().numpy()", "plt.imshow(output)", "plt.show()"], "performance": {"dataset": "10 distinct datasets", "accuracy": "Multi-objective optimization"}, "description": "MiDaS computes relative inverse depth from a single image. The repository provides multiple models that cover different use cases ranging from a small, high-speed model to a very large model that provide the highest accuracy. The models have been trained on 10 distinct datasets using multi-objective optimization to ensure high quality on a wide range of inputs."}}
{"code": "{'Instruction': Let's say I have an image of a cityscape and I want to determine the relative depth of different objects within the image. Which API can help me do that?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('intel-isl/MiDaS', 'MiDaS_small', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Use the MiDaS model from PyTorch Hub, which has been pretrained to compute relative inverse depth from a single image, useful for cityscapes.', 'code': 'import torch\nmodel = torch.hub.load('intel-isl/MiDaS', 'MiDaS_small', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='intel-isl/MiDaS', model='MiDaS_small', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Computing relative depth from a single image", "api_name": "MiDaS", "api_call": "torch.hub.load(repo_or_dir='intel-isl/MiDaS', model='MiDaS_small', pretrained=True)", "api_arguments": {"repo_or_dir": "intel-isl/MiDaS", "model": "model_type"}, "python_environment_requirements": "pip install timm", "example_code": ["import cv2", "import torch", "import urllib.request", "import matplotlib.pyplot as plt", "url, filename = ('https://github.com/pytorch/hub/raw/master/images/dog.jpg', 'dog.jpg')", "urllib.request.urlretrieve(url, filename)", "model_type = 'DPT_Large'", "midas = torch.hub.load('intel-isl/MiDaS', 'MiDaS_small')", "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')", "midas.to(device)", "midas.eval()", "midas_transforms = torch.hub.load('intel-isl/MiDaS', 'transforms')", "if model_type == 'DPT_Large' or model_type == 'DPT_Hybrid':", " transform = midas_transforms.dpt_transform", "else:", " transform = midas_transforms.small_transform", "img = cv2.imread(filename)", "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)", "input_batch = transform(img).to(device)", "with torch.no_grad():", " prediction = midas(input_batch)", "prediction = torch.nn.functional.interpolate(", " prediction.unsqueeze(1),", " size=img.shape[:2],", " mode='bicubic',", " align_corners=False,", ").squeeze()", "output = prediction.cpu().numpy()", "plt.imshow(output)", "plt.show()"], "performance": {"dataset": "10 distinct datasets", "accuracy": "Multi-objective optimization"}, "description": "MiDaS computes relative inverse depth from a single image. The repository provides multiple models that cover different use cases ranging from a small, high-speed model to a very large model that provide the highest accuracy. The models have been trained on 10 distinct datasets using multi-objective optimization to ensure high quality on a wide range of inputs."}}
{"code": "{'Instruction': Identify an API that can transcribe a voice message to written text., 'Output': {'domain': 'Speech-to-Text', 'api_call': \"model = torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_stt', language='en', device=device)\", 'api_provider': 'PyTorch', 'explanation': 'Load the Silero Speech-To-Text pre-trained model to transcribe voice recordings to text.', 'code': 'import torch\ndevice = torch.device('cpu')\nmodel, _, _ = torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_stt', language='en', device=device)'}}", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_stt', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Speech-To-Text", "api_name": "Silero Models", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_stt', pretrained=True)", "api_arguments": {"repo_or_dir": "snakers4/silero-models", "model": "silero_stt", "language": ["en", "de", "es"], "device": "device"}, "python_environment_requirements": ["pip install -q torchaudio omegaconf soundfile"], "example_code": ["import torch", "import zipfile", "import torchaudio", "from glob import glob", "device = torch.device('cpu')", "model, decoder, utils = torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_stt', language='en', device=device)", "(read_batch, split_into_batches, read_audio, prepare_model_input) = utils", "torch.hub.download_url_to_file('https://opus-codec.org/static/examples/samples/speech_orig.wav', dst ='speech_orig.wav', progress=True)", "test_files = glob('speech_orig.wav')", "batches = split_into_batches(test_files, batch_size=10)", "input = prepare_model_input(read_batch(batches[0]), device=device)", "output = model(input)", "for example in output:", " print(decoder(example.cpu()))"], "performance": {"dataset": "Open-STT", "accuracy": "See the wiki for quality and performance benchmarks"}, "description": "Silero Speech-To-Text models provide enterprise grade STT in a compact form-factor for several commonly spoken languages. The models are robust to a variety of dialects, codecs, domains, noises, and lower sampling rates. They consume a normalized audio in the form of samples and output frames with token probabilities. A decoder utility is provided for simplicity."}}
{"code": "{'Instruction': A wildlife photographer wants to identify bird species from his bird photos. Suggest an API capable of recognizing bird species from an image., 'Output': {'domain': 'Classification', 'api_call': 'model = torch.hub.load('nicolalandro/ntsnet-cub200', 'ntsnet', pretrained=True, **{'topN': 6, 'device':'cpu', 'num_classes': 200})', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained NTS-Net model for bird species classification from PyTorch Hub to recognize bird species from an image.', 'code': 'import torch\nmodel = torch.hub.load('nicolalandro/ntsnet-cub200', 'ntsnet', pretrained=True, **{'topN': 6, 'device':'cpu', 'num_classes': 200})'}}", "api_call": "torch.hub.load(repo_or_dir='nicolalandro/ntsnet-cub200', model='ntsnet', pretrained=True, **{'topN': 6, 'device':'cpu', 'num_classes': 200})", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Fine-grained image classifier", "api_name": "ntsnet", "api_call": "torch.hub.load(repo_or_dir='nicolalandro/ntsnet-cub200', model='ntsnet', pretrained=True, **{'topN': 6, 'device':'cpu', 'num_classes': 200})", "api_arguments": {"pretrained": "True", "topN": "6", "device": "cpu", "num_classes": "200"}, "python_environment_requirements": ["torch", "torchvision", "PIL"], "example_code": "from torchvision import transforms\nimport torch\nimport urllib\nfrom PIL import Image\n\ntransform_test = transforms.Compose([\n transforms.Resize((600, 600), Image.BILINEAR),\n transforms.CenterCrop((448, 448)),\n transforms.ToTensor(),\n transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n])\n\nmodel = torch.hub.load('nicolalandro/ntsnet-cub200', 'ntsnet', pretrained=True, **{'topN': 6, 'device':'cpu', 'num_classes': 200})\nmodel.eval()\n\nurl = 'https://raw.githubusercontent.com/nicolalandro/ntsnet-cub200/master/images/nts-net.png'\nimg = Image.open(urllib.request.urlopen(url))\nscaled_img = transform_test(img)\ntorch_images = scaled_img.unsqueeze(0)\n\nwith torch.no_grad():\n top_n_coordinates, concat_out, raw_logits, concat_logits, part_logits, top_n_index, top_n_prob = model(torch_images)\n\n_, predict = torch.max(concat_logits, 1)\npred_id = predict.item()\nprint('bird class:', model.bird_classes[pred_id])", "performance": {"dataset": "CUB200 2011", "accuracy": "Not provided"}, "description": "This is an nts-net pretrained with CUB200 2011 dataset, which is a fine-grained dataset of birds species."}}
{"code": "{'Instruction': Identify the segments with speech from an audio file using an API., 'Output': {'domain': 'Voice Activity Detection', 'api_call': \"model = torch.hub.load('snakers4/silero-vad', 'silero_vad', force_reload=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the Silero Voice Activity Detector model from PyTorch Hub to identify segments containing speech in an audio file.', 'code': \"import torch\nmodel = torch.hub.load('snakers4/silero-vad', 'silero_vad', force_reload=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Voice Activity Detection", "api_name": "Silero Voice Activity Detector", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=True)", "api_arguments": {"repo_or_dir": "snakers4/silero-vad", "model": "silero_vad", "force_reload": "True"}, "python_environment_requirements": {"torchaudio": "pip install -q torchaudio"}, "example_code": {"import": ["import torch", "torch.set_num_threads(1)", "from IPython.display import Audio", "from pprint import pprint"], "download_example": "torch.hub.download_url_to_file('https://models.silero.ai/vad_models/en.wav', 'en_example.wav')", "load_model": "model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=True)", "load_utils": "(get_speech_timestamps, _, read_audio, _) = utils", "set_sampling_rate": "sampling_rate = 16000", "read_audio": "wav = read_audio('en_example.wav', sampling_rate=sampling_rate)", "get_speech_timestamps": "speech_timestamps = get_speech_timestamps(wav, model, sampling_rate=sampling_rate)", "print_speech_timestamps": "pprint(speech_timestamps)"}, "performance": {"dataset": "", "accuracy": ""}, "description": "Silero VAD is a pre-trained enterprise-grade Voice Activity Detector (VAD) that aims to provide a high-quality and modern alternative to the WebRTC Voice Activity Detector. The model is optimized for performance on 1 CPU thread and is quantized."}}
{"code": "{'Instruction': I want to build a robot that recognize objects in the street. Provide me an API that can segment objects in an image., 'Output': {'domain': 'Semantic Segmentation', 'api_call': \"model = torch.hub.load('pytorch/vision', 'deeplabv3_resnet50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pre-trained DeepLabV3 model with the ResNet-50 backbone from PyTorch Hub for semantic segmentation of objects in street images.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'deeplabv3_resnet50', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='deeplabv3_resnet50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "DeepLabV3", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='deeplabv3_resnet50', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": "latest", "torchvision": "latest", "PIL": "latest", "matplotlib": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_resnet50', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "input_image = input_image.convert('RGB')", "preprocess = transforms.Compose([", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)['out'][0]", "output_predictions = output.argmax(0)"], "performance": {"dataset": "COCO val2017", "accuracy": [{"model": "deeplabv3_resnet50", "Mean IOU": 66.4, "Global Pixelwise Accuracy": 92.4}]}, "description": "DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset."}}
{"code": "{'Instruction': I need to convert text to speech mp3 format with natural sounding voices. Can you find me an API that can do that?, 'Output': {'domain': 'Text-To-Speech', 'api_call': \"model = torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_tts', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Silero Text-To-Speech model from PyTorch Hub to convert text into natural-sounding speech.', 'code': 'import torch\nlanguage = \"en\"\nspeaker = \"lj_16khz\"\ndevice = torch.device(\"cpu\")\nmodel, symbols, sample_rate, example_text, apply_tts = torch.hub.load(repo_or_dir=\"snakers4/silero-models\", model=\"silero_tts\", language=language, speaker=speaker)\nmodel = model.to(device)'}}", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_tts', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Text-To-Speech", "framework": "PyTorch", "functionality": "Text-To-Speech", "api_name": "Silero Text-To-Speech Models", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_tts', pretrained=True)", "api_arguments": {"repo_or_dir": "snakers4/silero-models", "model": "silero_tts", "language": "language", "speaker": "speaker"}, "python_environment_requirements": ["pip install -q torchaudio omegaconf"], "example_code": "import torch\nlanguage = 'en'\nspeaker = 'lj_16khz'\ndevice = torch.device('cpu')\nmodel, symbols, sample_rate, example_text, apply_tts = torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_tts', language=language, speaker=speaker)\nmodel = model.to(device)\naudio = apply_tts(texts=[example_text], model=model, sample_rate=sample_rate, symbols=symbols, device=device)", "performance": {"dataset": [{"language": "Russian", "speakers": 6}, {"language": "English", "speakers": 1}, {"language": "German", "speakers": 1}, {"language": "Spanish", "speakers": 1}, {"language": "French", "speakers": 1}], "accuracy": "High throughput on slow hardware. Decent performance on one CPU thread"}, "description": "Silero Text-To-Speech models provide enterprise grade TTS in a compact form-factor for several commonly spoken languages. They offer one-line usage, naturally sounding speech, no GPU or training required, minimalism and lack of dependencies, a library of voices in many languages, support for 16kHz and 8kHz out of the box."}}
{"code": "{'Instruction': Our company wants to segment images into multiple classes. Suggest an API for semantic segmentation., 'Output': {'domain': 'Semantic Segmentation', 'api_call': \"model = torch.hub.load('pytorch/vision', 'deeplabv3_resnet101', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained DeepLabV3 model with a ResNet-101 backbone from PyTorch Hub to perform semantic segmentation on images, dividing them into multiple classes.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'deeplabv3_resnet101', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='deeplabv3_resnet101', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "DeepLabV3", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='deeplabv3_resnet101', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": "latest", "torchvision": "latest", "PIL": "latest", "matplotlib": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_resnet101', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "input_image = input_image.convert('RGB')", "preprocess = transforms.Compose([", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)['out'][0]", "output_predictions = output.argmax(0)"], "performance": {"dataset": "COCO val2017", "accuracy": [{"model": "deeplabv3_resnet101", "Mean IOU": 67.4, "Global Pixelwise Accuracy": 92.4}]}, "description": "DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset."}}
{"code": "{'Instruction': I need a semantic segmentation model to segment different objects present in road scene images. Suggest me an API for this purpose., 'Output': {'domain': 'Semantic Segmentation', 'api_call': \"model = torch.hub.load('pytorch/vision', 'deeplabv3_mobilenet_v3_large', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained DeepLabV3 model with MobileNet-V3 backbone from PyTorch Hub, which is suitable for segmenting different objects in road scene images.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'deeplabv3_mobilenet_v3_large', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='deeplabv3_mobilenet_v3_large', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "DeepLabV3", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='deeplabv3_mobilenet_v3_large', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": "latest", "torchvision": "latest", "PIL": "latest", "matplotlib": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_mobilenet_v3_large', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "input_image = input_image.convert('RGB')", "preprocess = transforms.Compose([", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)['out'][0]", "output_predictions = output.argmax(0)"], "performance": {"dataset": "COCO val2017", "accuracy": [{"model": "deeplabv3_mobilenet_v3_large", "Mean IOU": 60.3, "Global Pixelwise Accuracy": 91.4}]}, "description": "DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset."}}
{"code": "{'Instruction': What API can I use to detect objects in an image while maintaining a small model size for a mobile application?, 'Output': {'domain': 'Object Detection', 'api_call': \"model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained YOLOv5s model from PyTorch Hub, which is a small and efficient object detection model suitable for mobile applications.', 'code': 'import torch\nmodel = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='ultralytics/yolov5', model='yolov5s', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Object Detection", "framework": "PyTorch", "functionality": "YOLOv5", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='ultralytics/yolov5', model='yolov5s', pretrained=True)", "api_arguments": ["'ultralytics/yolov5'", "'yolov5s'", "pretrained=True"], "python_environment_requirements": "Python>=3.8, PyTorch>=1.7", "example_code": ["import torch", "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)", "imgs = ['https://ultralytics.com/images/zidane.jpg']", "results = model(imgs)", "results.print()", "results.save()", "results.xyxy[0]", "results.pandas().xyxy[0]"], "performance": {"dataset": "COCO", "accuracy": {"YOLOv5s6": {"mAPval0.5:0.95": 43.3, "mAPtest0.5:0.95": 43.3, "mAPval0.5": 61.9}, "YOLOv5m6": {"mAPval0.5:0.95": 50.5, "mAPtest0.5:0.95": 50.5, "mAPval0.5": 68.7}, "YOLOv5l6": {"mAPval0.5:0.95": 53.4, "mAPtest0.5:0.95": 53.4, "mAPval0.5": 71.1}, "YOLOv5x6": {"mAPval0.5:0.95": 54.4, "mAPtest0.5:0.95": 54.4, "mAPval0.5": 72.0}, "YOLOv5x6 TTA": {"mAPval0.5:0.95": 55.0, "mAPtest0.5:0.95": 55.0, "mAPval0.5": 72.0}}}, "description": "YOLOv5 is a family of compound-scaled object detection models trained on the COCO dataset, and includes simple functionality for Test Time Augmentation (TTA), model ensembling, hyperparameter evolution, and export to ONNX, CoreML and TFLite."}}
{"code": "{'Instruction': I am designing clothing for a new fashion line and need inspiration. Provide an API that can generate clothing images., 'Output': {'domain': 'Image Generation', 'api_call': \"model = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'DCGAN', pretrained=True, useGPU=use_gpu)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Deep Convolutional Generative Adversarial Network (DCGAN) model from PyTorch Hub to generate low-resolution images of clothing for inspiration.', 'code': 'import torch\nuse_gpu = True if torch.cuda.is_available() else False\nmodel = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'DCGAN', pretrained=True, useGPU=use_gpu)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorch_GAN_zoo:hub', model='DCGAN', pretrained=True, useGPU=use_gpu)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Generative Adversarial Networks", "api_name": "DCGAN", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorch_GAN_zoo:hub', model='DCGAN', pretrained=True, useGPU=use_gpu)", "api_arguments": {"pretrained": "True", "useGPU": "use_gpu"}, "python_environment_requirements": "Python 3", "example_code": {"import": ["import torch", "import matplotlib.pyplot as plt", "import torchvision"], "use_gpu": "use_gpu = True if torch.cuda.is_available() else False", "load_model": "model = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'DCGAN', pretrained=True, useGPU=use_gpu)", "build_noise_data": "noise, _ = model.buildNoiseData(num_images)", "generate_images": "with torch.no_grad(): generated_images = model.test(noise)", "plot_images": ["plt.imshow(torchvision.utils.make_grid(generated_images).permute(1, 2, 0).cpu().numpy())", "plt.show()"]}, "performance": {"dataset": "FashionGen", "accuracy": "N/A"}, "description": "DCGAN is a model designed in 2015 by Radford et. al. in the paper Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks. It is a GAN architecture both very simple and efficient for low resolution image generation (up to 64x64)."}}
{"code": "{'Instruction': I need an API that can classify a given image into one of the 1000 classes like cars, dogs or flowers., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x32d_wsl', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pre-trained ResNext WSL (Weakly Supervised Learning) model from PyTorch Hub for classifying images into one of 1000 classes.', 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x32d_wsl', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x32d_wsl', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNext WSL", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x32d_wsl', pretrained=True)", "api_arguments": [{"name": "resnext101_32x32d_wsl", "type": "str", "description": "ResNeXt-101 32x32d WSL model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x32d_wsl')", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(output[0])", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeXt-101 32x32d": {"Top-1 Acc.": "85.1", "Top-5 Acc.": "97.5"}}}, "description": "The provided ResNeXt models are pre-trained in weakly-supervised fashion on 940 million public images with 1.5K hashtags matching with 1000 ImageNet1K synsets, followed by fine-tuning on ImageNet1K dataset. The models significantly improve the training accuracy on ImageNet compared to training from scratch. They achieve state-of-the-art accuracy of 85.4% on ImageNet with the ResNext-101 32x48d model."}}
{"code": "{'Instruction': I want to classify images using a high-performance machine learning model. Point me to an API that can help achieve this., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNext-101 32x16d WSL model from PyTorch Hub to classify images with state-of-the-art accuracy when fine-tuned on the ImageNet1K dataset.', 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x16d_wsl', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNext WSL", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x16d_wsl', pretrained=True)", "api_arguments": [{"name": "resnext101_32x16d_wsl", "type": "str", "description": "ResNeXt-101 32x16d WSL model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl')", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(output[0])", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeXt-101 32x16d": {"Top-1 Acc.": "84.2", "Top-5 Acc.": "97.2"}}}, "description": "The provided ResNeXt models are pre-trained in weakly-supervised fashion on 940 million public images with 1.5K hashtags matching with 1000 ImageNet1K synsets, followed by fine-tuning on ImageNet1K dataset. The models significantly improve the training accuracy on ImageNet compared to training from scratch. They achieve state-of-the-art accuracy of 85.4% on ImageNet with the ResNext-101 32x48d model."}}
{"code": "{'Instruction': Design an API that can classify images into different categories using a pretrained model., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x48d_wsl', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNext WSL model from PyTorch Hub to classify images into 1000 different categories.', 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x48d_wsl', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x48d_wsl', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNext WSL", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x48d_wsl', pretrained=True)", "api_arguments": [{"name": "resnext101_32x48d_wsl", "type": "str", "description": "ResNeXt-101 32x48d WSL model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x48d_wsl')", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(output[0])", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeXt-101 32x48d": {"Top-1 Acc.": "85.4", "Top-5 Acc.": "97.6"}}}, "description": "The provided ResNeXt models are pre-trained in weakly-supervised fashion on 940 million public images with 1.5K hashtags matching with 1000 ImageNet1K synsets, followed by fine-tuning on ImageNet1K dataset. The models significantly improve the training accuracy on ImageNet compared to training from scratch. They achieve state-of-the-art accuracy of 85.4% on ImageNet with the ResNext-101 32x48d model."}}
{"code": "{'Instruction': I am working on an app that generates unique user avatars. I need an API that creates realistic images of faces. Provide me with the API details and pre-trained model if available., 'Output': {'domain': 'Image Generation', 'api_call': \"model = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'PGAN', model_name='celebAHQ-512', pretrained=True, useGPU=use_gpu)\", 'api_provider': 'PyTorch', 'explanation': \"Use the pretrained Progressive Growing of GANs (PGAN) model from PyTorch Hub to generate high-quality, realistic images of human faces.\", 'code': 'import torch\nuse_gpu = True if torch.cuda.is_available() else False\nmodel = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'PGAN', model_name='celebAHQ-512', pretrained=True, useGPU=use_gpu)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorch_GAN_zoo:hub', model='PGAN', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Generative Adversarial Networks (GANs)", "api_name": "PGAN", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorch_GAN_zoo:hub', model='PGAN', pretrained=True)", "api_arguments": {"repo_or_dir": "facebookresearch/pytorch_GAN_zoo:hub", "model": "PGAN", "model_name": "celebAHQ-512", "pretrained": "True", "useGPU": "use_gpu"}, "python_environment_requirements": "Python 3", "example_code": {"import": "import torch", "use_gpu": "use_gpu = True if torch.cuda.is_available() else False", "load_model": "model = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'PGAN', model_name='celebAHQ-512', pretrained=True, useGPU=use_gpu)", "build_noise_data": "noise, _ = model.buildNoiseData(num_images)", "test": "generated_images = model.test(noise)", "plot_images": {"import_matplotlib": "import matplotlib.pyplot as plt", "import_torchvision": "import torchvision", "make_grid": "grid = torchvision.utils.make_grid(generated_images.clamp(min=-1, max=1), scale_each=True, normalize=True)", "imshow": "plt.imshow(grid.permute(1, 2, 0).cpu().numpy())", "show": "plt.show()"}}, "performance": {"dataset": "celebA", "accuracy": "High-quality celebrity faces"}, "description": "Progressive Growing of GANs (PGAN) is a method for generating high-resolution images using generative adversarial networks. The model is trained progressively, starting with low-resolution images and gradually increasing the resolution until the desired output is achieved. This implementation is based on the paper by Tero Karras et al., 'Progressive Growing of GANs for Improved Quality, Stability, and Variation'."}}
{"code": "{'Instruction': A research team is analyzing brain MRI images for abnormality segmentation. Suggest a suitable API to achieve this., 'Output': {'domain': 'Semantic Segmentation', 'api_call': \"model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained U-Net model for biomedical image segmentation from PyTorch Hub to segment abnormalities in brain MRI images.', 'code': 'import torch\nmodel = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='mateuszbuda/brain-segmentation-pytorch', model='unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "Biomedical Image Segmentation", "api_name": "U-Net for brain MRI", "api_call": "torch.hub.load(repo_or_dir='mateuszbuda/brain-segmentation-pytorch', model='unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)", "api_arguments": {"in_channels": 3, "out_channels": 1, "init_features": 32, "pretrained": true}, "python_environment_requirements": ["torch", "torchvision", "numpy", "PIL"], "example_code": ["import torch", "model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)", "import numpy as np", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "m, s = np.mean(input_image, axis=(0, 1)), np.std(input_image, axis=(0, 1))", "preprocess = transforms.Compose([transforms.ToTensor(),transforms.Normalize(mean=m, std=s)])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model = model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(torch.round(output[0]))"], "performance": {"dataset": "kaggle.com/mateuszbuda/lgg-mri-segmentation"}, "description": "U-Net with batch normalization for biomedical image segmentation with pretrained weights for abnormality segmentation in brain MRI. The model comprises four levels of blocks containing two convolutional layers with batch normalization and ReLU activation function, and one max pooling layer in the encoding part and up-convolutional layers instead in the decoding part. The number of convolutional filters in each block is 32, 64, 128, and 256. The bottleneck layer has 512 convolutional filters. From the encoding layers, skip connections are used to the corresponding layers in the decoding part. Input image is a 3-channel brain MRI slice from pre-contrast, FLAIR, and post-contrast sequences, respectively. Output is a one-channel probability map of abnormality regions with the same size as the input image."}}
{"code": "{'Instruction': Jane is developing a mobile app to identify animal species. Propose an API that classifies animals based on images., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resnet50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNet50 v1.5 model from PyTorch Hub, which can be fine-tuned for animal species classification using images.', 'code': 'import torch\nmodel = torch.hub.load(\"NVIDIA/DeepLearningExamples:torchhub\", \"nvidia_resnet50\", pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_resnet50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNet50", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_resnet50', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["pip install validators matplotlib"], "example_code": ["import torch", "from PIL import Image", "import torchvision.transforms as transforms", "import numpy as np", "import json", "import requests", "import matplotlib.pyplot as plt", "import warnings", "warnings.filterwarnings('ignore')", "%matplotlib inline", "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')", "print(f'Using {device} for inference')", "resnet50 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resnet50', pretrained=True)", "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_convnets_processing_utils')", "resnet50.eval().to(device)", "uris = [...]", "batch = torch.cat([utils.prepare_input_from_uri(uri) for uri in uris]).to(device)", "with torch.no_grad():", " output = torch.nn.functional.softmax(resnet50(batch), dim=1)", "results = utils.pick_n_best(predictions=output, n=5)", "for uri, result in zip(uris, results):", " img = Image.open(requests.get(uri, stream=True).raw)", " img.thumbnail((256,256), Image.ANTIALIAS)", " plt.imshow(img)", " plt.show()", " print(result)"], "performance": {"dataset": "ImageNet", "accuracy": "~0.5% top1 improvement over ResNet50 v1"}, "description": "The ResNet50 v1.5 model is a modified version of the original ResNet50 v1 model. The difference between v1 and v1.5 is that, in the bottleneck blocks which requires downsampling, v1 has stride = 2 in the first 1x1 convolution, whereas v1.5 has stride = 2 in the 3x3 convolution. This difference makes ResNet50 v1.5 slightly more accurate (~0.5% top1) than v1, but comes with a small performance drawback (~5% imgs/sec). The model is initialized as described in Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification. This model is trained with mixed precision using Tensor Cores on Volta, Turing, and the NVIDIA Ampere GPU architectures."}}
{"code": "{'Instruction': Find me an API to classify objects in images given a massive dataset of real life images., 'Output': {'domain': 'Image Classification', 'api_call': 'model = torch.hub.load('pytorch/vision', 'alexnet', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained AlexNet model from PyTorch Hub, which can be used for object classification in real-world images from a large dataset like ImageNet.\", 'code': \"import torch\nmodel = torch.hub.load('pytorch/vision', 'alexnet', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='alexnet', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "AlexNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='alexnet', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": ">=1.9.0", "torchvision": ">=0.10.0"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"top-1_error": 43.45, "top-5_error": 20.91}}, "description": "AlexNet is a deep convolutional neural network that achieved a top-5 error of 15.3% in the 2012 ImageNet Large Scale Visual Recognition Challenge. The main contribution of the original paper was the depth of the model, which was computationally expensive but made feasible through the use of GPUs during training. The pretrained AlexNet model in PyTorch can be used for image classification tasks."}}
{"code": "{'Instruction': I want to classify images in real-time during a robotics competition. Recommend an API for image classification., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resneXt', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNeXt101-32x4d model from the NVIDIA PyTorch Hub repository for real-time image classification during a robotics competition.', 'code': \"import torch\nmodel = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resneXt', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_resneXt', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNeXt101-32x4d", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_resneXt', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "validators", "matplotlib"], "example_code": ["import torch", "from PIL import Image", "import torchvision.transforms as transforms", "import numpy as np", "import json", "import requests", "import matplotlib.pyplot as plt", "import warnings", "warnings.filterwarnings('ignore')", "%matplotlib inline", "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")", "print(f'Using {device} for inference')", "resneXt = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resneXt')", "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_convnets_processing_utils')", "resneXt.eval().to(device)", "uris = [", " 'http://images.cocodataset.org/test-stuff2017/000000024309.jpg',", " 'http://images.cocodataset.org/test-stuff2017/000000028117.jpg',", " 'http://images.cocodataset.org/test-stuff2017/000000006149.jpg',", " 'http://images.cocodataset.org/test-stuff2017/000000004954.jpg',", "]", "batch = torch.cat(", " [utils.prepare_input_from_uri(uri) for uri in uris]", ").to(device)", "with torch.no_grad():", " output = torch.nn.functional.softmax(resneXt(batch), dim=1)", "results = utils.pick_n_best(predictions=output, n=5)", "for uri, result in zip(uris, results):", " img = Image.open(requests.get(uri, stream=True).raw)", " img.thumbnail((256,256), Image.ANTIALIAS)", " plt.imshow(img)", " plt.show()", " print(result)"], "performance": {"dataset": "IMAGENET", "accuracy": "Not provided"}, "description": "ResNeXt101-32x4d is a model introduced in the Aggregated Residual Transformations for Deep Neural Networks paper. It is based on regular ResNet model, substituting 3x3 convolutions inside the bottleneck block for 3x3 grouped convolutions. This model is trained with mixed precision using Tensor Cores on Volta, Turing, and the NVIDIA Ampere GPU architectures. It can be deployed for inference on the NVIDIA Triton Inference Server using TorchScript, ONNX Runtime or TensorRT as an execution backend."}}
{"code": "{'Instruction': A photographer at National Wildlife Foundation wants to automatically classify and organize the wildlife photos he has captured. Suggest an API that can classify animal images., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_se_resnext101_32x4d', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained SE-ResNeXt101-32x4d model from PyTorch Hub, designed for ImageNet classification, which can be fine-tuned to classify wildlife photographs into appropriate categories.', 'code': \"import torch\nmodel = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_se_resnext101_32x4d', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_se_resnext101_32x4d', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SE-ResNeXt101", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_se_resnext101_32x4d', pretrained=True)", "api_arguments": "N/A", "python_environment_requirements": "validators, matplotlib", "example_code": "import torch\nfrom PIL import Image\nimport torchvision.transforms as transforms\nimport numpy as np\nimport json\nimport requests\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nprint(f'Using {device} for inference')\nresneXt = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_se_resnext101_32x4d')\nutils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_convnets_processing_utils')\nresneXt.eval().to(device)\nuris = ['http://images.cocodataset.org/test-stuff2017/000000024309.jpg','http://images.cocodataset.org/test-stuff2017/000000028117.jpg','http://images.cocodataset.org/test-stuff2017/000000006149.jpg','http://images.cocodataset.org/test-stuff2017/000000004954.jpg']\nbatch = torch.cat([utils.prepare_input_from_uri(uri) for uri in uris]).to(device)\nwith torch.no_grad():\n output = torch.nn.functional.softmax(resneXt(batch), dim=1)\nresults = utils.pick_n_best(predictions=output, n=5)\nfor uri, result in zip(uris, results):\n img = Image.open(requests.get(uri, stream=True).raw)\n img.thumbnail((256,256), Image.ANTIALIAS)\n plt.imshow(img)\n plt.show()\n print(result)", "performance": {"dataset": "IMAGENET", "accuracy": "N/A"}, "description": "The SE-ResNeXt101-32x4d is a ResNeXt101-32x4d model with added Squeeze-and-Excitation module. This model is trained with mixed precision using Tensor Cores on Volta, Turing, and the NVIDIA Ampere GPU architectures, which allows researchers to get results 3x faster than training without Tensor Cores while experiencing the benefits of mixed precision training."}}
{"code": "{'Instruction': We need to develop a robotic assistant, and it needs to be able to speak given a text input. Provide a text-to-speech API that can convert a text input into natural-sounding speech., 'Output': {'domain': 'Text-to-Speech', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_waveglow', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Use the WaveGlow model, available from PyTorch Hub, to synthesize natural-sounding speech from an input text.', 'code': \"import torch\nmodel = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_waveglow', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_waveglow', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Text-to-Speech", "framework": "PyTorch", "functionality": "Speech Synthesis", "api_name": "WaveGlow", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_waveglow', pretrained=True)", "api_arguments": {"repo_or_dir": "NVIDIA/DeepLearningExamples:torchhub", "model": "nvidia_waveglow", "model_math": "fp32"}, "python_environment_requirements": ["numpy", "scipy", "librosa", "unidecode", "inflect", "libsndfile1"], "example_code": {"load_waveglow_model": "waveglow = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_waveglow', model_math='fp32')", "prepare_waveglow_model": ["waveglow = waveglow.remove_weightnorm(waveglow)", "waveglow = waveglow.to('cuda')", "waveglow.eval()"], "load_tacotron2_model": "tacotron2 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tacotron2', model_math='fp32')", "prepare_tacotron2_model": ["tacotron2 = tacotron2.to('cuda')", "tacotron2.eval()"], "synthesize_speech": ["text = \"hello world, I missed you so much\"", "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tts_utils')", "sequences, lengths = utils.prepare_input_sequence([text])", "with torch.no_grad():", " mel, _, _ = tacotron2.infer(sequences, lengths)", " audio = waveglow.infer(mel)", "audio_numpy = audio[0].data.cpu().numpy()", "rate = 22050"], "save_audio": "write(\"audio.wav\", rate, audio_numpy)", "play_audio": "Audio(audio_numpy, rate=rate)"}, "performance": {"dataset": "LJ Speech", "accuracy": null}, "description": "The Tacotron 2 and WaveGlow model form a text-to-speech system that enables users to synthesize natural-sounding speech from raw transcripts without any additional prosody information. The Tacotron 2 model produces mel spectrograms from input text using encoder-decoder architecture. WaveGlow is a flow-based model that consumes the mel spectrograms to generate speech."}}
{"code": "{'Instruction': Find me an API that can determine the sentiment of a given text review., 'Output': {'domain': 'Sentiment Analysis', 'api_call': 'model = torch.hub.load('pytorch/fairseq', 'roberta.large.mnli', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained RoBERTa model for natural language inference from PyTorch Hub to predict the sentiment of a given text review.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/fairseq', 'roberta.large.mnli', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/fairseq', model='roberta.large', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "RoBERTa", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='pytorch/fairseq', model='roberta.large', pretrained=True)", "api_arguments": ["'pytorch/fairseq'", "'roberta.large'"], "python_environment_requirements": ["regex", "requests", "hydra-core", "omegaconf"], "example_code": ["import torch", "roberta = torch.hub.load('pytorch/fairseq', 'roberta.large')", "roberta.eval()", "tokens = roberta.encode('Hello world!')", "last_layer_features = roberta.extract_features(tokens)", "all_layers = roberta.extract_features(tokens, return_all_hiddens=True)", "roberta = torch.hub.load('pytorch/fairseq', 'roberta.large.mnli')", "roberta.eval()", "tokens = roberta.encode('Roberta is a heavily optimized version of BERT.', 'Roberta is not very optimized.')", "prediction = roberta.predict('mnli', tokens).argmax().item()", "tokens = roberta.encode('Roberta is a heavily optimized version of BERT.', 'Roberta is based on BERT.')", "prediction = roberta.predict('mnli', tokens).argmax().item()", "roberta.register_classification_head('new_task', num_classes=3)", "logprobs = roberta.predict('new_task', tokens)"], "performance": {"dataset": "MNLI", "accuracy": "N/A"}, "description": "RoBERTa is a robustly optimized version of BERT, a revolutionary self-supervised pretraining technique that learns to predict intentionally hidden (masked) sections of text. RoBERTa builds on BERT's language masking strategy and modifies key hyperparameters, including removing BERT's next-sentence pretraining objective, and training with much larger mini-batches and learning rates. RoBERTa was also trained on an order of magnitude more data than BERT, for a longer amount of time, allowing it to generalize even better to downstream tasks."}}
{"code": "{'Instruction': I have a collection of photos of different objects and want to classify them. Suggest me an API that can help in this task., 'Output': {'domain': 'Image Classification', 'api_call': 'model = torch.hub.load('pytorch/vision', 'densenet121', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained DenseNet-121 model from PyTorch Hub, which is suitable for classifying images of different objects.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'densenet121', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet121', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Dense Convolutional Network", "api_name": "Densenet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet121', pretrained=True)", "api_arguments": [{"name": "densenet121", "type": "str", "description": "Densenet-121 model"}], "python_environment_requirements": {"torch": "latest", "torchvision": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet121', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"densenet121": {"Top-1 error": 25.35, "Top-5 error": 7.83}}}, "description": "Dense Convolutional Network (DenseNet) connects each layer to every other layer in a feed-forward fashion. It alleviates the vanishing-gradient problem, strengthens feature propagation, encourages feature reuse, and substantially reduces the number of parameters."}}
{"code": "{'Instruction': We need a deep learning model for image classification that is less computationally expensive and has a low number of parameters while maintaining the accuracy performance. Provide me an API for image classification., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'densenet169', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained DenseNet-169 model from PyTorch Hub, which has a low number of parameters, is less computationally expensive, and achieves good accuracy for image classification.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'densenet169', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet169', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Dense Convolutional Network", "api_name": "Densenet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet169', pretrained=True)", "api_arguments": [{"name": "densenet169", "type": "str", "description": "Densenet-169 model"}], "python_environment_requirements": {"torch": "latest", "torchvision": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet169', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"densenet169": {"Top-1 error": 24.0, "Top-5 error": 7.0}}}, "description": "Dense Convolutional Network (DenseNet) connects each layer to every other layer in a feed-forward fashion. It alleviates the vanishing-gradient problem, strengthens feature propagation, encourages feature reuse, and substantially reduces the number of parameters."}}
{"code": "{'Instruction': Design an image classifier that can differentiate between various classes. Recommend an API that can serve this purpose., 'Output': {'domain': 'Image Classification', 'api_call': 'model = torch.hub.load('pytorch/vision', 'densenet201', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pre-trained DenseNet-201 model from PyTorch Hub for image classification across multiple classes.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'densenet201', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet201', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Dense Convolutional Network", "api_name": "Densenet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet201', pretrained=True)", "api_arguments": [{"name": "densenet201", "type": "str", "description": "Densenet-201 model"}], "python_environment_requirements": {"torch": "latest", "torchvision": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet201', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"densenet201": {"Top-1 error": 22.8, "Top-5 error": 6.43}}}, "description": "Dense Convolutional Network (DenseNet) connects each layer to every other layer in a feed-forward fashion. It alleviates the vanishing-gradient problem, strengthens feature propagation, encourages feature reuse, and substantially reduces the number of parameters."}}
{"code": "{'Instruction': Suggest a suitable API that can classify images into different categories based on ImageNet dataset., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'densenet161', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained DenseNet-161 model from PyTorch Hub to classify images into different categories based on the ImageNet dataset.', 'code': 'import torch\nmodel = torch.hub.load(\\'pytorch/vision\\', \\'densenet161\\', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet161', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Dense Convolutional Network", "api_name": "Densenet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet161', pretrained=True)", "api_arguments": [{"name": "densenet161", "type": "str", "description": "Densenet-161 model"}], "python_environment_requirements": {"torch": "latest", "torchvision": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet161', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"densenet161": {"Top-1 error": 22.35, "Top-5 error": 6.2}}}, "description": "Dense Convolutional Network (DenseNet) connects each layer to every other layer in a feed-forward fashion. It alleviates the vanishing-gradient problem, strengthens feature propagation, encourages feature reuse, and substantially reduces the number of parameters."}}
{"code": "{'Instruction': Recommend an API for segmenting objects in images for an autonomous vehicle project., 'Output': {'domain': 'Semantic Segmentation', 'api_call': \"model = torch.hub.load('pytorch/vision', 'fcn_resnet50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Fully-Convolutional Network (FCN) model with a ResNet-50 backbone from PyTorch Hub for semantic segmentation, suitable for an autonomous vehicle project.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'fcn_resnet50', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='fcn_resnet50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "Fully-Convolutional Network", "api_name": "fcn_resnet50", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='fcn_resnet50', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "boolean", "description": "If True, returns a model pre-trained on COCO train2017"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "matplotlib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'fcn_resnet50', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "input_image = input_image.convert(\"RGB\")", "preprocess = transforms.Compose([", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)['out'][0]", "output_predictions = output.argmax(0)"], "performance": {"dataset": "COCO val2017", "accuracy": {"fcn_resnet50": {"Mean IOU": 60.5, "Global Pixelwise Accuracy": 91.4}}}, "description": "FCN-ResNet is a Fully-Convolutional Network model using a ResNet-50 or a ResNet-101 backbone. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset."}}
{"code": "{'Instruction': I need an API that generates semantic segmentation of an input image taken by a self-driving car., 'Output': {'domain': 'Semantic Segmentation', 'api_call': \"model = torch.hub.load('pytorch/vision', 'fcn_resnet101', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pre-trained FCN-ResNet101 model from PyTorch Hub for semantic segmentation of an input image.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'fcn_resnet101', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='fcn_resnet101', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "Fully-Convolutional Network", "api_name": "fcn_resnet101", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='fcn_resnet101', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "boolean", "description": "If True, returns a model pre-trained on COCO train2017"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "matplotlib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'fcn_resnet101', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "input_image = input_image.convert(\"RGB\")", "preprocess = transforms.Compose([", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)['out'][0]", "output_predictions = output.argmax(0)"], "performance": {"dataset": "COCO val2017", "accuracy": {"fcn_resnet101": {"Mean IOU": 63.7, "Global Pixelwise Accuracy": 91.9}}}, "description": "FCN-ResNet is a Fully-Convolutional Network model using a ResNet-50 or a ResNet-101 backbone. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset."}}
{"code": "{'Instruction': Weddings and events are photographed every day. What API can help me label these photos by category for my wedding business?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet39ds', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained HarDNet model from PyTorch Hub for image classification. This model is efficient and can help label your wedding photos by category.', 'code': 'import torch\nmodel = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet39ds', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet39ds', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "HarDNet", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet39ds', pretrained=True)", "api_arguments": [{"name": "hardnet39ds", "type": "str", "description": "HarDNet-39DS model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet39ds', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"hardnet39ds": {"Top-1 error": 27.92, "Top-5 error": 9.57}}}, "description": "Harmonic DenseNet (HarDNet) is a low memory traffic CNN model, which is fast and efficient. The basic concept is to minimize both computational cost and memory access cost at the same time, such that the HarDNet models are 35% faster than ResNet running on GPU comparing to models with the same accuracy (except the two DS models that were designed for comparing with MobileNet)."}}
{"code": "{'Instruction': Discover an efficient API that can classify the types objects in an image., 'Output': {'domain': 'Image Classification', 'api_call': 'model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet85', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained HarDNet-85 model from PyTorch Hub, an efficient image classification model with low memory traffic.', 'code': 'import torch\nmodel = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet85', pretrained=True)\nmodel.eval()'}}", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet85', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "HarDNet", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet85', pretrained=True)", "api_arguments": [{"name": "hardnet85", "type": "str", "description": "HarDNet-85 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet85', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"hardnet85": {"Top-1 error": 21.96, "Top-5 error": 6.11}}}, "description": "Harmonic DenseNet (HarDNet) is a low memory traffic CNN model, which is fast and efficient. The basic concept is to minimize both computational cost and memory access cost at the same time, such that the HarDNet models are 35% faster than ResNet running on GPU comparing to models with the same accuracy (except the two DS models that were designed for comparing with MobileNet)."}}
{"code": "{'Instruction': I am creating a mobile app to identify plants. I need an API that has a lighter-weight model for image classification., 'Output': {'domain': 'Classification', 'api_call': 'model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet68', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained HarDNet-68 model from PyTorch Hub, which is a lightweight model with low memory traffic, suitable for mobile apps and can be fine-tuned for plant identification.\", 'code': 'import torch\nmodel = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet68', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet68', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "HarDNet", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet68', pretrained=True)", "api_arguments": [{"name": "hardnet68", "type": "str", "description": "HarDNet-68 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet68', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"hardnet68": {"Top-1 error": 23.52, "Top-5 error": 6.99}}}, "description": "Harmonic DenseNet (HarDNet) is a low memory trafficCNN model, which is fast and efficient. The basic concept is to minimize both computational cost and memory access cost at the same time, such that the HarDNet models are 35% faster than ResNet running on GPU comparing to models with the same accuracy (except the two DS models that were designed for comparing with MobileNet)."}}
{"code": "{'Instruction': Recommend an API that can classify images into different categories using a model designed by Google., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'googlenet', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained GoogLeNet model for image classification from PyTorch Hub, which was created by Google.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'googlenet', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='googlenet', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "GoogLeNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='googlenet', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": ">=1.0.0", "torchvision": ">=0.2.2"}, "example_code": {"import": ["import torch", "import urllib", "from PIL import Image", "from torchvision import transforms"], "load_model": "model = torch.hub.load('pytorch/vision:v0.10.0', 'googlenet', pretrained=True)", "model_eval": "model.eval()", "image_preprocessing": ["input_image = Image.open(filename)", "preprocess = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)"], "model_execution": ["if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)"], "output_processing": ["probabilities = torch.nn.functional.softmax(output[0], dim=0)", "top5_prob, top5_catid = torch.topk(probabilities, 5)"]}, "performance": {"dataset": "ImageNet", "accuracy": {"Top-1 error": "30.22", "Top-5 error": "10.47"}}, "description": "GoogLeNet is based on a deep convolutional neural network architecture codenamed 'Inception', which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014)."}}
{"code": "{'Instruction': I want to build a mobile application that can classify an object based on its image. Recommend a lightweight API that can provide high performance on mobile devices., 'Output': {'domain': 'Image Classification', 'api_call': 'model = torch.hub.load('huawei-noah/ghostnet', 'ghostnet_1x', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained GhostNet model from PyTorch Hub, a lightweight and efficient architecture for image classification, suitable for use on mobile devices.', 'code': 'import torch\nmodel = torch.hub.load('huawei-noah/ghostnet', 'ghostnet_1x', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/ghostnet', model='ghostnet_1x', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Efficient networks by generating more features from cheap operations", "api_name": "GhostNet", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/ghostnet', model='ghostnet_1x', pretrained=True)", "api_arguments": ["pretrained"], "python_environment_requirements": ["torch", "torchvision", "PIL"], "example_code": ["import torch", "model = torch.hub.load('huawei-noah/ghostnet', 'ghostnet_1x', pretrained=True)", "model.eval()", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)", "print(probabilities)"], "performance": {"dataset": "ImageNet", "accuracy": {"Top-1 acc": "73.98", "Top-5 acc": "91.46"}}, "description": "The GhostNet architecture is based on an Ghost module structure which generates more features from cheap operations. Based on a set of intrinsic feature maps, a series of cheap operations are applied to generate many ghost feature maps that could fully reveal information underlying intrinsic features. Experiments conducted on benchmarks demonstrate the superiority of GhostNet in terms of speed and accuracy tradeoff."}}
{"code": "{'Instruction': In a new computer vision project, we need a model that can classify images into multiple possible categories. Recommend a suitable pre-trained model., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('XingangPan/IBN-Net', 'resnet50_ibn_a', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNet-50-IBN-a model from PyTorch Hub, which can classify images into multiple categories using IBN-Net for domain/appearance invariance.', 'code': 'import torch\nmodel = torch.hub.load('XingangPan/IBN-Net', 'resnet50_ibn_a', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnet50_ibn_a', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "IBN-Net", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnet50_ibn_a', pretrained=True)", "api_arguments": [{"name": "resnet50_ibn_a", "type": "str", "description": "ResNet-50-IBN-a model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('XingangPan/IBN-Net', 'resnet50_ibn_a', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"resnet50_ibn_a": {"Top-1 acc": 77.46, "Top-5 acc": 93.68}}}, "description": "IBN-Net is a CNN model with domain/appearance invariance. Motivated by style transfer works, IBN-Net carefully unifies instance normalization and batch normalization in a single deep network. It provides a simple way to increase both modeling and generalization capacities without adding model complexity. IBN-Net is especially suitable for cross domain or person/vehicle re-identification tasks."}}
{"code": "{'Instruction': An AI engineer is developing an image recognition system for Intel Corporation. Provide me an API to classify images using IBN-Net model., 'Output': {'domain': 'Image Classification', 'api_call': 'model = torch.hub.load('XingangPan/IBN-Net', 'resnet101_ibn_a', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNet-101-IBN-a model from PyTorch Hub to classify images using the IBN-Net model, which is suitable for cross-domain image recognition tasks.', 'code': 'import torch\nmodel = torch.hub.load('XingangPan/IBN-Net', 'resnet101_ibn_a', pretrained=True)\nmodel.eval()'}}", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnet101_ibn_a', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "IBN-Net", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnet101_ibn_a', pretrained=True)", "api_arguments": [{"name": "resnet101_ibn_a", "type": "str", "description": "ResNet-101-IBN-a model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('XingangPan/IBN-Net', 'resnet101_ibn_a', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"resnet101_ibn_a": {"Top-1 acc": 78.61, "Top-5 acc": 94.41}}}, "description": "IBN-Net is a CNN model with domain/appearance invariance. Motivated by style transfer works, IBN-Net carefully unifies instance normalization and batch normalization in a single deep network. It provides a simple way to increase both modeling and generalization capacities without adding model complexity. IBN-Net is especially suitable for cross domain or person/vehicle re-identification tasks."}}
{"code": "{'Instruction': Looking for something to help re-identify a person from diferent photos. Find me an API for that., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('XingangPan/IBN-Net', 'resnext101_ibn_a', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNeXt-101-IBN-a model from PyTorch Hub, which is suitable for person re-identification tasks.', 'code': 'import torch\nmodel = torch.hub.load('XingangPan/IBN-Net', 'resnext101_ibn_a', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnext101_ibn_a', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "IBN-Net", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnext101_ibn_a', pretrained=True)", "api_arguments": [{"name": "resnext101_ibn_a", "type": "str", "description": "ResNeXt-101-IBN-a model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('XingangPan/IBN-Net', 'resnext101_ibn_a', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"resnext101_ibn_a": {"Top-1 acc": 79.12, "Top-5 acc": 94.58}}}, "description": "IBN-Net is a CNN model with domain/appearance invariance. Motivated by style transfer works, IBN-Net carefully unifies instance normalization and batch normalization in a single deep network. It provides a simple way to increase both modeling and generalization capacities without adding model complexity. IBN-Net is especially suitable for cross domain or person/vehicle re-identification tasks."}}
{"code": "{'Instruction': Can you provide an API that could identify different types of cars given an image of the scene?, 'Output': {'domain': 'Image Classification', 'api_call': 'model = torch.hub.load('pytorch/vision', 'inception_v3', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the Inception v3 model from PyTorch Hub with pre-trained weights. This model can be fine-tuned to identify different types of cars given an input image.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'inception_v3', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='inception_v3', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "Inception_v3", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='inception_v3', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": "1.9.0", "torchvision": "0.10.0"}, "example_code": {"import_libraries": "import torch", "load_model": "model = torch.hub.load('pytorch/vision:v0.10.0', 'inception_v3', pretrained=True)", "model_evaluation": "model.eval()"}, "performance": {"dataset": "imagenet", "accuracy": {"top-1_error": 22.55, "top-5_error": 6.44}}, "description": "Inception v3, also called GoogleNetv3, is a famous Convolutional Neural Network trained on the ImageNet dataset from 2015. It is based on the exploration of ways to scale up networks to utilize the added computation as efficiently as possible by using suitably factorized convolutions and aggressive regularization. The model achieves a top-1 error of 22.55% and a top-5 error of 6.44% on the ImageNet dataset."}}
{"code": "{'Instruction': I need to classify different types of animals in images taken from the wild., 'Output': {'domain': 'Classification', 'api_call': 'model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest200', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the ResNeSt-200 model from PyTorch Hub with pre-trained weights for image classification of different types of animals.', 'code': 'import torch\nmodel = torch.hub.load('zhanghang1989/ResNeSt', 'resnest200', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest200', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNeSt", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest200', pretrained=True)", "api_arguments": "resnest200", "python_environment_requirements": {"torch": "1.0.0", "torchvision": "0.2.2"}, "example_code": ["import torch", "model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest200', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeSt-200": 83.84}}, "description": "ResNeSt models are from the ResNeSt: Split-Attention Networks paper. They are a new ResNet variant that enables attention across feature-map groups. By stacking Split-Attention blocks ResNet-style, ResNeSt models outperform other networks with similar model complexities, and also help downstream tasks including object detection, instance segmentation, and semantic segmentation."}}
{"code": "{'Instruction': We need to implement an image classification model for our AI platform. Can you suggest an appropriate API we can use as a base?, 'Output': {'domain': 'Image Classification', 'api_call': 'model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest101', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pre-trained ResNeSt-101 model from PyTorch Hub, which offers excellent image classification performance and uses the Split-Attention Networks architecture.', 'code': 'import torch\nmodel = torch.hub.load('zhanghang1989/ResNeSt', 'resnest101', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest101', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNeSt", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest101', pretrained=True)", "api_arguments": "resnest101", "python_environment_requirements": {"torch": "1.0.0", "torchvision": "0.2.2"}, "example_code": ["import torch", "model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest101', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeSt-101": 82.83}}, "description": "ResNeSt models are from the ResNeSt: Split-Attention Networks paper. They are a new ResNet variant that enables attention across feature-map groups. By stacking Split-Attention blocks ResNet-style, ResNeSt models outperform other networks with similar model complexities, and also help downstream tasks including object detection, instance segmentation, and semantic segmentation."}}
{"code": "{'Instruction': Provide an API to classify buildings to residential, commercial or industrial based on their locations from images., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNeSt-50 model from PyTorch Hub, which can be fine-tuned for building classification into residential, commercial or industrial.', 'code': 'import torch\nmodel = torch.hub.load('zhanghang1989/ResNeSt', 'resnest50', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNeSt", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest50', pretrained=True)", "api_arguments": "resnest50", "python_environment_requirements": {"torch": "1.0.0", "torchvision": "0.2.2"}, "example_code": ["import torch", "model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest50', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeSt-50": 81.03}}, "description": "ResNeSt models are from the ResNeSt: Split-Attention Networks paper. They are a new ResNet variant that enables attention across feature-map groups. By stacking Split-Attention blocks ResNet-style, ResNeSt models outperform other networks with similar model complexities, and also help downstream tasks including object detection, instance segmentation, and semantic segmentation."}}
{"code": "{'Instruction': I work at a tech company and I need to identify dog breeds in images. Provide me with an API that can classify dog breeds based on an input image., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest269', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNeSt269 model from PyTorch Hub, which can be fine-tuned for dog breed classification.', 'code': 'import torch\nmodel = torch.hub.load('zhanghang1989/ResNeSt', 'resnest269', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest269', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNeSt", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest269', pretrained=True)", "api_arguments": "resnest269", "python_environment_requirements": {"torch": "1.0.0", "torchvision": "0.2.2"}, "example_code": ["import torch", "model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest269', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeSt-269": 84.54}}, "description": "ResNeSt models are from the ResNeSt: Split-Attention Networks paper. They are a new ResNet variant that enables attention across feature-map groups. By stacking Split-Attention blocks ResNet-style, ResNeSt models outperform other networks with similar model complexities, and also help downstream tasks including object detection, instance segmentation, and semantic segmentation."}}
{"code": "{'Instruction': Create a mobile app to detect objects. Recommend me an API that would ideally suit this purpose., 'Output': {'domain': 'Object Detection', 'api_call': \"model = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_mobile', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained ProxylessNAS model optimized for mobile devices from PyTorch Hub, which can be fine-tuned for object detection in a mobile app.\", 'code': 'import torch\nmodel = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_mobile', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_cpu', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "ProxylessNAS", "api_name": "mit-han-lab/ProxylessNAS", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_cpu', pretrained=True)", "api_arguments": [{"name": "proxylessnas_cpu", "type": "str", "description": "ProxylessNAS optimized for CPU"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "target_platform = 'proxyless_cpu'", "model = torch.hub.load('mit-han-lab/ProxylessNAS', target_platform, pretrained=True)", "model.eval()"], "performance": {"dataset": [{"model_structure": "proxylessnas_cpu", "accuracy": 75.3}]}, "description": "ProxylessNAS models are from the ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware paper. They specialize CNN architectures for different hardware platforms, offering free yet significant performance boost on all three platforms (CPU, GPU, and Mobile) with similar accuracy."}}
{"code": "{'Instruction': I need an API that can classify objects in images and has a lightweight model suitable for mobile devices., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'mobilenet_v2', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MobileNet v2 model from PyTorch Hub, which is designed for efficient image classification on mobile devices.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'mobilenet_v2', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='mobilenet_v2', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MobileNet v2", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='mobilenet_v2', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision', 'mobilenet_v2', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)"], "performance": {"dataset": "ImageNet", "accuracy": {"top-1_error": 28.12, "top-5_error": 9.71}}, "description": "The MobileNet v2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input. MobileNet v2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, non-linearities in the narrow layers were removed in order to maintain representational power."}}
{"code": "{'Instruction': Develop a mobile application that can classify objects in images captured by the phone camera. Suggest an API that can do the classification., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_mobile', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ProxylessNAS model optimized for mobile devices from PyTorch Hub to classify objects in images captured by the phone camera.', 'code': 'import torch\nmodel = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_mobile', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_mobile', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "ProxylessNAS", "api_name": "mit-han-lab/ProxylessNAS", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_mobile', pretrained=True)", "api_arguments": [{"name": "proxylessnas_mobile", "type": "str", "description": "ProxylessNAS optimized for Mobile"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "target_platform = 'proxyless_mobile'", "model = torch.hub.load('mit-han-lab/ProxylessNAS', target_platform, pretrained=True)", "model.eval()"], "performance": {"dataset": [{"model_structure": "proxylessnas_mobile", "accuracy": 74.6}]}, "description": "ProxylessNAS models are from the ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware paper. They specialize CNN architectures for different hardware platforms, offering free yet significant performance boost on all three platforms (CPU, GPU, and Mobile) with similar accuracy."}}
{"code": "{'Instruction': To help an online store to label images taken from the deep sea, pick the appropriate API for this task., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'resnet50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNet50 model from PyTorch Hub, which can be fine-tuned to classify images taken from the deep sea.', 'code': 'import torch\nmodel = torch.hub.load(\"pytorch/vision\", \"resnet50\", pretrained=True)\nmodel.eval()'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Deep Residual Networks", "api_name": "ResNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet50', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "bool", "default": "False", "description": "If True, returns a model pre-trained on ImageNet"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "resnet50", "top-1_error": 23.85, "top-5_error": 7.13}}, "description": "ResNet models are deep residual networks pre-trained on ImageNet. They were proposed in the paper 'Deep Residual Learning for Image Recognition'. Available model variants include ResNet18, ResNet34, ResNet50, ResNet101, and ResNet152."}}
{"code": "{'Instruction': Suppose you work as a data scientist in IMB and you need an image classifier for your project on animals. Can you provide me the code that loads a model for image classification?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'resnet18', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pre-trained ResNet18 model from PyTorch Hub for image classification, which can be fine-tuned for an animal classification task.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'resnet18', pretrained=True)\nmodel.eval()' }}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet18', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Deep Residual Networks", "api_name": "ResNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet18', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "bool", "default": "False", "description": "If True, returns a model pre-trained on ImageNet"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "resnet18", "top-1_error": 30.24, "top-5_error": 10.92}}, "description": "ResNet models are deep residual networks pre-trained on ImageNet. They were proposed in the paper 'Deep Residual Learning for Image Recognition'. Available model variants include ResNet18, ResNet34, ResNet50, ResNet101, and ResNet152."}}
{"code": "{'Instruction': I am organizing my photo library and need an API to identify what's in each photo, can you make a code for me?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'resnet34', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNet34 model from PyTorch Hub for image recognition and use it to identify the contents of your photo library.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'resnet34', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet34', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Deep Residual Networks", "api_name": "ResNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet34', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "bool", "default": "False", "description": "If True, returns a model pre-trained on ImageNet"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet34', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "resnet34", "top-1_error": 26.7, "top-5_error": 8.58}}, "description": "ResNet models are deep residual networks pre-trained on ImageNet. They were proposed in the paper 'Deep Residual Learning for Image Recognition'. Available model variants include ResNet18, ResNet34, ResNet50, ResNet101, and ResNet152."}}
{"code": "{'Instruction': An application is being built that can identify different vehicles in photos. Suggest an API that can be used for this purpose., 'Output': {'domain': 'Classification', 'api_call': 'model = torch.hub.load('pytorch/vision', 'resnet101', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNet101 model from PyTorch Hub, which can be fine-tuned for vehicle classification in images.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'resnet101', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet101', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Deep Residual Networks", "api_name": "ResNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet101', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "bool", "default": "False", "description": "If True, returns a model pre-trained on ImageNet"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet101', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "resnet101", "top-1_error": 22.63, "top-5_error": 6.44}}, "description": "ResNet models are deep residual networks pre-trained on ImageNet. They were proposed in the paper 'Deep Residual Learning for Image Recognition'. Available model variants include ResNet18, ResNet34, ResNet50, ResNet101, and ResNet152."}}
{"code": "{'Instruction': I need an API for image classification that employs deep residual networks. Can you provide me with one?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'resnet152', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the ResNet152 model from PyTorch Hub, which is a deep residual network pretrained on ImageNet, suitable for image classification tasks.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'resnet152', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet152', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Deep Residual Networks", "api_name": "ResNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet152', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "bool", "default": "False", "description": "If True, returns a model pre-trained on ImageNet"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet152', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "resnet152", "top-1_error": 21.69, "top-5_error": 5.94}}, "description": "ResNet models are deep residual networks pre-trained on ImageNet. They were proposed in the paper 'Deep Residual Learning for Image Recognition'. Available model variants include ResNet18, ResNet34, ResNet50, ResNet101, and ResNet152."}}
{"code": "{'Instruction': A photographer at Adobe Stock wants to automatically tag their large photo inventory using AI. Point them to an ready-to-use API that accomplishes this task., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'resnext101_32x4d', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNext model for image classification from PyTorch Hub to automatically tag images based on their content.', 'code': 'import torch\nmodel = torch.hub.load(\"pytorch/vision\", \"resnext101_32x4d\", pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnext101_32x4d', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNext", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnext101_32x4d', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "PIL"], "example_code": ["import torch", "from PIL import Image", "from torchvision import transforms", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnext101_32x4d', pretrained=True)", "model.eval()", "input_image = Image.open('dog.jpg')", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)"], "performance": {"dataset": "ImageNet", "resnext101_32x8d": {"top-1": 20.69, "top-5": 5.47}}, "description": "ResNext is a next-generation ResNet architecture for image classification. It is more efficient and accurate than the original ResNet. This implementation includes two versions of the model, resnext50_32x4d and resnext101_32x8d, with 50 and 101 layers respectively."}}
{"code": "{'Instruction': To create an app that tells a user if the food they take a photo of is safe to eat, provide me with an appropriate API that can classify images of food products., 'Output': {'domain': 'Image Classification', 'api_call': 'model = torch.hub.load('pytorch/vision', 'resnext50_32x4d', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNext50_32x4d model from PyTorch Hub, which can be fine-tuned for food safety classification given an image of a food product.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'resnext50_32x4d', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnext50_32x4d', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNext", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnext50_32x4d', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "PIL"], "example_code": ["import torch", "from PIL import Image", "from torchvision import transforms", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnext50_32x4d', pretrained=True)", "model.eval()", "input_image = Image.open('dog.jpg')", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)"], "performance": {"dataset": "ImageNet", "accuracy": {"resnext50_32x4d": {"top-1": 22.38, "top-5": 6.3}}}, "description": "ResNext is a next-generation ResNet architecture for image classification. It is more efficient and accurate than the original ResNet. This implementation includes two versions of the model, resnext50_32x4d and resnext101_32x8d, with 50 and 101 layers respectively."}}
{"code": "{'Instruction': Find me an API capable of image classification based on Spiking Neural Networks., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_b', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained SNNMLP Base model from PyTorch Hub, which is an image classification model based on Spiking Neural Networks for efficient and accurate classification.', 'code': 'import torch\nmodel = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_b', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_b', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SNNMLP", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_b', pretrained=True)", "api_arguments": [{"name": "snnmlp_b", "type": "str", "description": "SNNMLP Base model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_b', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "SNNMLP Base", "top-1": 85.59}}, "description": "SNNMLP incorporates the mechanism of LIF neurons into the MLP models, to achieve better accuracy without extra FLOPs. We propose a full-precision LIF operation to communicate between patches, including horizontal LIF and vertical LIF in different directions. We also propose to use group LIF to extract better local features. With LIF modules, our SNNMLP model achieves 81.9%, 83.3% and 83.6% top-1 accuracy on ImageNet dataset with only 4.4G, 8.5G and 15.2G FLOPs, respectively."}}
{"code": "{'Instruction': What is an API that can be provided to a person working on the Hulu app for Apple TV to classify images?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_t', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pre-trained SNNMLP model from PyTorch Hub to perform efficient image classification with a high accuracy.', 'code': 'import torch\nmodel = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_t', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_t', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SNNMLP", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_t', pretrained=True)", "api_arguments": [{"name": "snnmlp_t", "type": "str", "description": "SNNMLP Tiny model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_t', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "SNNMLP Tiny", "top-1": 81.88}}, "description": "SNNMLP incorporates the mechanism of LIF neurons into the MLP models, to achieve better accuracy without extra FLOPs. We propose a full-precision LIF operation to communicate between patches, including horizontal LIF and vertical LIF in different directions. We also propose to use group LIF to extract better local features. With LIF modules, our SNNMLP model achieves 81.9%, 83.3% and 83.6% top-1 accuracy on ImageNet dataset with only 4.4G, 8.5G and 15.2G FLOPs, respectively."}}
{"code": "{'Instruction': An e-commerce website wants to integrate an AI assistant to differentiate between various items being uploaded by sellers. Give me an API to classify the uploaded items based on their image., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_s', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained SNNMLP Small model from PyTorch Hub, which can perform image classification on the uploaded items, allowing the AI assistant to differentiate between various products.', 'code': 'import torch\nmodel = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_s', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_s', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SNNMLP", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_s', pretrained=True)", "api_arguments": [{"name": "snnmlp_s", "type": "str", "description": "SNNMLP Small model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_s', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "SNNMLP Small", "top-1": 83.3}}, "description": "SNNMLP incorporates the mechanism of LIF neurons into the MLP models, to achieve better accuracy without extra FLOPs. We propose a full-precision LIF operation to communicate between patches, including horizontal LIF and vertical LIF in different directions. We also propose to use group LIF to extract better local features. With LIF modules, our SNNMLP model achieves 81.9%, 83.3% and 83.6% top-1 accuracy on ImageNet dataset with only 4.4G, 8.5G and 15.2G FLOPs, respectively."}}
{"code": "{'Instruction': An NGO ingin Gitonga is building an AI bot that identifies trees using images from the Amazon rainforest. Help me find an API that can classify images of trees using pre-trained models., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'squeezenet1_0', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained SqueezeNet model from PyTorch Hub, which serves as a base for image classification and can be fine-tuned to classify trees from the Amazon rainforest.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'squeezenet1_0', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='squeezenet1_0', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SqueezeNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='squeezenet1_0', pretrained=True)", "api_arguments": {"version": "v0.10.0", "model": ["squeezenet1_0"], "pretrained": "True"}, "python_environment_requirements": {"torch": ">=1.9.0", "torchvision": ">=0.10.0"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'squeezenet1_0', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)", "print(probabilities)"], "performance": {"dataset": "ImageNet", "accuracy": {"squeezenet1_0": {"Top-1 error": 41.9, "Top-5 error": 19.58}}}, "description": "SqueezeNet is an image classification model that achieves AlexNet-level accuracy with 50x fewer parameters. It has two versions: squeezenet1_0 and squeezenet1_1, with squeezenet1_1 having 2.4x less computation and slightly fewer parameters than squeezenet1_0, without sacrificing accuracy."}}
{"code": "{'Instruction': I want a lightweight PyTorch API for object recognition for my application. What would you suggest?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg11', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained VGG11 model from PyTorch Hub, which is a lightweight option for object recognition tasks.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg11', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg11', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg11', pretrained=True)", "api_arguments": [{"name": "vgg11", "type": "str", "description": "VGG11 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg11', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg11": {"Top-1 error": 30.98, "Top-5 error": 11.37}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': I want to categorize images from a dataset with pretrained API. Give me an ideal choice., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg11_bn', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained VGG11 model with batch normalization from PyTorch Hub for large-scale image recognition and classification.\", 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg11_bn', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg11_bn', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg11_bn', pretrained=True)", "api_arguments": [{"name": "vgg11_bn", "type": "str", "description": "VGG11 model with batch normalization"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg11_bn', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg11_bn": {"Top-1 error": 26.7, "Top-5 error": 8.58}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': I am a researcher in the field of computer vision, and I need to classify images using a pre-trained model with good performance. Can you give me an API for that?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg13', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained VGG13 model from PyTorch Hub for image classification tasks. VGG models are widely used for large-scale image recognition tasks and have good performance.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg13', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg13', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg13', pretrained=True)", "api_arguments": [{"name": "vgg13", "type": "str", "description": "VGG13 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg13', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg13": {"Top-1 error": 30.07, "Top-5 error": 10.75}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn,vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': I need a PyTorch API to recognize various objects in images for my project. Can you provide one?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg16', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained VGG16 model from PyTorch Hub, which can be used for object recognition in images.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg16', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg16', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg16', pretrained=True)", "api_arguments": [{"name": "vgg16", "type": "str", "description": "VGG16 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg16": {"Top-1 error": 28.41, "Top-5 error": 9.62}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': Can you provide me with an API capable of image recognition for my autonomous vehicle project?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg16_bn', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the VGG16 model with batch normalization from the PyTorch Vision library for image recognition tasks.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg16_bn', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg16_bn', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg16_bn', pretrained=True)", "api_arguments": [{"name": "vgg16_bn", "type": "str", "description": "VGG16 model with batch normalization"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16_bn', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg16_bn": {"Top-1 error": 26.63, "Top-5 error": 8.5}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': Describe an API that can help identify objects and scenes in a series of photos from a vacation., 'Output': {'domain': 'Classification', 'api_call': 'model = torch.hub.load('pytorch/vision', 'vgg19', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained VGG19 model from PyTorch Hub, which can be used for large-scale image recognition tasks to identify objects and scenes in vacation photos.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg19', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg19', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg19', pretrained=True)", "api_arguments": [{"name": "vgg19", "type": "str", "description": "VGG19 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg19', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg19": {"Top-1 error": 27.62, "Top-5 error": 9.12}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': I need an API that can recognize the dog breed from a given image of a dog., 'Output': {'domain': 'Image Recognition', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg19_bn', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained VGG19 model with batch normalization from PyTorch Hub, which can be fine-tuned for dog breed recognition from a given image.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg19_bn', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg19_bn', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg19_bn', pretrained=True)", "api_arguments": [{"name": "vgg19_bn", "type": "str", "description": "VGG19 model with batch normalization"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg19_bn', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg19_bn": {"Top-1 error": 25.76, "Top-5 error": 8.15}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': A computer vision company needs an efficient image recognizer API for a security project. Find a suitable API to classify objects in an image., 'Output': {'domain': 'Image Classification', 'api_call': \"torch.hub.load('pytorch/vision', 'wide_resnet50_2', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the Wide ResNet50_2 API from PyTorch Hub, which is pretrained for object recognition using the ImageNet database.', 'code': 'import torch\nmodel = torch.hub.load(\"pytorch/vision\", \"wide_resnet50_2\", pretrained=True)\nmodel.eval()'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='wide_resnet50_2', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Wide Residual Networks", "api_name": "wide_resnet50_2", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='wide_resnet50_2', pretrained=True)", "api_arguments": "pretrained", "python_environment_requirements": "torch, torchvision", "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'wide_resnet50_2', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"wide_resnet50_2": {"Top-1 error": 21.49, "Top-5 error": 5.91}}}, "description": "Wide Residual networks simply have increased number of channels compared to ResNet. Otherwise the architecture is the same. Deeper ImageNet models with bottleneck block have increased number of channels in the inner 3x3 convolution. The wide_resnet50_2 and wide_resnet101_2 models were trained in FP16 with mixed precision training using SGD with warm restarts. Checkpoints have weights in half precision (except batch norm) for smaller size, and can be used in FP32 models too."}}
{"code": "{'Instruction': Provide a pretrained API that can identify and classify dogs in images., 'Output': {'domain': 'Classification', 'api_call': 'model = torch.hub.load('pytorch/vision', 'wide_resnet101_2', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Wide ResNet-101-2 model from PyTorch Hub, which can be used for image classification tasks such as identifying and classifying dogs.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'wide_resnet101_2', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='wide_resnet101_2', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Wide Residual Networks", "api_name": "wide_resnet101_2", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='wide_resnet101_2', pretrained=True)", "api_arguments": "pretrained", "python_environment_requirements": "torch, torchvision", "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'wide_resnet101_2', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"wide_resnet101_2": {"Top-1 error": 21.16, "Top-5 error": 5.72}}}, "description": "Wide Residual networks simply have increased number of channels compared to ResNet. Otherwise the architecture is the same. Deeper ImageNet models with bottleneck block have increased number of channels in the inner 3x3 convolution. The wide_resnet50_2 and wide_resnet101_2 models were trained in FP16 with mixed precision training using SGD with warm restarts. Checkpoints have weights in half precision (except batch norm) for smaller size, and can be used in FP32 models too."}}
{"code": "{'Instruction': A company called \"VideoTag\" wants to classify videos into different categories using machine learning. Provide an API that can help them with video classification., 'Output': {'domain': 'Video Classification', 'api_call': \"model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained SlowFast model for video classification from PyTorch Hub to categorize video clips into various action classes.', 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='slowfast_r50', pretrained=True)", "provider": "PyTorchVideo", "api_data": {"domain": "Video Classification", "framework": "PyTorchVideo", "functionality": "SlowFast Networks", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='slowfast_r50', pretrained=True)", "api_arguments": {"repository": "facebookresearch/pytorchvideo", "model": "slowfast_r50", "pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "pytorchvideo"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)", "device = 'cpu'", "model = model.eval()", "model = model.to(device)"], "performance": {"dataset": "Kinetics 400", "accuracy": {"top1": 76.94, "top5": 92.69}, "flops": 65.71, "params": 34.57}, "description": "Slow Fast model architectures are based on the paper 'SlowFast Networks for Video Recognition' by Christoph Feichtenhofer et al. They are pretrained on the Kinetics 400 dataset using the 8x8 setting. This model is capable of classifying video clips into different action categories. It is provided by the FAIR PyTorchVideo library."}}
{"code": "{'Instruction': Recommend an API that can classify images quickly and efficiently using an NVIDIA GPU., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_gpunet', pretrained=True, model_type=model_type, model_math=precision)\", 'api_provider': 'PyTorch', 'explanation': 'Load the NVIDIA GPUNet model from PyTorch Hub, which is designed to classify images quickly and efficiently using an NVIDIA GPU.', 'code': 'import torch\nmodel_type = \"GPUNet-0\"\nprecision = \"fp32\"\nmodel = torch.hub.load(\"NVIDIA/DeepLearningExamples:torchhub\", \"nvidia_gpunet\", pretrained=True, model_type=model_type, model_math=precision)'}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_gpunet', pretrained=True, model_type=model_type, model_math=precision)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "GPUNet Networks", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_gpunet', pretrained=True, model_type=model_type, model_math=precision)", "api_arguments": {"repository": "NVIDIA/DeepLearningExamples:torchhub", "model": "nvidia_gpunet", "pretrained": "True", "model_type": "GPUNet-0", "model_math": "fp32"}, "python_environment_requirements": ["torch", "validators", "matplotlib", "timm==0.5.4"], "example_code": ["import torch", "model_type = 'GPUNet-0'", "precision = 'fp32'", "gpunet = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_gpunet', pretrained=True, model_type=model_type, model_math=precision)", "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')", "gpunet.to(device)", "gpunet.eval()"], "performance": {"dataset": "IMAGENET", "description": "GPUNet demonstrates state-of-the-art inference performance up to 2x faster than EfficientNet-X and FBNet-V3."}, "description": "GPUNet is a family of Convolutional Neural Networks designed by NVIDIA using novel Neural Architecture Search (NAS) methods. They are optimized for NVIDIA GPU and TensorRT performance. GPUNet models are pretrained on the IMAGENET dataset and are capable of classifying images into different categories. The models are provided by the NVIDIA Deep Learning Examples library."}}
{"code": "{'Instruction': A news aggregator wants to categorize and summarize the news article. Which API they can use for text classification and understanding?, 'Output': {'domain': 'Natural Language Processing', 'api_call': 'model = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-cased')', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained BERT model for text classification and understanding from PyTorch Hub to categorize and summarize news articles.', 'code': 'import torch\nmodel = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-cased)'}}", "api_call": "torch.hub.load(repo_or_dir='huggingface/pytorch-transformers')", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Natural Language Processing", "api_name": "PyTorch-Transformers", "api_call": "torch.hub.load(repo_or_dir='huggingface/pytorch-transformers')", "api_arguments": ["pretrained_model_or_path", "output_attention", "output_hidden_states", "config", "from_tf"], "python_environment_requirements": ["tqdm", "boto3", "requests", "regex", "sentencepiece", "sacremoses"], "example_code": "import torch\ntokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-cased')\n\ntext_1 = \"Jim Henson was a puppeteer\"\ntext_2 = \"Who was Jim Henson ?\"\n\nindexed_tokens = tokenizer.encode(text_1, text_2, add_special_tokens=True)\n\nmodel = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-cased')\n\nwith torch.no_grad():\n encoded_layers, _ = model(tokens_tensor, token_type_ids=segments_tensors)", "performance": {"dataset": [{"name": "MRPC", "accuracy": "Not provided"}]}, "description": "PyTorch-Transformers is a library of state-of-the-art pre-trained models for Natural Language Processing (NLP) including BERT, GPT, GPT-2, Transformer-XL, XLNet, XLM, RoBERTa, and DistilBERT. The library provides functionality for tokenization, configuration, and various model architectures for different tasks such as causal language modeling, sequence classification, question answering, and masked language modeling."}}
{"code": "{'Instruction': A fintech company wants to build a model that can identify the pattern on a credit card. What API can they use to achieve this classification?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('mit-han-lab/once-for-all', 'ofa_supernet_mbv3_w10', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Use the pretrained Once-for-all (OFA) network from PyTorch Hub to classify patterns on credit cards. This model is efficient and provides excellent performance for image classification tasks.', 'code': 'import torch\nmodel = torch.hub.load('mit-han-lab/once-for-all', 'ofa_supernet_mbv3_w10', pretrained=True).eval()'}}", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/once-for-all', model='ofa_supernet_mbv3_w10', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Once-for-all (OFA) Networks", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/once-for-all', model='ofa_supernet_mbv3_w10', pretrained=True)", "api_arguments": {"repository": "mit-han-lab/once-for-all", "model": "ofa_supernet_mbv3_w10", "pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "super_net_name = 'ofa_supernet_mbv3_w10'", "super_net = torch.hub.load('mit-han-lab/once-for-all', super_net_name, pretrained=True).eval()"], "performance": {"description": "OFA networks outperform state-of-the-art NAS methods (up to 4.0% ImageNet top1 accuracy improvement over MobileNetV3, or same accuracy but 1.5x faster than MobileNetV3, 2.6x faster than EfficientNet w.r.t measured latency) while reducing many orders of magnitude GPU hours and CO2 emission."}, "description": "Once-for-all (OFA) networks are a family of neural networks designed by MIT Han Lab. They decouple training and search, achieving efficient inference across various edge devices and resource constraints. OFA networks are pretrained on the IMAGENET dataset and are capable of classifying images into different categories."}}
{"code": "{'Instruction': What is an API to provide me with a pretrained image classifier that would recognize object types in images?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models', 'resnet18_swsl', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the ResNet-18 swsl model from Facebook Research via PyTorch Hub. This model is pretrained with semi-supervised and semi-weakly supervised ImageNet models and can recognize object types in images.\", 'code': 'import torch\nmodel = torch.hub.load(\"facebookresearch/semi-supervised-ImageNet1K-models\", \"resnet18_swsl\", pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/semi-supervised-ImageNet1K-models', model='resnet18_swsl', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Semi-supervised and semi-weakly supervised ImageNet Models", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/semi-supervised-ImageNet1K-models', model='resnet18_swsl', pretrained=True)", "api_arguments": {"repository": "facebookresearch/semi-supervised-ImageNet1K-models", "model": "resnet18_swsl", "pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models', 'resnet18_swsl', pretrained=True)", "model.eval()"], "performance": {"description": "Semi-supervised and semi-weakly supervised ImageNet models achieve state-of-the-art accuracy of 81.2% on ImageNet for the widely used/adopted ResNet-50 model architecture."}, "description": "Semi-supervised and semi-weakly supervised ImageNet Models are introduced in the 'Billion scale semi-supervised learning for image classification' paper. These models are pretrained on a subset of unlabeled YFCC100M public image dataset and fine-tuned with the ImageNet1K training dataset. They are capable of classifying images into different categories and are provided by the Facebook Research library."}}
{"code": "{'Instruction': Provide me with a Machine Learning API that can classify objects in images with high accuracy and low computational cost., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_efficientnet_b0', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained NVIDIA EfficientNet-B0 model from PyTorch Hub, which offers high accuracy and low computational cost for image classification tasks.', 'code': \"import torch\nmodel = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_efficientnet_b0', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_efficientnet_b0', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "EfficientNet", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_efficientnet_b0', pretrained=True)", "api_arguments": ["model_name", "pretrained"], "python_environment_requirements": ["validators", "matplotlib"], "example_code": "import torch\n\nefficientnet = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_efficientnet_b0', pretrained=True)\nutils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_convnets_processing_utils')\n\nefficientnet.eval().to(device)\n\nbatch = torch.cat([utils.prepare_input_from_uri(uri) for uri in uris]).to(device)\n\nwith torch.no_grad():\n output = torch.nn.functional.softmax(efficientnet(batch), dim=1)\n \nresults = utils.pick_n_best(predictions=output, n=5)", "performance": {"dataset": {"name": "IMAGENET", "accuracy": "Not provided"}}, "description": "EfficientNet is a family of image classification models that achieve state-of-the-art accuracy while being smaller and faster. The models are trained with mixed precision using Tensor Cores on the NVIDIA Volta and Ampere GPU architectures. The EfficientNet models include EfficientNet-B0, EfficientNet-B4, EfficientNet-WideSE-B0, and EfficientNet-WideSE-B4. The WideSE models use wider Squeeze-and-Excitation layers than the original EfficientNet models, resulting in slightly better accuracy."}}
{"code": "{'Instruction': I need to translate a paragraph from English to French. What API would you recommend for this task?, 'Output': {'domain': 'Translation', 'api_call': \"model = torch.hub.load('pytorch/fairseq', 'transformer.wmt14.en-fr', tokenizer='moses', bpe='subword_nmt')\", 'api_provider': 'PyTorch', 'explanation': \"Use the Transformer NMT model provided by PyTorch Hub to translate a paragraph from English to French. This model is based on the 'Attention Is All You Need' paper and has been trained using large-scale semi-supervised training, back-translation, and noisy-channel reranking.\", 'code': \"import torch\nmodel = torch.hub.load('pytorch/fairseq', 'transformer.wmt14.en-fr', tokenizer='moses', bpe='subword_nmt')\"}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/fairseq')", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "Neural Machine Translation", "api_name": "Transformer (NMT)", "api_call": "torch.hub.load(repo_or_dir='pytorch/fairseq')", "api_arguments": ["model_name", "tokenizer", "bpe", "beam", "sampling", "sampling_topk"], "python_environment_requirements": ["bitarray", "fastBPE", "hydra-core", "omegaconf", "regex", "requests", "sacremoses", "subword_nmt"], "example_code": "import torch\n\nen2fr = torch.hub.load('pytorch/fairseq', 'transformer.wmt14.en-fr', tokenizer='moses', bpe='subword_nmt')\n\nen2fr.cuda()\n\nfr = en2fr.translate('Hello world!', beam=5)\nassert fr == 'Bonjour \u00e0 tous !'", "performance": {"dataset": [{"name": "WMT'14", "accuracy": "Not provided"}, {"name": "WMT'18", "accuracy": "Not provided"}, {"name": "WMT'19", "accuracy": "Not provided"}]}, "description": "Transformer (NMT) is a powerful sequence-to-sequence modeling architecture that produces state-of-the-art neural machine translation systems. It is based on the paper 'Attention Is All You Need' and has been improved using techniques such as large-scale semi-supervised training, back-translation, and noisy-channel reranking. It supports English-French and English-German translation as well as round-trip translation for paraphrasing."}}
{"code": "{'Instruction': A CEO of a podcast company is trying to write summaries of the podcasts. She needs an API that can convert the main points of a podcast to text., 'Output': {'domain': 'Text-to-Speech', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tacotron2', model_math='fp16')\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Tacotron 2 model from PyTorch Hub to convert the main points of a podcast to text.', 'code': 'import torch\ntacotron2 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tacotron2', model_math='fp16')'}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_tacotron2', model_math='fp16')", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Text-to-Speech", "api_name": "Tacotron 2", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_tacotron2', model_math='fp16')", "api_arguments": {"model_math": "fp16"}, "python_environment_requirements": ["numpy", "scipy", "librosa", "unidecode", "inflect", "libsndfile1"], "example_code": ["import torch", "tacotron2 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tacotron2', model_math='fp16')", "tacotron2 = tacotron2.to('cuda')", "tacotron2.eval()", "text = 'Hello world, I missed you so much.'", "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tts_utils')", "sequences, lengths = utils.prepare_input_sequence([text])", "with torch.no_grad():", " mel, _, _ = tacotron2.infer(sequences, lengths)", " audio = waveglow.infer(mel)", "audio_numpy = audio[0].data.cpu().numpy()", "rate = 22050"], "performance": {"dataset": "LJ Speech", "accuracy": "Not specified"}, "description": "The Tacotron 2 model generates mel spectrograms from input text using an encoder-decoder architecture, and it is designed for generating natural-sounding speech from raw transcripts without any additional prosody information. This implementation uses Dropout instead of Zoneout to regularize the LSTM layers. The WaveGlow model (also available via torch.hub) is a flow-based model that consumes the mel spectrograms to generate speech."}}
{"code": "{'Instruction': As a developer, I'm looking for an image classification API that has high accuracy., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', 'mealv2_resnest50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'For a high accuracy image classification task, load the pretrained MEAL V2 model with ResNet-50 architecture from PyTorch Hub.', 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', 'mealv2_resnest50', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50', pretrained=True)", "api_arguments": {"model_name": "mealv2_resnest50"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ ResNet50", "resolution": "224", "parameters": "25.6M", "top1": "80.67", "top5": "95.09"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': Tell me how to classify an image based on different object categories with the highest performance. Give me an API to do that., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50_cutmix', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MEAL-V2 (MEAL Version 2) model from PyTorch Hub for state-of-the-art image classification into different object categories.', 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50_cutmix', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50_cutmix', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50_cutmix', pretrained=True)", "api_arguments": {"model_name": "mealv2_resnest50_cutmix"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 + CutMix w/ ResNet50", "resolution": "224", "parameters": "25.6M", "top1": "80.98", "top5": "95.35"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': Recommend a highly-performing image classification model that does not rely on tricks such as architecture modification, data augmentation or label smoothing., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MEAL-V2 model for image classification from PyTorch Hub. It achieves state-of-the-art results without using common tricks like architecture modification, data augmentation, or label smoothing.', 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50_380x380', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50_380x380', pretrained=True)", "api_arguments": {"model_name": "mealv2_resnest50_380x380"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ ResNet50", "resolution": "380", "parameters": "25.6M", "top1": "81.72", "top5": "95.81"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': I need an image classification API to predict the highest possible accuracy on ImageNet., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenetv3_small_075', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MEAL V2 model based on ResNet50 and MobileNet V3 architectures to achieve a high top-1 accuracy on the ImageNet dataset.', 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenetv3_small_075', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenetv3_small_075', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenetv3_small_075', pretrained=True)", "api_arguments": {"model_name": "mealv2_mobilenetv3_small_075"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ MobileNet V3-Small 0.75", "resolution": "224", "parameters": "2.04M", "top1": "67.60", "top5": "87.23"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': A developer in your team is working on an image classification app and would like to use an efficient pretrained model. Offer an API that can assist him., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', 'mealv2_mobilenet_v3_large_100', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MEAL V2 model from PyTorch Hub, which has a MobileNet V3-Large architecture, providing a balance between accuracy and efficiency for image classification tasks.', 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', 'mealv2_mobilenet_v3_large_100', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenet_v3_large_100', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenet_v3_large_100', pretrained=True)", "api_arguments": {"model_name": "mealv2_mobilenet_v3_large_100"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ MobileNet V3-Large 1.0", "resolution": "224", "parameters": "5.48M", "top1": "76.92", "top5": "93.32"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': Help me find an API to classify different dog breeds from their image. Our product integrates with Instagram., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', 'mealv2_mobilenetv3_small_100', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MEAL V2 model with MobileNet V3-Small 1.0 architecture for image classification, which can be fine-tuned to classify different dog breeds based on their images from Instagram.', 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', 'mealv2_mobilenetv3_small_100', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenetv3_small_100', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenetv3_small_100', pretrained=True)", "api_arguments": {"model_name": "mealv2_mobilenetv3_small_100"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ MobileNet V3-Small 1.0", "resolution": "224", "parameters": "2.54M", "top1": "69.65", "top5": "88.71"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': Generate an API that classifies images with enhanced accuracy and without depending on architecture modification, data augmentation, or other training tricks., 'Output': {'domain': 'Image Classification', 'api_call': 'model = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_efficientnet_b0', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MEAL V2 EfficientNet-B0 model from PyTorch Hub to classify images with high accuracy without using external training data, architecture modifications, or other tricks.', 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_efficientnet_b0', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_efficientnet_b0', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_efficientnet_b0', pretrained=True)", "api_arguments": {"model_name": "mealv2_efficientnet_b0"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ EfficientNet-B0", "resolution": "224", "parameters": "5.29M", "top1": "78.29", "top5": "93.95"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': Find an API that can perform scene recognition in videos and is pretrained on a large dataset for enhanced accuracy., 'Output': {'domain': 'Video Classification', 'api_call': \"model = torch.hub.load('facebookresearch/pytorchvideo', 'slow_r50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained 3D ResNet model (slow_r50 variant) from PyTorch Hub, which performs scene recognition in videos using the Kinetics 400 dataset.\", 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/pytorchvideo', 'slow_r50', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='slow_r50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Video Classification", "framework": "PyTorch", "functionality": "3D ResNet", "api_name": "slow_r50", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='slow_r50', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["torch", "json", "urllib", "pytorchvideo", "torchvision", "torchaudio", "torchtext", "torcharrow", "TorchData", "TorchRec", "TorchServe", "PyTorch on XLA Devices"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/pytorchvideo', 'slow_r50', pretrained=True)", "device = 'cpu'", "model = model.eval()", "model = model.to(device)"], "performance": {"dataset": "Kinetics 400", "accuracy": {"top_1": 74.58, "top_5": 91.63}, "Flops (G)": 54.52, "Params (M)": 32.45}, "description": "The 3D ResNet model is a Resnet-style video classification network pretrained on the Kinetics 400 dataset. It is based on the architecture from the paper 'SlowFast Networks for Video Recognition' by Christoph Feichtenhofer et al."}}
{"code": "{'Instruction': I need an API to detect objects in a traffic video, segment drivable areas, and detect lane lines. Provide an API that can perform this task., 'Output': {'domain': 'Object Detection', 'api_call': \"model = torch.hub.load('datvuthanh/hybridnets', 'hybridnets', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained HybridNets model from PyTorch Hub, which is an end-to-end perception neural network that can detect traffic objects, segment drivable areas, and perform lane detection.\", 'code': \"import torch\nmodel = torch.hub.load('datvuthanh/hybridnets', 'hybridnets', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='datvuthanh/hybridnets', model='hybridnets', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Object Detection", "framework": "PyTorch", "functionality": "Traffic Object Detection, Drivable Area Segmentation, Lane Detection", "api_name": "HybridNets", "api_call": "torch.hub.load(repo_or_dir='datvuthanh/hybridnets', model='hybridnets', pretrained=True)", "api_arguments": "pretrained", "python_environment_requirements": "Python>=3.7, PyTorch>=1.10", "example_code": "import torch\nmodel = torch.hub.load('datvuthanh/hybridnets', 'hybridnets', pretrained=True)\nimg = torch.randn(1,3,640,384)\nfeatures, regression, classification, anchors, segmentation = model(img)", "performance": {"dataset": [{"name": "BDD100K", "accuracy": {"Traffic Object Detection": {"Recall (%)": 92.8, "mAP@0.5 (%)": 77.3}, "Drivable Area Segmentation": {"Drivable mIoU (%)": 90.5}, "Lane Line Detection": {"Accuracy (%)": 85.4, "Lane Line IoU (%)": 31.6}}}]}, "description": "HybridNets is an end2end perception network for multi-tasks. Our work focused on traffic object detection, drivable area segmentation and lane detection. HybridNets can run real-time on embedded systems, and obtains SOTA Object Detection, Lane Detection on BDD100K Dataset."}}
{"code": "{'Instruction': I am designing an autonomous navigation system for a robot. What API should I use that detects objects, drivable areas, and lane lines simultaneously?, 'Output': {'domain': 'Object Detection', 'api_call': \"model = torch.hub.load('hustvl/yolop', 'yolop', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Use the YOLOP model from PyTorch Hub, which is an efficient multi-task network for object detection, drivable area segmentation, and lane detection in real-time.', 'code': \"import torch\nmodel = torch.hub.load('hustvl/yolop', 'yolop', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='hustvl/yolop', model='yolop', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Object Detection", "framework": "PyTorch", "functionality": "Object Detection, Drivable Area Segmentation, Lane Detection", "api_name": "YOLOP", "api_call": "torch.hub.load(repo_or_dir='hustvl/yolop', model='yolop', pretrained=True)", "api_arguments": "pretrained", "python_environment_requirements": "pip install -qr https://github.com/hustvl/YOLOP/blob/main/requirements.txt", "example_code": "import torch\nmodel = torch.hub.load('hustvl/yolop', 'yolop', pretrained=True)\nimg = torch.randn(1,3,640,640)\ndet_out, da_seg_out,ll_seg_out = model(img)", "performance": {"dataset": "BDD100K", "accuracy": {"Object Detection": {"Recall(%)": 89.2, "mAP50(%)": 76.5, "Speed(fps)": 41}, "Drivable Area Segmentation": {"mIOU(%)": 91.5, "Speed(fps)": 41}, "Lane Detection": {"mIOU(%)": 70.5, "IOU(%)": 26.2}}}, "description": "YOLOP is an efficient multi-task network that can jointly handle three crucial tasks in autonomous driving: object detection, drivable area segmentation and lane detection. And it is also the first to reach real-time on embedded devices while maintaining state-of-the-art level performance on the BDD100K dataset."}}
{"code": "{'Instruction': Is there any technique that can compute relative depth from a single image? Which API can do this job?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('intel-isl/MiDaS', 'MiDaS_small', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MiDaS model from PyTorch Hub, which can compute relative depth from a single image.', 'code': 'import torch\nmodel = torch.hub.load('intel-isl/MiDaS', 'MiDaS_small', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='intel-isl/MiDaS', model='MiDaS_small', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Computing relative depth from a single image", "api_name": "MiDaS", "api_call": "torch.hub.load(repo_or_dir='intel-isl/MiDaS', model='MiDaS_small', pretrained=True)", "api_arguments": {"repo_or_dir": "intel-isl/MiDaS", "model": "model_type"}, "python_environment_requirements": "pip install timm", "example_code": ["import cv2", "import torch", "import urllib.request", "import matplotlib.pyplot as plt", "url, filename = ('https://github.com/pytorch/hub/raw/master/images/dog.jpg', 'dog.jpg')", "urllib.request.urlretrieve(url, filename)", "model_type = 'DPT_Large'", "midas = torch.hub.load('intel-isl/MiDaS', 'MiDaS_small')", "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')", "midas.to(device)", "midas.eval()", "midas_transforms = torch.hub.load('intel-isl/MiDaS', 'transforms')", "if model_type == 'DPT_Large' or model_type == 'DPT_Hybrid':", " transform = midas_transforms.dpt_transform", "else:", " transform = midas_transforms.small_transform", "img = cv2.imread(filename)", "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)", "input_batch = transform(img).to(device)", "with torch.no_grad():", " prediction = midas(input_batch)", "prediction = torch.nn.functional.interpolate(", " prediction.unsqueeze(1),", " size=img.shape[:2],", " mode='bicubic',", " align_corners=False,", ").squeeze()", "output = prediction.cpu().numpy()", "plt.imshow(output)", "plt.show()"], "performance": {"dataset": "10 distinct datasets", "accuracy": "Multi-objective optimization"}, "description": "MiDaS computes relative inverse depth from a single image. The repository provides multiple models that cover different use cases ranging from a small, high-speed model to a very large model that provide the highest accuracy. The models have been trained on 10 distinct datasets using multi-objective optimization to ensure high quality on a wide range of inputs."}}
{"code": "{'Instruction': Help me extract the depth information from a single image taken with my camera. Provide an API that can do this., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('intel-isl/MiDaS', 'DPT_Hybrid', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained MiDaS DPT_Hybrid model from PyTorch Hub, which can compute relative depth information from a single image.\", 'code': \"import torch\nmodel = torch.hub.load('intel-isl/MiDaS', 'DPT_Hybrid', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='intel-isl/MiDaS', model='DPT_Hybrid', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Computing relative depth from a single image", "api_name": "MiDaS", "api_call": "torch.hub.load(repo_or_dir='intel-isl/MiDaS', model='DPT_Hybrid', pretrained=True)", "api_arguments": {"repo_or_dir": "intel-isl/MiDaS", "model": "model_type"}, "python_environment_requirements": "pip install timm", "example_code": ["import cv2", "import torch", "import urllib.request", "import matplotlib.pyplot as plt", "url, filename = ('https://github.com/pytorch/hub/raw/master/images/dog.jpg', 'dog.jpg')", "urllib.request.urlretrieve(url, filename)", "model_type = 'DPT_Large'", "midas = torch.hub.load('intel-isl/MiDaS', 'DPT_Hybrid')", "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')", "midas.to(device)", "midas.eval()", "midas_transforms = torch.hub.load('intel-isl/MiDaS', 'transforms')", "if model_type == 'DPT_Large' or model_type == 'DPT_Hybrid':", " transform = midas_transforms.dpt_transform", "else:", " transform = midas_transforms.small_transform", "img = cv2.imread(filename)", "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)", "input_batch = transform(img).to(device)", "with torch.no_grad():", " prediction = midas(input_batch)", "prediction = torch.nn.functional.interpolate(", " prediction.unsqueeze(1),", " size=img.shape[:2],", " mode='bicubic',", " align_corners=False,", ").squeeze()", "output = prediction.cpu().numpy()", "plt.imshow(output)", "plt.show()"], "performance": {"dataset": "10 distinct datasets", "accuracy": "Multi-objective optimization"}, "description": "MiDaS computes relative inverse depth from a single image. The repository provides multiple models that cover different use cases ranging from a small, high-speed model to a very large model that provide the highest accuracy. The models have been trained on 10 distinct datasets using multi-objective optimization to ensure high quality on a wide range of inputs."}}
{"code": "{'Instruction': I have a mix of pop songs and I need an API that can break it into separate tracks for vocals, drums, etc., 'Output': {'domain': 'Audio Separation', 'api_call': 'separator = torch.hub.load('sigsep/open-unmix-pytorch', 'umxhq', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Open-Unmix model from PyTorch Hub to separate pop music into vocals, drums, bass, and other instruments.', 'code': 'import torch\nseparator = torch.hub.load('sigsep/open-unmix-pytorch', 'umxhq', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='sigsep/open-unmix-pytorch', model='umxhq', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Audio Separation", "framework": "PyTorch", "functionality": "Music Source Separation", "api_name": "Open-Unmix", "api_call": "torch.hub.load(repo_or_dir='sigsep/open-unmix-pytorch', model='umxhq', pretrained=True)", "api_arguments": ["model_name"], "python_environment_requirements": ["PyTorch >=1.6.0", "torchaudio"], "example_code": ["import torch", "separator = torch.hub.load('sigsep/open-unmix-pytorch', 'umxhq')", "audio = torch.rand((1, 2, 100000))", "original_sample_rate = separator.sample_rate", "estimates = separator(audio)"], "performance": {"dataset": "MUSDB18", "accuracy": "N/A"}, "description": "Open-Unmix provides ready-to-use models that allow users to separate pop music into four stems: vocals, drums, bass and the remaining other instruments. The models were pre-trained on the freely available MUSDB18 dataset."}}
{"code": "{'Instruction': Help me choose an API that can identify the specific species of the bird in a given photograph., 'Output': {'domain': 'Fine-grained Classification', 'api_call': \"model = torch.hub.load('nicolalandro/ntsnet-cub200', 'ntsnet', pretrained=True, **{'topN': 6, 'device':'cpu', 'num_classes': 200})\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained NTS-Net model for bird species identification from PyTorch Hub. The model is fine-tuned on the CUB200 2011 dataset, which consists of 200 bird species.', 'code': \"import torch\nmodel = torch.hub.load('nicolalandro/ntsnet-cub200', 'ntsnet', pretrained=True, **{'topN': 6, 'device':'cpu', 'num_classes': 200})\"}}", "api_call": "torch.hub.load(repo_or_dir='nicolalandro/ntsnet-cub200', model='ntsnet', pretrained=True, **{'topN': 6, 'device':'cpu', 'num_classes': 200})", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Fine-grained image classifier", "api_name": "ntsnet", "api_call": "torch.hub.load(repo_or_dir='nicolalandro/ntsnet-cub200', model='ntsnet', pretrained=True, **{'topN': 6, 'device':'cpu', 'num_classes': 200})", "api_arguments": {"pretrained": "True", "topN": "6", "device": "cpu", "num_classes": "200"}, "python_environment_requirements": ["torch", "torchvision", "PIL"], "example_code": "from torchvision import transforms\nimport torch\nimport urllib\nfrom PIL import Image\n\ntransform_test = transforms.Compose([\n transforms.Resize((600, 600), Image.BILINEAR),\n transforms.CenterCrop((448, 448)),\n transforms.ToTensor(),\n transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n])\n\nmodel = torch.hub.load('nicolalandro/ntsnet-cub200', 'ntsnet', pretrained=True, **{'topN': 6, 'device':'cpu', 'num_classes': 200})\nmodel.eval()\n\nurl = 'https://raw.githubusercontent.com/nicolalandro/ntsnet-cub200/master/images/nts-net.png'\nimg = Image.open(urllib.request.urlopen(url))\nscaled_img = transform_test(img)\ntorch_images = scaled_img.unsqueeze(0)\n\nwith torch.no_grad():\n top_n_coordinates, concat_out, raw_logits, concat_logits, part_logits, top_n_index, top_n_prob = model(torch_images)\n\n_, predict = torch.max(concat_logits, 1)\npred_id = predict.item()\nprint('bird class:', model.bird_classes[pred_id])", "performance": {"dataset": "CUB200 2011", "accuracy": "Not provided"}, "description": "This is an nts-net pretrained with CUB200 2011 dataset, which is a fine-grained dataset of birds species."}}
{"code": "{'Instruction': Create a high-resolution image segmentation model that can distinguish between objects in a scene., 'Output': {'domain': 'Semantic Segmentation', 'api_call': \"model = torch.hub.load('pytorch/vision', 'deeplabv3_resnet50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the DeepLabV3 model with a ResNet-50 backbone pretrained on the COCO train2017 dataset to perform high-resolution semantic segmentation of input images.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'deeplabv3_resnet50', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='deeplabv3_resnet50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "DeepLabV3", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='deeplabv3_resnet50', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": "latest", "torchvision": "latest", "PIL": "latest", "matplotlib": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_resnet50', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "input_image = input_image.convert('RGB')", "preprocess = transforms.Compose([", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)['out'][0]", "output_predictions = output.argmax(0)"], "performance": {"dataset": "COCO val2017", "accuracy": [{"model": "deeplabv3_resnet50", "Mean IOU": 66.4, "Global Pixelwise Accuracy": 92.4}]}, "description": "DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset."}}
{"code": "{'Instruction': What is an efficient method to transcribe voicemails into text using an API?, 'Output': {'domain': 'Speech-To-Text', 'api_call': \"model = torch.hub.load('snakers4/silero-models', 'silero_stt', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Use the pretrained Silero Speech-to-Text model from PyTorch Hub to efficiently transcribe voicemails into text.', 'code': 'import torch\nmodel, decoder, utils = torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_stt', language='en', device=torch.device('cpu'))'}}", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_stt', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Speech-To-Text", "api_name": "Silero Models", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_stt', pretrained=True)", "api_arguments": {"repo_or_dir": "snakers4/silero-models", "model": "silero_stt", "language": ["en", "de", "es"], "device": "device"}, "python_environment_requirements": ["pip install -q torchaudio omegaconf soundfile"], "example_code": ["import torch", "import zipfile", "import torchaudio", "from glob import glob", "device = torch.device('cpu')", "model, decoder, utils = torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_stt', language='en', device=device)", "(read_batch, split_into_batches, read_audio, prepare_model_input) = utils", "torch.hub.download_url_to_file('https://opus-codec.org/static/examples/samples/speech_orig.wav', dst ='speech_orig.wav', progress=True)", "test_files = glob('speech_orig.wav')", "batches = split_into_batches(test_files, batch_size=10)", "input = prepare_model_input(read_batch(batches[0]), device=device)", "output = model(input)", "for example in output:", " print(decoder(example.cpu()))"], "performance": {"dataset": "Open-STT", "accuracy": "See the wiki for quality and performance benchmarks"}, "description": "Silero Speech-To-Text models provide enterprise grade STT in a compact form-factor for several commonly spoken languages. The models are robust to a variety of dialects, codecs, domains, noises, and lower sampling rates. They consume a normalized audio in the form of samples and output frames with token probabilities. A decoder utility is provided for simplicity."}}
{"code": "{'Instruction': An edTech company wants to build an app that reads content from their textbook to the students. Suggest an API that can convert written text to speech., 'Output': {'domain': 'Text-To-Speech', 'api_call': \"model = torch.hub.load('snakers4/silero-models', 'silero_tts', language=language, speaker=speaker, pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Silero Text-To-Speech model from PyTorch Hub, which can convert written text to naturally sounding speech for educators and students.', 'code': \"import torch\nlanguage = 'en'\nspeaker = 'lj_16khz'\ndevice = torch.device('cpu')\nmodel, symbols, sample_rate, example_text, apply_tts = torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_tts', language=language, speaker=speaker)\nmodel = model.to(device)\"}}", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_tts', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Text-To-Speech", "framework": "PyTorch", "functionality": "Text-To-Speech", "api_name": "Silero Text-To-Speech Models", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_tts', pretrained=True)", "api_arguments": {"repo_or_dir": "snakers4/silero-models", "model": "silero_tts", "language": "language", "speaker": "speaker"}, "python_environment_requirements": ["pip install -q torchaudio omegaconf"], "example_code": "import torch\nlanguage = 'en'\nspeaker = 'lj_16khz'\ndevice = torch.device('cpu')\nmodel, symbols, sample_rate, example_text, apply_tts = torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_tts', language=language, speaker=speaker)\nmodel = model.to(device)\naudio = apply_tts(texts=[example_text], model=model, sample_rate=sample_rate, symbols=symbols, device=device)", "performance": {"dataset": [{"language": "Russian", "speakers": 6}, {"language": "English", "speakers": 1}, {"language": "German", "speakers": 1}, {"language": "Spanish", "speakers": 1}, {"language": "French", "speakers": 1}], "accuracy": "High throughput on slow hardware. Decent performance on one CPU thread"}, "description": "Silero Text-To-Speech models provide enterprise grade TTS in a compact form-factor for several commonly spoken languages. They offer one-line usage, naturally sounding speech, no GPU or training required, minimalism and lack of dependencies, a library of voices in many languages, support for 16kHz and 8kHz out of the box."}}
{"code": "{'Instruction': Find me an API that can generate low-resolution images of clothing items., 'Output': {'domain': 'Image Generation', 'api_call': \"model = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'DCGAN', pretrained=True, useGPU=use_gpu)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Deep Convolutional Generative Adversarial Networks (DCGAN) model from PyTorch Hub, which is efficient for generating low-resolution images of clothing items.', 'code': 'import torch\nuse_gpu = True if torch.cuda.is_available() else False\nmodel = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'DCGAN', pretrained=True, useGPU=use_gpu)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorch_GAN_zoo:hub', model='DCGAN', pretrained=True, useGPU=use_gpu)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Generative Adversarial Networks", "api_name": "DCGAN", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorch_GAN_zoo:hub', model='DCGAN', pretrained=True, useGPU=use_gpu)", "api_arguments": {"pretrained": "True", "useGPU": "use_gpu"}, "python_environment_requirements": "Python 3", "example_code": {"import": ["import torch", "import matplotlib.pyplot as plt", "import torchvision"], "use_gpu": "use_gpu = True if torch.cuda.is_available() else False", "load_model": "model = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'DCGAN', pretrained=True, useGPU=use_gpu)", "build_noise_data": "noise, _ = model.buildNoiseData(num_images)", "generate_images": "with torch.no_grad(): generated_images = model.test(noise)", "plot_images": ["plt.imshow(torchvision.utils.make_grid(generated_images).permute(1, 2, 0).cpu().numpy())", "plt.show()"]}, "performance": {"dataset": "FashionGen", "accuracy": "N/A"}, "description": "DCGAN is a model designed in 2015 by Radford et. al. in the paper Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks. It is a GAN architecture both very simple and efficient for low resolution image generation (up to 64x64)."}}
{"code": "{'Instruction': A startup wants to build an application that helps users categorize the images they upload. Can you recommend an API for that?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x8d_wsl', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the ResNext WSL model pretrained on PyTorch Hub for high-accuracy image classification.', 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x8d_wsl', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x8d_wsl', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNext WSL", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x8d_wsl', pretrained=True)", "api_arguments": [{"name": "resnext101_32x8d_wsl", "type": "str", "description": "ResNeXt-101 32x8d WSL model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x8d_wsl')", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(output[0])", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeXt-101 32x8d": {"Top-1 Acc.": "82.2", "Top-5 Acc.": "96.4"}}}, "description": "The provided ResNeXt models are pre-trained in weakly-supervised fashion on 940 million public images with 1.5K hashtags matching with 1000 ImageNet1K synsets, followed by fine-tuning on ImageNet1K dataset. The models significantly improve the training accuracy on ImageNet compared to training from scratch. They achieve state-of-the-art accuracy of 85.4% on ImageNet with the ResNext-101 32x48d model."}}
{"code": "{'Instruction': Provide me with an image classification API that uses weakly-supervised training and can achieve high accuracy on the ImageNet dataset., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNext WSL model from PyTorch Hub, which is trained using weakly-supervised learning on a large-scale dataset and achieves high accuracy on the ImageNet dataset.', 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x16d_wsl', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNext WSL", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x16d_wsl', pretrained=True)", "api_arguments": [{"name": "resnext101_32x16d_wsl", "type": "str", "description": "ResNeXt-101 32x16d WSL model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl')", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(output[0])", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeXt-101 32x16d": {"Top-1 Acc.": "84.2", "Top-5 Acc.": "97.2"}}}, "description": "The provided ResNeXt models are pre-trained in weakly-supervised fashion on 940 million public images with 1.5K hashtags matching with 1000 ImageNet1K synsets, followed by fine-tuning on ImageNet1K dataset. The models significantly improve the training accuracy on ImageNet compared to training from scratch. They achieve state-of-the-art accuracy of 85.4% on ImageNet with the ResNext-101 32x48d model."}}
{"code": "{'Instruction': I am working on a computer vision task for my company, and I would like a well-performing machine learning API that can provide correct accuracy in classifying images of objects., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x32d_wsl', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the ResNext WSL model from PyTorch Hub with pre-trained weights for classification of objects in images. The model is trained on ImageNet, ensuring high accuracy.', 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x32d_wsl', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x32d_wsl', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNext WSL", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x32d_wsl', pretrained=True)", "api_arguments": [{"name": "resnext101_32x32d_wsl", "type": "str", "description": "ResNeXt-101 32x32d WSL model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x32d_wsl')", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(output[0])", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeXt-101 32x32d": {"Top-1 Acc.": "85.1", "Top-5 Acc.": "97.5"}}}, "description": "The provided ResNeXt models are pre-trained in weakly-supervised fashion on 940 million public images with 1.5K hashtags matching with 1000 ImageNet1K synsets, followed by fine-tuning on ImageNet1K dataset. The models significantly improve the training accuracy on ImageNet compared to training from scratch. They achieve state-of-the-art accuracy of 85.4% on ImageNet with the ResNext-101 32x48d model."}}
{"code": "{'Instruction': I have a brain MRI scan and I want to detect abnormalities in the scan. Can you recommend an API for this task?, 'Output': {'domain': 'Medical Image Analysis', 'api_call': \"model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Use the pretrained U-Net model for brain MRI segmentation in PyTorch Hub. This model can segment abnormalities in brain MRI scans with 3 input channels from pre-contrast, FLAIR, and post-contrast sequences.', 'code': 'import torch\nmodel = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='mateuszbuda/brain-segmentation-pytorch', model='unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "Biomedical Image Segmentation", "api_name": "U-Net for brain MRI", "api_call": "torch.hub.load(repo_or_dir='mateuszbuda/brain-segmentation-pytorch', model='unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)", "api_arguments": {"in_channels": 3, "out_channels": 1, "init_features": 32, "pretrained": true}, "python_environment_requirements": ["torch", "torchvision", "numpy", "PIL"], "example_code": ["import torch", "model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)", "import numpy as np", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "m, s = np.mean(input_image, axis=(0, 1)), np.std(input_image, axis=(0, 1))", "preprocess = transforms.Compose([transforms.ToTensor(),transforms.Normalize(mean=m, std=s)])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model = model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(torch.round(output[0]))"], "performance": {"dataset": "kaggle.com/mateuszbuda/lgg-mri-segmentation"}, "description": "U-Net with batch normalization for biomedical image segmentation with pretrained weights for abnormality segmentation in brain MRI. The model comprises four levels of blocks containing two convolutional layers with batch normalization and ReLU activation function, and one max pooling layer in the encoding part and up-convolutional layers instead in the decoding part. The number of convolutional filters in each block is 32, 64, 128, and 256. The bottleneck layer has 512 convolutional filters. From the encoding layers, skip connections are used to the corresponding layers in the decoding part. Input image is a 3-channel brain MRI slice from pre-contrast, FLAIR, and post-contrast sequences, respectively. Output is a one-channel probability map of abnormality regions with the same size as the input image."}}
{"code": "{'Instruction': A prankster is building a fake celebrity face generator. Recommend an API that can generate realistic fake celebrity faces., 'Output': {'domain': 'Classification', 'api_call': 'model = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'PGAN', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Progressive Growing of GANs (PGAN) model from PyTorch Hub, which is capable of generating realistic, high-quality fake celebrity faces.', 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'PGAN', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorch_GAN_zoo:hub', model='PGAN', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Generative Adversarial Networks (GANs)", "api_name": "PGAN", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorch_GAN_zoo:hub', model='PGAN', pretrained=True)", "api_arguments": {"repo_or_dir": "facebookresearch/pytorch_GAN_zoo:hub", "model": "PGAN", "model_name": "celebAHQ-512", "pretrained": "True", "useGPU": "use_gpu"}, "python_environment_requirements": "Python 3", "example_code": {"import": "import torch", "use_gpu": "use_gpu = True if torch.cuda.is_available() else False", "load_model": "model = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'PGAN', model_name='celebAHQ-512', pretrained=True, useGPU=use_gpu)", "build_noise_data": "noise, _ = model.buildNoiseData(num_images)", "test": "generated_images = model.test(noise)", "plot_images": {"import_matplotlib": "import matplotlib.pyplot as plt", "import_torchvision": "import torchvision", "make_grid": "grid = torchvision.utils.make_grid(generated_images.clamp(min=-1, max=1), scale_each=True, normalize=True)", "imshow": "plt.imshow(grid.permute(1, 2, 0).cpu().numpy())", "show": "plt.show()"}}, "performance": {"dataset": "celebA", "accuracy": "High-quality celebrity faces"}, "description": "Progressive Growing of GANs (PGAN) is a method for generating high-resolution images using generative adversarial networks. The model is trained progressively, starting with low-resolution images and gradually increasing the resolution until the desired output is achieved. This implementation is based on the paper by Tero Karras et al., 'Progressive Growing of GANs for Improved Quality, Stability, and Variation'."}}
{"code": "{'Instruction': Abby wants to identify the objects present in her collection of images. Identify a model that can analyze Abby's images., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resneXt', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained NVIDIA ResNeXt101-32x4d model from PyTorch Hub to classify the objects in the images collected by Abby.', 'code': 'import torch\nmodel = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resneXt', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_resneXt', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNeXt101-32x4d", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_resneXt', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "validators", "matplotlib"], "example_code": ["import torch", "from PIL import Image", "import torchvision.transforms as transforms", "import numpy as np", "import json", "import requests", "import matplotlib.pyplot as plt", "import warnings", "warnings.filterwarnings('ignore')", "%matplotlib inline", "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")", "print(f'Using {device} for inference')", "resneXt = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resneXt')", "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_convnets_processing_utils')", "resneXt.eval().to(device)", "uris = [", " 'http://images.cocodataset.org/test-stuff2017/000000024309.jpg',", " 'http://images.cocodataset.org/test-stuff2017/000000028117.jpg',", " 'http://images.cocodataset.org/test-stuff2017/000000006149.jpg',", " 'http://images.cocodataset.org/test-stuff2017/000000004954.jpg',", "]", "batch = torch.cat(", " [utils.prepare_input_from_uri(uri) for uri in uris]", ").to(device)", "with torch.no_grad():", " output = torch.nn.functional.softmax(resneXt(batch), dim=1)", "results = utils.pick_n_best(predictions=output, n=5)", "for uri, result in zip(uris, results):", " img = Image.open(requests.get(uri, stream=True).raw)", " img.thumbnail((256,256), Image.ANTIALIAS)", " plt.imshow(img)", " plt.show()", " print(result)"], "performance": {"dataset": "IMAGENET", "accuracy": "Not provided"}, "description": "ResNeXt101-32x4d is a model introduced in the Aggregated Residual Transformations for Deep Neural Networks paper. It is based on regular ResNet model, substituting 3x3 convolutions inside the bottleneck block for 3x3 grouped convolutions. This model is trained with mixed precision using Tensor Cores on Volta, Turing, and the NVIDIA Ampere GPU architectures. It can be deployed for inference on the NVIDIA Triton Inference Server using TorchScript, ONNX Runtime or TensorRT as an execution backend."}}
{"code": "{'Instruction': Imagine you received a huge dataset of unidentified animals, and you need to classify them according to their species. Provide me an API that can classify them given their image., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'alexnet', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained AlexNet model from PyTorch Hub for image classification tasks, which can be fine-tuned to classify animal species given their images.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'alexnet', pretrained=True)\nmodel.eval()'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='alexnet', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "AlexNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='alexnet', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": ">=1.9.0", "torchvision": ">=0.10.0"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"top-1_error": 43.45, "top-5_error": 20.91}}, "description": "AlexNet is a deep convolutional neural network that achieved a top-5 error of 15.3% in the 2012 ImageNet Large Scale Visual Recognition Challenge. The main contribution of the original paper was the depth of the model, which was computationally expensive but made feasible through the use of GPUs during training. The pretrained AlexNet model in PyTorch can be used for image classification tasks."}}
{"code": "{'Instruction': How can I get an accurate text classification model based on BERT but with improvements?, 'Output': {'domain': 'Text Classification', 'api_call': \"model = torch.hub.load('pytorch/fairseq', 'roberta.large', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained RoBERTa model from PyTorch Hub, which is an improved version of BERT fine-tuned for better text classification.', 'code': 'import torch\nmodel = torch.hub.load(\"pytorch/fairseq\", \"roberta.large\", pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/fairseq', model='roberta.large', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "RoBERTa", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='pytorch/fairseq', model='roberta.large', pretrained=True)", "api_arguments": ["'pytorch/fairseq'", "'roberta.large'"], "python_environment_requirements": ["regex", "requests", "hydra-core", "omegaconf"], "example_code": ["import torch", "roberta = torch.hub.load('pytorch/fairseq', 'roberta.large')", "roberta.eval()", "tokens = roberta.encode('Hello world!')", "last_layer_features = roberta.extract_features(tokens)", "all_layers = roberta.extract_features(tokens, return_all_hiddens=True)", "roberta = torch.hub.load('pytorch/fairseq', 'roberta.large.mnli')", "roberta.eval()", "tokens = roberta.encode('Roberta is a heavily optimized version of BERT.', 'Roberta is not very optimized.')", "prediction = roberta.predict('mnli', tokens).argmax().item()", "tokens = roberta.encode('Roberta is a heavily optimized version of BERT.', 'Roberta is based on BERT.')", "prediction = roberta.predict('mnli', tokens).argmax().item()", "roberta.register_classification_head('new_task', num_classes=3)", "logprobs = roberta.predict('new_task', tokens)"], "performance": {"dataset": "MNLI", "accuracy": "N/A"}, "description": "RoBERTa is a robustly optimized version of BERT, a revolutionary self-supervised pretraining technique that learns to predict intentionally hidden (masked) sections of text. RoBERTa builds on BERT's language masking strategy and modifies key hyperparameters, including removing BERT's next-sentence pretraining objective, and training with much larger mini-batches and learning rates. RoBERTa was also trained on an order of magnitude more data than BERT, for a longer amount of time, allowing it to generalize even better to downstream tasks."}}
{"code": "{'Instruction': I need an API to classify objects within images efficiently using mixed precision training for better performance., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_se_resnext101_32x4d')\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained SE-ResNeXt101-32x4d model from PyTorch Hub, which uses mixed precision training and Tensor Cores on NVIDIA GPUs for efficient object classification within images.', 'code': \"import torch\nmodel = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_se_resnext101_32x4d')\"}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_se_resnext101_32x4d', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SE-ResNeXt101", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_se_resnext101_32x4d', pretrained=True)", "api_arguments": "N/A", "python_environment_requirements": "validators, matplotlib", "example_code": "import torch\nfrom PIL import Image\nimport torchvision.transforms as transforms\nimport numpy as np\nimport json\nimport requests\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nprint(f'Using {device} for inference')\nresneXt = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_se_resnext101_32x4d')\nutils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_convnets_processing_utils')\nresneXt.eval().to(device)\nuris = ['http://images.cocodataset.org/test-stuff2017/000000024309.jpg','http://images.cocodataset.org/test-stuff2017/000000028117.jpg','http://images.cocodataset.org/test-stuff2017/000000006149.jpg','http://images.cocodataset.org/test-stuff2017/000000004954.jpg']\nbatch = torch.cat([utils.prepare_input_from_uri(uri) for uri in uris]).to(device)\nwith torch.no_grad():\n output = torch.nn.functional.softmax(resneXt(batch), dim=1)\nresults = utils.pick_n_best(predictions=output, n=5)\nfor uri, result in zip(uris, results):\n img = Image.open(requests.get(uri, stream=True).raw)\n img.thumbnail((256,256), Image.ANTIALIAS)\n plt.imshow(img)\n plt.show()\n print(result)", "performance": {"dataset": "IMAGENET", "accuracy": "N/A"}, "description": "The SE-ResNeXt101-32x4d is a ResNeXt101-32x4d model with added Squeeze-and-Excitation module. This model is trained with mixed precision using Tensor Cores on Volta, Turing, and the NVIDIA Ampere GPU architectures, which allows researchers to get results 3x faster than training without Tensor Cores while experiencing the benefits of mixed precision training."}}
{"code": "{'Instruction': Convert an English text to speech while preserving the naturalness of the voice. Recommend me a suitable API., 'Output': {'domain': 'Text-to-Speech', 'api_call': 'waveglow = torch.hub.load(\"NVIDIA/DeepLearningExamples:torchhub\", \"nvidia_waveglow\", model_math=\"fp32\")', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained WaveGlow model from PyTorch Hub, which works in conjunction with the Tacotron 2 model, to synthesize natural-sounding speech from text input.', 'code': 'import torch\nwaveglow = torch.hub.load(\"NVIDIA/DeepLearningExamples:torchhub\", \"nvidia_waveglow\", model_math=\"fp32\")'}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_waveglow', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Text-to-Speech", "framework": "PyTorch", "functionality": "Speech Synthesis", "api_name": "WaveGlow", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_waveglow', pretrained=True)", "api_arguments": {"repo_or_dir": "NVIDIA/DeepLearningExamples:torchhub", "model": "nvidia_waveglow", "model_math": "fp32"}, "python_environment_requirements": ["numpy", "scipy", "librosa", "unidecode", "inflect", "libsndfile1"], "example_code": {"load_waveglow_model": "waveglow = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_waveglow', model_math='fp32')", "prepare_waveglow_model": ["waveglow = waveglow.remove_weightnorm(waveglow)", "waveglow = waveglow.to('cuda')", "waveglow.eval()"], "load_tacotron2_model": "tacotron2 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tacotron2', model_math='fp32')", "prepare_tacotron2_model": ["tacotron2 = tacotron2.to('cuda')", "tacotron2.eval()"], "synthesize_speech": ["text = \"hello world, I missed you so much\"", "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tts_utils')", "sequences, lengths = utils.prepare_input_sequence([text])", "with torch.no_grad():", " mel, _, _ = tacotron2.infer(sequences, lengths)", " audio = waveglow.infer(mel)", "audio_numpy = audio[0].data.cpu().numpy()", "rate = 22050"], "save_audio": "write(\"audio.wav\", rate, audio_numpy)", "play_audio": "Audio(audio_numpy, rate=rate)"}, "performance": {"dataset": "LJ Speech", "accuracy": null}, "description": "The Tacotron 2 and WaveGlow model form a text-to-speech system that enables users to synthesize natural-sounding speech from raw transcripts without any additional prosody information. The Tacotron 2 model produces mel spectrograms from input text using encoder-decoder architecture. WaveGlow is a flow-based model that consumes the mel spectrograms to generate speech."}}
{"code": "{'Instruction': I am a data scientist who needs to classify images using a neural network with a dense structure. Recommend an API for this purpose., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'densenet121', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained DenseNet-121 model from PyTorch Hub for dense structure image classification.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'densenet121', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet121', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Dense Convolutional Network", "api_name": "Densenet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet121', pretrained=True)", "api_arguments": [{"name": "densenet121", "type": "str", "description": "Densenet-121 model"}], "python_environment_requirements": {"torch": "latest", "torchvision": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet121', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"densenet121": {"Top-1 error": 25.35, "Top-5 error": 7.83}}}, "description": "Dense Convolutional Network (DenseNet) connects each layer to every other layer in a feed-forward fashion. It alleviates the vanishing-gradient problem, strengthens feature propagation, encourages feature reuse, and substantially reduces the number of parameters."}}
{"code": "{'Instruction': I am building an image recognition app, and I need to classify objects in images. Which API should I use?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'densenet169', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Densenet169 model from PyTorch Hub for image classification, which can recognize objects in images.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'densenet169', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet169', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Dense Convolutional Network", "api_name": "Densenet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet169', pretrained=True)", "api_arguments": [{"name": "densenet169", "type": "str", "description": "Densenet-169 model"}], "python_environment_requirements": {"torch": "latest", "torchvision": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet169', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"densenet169": {"Top-1 error": 24.0, "Top-5 error": 7.0}}}, "description": "Dense Convolutional Network (DenseNet) connects each layer to every other layer in a feed-forward fashion. It alleviates the vanishing-gradient problem, strengthens feature propagation, encourages feature reuse, and substantially reduces the number of parameters."}}
{"code": "{'Instruction': I need an API suitable for classifying skin conditions based on images. What do you suggest?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'densenet201', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained DenseNet-201 model from PyTorch Hub, which can be fine-tuned for skin condition classification tasks.', 'code': 'import torch\nmodel = torch.hub.load(\"pytorch/vision\", \"densenet201\", pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet201', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Dense Convolutional Network", "api_name": "Densenet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet201', pretrained=True)", "api_arguments": [{"name": "densenet201", "type": "str", "description": "Densenet-201 model"}], "python_environment_requirements": {"torch": "latest", "torchvision": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet201', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"densenet201": {"Top-1 error": 22.8, "Top-5 error": 6.43}}}, "description": "Dense Convolutional Network (DenseNet) connects each layer to every other layer in a feed-forward fashion. It alleviates the vanishing-gradient problem, strengthens feature propagation, encourages feature reuse, and substantially reduces the number of parameters."}}
{"code": "{'Instruction': Which API do I use for segmenting objects in an image based on their class?, 'Output': {'domain': 'Semantic Segmentation', 'api_call': 'model = torch.hub.load('pytorch/vision', 'fcn_resnet50', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Use the FCN-ResNet50 semantic segmentation API from PyTorch Hub, which is pretrained on the COCO dataset and can segment objects in an image based on their class.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'fcn_resnet50', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='fcn_resnet50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "Fully-Convolutional Network", "api_name": "fcn_resnet50", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='fcn_resnet50', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "boolean", "description": "If True, returns a model pre-trained on COCO train2017"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "matplotlib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'fcn_resnet50', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "input_image = input_image.convert(\"RGB\")", "preprocess = transforms.Compose([", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)['out'][0]", "output_predictions = output.argmax(0)"], "performance": {"dataset": "COCO val2017", "accuracy": {"fcn_resnet50": {"Mean IOU": 60.5, "Global Pixelwise Accuracy": 91.4}}}, "description": "FCN-ResNet is a Fully-Convolutional Network model using a ResNet-50 or a ResNet-101 backbone. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset."}}
{"code": "{'Instruction': I am developing an AI vacuum cleaner that can identify type of object on the floor so it can avoid it. I am looking for an API to help me in idenfifying common objects like shoes, toys, type of dirt. You may assume I already have pictures of the floor., 'Output': {'domain': 'Object Detection', 'api_call': \"model = torch.hub.load('pytorch/vision', 'densenet161', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained DenseNet from the PyTorch Hub for identifying common objects on the floor, such as shoes, toys, or types of dirt.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision:v0.10.0', 'densenet161', pretrained=True)\nmodel.eval()'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet161', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Dense Convolutional Network", "api_name": "Densenet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet161', pretrained=True)", "api_arguments": [{"name": "densenet161", "type": "str", "description": "Densenet-161 model"}], "python_environment_requirements": {"torch": "latest", "torchvision": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet161', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"densenet161": {"Top-1 error": 22.35, "Top-5 error": 6.2}}}, "description": "Dense Convolutional Network (DenseNet) connects each layer to every other layer in a feed-forward fashion. It alleviates the vanishing-gradient problem, strengthens feature propagation, encourages feature reuse, and substantially reduces the number of parameters."}}
{"code": "{'Instruction': I am working on environmental analysis and I want to analyze each pixel in an aerial image and identify its class. What API could help me with that?, 'Output': {'domain': 'Semantic Segmentation', 'api_call': \"model = torch.hub.load('pytorch/vision', 'fcn_resnet101', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Use the pretrained FCN-ResNet101 model for semantic segmentation from PyTorch Hub to analyze each pixel in the aerial image and classify it into corresponding categories.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'fcn_resnet101', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='fcn_resnet101', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "Fully-Convolutional Network", "api_name": "fcn_resnet101", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='fcn_resnet101', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "boolean", "description": "If True, returns a model pre-trained on COCO train2017"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "matplotlib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'fcn_resnet101', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "input_image = input_image.convert(\"RGB\")", "preprocess = transforms.Compose([", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)['out'][0]", "output_predictions = output.argmax(0)"], "performance": {"dataset": "COCO val2017", "accuracy": {"fcn_resnet101": {"Mean IOU": 63.7, "Global Pixelwise Accuracy": 91.9}}}, "description": "FCN-ResNet is a Fully-Convolutional Network model using a ResNet-50 or a ResNet-101 backbone. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset."}}
{"code": "{'Instruction': I'm looking for a computationally efficient image classifier. Please suggest a suitable API for this purpose., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet39ds', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Harmonic DenseNet (HarDNet) model from PyTorch Hub for computationally efficient image classification.', 'code': 'import torch\nmodel = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet39ds', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet39ds', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "HarDNet", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet39ds', pretrained=True)", "api_arguments": [{"name": "hardnet39ds", "type": "str", "description": "HarDNet-39DS model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet39ds', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"hardnet39ds": {"Top-1 error": 27.92, "Top-5 error": 9.57}}}, "description": "Harmonic DenseNet (HarDNet) is a low memory traffic CNN model, which is fast and efficient. The basic concept is to minimize both computational cost and memory access cost at the same time, such that the HarDNet models are 35% faster than ResNet running on GPU comparing to models with the same accuracy (except the two DS models that were designed for comparing with MobileNet)."}}
{"code": "{'Instruction': I have a collection of images to classify. Can you suggest an API with good accuracy and less computational cost?, 'Output': {'domain': 'Image Classification', 'api_call': 'model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet68', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained HarDNet-68 model from PyTorch Hub, which is efficient, fast and has low memory traffic, making it suitable for image classification tasks with good accuracy.', 'code': 'import torch\nmodel = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet68', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet68', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "HarDNet", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet68', pretrained=True)", "api_arguments": [{"name": "hardnet68", "type": "str", "description": "HarDNet-68 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet68', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"hardnet68": {"Top-1 error": 23.52, "Top-5 error": 6.99}}}, "description": "Harmonic DenseNet (HarDNet) is a low memory trafficCNN model, which is fast and efficient. The basic concept is to minimize both computational cost and memory access cost at the same time, such that the HarDNet models are 35% faster than ResNet running on GPU comparing to models with the same accuracy (except the two DS models that were designed for comparing with MobileNet)."}}
{"code": "{'Instruction': I am trying to design an app that can classify shoes, clothes, and other fashion items from images taken by users. Recommend me an API that can perform this task., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'harDnet68ds', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained HarDNet-68DS model from PyTorch Hub, which can be fine-tuned for classification of fashion items such as shoes and clothes.', 'code': 'import torch\nmodel = torch.hub.load(\"PingoLH/Pytorch-HarDNet\", \"harDnet68ds\", pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet68ds', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "HarDNet", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet68ds', pretrained=True)", "api_arguments": [{"name": "hardnet68ds", "type": "str", "description": "HarDNet-68DS model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet68ds', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"hardnet68ds": {"Top-1 error": 25.71, "Top-5 error": 8.13}}}, "description": "Harmonic DenseNet (HarDNet) is a low memory traffic CNN model, which is fast and efficient. The basic concept is to minimize both computational cost and memory access cost at the same time, such that the HarDNet models are 35% faster than ResNet running on GPU comparing to models with the same accuracy (except the two DS models that were designed for comparing with MobileNet)."}}
{"code": "{'Instruction': Recommend me an API to classify images of cats and dogs., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet85', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained HarDNet-85 model from PyTorch Hub, which can be fine-tuned for cat and dog classification given an input image.\", 'code': 'import torch\nmodel = torch.hub.load(\u2018PingoLH/Pytorch-HarDNet\u2019, \u2018hardnet85\u2019, pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet85', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "HarDNet", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet85', pretrained=True)", "api_arguments": [{"name": "hardnet85", "type": "str", "description": "HarDNet-85 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet85', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"hardnet85": {"Top-1 error": 21.96, "Top-5 error": 6.11}}}, "description": "Harmonic DenseNet (HarDNet) is a low memory traffic CNN model, which is fast and efficient. The basic concept is to minimize both computational cost and memory access cost at the same time, such that the HarDNet models are 35% faster than ResNet running on GPU comparing to models with the same accuracy (except the two DS models that were designed for comparing with MobileNet)."}}
{"code": "{'Instruction': Tell me an API that can classify photos of plants into their respective species., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'googlenet', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained GoogLeNet model from PyTorch Hub, which can be fine-tuned for plant species classification.\", 'code': \"import torch\nmodel = torch.hub.load('pytorch/vision', 'googlenet', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='googlenet', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "GoogLeNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='googlenet', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": ">=1.0.0", "torchvision": ">=0.2.2"}, "example_code": {"import": ["import torch", "import urllib", "from PIL import Image", "from torchvision import transforms"], "load_model": "model = torch.hub.load('pytorch/vision:v0.10.0', 'googlenet', pretrained=True)", "model_eval": "model.eval()", "image_preprocessing": ["input_image = Image.open(filename)", "preprocess = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)"], "model_execution": ["if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)"], "output_processing": ["probabilities = torch.nn.functional.softmax(output[0], dim=0)", "top5_prob, top5_catid = torch.topk(probabilities, 5)"]}, "performance": {"dataset": "ImageNet", "accuracy": {"Top-1 error": "30.22", "Top-5 error": "10.47"}}, "description": "GoogLeNet is based on a deep convolutional neural network architecture codenamed 'Inception', which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014)."}}
{"code": "{'Instruction': I need an API for classifying types of plants through an image. Write me an example., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('XingangPan/IBN-Net', 'resnet50_ibn_a', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained IBN-Net model from PyTorch Hub, which can be fine-tuned for plant classification tasks given an input image.', 'code': 'import torch\nmodel = torch.hub.load('XingangPan/IBN-Net', 'resnet50_ibn_a', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnet50_ibn_a', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "IBN-Net", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnet50_ibn_a', pretrained=True)", "api_arguments": [{"name": "resnet50_ibn_a", "type": "str", "description": "ResNet-50-IBN-a model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('XingangPan/IBN-Net', 'resnet50_ibn_a', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"resnet50_ibn_a": {"Top-1 acc": 77.46, "Top-5 acc": 93.68}}}, "description": "IBN-Net is a CNN model with domain/appearance invariance. Motivated by style transfer works, IBN-Net carefully unifies instance normalization and batch normalization in a single deep network. It provides a simple way to increase both modeling and generalization capacities without adding model complexity. IBN-Net is especially suitable for cross domain or person/vehicle re-identification tasks."}}
{"code": "{'Instruction': Help me find an API that can recognise appearances and domains in cross domain tasks like person or vehicle re-identification., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('XingangPan/IBN-Net', 'resnet101_ibn_a', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained IBN-Net model for ResNet-101-IBN-a from PyTorch Hub, which recognizes appearances and domains in cross-domain tasks such as person or vehicle re-identification.', 'code': 'import torch\nmodel = torch.hub.load('XingangPan/IBN-Net', 'resnet101_ibn_a', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnet101_ibn_a', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "IBN-Net", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnet101_ibn_a', pretrained=True)", "api_arguments": [{"name": "resnet101_ibn_a", "type": "str", "description": "ResNet-101-IBN-a model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('XingangPan/IBN-Net', 'resnet101_ibn_a', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"resnet101_ibn_a": {"Top-1 acc": 78.61, "Top-5 acc": 94.41}}}, "description": "IBN-Net is a CNN model with domain/appearance invariance. Motivated by style transfer works, IBN-Net carefully unifies instance normalization and batch normalization in a single deep network. It provides a simple way to increase both modeling and generalization capacities without adding model complexity. IBN-Net is especially suitable for cross domain or person/vehicle re-identification tasks."}}
{"code": "{'Instruction': One of the largest retailers, Walmart, wants to develop a system for automating price audits to replace the manual way of checking prices. Suggest an API that can identify items in a picture and compare them with data from the server to achieve the retailer's goal., 'Output': {'domain': 'Object Classification', 'api_call': 'model = torch.hub.load('huawei-noah/ghostnet', 'ghostnet_1x', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained GhostNet model from PyTorch Hub, which can be fine-tuned for object classification of items in a picture, enabling price audit automation at the retailer.', 'code': 'import torch\nmodel = torch.hub.load('huawei-noah/ghostnet', 'ghostnet_1x', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/ghostnet', model='ghostnet_1x', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Efficient networks by generating more features from cheap operations", "api_name": "GhostNet", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/ghostnet', model='ghostnet_1x', pretrained=True)", "api_arguments": ["pretrained"], "python_environment_requirements": ["torch", "torchvision", "PIL"], "example_code": ["import torch", "model = torch.hub.load('huawei-noah/ghostnet', 'ghostnet_1x', pretrained=True)", "model.eval()", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)", "print(probabilities)"], "performance": {"dataset": "ImageNet", "accuracy": {"Top-1 acc": "73.98", "Top-5 acc": "91.46"}}, "description": "The GhostNet architecture is based on an Ghost module structure which generates more features from cheap operations. Based on a set of intrinsic feature maps, a series of cheap operations are applied to generate many ghost feature maps that could fully reveal information underlying intrinsic features. Experiments conducted on benchmarks demonstrate the superiority of GhostNet in terms of speed and accuracy tradeoff."}}
{"code": "{'Instruction': Identify an API that can deal with vehicle re-identification tasks., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('XingangPan/IBN-Net', 'resnext101_ibn_a', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained IBN-Net (ResNeXt-101-IBN-a) model from PyTorch Hub for person/vehicle re-identification tasks, which unifies instance normalization and batch normalization.', 'code': 'import torch\nmodel = torch.hub.load('XingangPan/IBN-Net', 'resnext101_ibn_a', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnext101_ibn_a', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "IBN-Net", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnext101_ibn_a', pretrained=True)", "api_arguments": [{"name": "resnext101_ibn_a", "type": "str", "description": "ResNeXt-101-IBN-a model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('XingangPan/IBN-Net', 'resnext101_ibn_a', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"resnext101_ibn_a": {"Top-1 acc": 79.12, "Top-5 acc": 94.58}}}, "description": "IBN-Net is a CNN model with domain/appearance invariance. Motivated by style transfer works, IBN-Net carefully unifies instance normalization and batch normalization in a single deep network. It provides a simple way to increase both modeling and generalization capacities without adding model complexity. IBN-Net is especially suitable for cross domain or person/vehicle re-identification tasks."}}
{"code": "{'Instruction': I need an AI-based image recognition technology to enhance a photo journal app. Please recommend an API., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'inception_v3', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Inception v3 model from PyTorch Hub, an AI-based image recognition technology suitable for enhancing photo journal apps.', 'code': \"import torch\nmodel = torch.hub.load('pytorch/vision', 'inception_v3', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='inception_v3', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "Inception_v3", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='inception_v3', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": "1.9.0", "torchvision": "0.10.0"}, "example_code": {"import_libraries": "import torch", "load_model": "model = torch.hub.load('pytorch/vision:v0.10.0', 'inception_v3', pretrained=True)", "model_evaluation": "model.eval()"}, "performance": {"dataset": "imagenet", "accuracy": {"top-1_error": 22.55, "top-5_error": 6.44}}, "description": "Inception v3, also called GoogleNetv3, is a famous Convolutional Neural Network trained on the ImageNet dataset from 2015. It is based on the exploration of ways to scale up networks to utilize the added computation as efficiently as possible by using suitably factorized convolutions and aggressive regularization. The model achieves a top-1 error of 22.55% and a top-5 error of 6.44% on the ImageNet dataset."}}
{"code": "{'Instruction': I'm facing a classification problem that involves a high level of domain adaptation. Suggest me an API that suits my task., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('XingangPan/IBN-Net', 'se_resnet101_ibn_a', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained IBN-Net model from PyTorch Hub, which is designed with domain/appearance invariance for cross-domain classification tasks.', 'code': 'import torch\nmodel = torch.hub.load('XingangPan/IBN-Net', 'se_resnet101_ibn_a', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='se_resnet101_ibn_a', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "IBN-Net", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='se_resnet101_ibn_a', pretrained=True)", "api_arguments": [{"name": "se_resnet101_ibn_a", "type": "str", "description": "SE-ResNet-101-IBN-a model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('XingangPan/IBN-Net', 'se_resnet101_ibn_a', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"se_resnet101_ibn_a": {"Top-1 acc": 78.75, "Top-5 acc": 94.49}}}, "description": "IBN-Net is a CNN model with domain/appearance invariance. Motivated by style transfer works, IBN-Net carefully unifies instance normalization and batch normalization in a single deep network. It provides a simple way to increase both modeling and generalization capacities without adding model complexity. IBN-Net is especially suitable for cross domain or person/vehicle re-identification tasks."}}
{"code": "{'Instruction': Looking for an API that can classify thousands of different species of flowers from an image., 'Output': {'domain': 'Classification', 'api_call': 'model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest50', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNeSt50 model from PyTorch Hub, which can be fine-tuned for flower species classification from images.', 'code': 'import torch\nmodel = torch.hub.load('zhanghang1989/ResNeSt', 'resnest50', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNeSt", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest50', pretrained=True)", "api_arguments": "resnest50", "python_environment_requirements": {"torch": "1.0.0", "torchvision": "0.2.2"}, "example_code": ["import torch", "model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest50', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeSt-50": 81.03}}, "description": "ResNeSt models are from the ResNeSt: Split-Attention Networks paper. They are a new ResNet variant that enables attention across feature-map groups. By stacking Split-Attention blocks ResNet-style, ResNeSt models outperform other networks with similar model complexities, and also help downstream tasks including object detection, instance segmentation, and semantic segmentation."}}
{"code": "{'Instruction': Recommend an image classification API that can accurately predict items in a given image., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest269', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pre-trained ResNeSt-269 model from PyTorch Hub, an image classification model with high accuracy for predicting items in images.', 'code': 'import torch\nmodel = torch.hub.load('zhanghang1989/ResNeSt', 'resnest269', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest269', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNeSt", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest269', pretrained=True)", "api_arguments": "resnest269", "python_environment_requirements": {"torch": "1.0.0", "torchvision": "0.2.2"}, "example_code": ["import torch", "model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest269', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeSt-269": 84.54}}, "description": "ResNeSt models are from the ResNeSt: Split-Attention Networks paper. They are a new ResNet variant that enables attention across feature-map groups. By stacking Split-Attention blocks ResNet-style, ResNeSt models outperform other networks with similar model complexities, and also help downstream tasks including object detection, instance segmentation, and semantic segmentation."}}
{"code": "{'Instruction': I need to know which API would be suitable to perform image classification on CPU optimized hardware., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_cpu', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the ProxylessNAS model optimized for CPU from PyTorch Hub for efficient image classification on CPU-based hardware.\", 'code': 'import torch\nmodel = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_cpu', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_cpu', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "ProxylessNAS", "api_name": "mit-han-lab/ProxylessNAS", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_cpu', pretrained=True)", "api_arguments": [{"name": "proxylessnas_cpu", "type": "str", "description": "ProxylessNAS optimized for CPU"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "target_platform = 'proxyless_cpu'", "model = torch.hub.load('mit-han-lab/ProxylessNAS', target_platform, pretrained=True)", "model.eval()"], "performance": {"dataset": [{"model_structure": "proxylessnas_cpu", "accuracy": 75.3}]}, "description": "ProxylessNAS models are from the ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware paper. They specialize CNN architectures for different hardware platforms, offering free yet significant performance boost on all three platforms (CPU, GPU, and Mobile) with similar accuracy."}}
{"code": "{'Instruction': I am designing a wildlife preservation journal that needs to categorize pictures of different animal species. Suggest a machine learning API to classify the animal species in a given image., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest200', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNeSt model from PyTorch Hub, which can be fine-tuned for animal species classification given an input image of an animal.', 'code': 'import torch\nmodel = torch.hub.load('zhanghang1989/ResNeSt', 'resnest200', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest200', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNeSt", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest200', pretrained=True)", "api_arguments": "resnest200", "python_environment_requirements": {"torch": "1.0.0", "torchvision": "0.2.2"}, "example_code": ["import torch", "model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest200', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeSt-200": 83.84}}, "description": "ResNeSt models are from the ResNeSt: Split-Attention Networks paper. They are a new ResNet variant that enables attention across feature-map groups. By stacking Split-Attention blocks ResNet-style, ResNeSt models outperform other networks with similar model complexities, and also help downstream tasks including object detection, instance segmentation, and semantic segmentation."}}
{"code": "{'Instruction': Create a mobile application that optimizes image classification. Recommend an API that I can use for this purpose., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_mobile', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained ProxylessNAS Mobile model from PyTorch Hub, which is optimized for mobile platforms and offers high accuracy for image classification.\", 'code': \"import torch\nmodel = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_mobile', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_mobile', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "ProxylessNAS", "api_name": "mit-han-lab/ProxylessNAS", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_mobile', pretrained=True)", "api_arguments": [{"name": "proxylessnas_mobile", "type": "str", "description": "ProxylessNAS optimized for Mobile"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "target_platform = 'proxyless_mobile'", "model = torch.hub.load('mit-han-lab/ProxylessNAS', target_platform, pretrained=True)", "model.eval()"], "performance": {"dataset": [{"model_structure": "proxylessnas_mobile", "accuracy": 74.6}]}, "description": "ProxylessNAS models are from the ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware paper. They specialize CNN architectures for different hardware platforms, offering free yet significant performance boost on all three platforms (CPU, GPU, and Mobile) with similar accuracy."}}
{"code": "{'Instruction': I require a lightweight API that can perform image classification on mobile devices., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'mobilenet_v2', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load MobileNet v2 from PyTorch Hub, a lightweight model optimized for image classification tasks on mobile devices.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'mobilenet_v2', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='mobilenet_v2', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MobileNet v2", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='mobilenet_v2', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision', 'mobilenet_v2', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)"], "performance": {"dataset": "ImageNet", "accuracy": {"top-1_error": 28.12, "top-5_error": 9.71}}, "description": "The MobileNet v2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input. MobileNet v2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, non-linearities in the narrow layers were removed in order to maintain representational power."}}
{"code": "{'Instruction': I need help selecting an API that can classify an image based on an optimized CNN architecture for GPU devices., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_gpu', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Use the ProxylessNAS model from PyTorch Hub, optimized for GPU devices, to classify images using state-of-the-art neural architecture search.', 'code': 'import torch\nmodel = torch.hub.load(\"mit-han-lab/ProxylessNAS\", \"proxylessnas_gpu\", pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_gpu', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "ProxylessNAS", "api_name": "mit-han-lab/ProxylessNAS", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_gpu', pretrained=True)", "api_arguments": [{"name": "proxylessnas_gpu", "type": "str", "description": "ProxylessNAS optimized for GPU"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "target_platform = 'proxyless_gpu'", "model = torch.hub.load('mit-han-lab/ProxylessNAS', target_platform, pretrained=True)", "model.eval()"], "performance": {"dataset": [{"model_structure": "proxylessnas_gpu", "accuracy": 75.1}]}, "description": "ProxylessNAS models are from the ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware paper. They specialize CNN architectures for different hardware platforms, offering free yet significant performance boost on all three platforms (CPU, GPU, and Mobile) with similar accuracy."}}
{"code": "{'Instruction': I need to execute an image classification API that can identify objects in a given image. Recommend an API and show me how to use it., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'resnet18', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained ResNet18 model from PyTorch Hub for image classification tasks. The model can identify objects in a given image using its pre-trained weights.\", 'code': 'import torch\nmodel = torch.hub.load(\"pytorch/vision\", \"resnet18\", pretrained=True)\nmodel.eval()'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet18', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Deep Residual Networks", "api_name": "ResNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet18', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "bool", "default": "False", "description": "If True, returns a model pre-trained on ImageNet"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "resnet18", "top-1_error": 30.24, "top-5_error": 10.92}}, "description": "ResNet models are deep residual networks pre-trained on ImageNet. They were proposed in the paper 'Deep Residual Learning for Image Recognition'. Available model variants include ResNet18, ResNet34, ResNet50, ResNet101, and ResNet152."}}
{"code": "{'Instruction': Find me a pre-trained model for general image recognition that I can use for an e-commerce website's product categorization., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'resnet34', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pre-trained ResNet-34 model from PyTorch Hub for general image recognition, which can be used for e-commerce product categorization.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'resnet34', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet34', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Deep Residual Networks", "api_name": "ResNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet34', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "bool", "default": "False", "description": "If True, returns a model pre-trained on ImageNet"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet34', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "resnet34", "top-1_error": 26.7, "top-5_error": 8.58}}, "description": "ResNet models are deep residual networks pre-trained on ImageNet. They were proposed in the paper 'Deep Residual Learning for Image Recognition'. Available model variants include ResNet18, ResNet34, ResNet50, ResNet101, and ResNet152."}}
{"code": "{'Instruction': I want to classify an image using a pre-trained deep learning model. Find me an API I could use., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'resnet152', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNet152 model from PyTorch Hub to perform image classification on a given image.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'resnet152', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet152', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Deep Residual Networks", "api_name": "ResNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet152', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "bool", "default": "False", "description": "If True, returns a model pre-trained on ImageNet"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet152', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "resnet152", "top-1_error": 21.69, "top-5_error": 5.94}}, "description": "ResNet models are deep residual networks pre-trained on ImageNet. They were proposed in the paper 'Deep Residual Learning for Image Recognition'. Available model variants include ResNet18, ResNet34, ResNet50, ResNet101, and ResNet152."}}
{"code": "{'Instruction': I am working on building a dog breed identification app, recommend an API that can be used for this task., 'Output': {'domain': 'Classification', 'api_call': 'model = torch.hub.load('pytorch/vision', 'resnet50', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNet50 model from PyTorch Hub, which can be fine-tuned for dog breed identification.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'resnet50', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Deep Residual Networks", "api_name": "ResNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet50', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "bool", "default": "False", "description": "If True, returns a model pre-trained on ImageNet"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "resnet50", "top-1_error": 23.85, "top-5_error": 7.13}}, "description": "ResNet models are deep residual networks pre-trained on ImageNet. They were proposed in the paper 'Deep Residual Learning for Image Recognition'. Available model variants include ResNet18, ResNet34, ResNet50, ResNet101, and ResNet152."}}
{"code": "{'Instruction': I would like an API that can classify land animals from an image using the latest deep learning techniques., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'resnext50_32x4d', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNext model from PyTorch Hub, which can be fine-tuned for classifying land animals from images.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'resnext50_32x4d', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnext50_32x4d', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNext", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnext50_32x4d', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "PIL"], "example_code": ["import torch", "from PIL import Image", "from torchvision import transforms", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnext50_32x4d', pretrained=True)", "model.eval()", "input_image = Image.open('dog.jpg')", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)"], "performance": {"dataset": "ImageNet", "accuracy": {"resnext50_32x4d": {"top-1": 22.38, "top-5": 6.3}}}, "description": "ResNext is a next-generation ResNet architecture for image classification. It is more efficient and accurate than the original ResNet. This implementation includes two versions of the model, resnext50_32x4d and resnext101_32x8d, with 50 and 101 layers respectively."}}
{"code": "{'Instruction': A wildlife photographer needs an API to help in sorting birds and mammals in his photo collection. Suggest an API., 'Output': {'domain': 'Classification', 'api_call': 'model = torch.hub.load('pytorch/vision', 'resnext101_32x4d', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNext101 model from PyTorch Hub, which can be used for image classification and fine-tuned for sorting birds and mammals in the photo collection.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'resnext101_32x4d', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnext101_32x4d', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNext", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnext101_32x4d', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "PIL"], "example_code": ["import torch", "from PIL import Image", "from torchvision import transforms", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnext101_32x4d', pretrained=True)", "model.eval()", "input_image = Image.open('dog.jpg')", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)"], "performance": {"dataset": "ImageNet", "resnext101_32x8d": {"top-1": 20.69, "top-5": 5.47}}, "description": "ResNext is a next-generation ResNet architecture for image classification. It is more efficient and accurate than the original ResNet. This implementation includes two versions of the model, resnext50_32x4d and resnext101_32x8d, with 50 and 101 layers respectively."}}
{"code": "{'Instruction': Identify a library API suitable for classifying images with high accuracy while keeping FLOPs low., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_t', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the SNNMLP Tiny model from PyTorch Hub, which provides high accuracy image classification with low FLOPs, making it efficient for your task.\", 'code': 'import torch\nmodel = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_t', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_t', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SNNMLP", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_t', pretrained=True)", "api_arguments": [{"name": "snnmlp_t", "type": "str", "description": "SNNMLP Tiny model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_t', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "SNNMLP Tiny", "top-1": 81.88}}, "description": "SNNMLP incorporates the mechanism of LIF neurons into the MLP models, to achieve better accuracy without extra FLOPs. We propose a full-precision LIF operation to communicate between patches, including horizontal LIF and vertical LIF in different directions. We also propose to use group LIF to extract better local features. With LIF modules, our SNNMLP model achieves 81.9%, 83.3% and 83.6% top-1 accuracy on ImageNet dataset with only 4.4G, 8.5G and 15.2G FLOPs, respectively."}}
{"code": "{'Instruction': I want to classify an image with a neural network that has lower operation complexity. What's the best API to use?, 'Output': {'domain': 'Image Classification', 'api_call': 'model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_s', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained SNNMLP Small model from PyTorch Hub, which provides high accuracy with lower FLOPs, making it ideal for image classification tasks with low operation complexity.', 'code': 'import torch\nmodel = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_s', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_s', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SNNMLP", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_s', pretrained=True)", "api_arguments": [{"name": "snnmlp_s", "type": "str", "description": "SNNMLP Small model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_s', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "SNNMLP Small", "top-1": 83.3}}, "description": "SNNMLP incorporates the mechanism of LIF neurons into the MLP models, to achieve better accuracy without extra FLOPs. We propose a full-precision LIF operation to communicate between patches, including horizontal LIF and vertical LIF in different directions. We also propose to use group LIF to extract better local features. With LIF modules, our SNNMLP model achieves 81.9%, 83.3% and 83.6% top-1 accuracy on ImageNet dataset with only 4.4G, 8.5G and 15.2G FLOPs, respectively."}}
{"code": "{'Instruction': Is there an API I can use for image categorization in a mobile app? I need it to be lightweight and fast., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'squeezenet1_1', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained SqueezeNet 1.1 model from PyTorch Hub, a lightweight and fast image classification model suitable for mobile applications.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'squeezenet1_1', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='squeezenet1_1', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SqueezeNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='squeezenet1_1', pretrained=True)", "api_arguments": {"version": "v0.10.0", "model": ["squeezenet1_1"], "pretrained": "True"}, "python_environment_requirements": {"torch": ">=1.9.0", "torchvision": ">=0.10.0"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'squeezenet1_1', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)", "print(probabilities)"], "performance": {"dataset": "ImageNet", "accuracy": {"squeezenet1_1": {"Top-1 error": 41.81, "Top-5 error": 19.38}}}, "description": "SqueezeNet is an image classification model that achieves AlexNet-level accuracy with 50x fewer parameters. It has two versions: squeezenet1_0 and squeezenet1_1, with squeezenet1_1 having 2.4x less computation and slightly fewer parameters than squeezenet1_0, without sacrificing accuracy."}}
{"code": "{'Instruction': I am an independent developer working on a chatbot. I need my chatbot to identify what is in the user's picture. Suggest an pre-trained API that can classify objects in an image., 'Output': {'domain': 'Image Classification', 'api_call': 'model = torch.hub.load('pytorch/vision', 'shufflenet_v2_x1_0', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ShuffleNet v2 model from PyTorch Hub, which can classify objects in images with high accuracy.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'shufflenet_v2_x1_0', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='shufflenet_v2_x1_0', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ShuffleNet v2", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='shufflenet_v2_x1_0', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": "torch", "torchvision": "torchvision", "PIL": "Image", "urllib": "urllib"}, "example_code": {"import_libraries": ["import torch", "from PIL import Image", "from torchvision import transforms", "import urllib"], "load_model": ["model = torch.hub.load('pytorch/vision:v0.10.0', 'shufflenet_v2_x1_0', pretrained=True)", "model.eval()"], "load_image": ["url, filename = ('https://github.com/pytorch/hub/raw/master/images/dog.jpg', 'dog.jpg')", "try: urllib.URLopener().retrieve(url, filename)", "except: urllib.request.urlretrieve(url, filename)", "input_image = Image.open(filename)"], "preprocess_image": ["preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)"], "run_inference": ["if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)"], "get_probabilities": ["probabilities = torch.nn.functional.softmax(output[0], dim=0)"], "top_categories": ["top5_prob, top5_catid = torch.topk(probabilities, 5)", "for i in range(top5_prob.size(0)):", " print(categories[top5_catid[i]], top5_prob[i].item())"]}, "performance": {"dataset": "Imagenet", "accuracy": {"top-1_error": 30.64, "top-5_error": 11.68}}, "description": "ShuffleNet V2 is an efficient ConvNet optimized for speed and memory, pre-trained on Imagenet. It is designed based on practical guidelines for efficient network design, including speed and accuracy tradeoff."}}
{"code": "{'Instruction': I need an image classification API for a robot that can recognize an object in real time with high accuracy., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_b', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the SNNMLP Base model from PyTorch Hub, which is a high accuracy image classification model suitable for real-time object recognition.', 'code': 'import torch\nmodel = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_b', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_b', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SNNMLP", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_b', pretrained=True)", "api_arguments": [{"name": "snnmlp_b", "type": "str", "description": "SNNMLP Base model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_b', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "SNNMLP Base", "top-1": 85.59}}, "description": "SNNMLP incorporates the mechanism of LIF neurons into the MLP models, to achieve better accuracy without extra FLOPs. We propose a full-precision LIF operation to communicate between patches, including horizontal LIF and vertical LIF in different directions. We also propose to use group LIF to extract better local features. With LIF modules, our SNNMLP model achieves 81.9%, 83.3% and 83.6% top-1 accuracy on ImageNet dataset with only 4.4G, 8.5G and 15.2G FLOPs, respectively."}}
{"code": "{'Instruction': A retail analyst wants to process an image containing a collection of consumer products and classify the objects in the image. Find an API that helps categorize the objects in the image and provide a confidence score for each category., 'Input': '<noinput>', 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'squeezenet1_0', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Use the pretrained SqueezeNet model, an image classification model with 50x fewer parameters than AlexNet, to classify the objects in the image and provide confidence scores for each category.', 'code': 'import torch\nmodel = torch.hub.load(\"pytorch/vision\", \"squeezenet1_0\", pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='squeezenet1_0', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SqueezeNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='squeezenet1_0', pretrained=True)", "api_arguments": {"version": "v0.10.0", "model": ["squeezenet1_0"], "pretrained": "True"}, "python_environment_requirements": {"torch": ">=1.9.0", "torchvision": ">=0.10.0"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'squeezenet1_0', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)", "print(probabilities)"], "performance": {"dataset": "ImageNet", "accuracy": {"squeezenet1_0": {"Top-1 error": 41.9, "Top-5 error": 19.58}}}, "description": "SqueezeNet is an image classification model that achieves AlexNet-level accuracy with 50x fewer parameters. It has two versions: squeezenet1_0 and squeezenet1_1, with squeezenet1_1 having 2.4x less computation and slightly fewer parameters than squeezenet1_0, without sacrificing accuracy."}}
{"code": "{'Instruction': I am building a photo app and I need an API that can classify images into categories. Do you have any suggestions?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg13', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Use the pre-trained VGG13 model from PyTorch Hub for image recognition and classification into categories.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg13', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg13', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg13', pretrained=True)", "api_arguments": [{"name": "vgg13", "type": "str", "description": "VGG13 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg13', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg13": {"Top-1 error": 30.07, "Top-5 error": 10.75}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn,vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': Develop a mobile app which can identify 1000 objects based on the camera, provide an API that can be used to develop it., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load(repo_or_dir='pytorch/vision', model='vgg11_bn', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained VGG11 model with batch normalization for image recognition from PyTorch Hub to identify 1000 different objects in a mobile app.', 'code': 'import torch\nmodel = torch.hub.load(repo_or_dir=\"pytorch/vision\", model=\"vgg11_bn\", pretrained=True)\nmodel.eval()' }}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg11_bn', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg11_bn', pretrained=True)", "api_arguments": [{"name": "vgg11_bn", "type": "str", "description": "VGG11 model with batch normalization"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg11_bn', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg11_bn": {"Top-1 error": 26.7, "Top-5 error": 8.58}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': A team is working on automatically tagging animals in pictures taken by wildlife cameras. Provide an API for a model that can perform this task., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg11', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained VGG11 model from PyTorch Hub, which can be fine-tuned for wildlife animal classification given images from wildlife cameras.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg11', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg11', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg11', pretrained=True)", "api_arguments": [{"name": "vgg11", "type": "str", "description": "VGG11 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg11', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg11": {"Top-1 error": 30.98, "Top-5 error": 11.37}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': I want to recognize objects in images using a VGG13 model with batch normalization. What API should I use for this?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg13_bn', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained VGG13 model with batch normalization from PyTorch Hub to perform object recognition in images.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg13_bn', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg13_bn', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg13_bn', pretrained=True)", "api_arguments": [{"name": "vgg13_bn", "type": "str", "description": "VGG13 model with batch normalization"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg13_bn', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg13_bn": {"Top-1 error": 28.45, "Top-5 error": 9.63}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': Identify an API that can recognize objects inside photos. Ensure that it is pretrained and ready for use., 'Output': {'domain': 'Object Recognition', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg16_bn', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained VGG16 model with batch normalization from PyTorch Hub. This model is suitable for recognizing objects within images.', 'code': 'import torch\nmodel = torch.hub.load(\"pytorch/vision\", \"vgg16_bn\", pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg16_bn', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg16_bn', pretrained=True)", "api_arguments": [{"name": "vgg16_bn", "type": "str", "description": "VGG16 model with batch normalization"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16_bn', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg16_bn": {"Top-1 error": 26.63, "Top-5 error": 8.5}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': Identify an API that can recognize what type of buildings are in an image. , 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg19', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the VGG19 model from the PyTorch hub and fine-tune it to classify images of different types of buildings.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg19', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg19', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg19', pretrained=True)", "api_arguments": [{"name": "vgg19", "type": "str", "description": "VGG19 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg19', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg19": {"Top-1 error": 27.62, "Top-5 error": 9.12}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': Can you recommend an API that can recognize objects in images, like different species of animals?, 'Output': {'domain': 'Image Recognition', 'api_call': \"model = torch.hub.load('pytorch/vision', model='vgg16', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained VGG16 model from PyTorch Hub to recognize objects in images, such as different species of animals, using the ImageNet dataset's trained weights.\", 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', model=\\'vgg16\\', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg16', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg16', pretrained=True)", "api_arguments": [{"name": "vgg16", "type": "str", "description": "VGG16 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg16": {"Top-1 error": 28.41, "Top-5 error": 9.62}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': An automotive website has different types of vehicles in a gallery; they want to automatically classify these vehicles into different categories based on an image. Tell me how to achieve this using an API., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg19_bn', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained VGG19 model with batch normalization from PyTorch Hub, which can be fine-tuned for classifying vehicles into different categories based on images.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg19_bn', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg19_bn', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg19_bn', pretrained=True)", "api_arguments": [{"name": "vgg19_bn", "type": "str", "description": "VGG19 model with batch normalization"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg19_bn', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg19_bn": {"Top-1 error": 25.76, "Top-5 error": 8.15}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': Generate code that will help me classify different dog breeds in a photo., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'wide_resnet101_2', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Wide ResNet-101-2 model from PyTorch Hub, which can be fine-tuned for dog breed classification.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'wide_resnet101_2', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='wide_resnet101_2', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Wide Residual Networks", "api_name": "wide_resnet101_2", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='wide_resnet101_2', pretrained=True)", "api_arguments": "pretrained", "python_environment_requirements": "torch, torchvision", "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'wide_resnet101_2', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"wide_resnet101_2": {"Top-1 error": 21.16, "Top-5 error": 5.72}}}, "description": "Wide Residual networks simply have increased number of channels compared to ResNet. Otherwise the architecture is the same. Deeper ImageNet models with bottleneck block have increased number of channels in the inner 3x3 convolution. The wide_resnet50_2 and wide_resnet101_2 models were trained in FP16 with mixed precision training using SGD with warm restarts. Checkpoints have weights in half precision (except batch norm) for smaller size, and can be used in FP32 models too."}}
{"code": "{'Instruction': I need an API that can help me classify various objects in an image from a popular repository. Provide a pretrained model for the same., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'wide_resnet50_2', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Wide ResNet-50-2 model from PyTorch Hub, which can be used for classifying various objects in an image.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'wide_resnet50_2', pretrained=True)\nmodel.eval()'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='wide_resnet50_2', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Wide Residual Networks", "api_name": "wide_resnet50_2", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='wide_resnet50_2', pretrained=True)", "api_arguments": "pretrained", "python_environment_requirements": "torch, torchvision", "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'wide_resnet50_2', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"wide_resnet50_2": {"Top-1 error": 21.49, "Top-5 error": 5.91}}}, "description": "Wide Residual networks simply have increased number of channels compared to ResNet. Otherwise the architecture is the same. Deeper ImageNet models with bottleneck block have increased number of channels in the inner 3x3 convolution. The wide_resnet50_2 and wide_resnet101_2 models were trained in FP16 with mixed precision training using SGD with warm restarts. Checkpoints have weights in half precision (except batch norm) for smaller size, and can be used in FP32 models too."}}
{"code": "{'Instruction': As a content moderator, I need to classify videos into categories Automatically. Please suggest an API for performing this task., 'Output': {'domain': 'Video Classification', 'api_call': \"model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pre-trained SlowFast model for video classification from PyTorch Hub to automatically classify video clips into different categories.', 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='slowfast_r50', pretrained=True)", "provider": "PyTorchVideo", "api_data": {"domain": "Video Classification", "framework": "PyTorchVideo", "functionality": "SlowFast Networks", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='slowfast_r50', pretrained=True)", "api_arguments": {"repository": "facebookresearch/pytorchvideo", "model": "slowfast_r50", "pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "pytorchvideo"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)", "device = 'cpu'", "model = model.eval()", "model = model.to(device)"], "performance": {"dataset": "Kinetics 400", "accuracy": {"top1": 76.94, "top5": 92.69}, "flops": 65.71, "params": 34.57}, "description": "Slow Fast model architectures are based on the paper 'SlowFast Networks for Video Recognition' by Christoph Feichtenhofer et al. They are pretrained on the Kinetics 400 dataset using the 8x8 setting. This model is capable of classifying video clips into different action categories. It is provided by the FAIR PyTorchVideo library."}}
{"code": "{'Instruction': Tell me an API that can classify a video clip according to the action in the clip., 'Output': {'domain': 'Video Classification', 'api_call': \"model = torch.hub.load('facebookresearch/pytorchvideo', 'x3d_s', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained X3D model from PyTorchVideo, which can classify video clips into different action categories based on the content of the clip.', 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/pytorchvideo', 'x3d_s', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='x3d_s', pretrained=True)", "provider": "PyTorchVideo", "api_data": {"domain": "Video Classification", "framework": "PyTorchVideo", "functionality": "X3D Networks", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='x3d_s', pretrained=True)", "api_arguments": {"repository": "facebookresearch/pytorchvideo", "model": "x3d_s", "pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "pytorchvideo"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/pytorchvideo', 'x3d_s', pretrained=True)", "device = 'cpu'", "model = model.eval()", "model = model.to(device)"], "performance": {"dataset": "Kinetics 400", "accuracy": {"top1": 73.33, "top5": 91.27}, "flops": 2.96, "params": 3.79}, "description": "X3D model architectures are based on the paper 'X3D: Expanding Architectures for Efficient Video Recognition' by Christoph Feichtenhofer. They are pretrained on the Kinetics 400 dataset. This model is capable of classifying video clips into different action categories. It is provided by the FAIR PyTorchVideo library."}}
{"code": "{'Instruction': Is there an NLP API that can summarize long articles or documents?, 'Output': {'domain': 'NLP', 'api_call': \"model = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-cased')\", 'api_provider': 'Hugging Face', 'explanation': 'Load the pretrained BERT model from Hugging Face PyTorch-Transformers, which can be fine-tuned for text summarization tasks on long articles or documents.', 'code': \"import torch\nmodel = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-cased')\"}}", "api_call": "torch.hub.load(repo_or_dir='huggingface/pytorch-transformers')", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Natural Language Processing", "api_name": "PyTorch-Transformers", "api_call": "torch.hub.load(repo_or_dir='huggingface/pytorch-transformers')", "api_arguments": ["pretrained_model_or_path", "output_attention", "output_hidden_states", "config", "from_tf"], "python_environment_requirements": ["tqdm", "boto3", "requests", "regex", "sentencepiece", "sacremoses"], "example_code": "import torch\ntokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-cased')\n\ntext_1 = \"Jim Henson was a puppeteer\"\ntext_2 = \"Who was Jim Henson ?\"\n\nindexed_tokens = tokenizer.encode(text_1, text_2, add_special_tokens=True)\n\nmodel = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-cased')\n\nwith torch.no_grad():\n encoded_layers, _ = model(tokens_tensor, token_type_ids=segments_tensors)", "performance": {"dataset": [{"name": "MRPC", "accuracy": "Not provided"}]}, "description": "PyTorch-Transformers is a library of state-of-the-art pre-trained models for Natural Language Processing (NLP) including BERT, GPT, GPT-2, Transformer-XL, XLNet, XLM, RoBERTa, and DistilBERT. The library provides functionality for tokenization, configuration, and various model architectures for different tasks such as causal language modeling, sequence classification, question answering, and masked language modeling."}}
{"code": "{'Instruction': Suggest a machine learning API that can be used to analyze images of protein structures to predict their cellular localization., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models', 'resnet18_swsl', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNet model from the Facebook Research Semi-supervised and Semi-weakly Supervised ImageNet Models collection. Fine-tune the model to classify protein structure images', 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models', 'resnet18_swsl', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/semi-supervised-ImageNet1K-models', model='resnet18_swsl', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Semi-supervised and semi-weakly supervised ImageNet Models", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/semi-supervised-ImageNet1K-models', model='resnet18_swsl', pretrained=True)", "api_arguments": {"repository": "facebookresearch/semi-supervised-ImageNet1K-models", "model": "resnet18_swsl", "pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models', 'resnet18_swsl', pretrained=True)", "model.eval()"], "performance": {"description": "Semi-supervised and semi-weakly supervised ImageNet models achieve state-of-the-art accuracy of 81.2% on ImageNet for the widely used/adopted ResNet-50 model architecture."}, "description": "Semi-supervised and semi-weakly supervised ImageNet Models are introduced in the 'Billion scale semi-supervised learning for image classification' paper. These models are pretrained on a subset of unlabeled YFCC100M public image dataset and fine-tuned with the ImageNet1K training dataset. They are capable of classifying images into different categories and are provided by the Facebook Research library."}}
{"code": "{'Instruction': I\u2019m working on a mobile app, and I need an efficient neural network for image classification. What API should I use?, 'Output': {'domain': 'Classification', 'api_call': 'model = torch.hub.load('mit-han-lab/once-for-all', 'ofa_supernet_mbv3_w10', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Use the Once-for-All (OFA) Networks, which are specifically designed for mobile and efficient image classification.', 'code': 'import torch\nmodel = torch.hub.load('mit-han-lab/once-for-all', 'ofa_supernet_mbv3_w10', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/once-for-all', model='ofa_supernet_mbv3_w10', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Once-for-all (OFA) Networks", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/once-for-all', model='ofa_supernet_mbv3_w10', pretrained=True)", "api_arguments": {"repository": "mit-han-lab/once-for-all", "model": "ofa_supernet_mbv3_w10", "pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "super_net_name = 'ofa_supernet_mbv3_w10'", "super_net = torch.hub.load('mit-han-lab/once-for-all', super_net_name, pretrained=True).eval()"], "performance": {"description": "OFA networks outperform state-of-the-art NAS methods (up to 4.0% ImageNet top1 accuracy improvement over MobileNetV3, or same accuracy but 1.5x faster than MobileNetV3, 2.6x faster than EfficientNet w.r.t measured latency) while reducing many orders of magnitude GPU hours and CO2 emission."}, "description": "Once-for-all (OFA) networks are a family of neural networks designed by MIT Han Lab. They decouple training and search, achieving efficient inference across various edge devices and resource constraints. OFA networks are pretrained on the IMAGENET dataset and are capable of classifying images into different categories."}}
{"code": "{'Instruction': The Amazon security team wants to auto-detect unattended package left at their headquarters. Propose an API that can detect objects in images., 'Output': {'domain': 'Object Detection', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained SSD (Single Shot MultiBox Detector) model from PyTorch Hub, which can be used to detect objects, including unattended packages, in images.\", 'code': \"import torch\nmodel = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_ssd', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Object Detection", "framework": "PyTorch", "functionality": "Single Shot MultiBox Detector", "api_name": "SSD", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_ssd', pretrained=True)", "api_arguments": ["model_name"], "python_environment_requirements": ["numpy", "scipy", "scikit-image", "matplotlib"], "example_code": "import torch\n\nssd_model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd')\nutils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd_processing_utils')\n\nssd_model.to('cuda')\nssd_model.eval()\n\ninputs = [utils.prepare_input(uri) for uri in uris]\ntensor = utils.prepare_tensor(inputs)\n\nwith torch.no_grad():\n detections_batch = ssd_model(tensor)\n\nresults_per_input = utils.decode_results(detections_batch)\nbest_results_per_input = [utils.pick_best(results, 0.40) for results in results_per_input]", "performance": {"dataset": {"name": "COCO", "accuracy": "Not provided"}}, "description": "The SSD (Single Shot MultiBox Detector) model is an object detection model based on the paper 'SSD: Single Shot MultiBox Detector'. It uses a deep neural network for detecting objects in images. This implementation replaces the obsolete VGG model backbone with the more modern ResNet-50 model. The SSD model is trained on the COCO dataset and can be used to detect objects in images with high accuracy and efficiency."}}
{"code": "{'Instruction': What machine learning API should I use to convert a text to speech?, 'Output': {'domain': 'Text-to-Speech', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tacotron2', model_math='fp16')\", 'api_provider': 'PyTorch', 'explanation': 'Use the Tacotron 2 model available through PyTorch Hub, which generates natural-sounding speech from input text without any additional prosody information.', 'code': 'import torch\nmodel = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tacotron2', model_math='fp16')'}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_tacotron2', model_math='fp16')", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Text-to-Speech", "api_name": "Tacotron 2", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_tacotron2', model_math='fp16')", "api_arguments": {"model_math": "fp16"}, "python_environment_requirements": ["numpy", "scipy", "librosa", "unidecode", "inflect", "libsndfile1"], "example_code": ["import torch", "tacotron2 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tacotron2', model_math='fp16')", "tacotron2 = tacotron2.to('cuda')", "tacotron2.eval()", "text = 'Hello world, I missed you so much.'", "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tts_utils')", "sequences, lengths = utils.prepare_input_sequence([text])", "with torch.no_grad():", " mel, _, _ = tacotron2.infer(sequences, lengths)", " audio = waveglow.infer(mel)", "audio_numpy = audio[0].data.cpu().numpy()", "rate = 22050"], "performance": {"dataset": "LJ Speech", "accuracy": "Not specified"}, "description": "The Tacotron 2 model generates mel spectrograms from input text using an encoder-decoder architecture, and it is designed for generating natural-sounding speech from raw transcripts without any additional prosody information. This implementation uses Dropout instead of Zoneout to regularize the LSTM layers. The WaveGlow model (also available via torch.hub) is a flow-based model that consumes the mel spectrograms to generate speech."}}
{"code": "{'Instruction': I need an efficient image classification API that can be used to recognize complex objects in images., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', 'mealv2_resnest50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MEAL V2 model with ResNet50 architecture from PyTorch Hub, which is capable of recognizing complex objects in images with high accuracy.', 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', 'mealv2_resnest50', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50', pretrained=True)", "api_arguments": {"model_name": "mealv2_resnest50"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ ResNet50", "resolution": "224", "parameters": "25.6M", "top1": "80.67", "top5": "95.09"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': The Vegan Society is doing research on vegan products by automatically labeling Instagram images depicting different types of food. Provide an API that can identify and classify objects in an image., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V1', 'mealv1_resnest50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MEAL V2 model based on ResNet50 from PyTorch Hub to perform image classification and identify objects in an image.', 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V1', 'mealv1_resnest50', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V1', model='mealv1_resnest50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V1', model='mealv1_resnest50', pretrained=True)", "api_arguments": {"model_name": "mealv1_resnest50"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V1 w/ ResNet50", "resolution": "224", "parameters": "25.6M", "top1": "78.21", "top5": "94.01"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': Recommend an API to enhance classification accuracy for a popular machine learning model for tagging animals in images., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MEAL V2 model with ResNet50 backbone and CutMix augmentation from PyTorch Hub to achieve enhanced classification accuracy for tagging animals in images.', 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50_cutmix', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50_cutmix', pretrained=True)", "api_arguments": {"model_name": "mealv2_resnest50_cutmix"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 + CutMix w/ ResNet50", "resolution": "224", "parameters": "25.6M", "top1": "80.98", "top5": "95.35"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': Design a machine learning API to identify the type of objects present in an image with high accuracy., 'Output':{'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenetv3_small_075', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MEAL V2 model from PyTorch Hub, which is designed to classify objects in images with high accuracy without relying on additional tricks.', 'code':\"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenetv3_small_075', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenetv3_small_075', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenetv3_small_075', pretrained=True)", "api_arguments": {"model_name": "mealv2_mobilenetv3_small_075"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ MobileNet V3-Small 0.75", "resolution": "224", "parameters": "2.04M", "top1": "67.60", "top5": "87.23"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': Create a tool to predict what would be in an image taken outdoors in the wild, and recommend an API that could accomplish this., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', 'mealv2_resnest50_380x380', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MEAL V2 model from PyTorch Hub with a ResNet50 backbone, which can predict the contents in outdoor wild images accurately.', 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', 'mealv2_resnest50_380x380', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50_380x380', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50_380x380', pretrained=True)", "api_arguments": {"model_name": "mealv2_resnest50_380x380"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ ResNet50", "resolution": "380", "parameters": "25.6M", "top1": "81.72", "top5": "95.81"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': Recommend an API that can identify different breeds of dogs from photographs., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', 'mealv2_efficientnet_b0', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MEAL_V2 EfficientNet-B0 model from PyTorch Hub, which can be fine-tuned for identifying different dog breeds from photographs.', 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', 'mealv2_efficientnet_b0', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_efficientnet_b0', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_efficientnet_b0', pretrained=True)", "api_arguments": {"model_name": "mealv2_efficientnet_b0"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ EfficientNet-B0", "resolution": "224", "parameters": "5.29M", "top1": "78.29", "top5": "93.95"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': Recommend an API to classify images with a high level of accuracy without using any tricks., 'Output': {'domain': 'Classification', 'api_call': \"torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', 'mealv2_mobilenetv3_small_100', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Use the MEAL V2 model from PyTorch Hub, which achieves state-of-the-art performance on image classification without relying on any tricks or modifications.', 'code': 'import torch\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_mobilenetv3_small_100', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenetv3_small_100', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenetv3_small_100', pretrained=True)", "api_arguments": {"model_name": "mealv2_mobilenetv3_small_100"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ MobileNet V3-Small 1.0", "resolution": "224", "parameters": "2.54M", "top1": "69.65", "top5": "88.71"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': I'm an app developer and I need an API to classify which type of animal is in a particular image. What do you suggest?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenet_v3_large_100', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MEAL V2 model for image classification from PyTorch Hub. This model is optimized for high accuracy and performance and can classify various objects, including animals.', 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenet_v3_large_100', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenet_v3_large_100', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenet_v3_large_100', pretrained=True)", "api_arguments": {"model_name": "mealv2_mobilenet_v3_large_100"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ MobileNet V3-Large 1.0", "resolution": "224", "parameters": "5.48M", "top1": "76.92", "top5": "93.32"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': Suggest a pre-trained video classification model capable of classifying human actions in a video., 'Output': {'domain': 'Video Classification', 'api_call': \"model = torch.hub.load('facebookresearch/pytorchvideo', 'slow_r50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained 3D ResNet model 'slow_r50' from PyTorch Hub, which can classify human actions in a video using its pre-trained knowledge from the Kinetics-400 dataset.\", 'code': 'import torch\nmodel = torch.hub.load(\\'facebookresearch/pytorchvideo\\', \\'slow_r50\\', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='slow_r50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Video Classification", "framework": "PyTorch", "functionality": "3D ResNet", "api_name": "slow_r50", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='slow_r50', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["torch", "json", "urllib", "pytorchvideo", "torchvision", "torchaudio", "torchtext", "torcharrow", "TorchData", "TorchRec", "TorchServe", "PyTorch on XLA Devices"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/pytorchvideo', 'slow_r50', pretrained=True)", "device = 'cpu'", "model = model.eval()", "model = model.to(device)"], "performance": {"dataset": "Kinetics 400", "accuracy": {"top_1": 74.58, "top_5": 91.63}, "Flops (G)": 54.52, "Params (M)": 32.45}, "description": "The 3D ResNet model is a Resnet-style video classification network pretrained on the Kinetics 400 dataset. It is based on the architecture from the paper 'SlowFast Networks for Video Recognition' by Christoph Feichtenhofer et al."}}
{"code": "{'Instruction': I need an API capable of detecting vehicles, drivable areas, and lane lines from images taken from a dashboard camera., 'Output': {'domain': 'Object Detection', 'api_call': \"model = torch.hub.load('datvuthanh/hybridnets', 'hybridnets', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained HybridNets model from PyTorch Hub, which is specialized for traffic object detection, drivable area segmentation, and lane detection from images taken by a dashboard camera.\", 'code': \"import torch\nmodel = torch.hub.load('datvuthanh/hybridnets', 'hybridnets', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='datvuthanh/hybridnets', model='hybridnets', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Object Detection", "framework": "PyTorch", "functionality": "Traffic Object Detection, Drivable Area Segmentation, Lane Detection", "api_name": "HybridNets", "api_call": "torch.hub.load(repo_or_dir='datvuthanh/hybridnets', model='hybridnets', pretrained=True)", "api_arguments": "pretrained", "python_environment_requirements": "Python>=3.7, PyTorch>=1.10", "example_code": "import torch\nmodel = torch.hub.load('datvuthanh/hybridnets', 'hybridnets', pretrained=True)\nimg = torch.randn(1,3,640,384)\nfeatures, regression, classification, anchors, segmentation = model(img)", "performance": {"dataset": [{"name": "BDD100K", "accuracy": {"Traffic Object Detection": {"Recall (%)": 92.8, "mAP@0.5 (%)": 77.3}, "Drivable Area Segmentation": {"Drivable mIoU (%)": 90.5}, "Lane Line Detection": {"Accuracy (%)": 85.4, "Lane Line IoU (%)": 31.6}}}]}, "description": "HybridNets is an end2end perception network for multi-tasks. Our work focused on traffic object detection, drivable area segmentation and lane detection. HybridNets can run real-time on embedded systems, and obtains SOTA Object Detection, Lane Detection on BDD100K Dataset."}}
{"code": "{'Instruction': Help me to find an API to assist autonomous vehicles providing object detection, drivable area segmentation and lane detection for safer navigation., 'Output': {'domain': 'Autonomous Vehicle Assistance', 'api_call': \"model = torch.hub.load('hustvl/yolop', 'yolop', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained YOLOP model from PyTorch Hub for object detection, drivable area segmentation, and lane detection to assist autonomous vehicles in safe navigation.', 'code': \"import torch\nmodel = torch.hub.load('hustvl/yolop', 'yolop', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='hustvl/yolop', model='yolop', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Object Detection", "framework": "PyTorch", "functionality": "Object Detection, Drivable Area Segmentation, Lane Detection", "api_name": "YOLOP", "api_call": "torch.hub.load(repo_or_dir='hustvl/yolop', model='yolop', pretrained=True)", "api_arguments": "pretrained", "python_environment_requirements": "pip install -qr https://github.com/hustvl/YOLOP/blob/main/requirements.txt", "example_code": "import torch\nmodel = torch.hub.load('hustvl/yolop', 'yolop', pretrained=True)\nimg = torch.randn(1,3,640,640)\ndet_out, da_seg_out,ll_seg_out = model(img)", "performance": {"dataset": "BDD100K", "accuracy": {"Object Detection": {"Recall(%)": 89.2, "mAP50(%)": 76.5, "Speed(fps)": 41}, "Drivable Area Segmentation": {"mIOU(%)": 91.5, "Speed(fps)": 41}, "Lane Detection": {"mIOU(%)": 70.5, "IOU(%)": 26.2}}}, "description": "YOLOP is an efficient multi-task network that can jointly handle three crucial tasks in autonomous driving: object detection, drivable area segmentation and lane detection. And it is also the first to reach real-time on embedded devices while maintaining state-of-the-art level performance on the BDD100K dataset."}}
{"code": "{'Instruction': I want an API that can deliver relative depth information from a single photo., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('intel-isl/MiDaS', 'MiDaS_small', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MiDaS model from PyTorch Hub, which can compute relative depth from a single image.', 'code': 'import torch\nmodel = torch.hub.load('intel-isl/MiDaS', 'MiDaS_small', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='intel-isl/MiDaS', model='MiDaS_small', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Computing relative depth from a single image", "api_name": "MiDaS", "api_call": "torch.hub.load(repo_or_dir='intel-isl/MiDaS', model='MiDaS_small', pretrained=True)", "api_arguments": {"repo_or_dir": "intel-isl/MiDaS", "model": "model_type"}, "python_environment_requirements": "pip install timm", "example_code": ["import cv2", "import torch", "import urllib.request", "import matplotlib.pyplot as plt", "url, filename = ('https://github.com/pytorch/hub/raw/master/images/dog.jpg', 'dog.jpg')", "urllib.request.urlretrieve(url, filename)", "model_type = 'DPT_Large'", "midas = torch.hub.load('intel-isl/MiDaS', 'MiDaS_small')", "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')", "midas.to(device)", "midas.eval()", "midas_transforms = torch.hub.load('intel-isl/MiDaS', 'transforms')", "if model_type == 'DPT_Large' or model_type == 'DPT_Hybrid':", " transform = midas_transforms.dpt_transform", "else:", " transform = midas_transforms.small_transform", "img = cv2.imread(filename)", "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)", "input_batch = transform(img).to(device)", "with torch.no_grad():", " prediction = midas(input_batch)", "prediction = torch.nn.functional.interpolate(", " prediction.unsqueeze(1),", " size=img.shape[:2],", " mode='bicubic',", " align_corners=False,", ").squeeze()", "output = prediction.cpu().numpy()", "plt.imshow(output)", "plt.show()"], "performance": {"dataset": "10 distinct datasets", "accuracy": "Multi-objective optimization"}, "description": "MiDaS computes relative inverse depth from a single image. The repository provides multiple models that cover different use cases ranging from a small, high-speed model to a very large model that provide the highest accuracy. The models have been trained on 10 distinct datasets using multi-objective optimization to ensure high quality on a wide range of inputs."}}
{"code": "{'Instruction': Suppose I have to design an autonomous car which uses a depth perception system. Suggest me an API which can compute relative depth from a single image., 'Output': {'domain': 'Depth Perception', 'api_call': \"model = torch.hub.load(repo_or_dir='intel-isl/MiDaS', model='DPT_Large', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MiDaS model from PyTorch Hub, which calculates relative depth from a single image for depth perception systems.', 'code': 'import torch\nmodel = torch.hub.load(repo_or_dir='intel-isl/MiDaS', model='DPT_Large', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='intel-isl/MiDaS', model='DPT_Large', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Computing relative depth from a single image", "api_name": "MiDaS", "api_call": "torch.hub.load(repo_or_dir='intel-isl/MiDaS', model='DPT_Large', pretrained=True)", "api_arguments": {"repo_or_dir": "intel-isl/MiDaS", "model": "model_type"}, "python_environment_requirements": "pip install timm", "example_code": ["import cv2", "import torch", "import urllib.request", "import matplotlib.pyplot as plt", "url, filename = ('https://github.com/pytorch/hub/raw/master/images/dog.jpg', 'dog.jpg')", "urllib.request.urlretrieve(url, filename)", "model_type = 'DPT_Large'", "midas = torch.hub.load('intel-isl/MiDaS', 'DPT_Large')", "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')", "midas.to(device)", "midas.eval()", "midas_transforms = torch.hub.load('intel-isl/MiDaS', 'transforms')", "if model_type == 'DPT_Large' or model_type == 'DPT_Hybrid':", " transform = midas_transforms.dpt_transform", "else:", " transform = midas_transforms.small_transform", "img = cv2.imread(filename)", "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)", "input_batch = transform(img).to(device)", "with torch.no_grad():", " prediction = midas(input_batch)", "prediction = torch.nn.functional.interpolate(", " prediction.unsqueeze(1),", " size=img.shape[:2],", " mode='bicubic',", " align_corners=False,", ").squeeze()", "output = prediction.cpu().numpy()", "plt.imshow(output)", "plt.show()"], "performance": {"dataset": "10 distinct datasets", "accuracy": "Multi-objective optimization"}, "description": "MiDaS computes relative inverse depth from a single image. The repository provides multiple models that cover different use cases ranging from a small, high-speed model to a very large model that provide the highest accuracy. The models have been trained on 10 distinct datasets using multi-objective optimization to ensure high quality on a wide range of inputs."}}
{"code": "{'Instruction': I have a single image and want to compute its depth map. Suggest me an API for this task., 'Output': {'domain': 'Depth Estimation', 'api_call': \"model = torch.hub.load('intel-isl/MiDaS', 'DPT_Hybrid', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MiDaS DPT_Hybrid model from PyTorch Hub, which can compute relative depth maps from a single input image.', 'code': 'import torch\nmodel = torch.hub.load('intel-isl/MiDaS', 'DPT_Hybrid', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='intel-isl/MiDaS', model='DPT_Hybrid', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Computing relative depth from a single image", "api_name": "MiDaS", "api_call": "torch.hub.load(repo_or_dir='intel-isl/MiDaS', model='DPT_Hybrid', pretrained=True)", "api_arguments": {"repo_or_dir": "intel-isl/MiDaS", "model": "model_type"}, "python_environment_requirements": "pip install timm", "example_code": ["import cv2", "import torch", "import urllib.request", "import matplotlib.pyplot as plt", "url, filename = ('https://github.com/pytorch/hub/raw/master/images/dog.jpg', 'dog.jpg')", "urllib.request.urlretrieve(url, filename)", "model_type = 'DPT_Large'", "midas = torch.hub.load('intel-isl/MiDaS', 'DPT_Hybrid')", "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')", "midas.to(device)", "midas.eval()", "midas_transforms = torch.hub.load('intel-isl/MiDaS', 'transforms')", "if model_type == 'DPT_Large' or model_type == 'DPT_Hybrid':", " transform = midas_transforms.dpt_transform", "else:", " transform = midas_transforms.small_transform", "img = cv2.imread(filename)", "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)", "input_batch = transform(img).to(device)", "with torch.no_grad():", " prediction = midas(input_batch)", "prediction = torch.nn.functional.interpolate(", " prediction.unsqueeze(1),", " size=img.shape[:2],", " mode='bicubic',", " align_corners=False,", ").squeeze()", "output = prediction.cpu().numpy()", "plt.imshow(output)", "plt.show()"], "performance": {"dataset": "10 distinct datasets", "accuracy": "Multi-objective optimization"}, "description": "MiDaS computes relative inverse depth from a single image. The repository provides multiple models that cover different use cases ranging from a small, high-speed model to a very large model that provide the highest accuracy. The models have been trained on 10 distinct datasets using multi-objective optimization to ensure high quality on a wide range of inputs."}}
{"code": "{'Instruction': How can I extract the vocal track and other stems from a song's audio? Suggest an API for this., 'Output': {'domain': 'Audio Separation', 'api_call': \"model = torch.hub.load('sigsep/open-unmix-pytorch', 'umxhq', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Open-Unmix model for music source separation from PyTorch Hub, which can separate a song into vocal, drums, bass and other instruments stems.', 'code': \"import torch\nseparator = torch.hub.load('sigsep/open-unmix-pytorch', 'umxhq', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='sigsep/open-unmix-pytorch', model='umxhq', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Audio Separation", "framework": "PyTorch", "functionality": "Music Source Separation", "api_name": "Open-Unmix", "api_call": "torch.hub.load(repo_or_dir='sigsep/open-unmix-pytorch', model='umxhq', pretrained=True)", "api_arguments": ["model_name"], "python_environment_requirements": ["PyTorch >=1.6.0", "torchaudio"], "example_code": ["import torch", "separator = torch.hub.load('sigsep/open-unmix-pytorch', 'umxhq')", "audio = torch.rand((1, 2, 100000))", "original_sample_rate = separator.sample_rate", "estimates = separator(audio)"], "performance": {"dataset": "MUSDB18", "accuracy": "N/A"}, "description": "Open-Unmix provides ready-to-use models that allow users to separate pop music into four stems: vocals, drums, bass and the remaining other instruments. The models were pre-trained on the freely available MUSDB18 dataset."}}
{"code": "{'Instruction': Can you find me an API that identifies a species of bird from an image?, 'Output': {'domain': 'Fine-grained Image Classification', 'api_call': 'model = torch.hub.load('nicolalandro/ntsnet-cub200', 'ntsnet', pretrained=True, topN=6, device='cpu', num_classes=200)', 'api_provider': 'PyTorch', 'explanation': 'Load the NTS-Net model from PyTorch Hub, which is pretrained on the CUB200 2011 dataset to perform fine-grained classification of bird species.', 'code': 'import torch\nmodel = torch.hub.load('nicolalandro/ntsnet-cub200', 'ntsnet', pretrained=True, topN=6, device='cpu', num_classes=200)'}}", "api_call": "torch.hub.load(repo_or_dir='nicolalandro/ntsnet-cub200', model='ntsnet', pretrained=True, **{'topN': 6, 'device':'cpu', 'num_classes': 200})", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Fine-grained image classifier", "api_name": "ntsnet", "api_call": "torch.hub.load(repo_or_dir='nicolalandro/ntsnet-cub200', model='ntsnet', pretrained=True, **{'topN': 6, 'device':'cpu', 'num_classes': 200})", "api_arguments": {"pretrained": "True", "topN": "6", "device": "cpu", "num_classes": "200"}, "python_environment_requirements": ["torch", "torchvision", "PIL"], "example_code": "from torchvision import transforms\nimport torch\nimport urllib\nfrom PIL import Image\n\ntransform_test = transforms.Compose([\n transforms.Resize((600, 600), Image.BILINEAR),\n transforms.CenterCrop((448, 448)),\n transforms.ToTensor(),\n transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n])\n\nmodel = torch.hub.load('nicolalandro/ntsnet-cub200', 'ntsnet', pretrained=True, **{'topN': 6, 'device':'cpu', 'num_classes': 200})\nmodel.eval()\n\nurl = 'https://raw.githubusercontent.com/nicolalandro/ntsnet-cub200/master/images/nts-net.png'\nimg = Image.open(urllib.request.urlopen(url))\nscaled_img = transform_test(img)\ntorch_images = scaled_img.unsqueeze(0)\n\nwith torch.no_grad():\n top_n_coordinates, concat_out, raw_logits, concat_logits, part_logits, top_n_index, top_n_prob = model(torch_images)\n\n_, predict = torch.max(concat_logits, 1)\npred_id = predict.item()\nprint('bird class:', model.bird_classes[pred_id])", "performance": {"dataset": "CUB200 2011", "accuracy": "Not provided"}, "description": "This is an nts-net pretrained with CUB200 2011 dataset, which is a fine-grained dataset of birds species."}}
{"code": "{'Instruction': I have a voice assistant project and I want to convert the user's speech to text. Can you recommend an API suitable for this?, 'Output': {'domain': 'Speech-To-Text', 'api_call': \"model = torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_stt', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the Silero Speech-To-Text models from PyTorch Hub, which are robust and can handle multiple languages.', 'code': 'import torch\nmodel, decoder, utils = torch.hub.load(repo_or_dir=\"snakers4/silero-models\", model=\"silero_stt\", language=\"en\", device=torch.device(\"cpu\"))'}}", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_stt', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Speech-To-Text", "api_name": "Silero Models", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_stt', pretrained=True)", "api_arguments": {"repo_or_dir": "snakers4/silero-models", "model": "silero_stt", "language": ["en", "de", "es"], "device": "device"}, "python_environment_requirements": ["pip install -q torchaudio omegaconf soundfile"], "example_code": ["import torch", "import zipfile", "import torchaudio", "from glob import glob", "device = torch.device('cpu')", "model, decoder, utils = torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_stt', language='en', device=device)", "(read_batch, split_into_batches, read_audio, prepare_model_input) = utils", "torch.hub.download_url_to_file('https://opus-codec.org/static/examples/samples/speech_orig.wav', dst ='speech_orig.wav', progress=True)", "test_files = glob('speech_orig.wav')", "batches = split_into_batches(test_files, batch_size=10)", "input = prepare_model_input(read_batch(batches[0]), device=device)", "output = model(input)", "for example in output:", " print(decoder(example.cpu()))"], "performance": {"dataset": "Open-STT", "accuracy": "See the wiki for quality and performance benchmarks"}, "description": "Silero Speech-To-Text models provide enterprise grade STT in a compact form-factor for several commonly spoken languages. The models are robust to a variety of dialects, codecs, domains, noises, and lower sampling rates. They consume a normalized audio in the form of samples and output frames with token probabilities. A decoder utility is provided for simplicity."}}
{"code": "{'Instruction': Recommend me an API for detecting the presence of speech in a given audio file., 'Output': {'domain': 'Voice Activity Detection', 'api_call': \"model = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the Silero VAD, a pre-trained enterprise-grade Voice Activity Detector (VAD) from PyTorch Hub, to detect the presence of speech in an audio file.', 'code': 'import torch\nmodel, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=True)'}}", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Voice Activity Detection", "api_name": "Silero Voice Activity Detector", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=True)", "api_arguments": {"repo_or_dir": "snakers4/silero-vad", "model": "silero_vad", "force_reload": "True"}, "python_environment_requirements": {"torchaudio": "pip install -q torchaudio"}, "example_code": {"import": ["import torch", "torch.set_num_threads(1)", "from IPython.display import Audio", "from pprint import pprint"], "download_example": "torch.hub.download_url_to_file('https://models.silero.ai/vad_models/en.wav', 'en_example.wav')", "load_model": "model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=True)", "load_utils": "(get_speech_timestamps, _, read_audio, _) = utils", "set_sampling_rate": "sampling_rate = 16000", "read_audio": "wav = read_audio('en_example.wav', sampling_rate=sampling_rate)", "get_speech_timestamps": "speech_timestamps = get_speech_timestamps(wav, model, sampling_rate=sampling_rate)", "print_speech_timestamps": "pprint(speech_timestamps)"}, "performance": {"dataset": "", "accuracy": ""}, "description": "Silero VAD is a pre-trained enterprise-grade Voice Activity Detector (VAD) that aims to provide a high-quality and modern alternative to the WebRTC Voice Activity Detector. The model is optimized for performance on 1 CPU thread and is quantized."}}
{"code": "{'Instruction': I need to extract objects from a picture of a city scene. Provide me with an API that can output the segmented objects from an image., 'Output': {'domain': 'Semantic Segmentation', 'api_call': \"model = torch.hub.load('pytorch/vision', 'deeplabv3_resnet101', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained DeepLabV3 model with ResNet-101 backbone from PyTorch Hub to segment objects from an input city scene image.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'deeplabv3_resnet101', pretrained=True)\nmodel.eval()' }}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='deeplabv3_resnet101', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "DeepLabV3", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='deeplabv3_resnet101', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": "latest", "torchvision": "latest", "PIL": "latest", "matplotlib": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_resnet101', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "input_image = input_image.convert('RGB')", "preprocess = transforms.Compose([", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)['out'][0]", "output_predictions = output.argmax(0)"], "performance": {"dataset": "COCO val2017", "accuracy": [{"model": "deeplabv3_resnet101", "Mean IOU": 67.4, "Global Pixelwise Accuracy": 92.4}]}, "description": "DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset."}}
{"code": "{'Instruction': Develop an API that detects objects in an image given the URL of the image. The API should work with high accuracy., 'Output': {'domain': 'Object Detection', 'api_call': \"model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained YOLOv5 model from PyTorch Hub to detect objects in an image provided by the URL, offering high accuracy for object detection.', 'code': 'import torch\nmodel = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='ultralytics/yolov5', model='yolov5s', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Object Detection", "framework": "PyTorch", "functionality": "YOLOv5", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='ultralytics/yolov5', model='yolov5s', pretrained=True)", "api_arguments": ["'ultralytics/yolov5'", "'yolov5s'", "pretrained=True"], "python_environment_requirements": "Python>=3.8, PyTorch>=1.7", "example_code": ["import torch", "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)", "imgs = ['https://ultralytics.com/images/zidane.jpg']", "results = model(imgs)", "results.print()", "results.save()", "results.xyxy[0]", "results.pandas().xyxy[0]"], "performance": {"dataset": "COCO", "accuracy": {"YOLOv5s6": {"mAPval0.5:0.95": 43.3, "mAPtest0.5:0.95": 43.3, "mAPval0.5": 61.9}, "YOLOv5m6": {"mAPval0.5:0.95": 50.5, "mAPtest0.5:0.95": 50.5, "mAPval0.5": 68.7}, "YOLOv5l6": {"mAPval0.5:0.95": 53.4, "mAPtest0.5:0.95": 53.4, "mAPval0.5": 71.1}, "YOLOv5x6": {"mAPval0.5:0.95": 54.4, "mAPtest0.5:0.95": 54.4, "mAPval0.5": 72.0}, "YOLOv5x6 TTA": {"mAPval0.5:0.95": 55.0, "mAPtest0.5:0.95": 55.0, "mAPval0.5": 72.0}}}, "description": "YOLOv5 is a family of compound-scaled object detection models trained on the COCO dataset, and includes simple functionality for Test Time Augmentation (TTA), model ensembling, hyperparameter evolution, and export to ONNX, CoreML and TFLite."}}
{"code": "{'Instruction': Find an API to correctly classify my travel photos based on the displayed location., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNext WSL model for image classification from PyTorch Hub to classify travel photos based on locations.', 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x16d_wsl', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNext WSL", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x16d_wsl', pretrained=True)", "api_arguments": [{"name": "resnext101_32x16d_wsl", "type": "str", "description": "ResNeXt-101 32x16d WSL model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl')", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(output[0])", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeXt-101 32x16d": {"Top-1 Acc.": "84.2", "Top-5 Acc.": "97.2"}}}, "description": "The provided ResNeXt models are pre-trained in weakly-supervised fashion on 940 million public images with 1.5K hashtags matching with 1000 ImageNet1K synsets, followed by fine-tuning on ImageNet1K dataset. The models significantly improve the training accuracy on ImageNet compared to training from scratch. They achieve state-of-the-art accuracy of 85.4% on ImageNet with the ResNext-101 32x48d model."}}
{"code": "{'Instruction': Develop a model to generate new clothing design samples using GAN (Generative Adversarial Networks)., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'DCGAN', pretrained=True, useGPU=use_gpu)\", 'api_provider': 'PyTorch', 'explanation': 'Load the DCGAN model from PyTorch Hub with pretrained weights on the FashionGen dataset for generating new clothing designs.', 'code': \"import torch\nuse_gpu = True if torch.cuda.is_available() else False\nmodel = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'DCGAN', pretrained=True, useGPU=use_gpu)\"}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorch_GAN_zoo:hub', model='DCGAN', pretrained=True, useGPU=use_gpu)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Generative Adversarial Networks", "api_name": "DCGAN", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorch_GAN_zoo:hub', model='DCGAN', pretrained=True, useGPU=use_gpu)", "api_arguments": {"pretrained": "True", "useGPU": "use_gpu"}, "python_environment_requirements": "Python 3", "example_code": {"import": ["import torch", "import matplotlib.pyplot as plt", "import torchvision"], "use_gpu": "use_gpu = True if torch.cuda.is_available() else False", "load_model": "model = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'DCGAN', pretrained=True, useGPU=use_gpu)", "build_noise_data": "noise, _ = model.buildNoiseData(num_images)", "generate_images": "with torch.no_grad(): generated_images = model.test(noise)", "plot_images": ["plt.imshow(torchvision.utils.make_grid(generated_images).permute(1, 2, 0).cpu().numpy())", "plt.show()"]}, "performance": {"dataset": "FashionGen", "accuracy": "N/A"}, "description": "DCGAN is a model designed in 2015 by Radford et. al. in the paper Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks. It is a GAN architecture both very simple and efficient for low resolution image generation (up to 64x64)."}}
{"code": "{'Instruction': I run an online marketplace where users want to categorize their items based on images. What API should they use for this?, 'Output': {'domain': 'Image Classification', 'api_call': 'model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x32d_wsl', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the ResNext WSL model for image classification from PyTorch Hub to categorize items based on images.', 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x32d_wsl', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x32d_wsl', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNext WSL", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x32d_wsl', pretrained=True)", "api_arguments": [{"name": "resnext101_32x32d_wsl", "type": "str", "description": "ResNeXt-101 32x32d WSL model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x32d_wsl')", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(output[0])", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeXt-101 32x32d": {"Top-1 Acc.": "85.1", "Top-5 Acc.": "97.5"}}}, "description": "The provided ResNeXt models are pre-trained in weakly-supervised fashion on 940 million public images with 1.5K hashtags matching with 1000 ImageNet1K synsets, followed by fine-tuning on ImageNet1K dataset. The models significantly improve the training accuracy on ImageNet compared to training from scratch. They achieve state-of-the-art accuracy of 85.4% on ImageNet with the ResNext-101 32x48d model."}}
{"code": "{'Instruction': Explain how to use an API for semantic segmentation of park scenes to identify different regions such as playgrounds, pavements and water., 'Output': {'domain': 'Semantic Segmentation', 'api_call': \"model = torch.hub.load('pytorch/vision', 'deeplabv3_mobilenet_v3_large', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained DeepLabV3 model with a MobileNet_V3 backbone for semantic segmentation.', 'code': 'import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load(\"pytorch/vision\", \"deeplabv3_mobilenet_v3_large\", pretrained=True)\nmodel.eval()\n\ninput_image = Image.open(\"park_scene.jpg\")\ninput_image = input_image.convert(\"RGB\")\n\npreprocess = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\ninput_batch = input_batch.to(\"cuda\") if torch.cuda.is_available() else input_batch\nmodel.to(\"cuda\") if torch.cuda.is_available() else model\n\nwith torch.no_grad():\n    output = model(input_batch)[\"out\"][0]\noutput_predictions = output.argmax(0)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='deeplabv3_mobilenet_v3_large', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "DeepLabV3", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='deeplabv3_mobilenet_v3_large', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": "latest", "torchvision": "latest", "PIL": "latest", "matplotlib": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_mobilenet_v3_large', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "input_image = input_image.convert('RGB')", "preprocess = transforms.Compose([", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)['out'][0]", "output_predictions = output.argmax(0)"], "performance": {"dataset": "COCO val2017", "accuracy": [{"model": "deeplabv3_mobilenet_v3_large", "Mean IOU": 60.3, "Global Pixelwise Accuracy": 91.4}]}, "description": "DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset."}}
{"code": "{'Instruction': Propose an API that can recognize objects in an image with high accuracy., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x48d_wsl', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNeXt WSL model from PyTorch Hub, which is capable of accurately recognizing objects in images.', 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x48d_wsl', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x48d_wsl', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNext WSL", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x48d_wsl', pretrained=True)", "api_arguments": [{"name": "resnext101_32x48d_wsl", "type": "str", "description": "ResNeXt-101 32x48d WSL model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x48d_wsl')", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(output[0])", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeXt-101 32x48d": {"Top-1 Acc.": "85.4", "Top-5 Acc.": "97.6"}}}, "description": "The provided ResNeXt models are pre-trained in weakly-supervised fashion on 940 million public images with 1.5K hashtags matching with 1000 ImageNet1K synsets, followed by fine-tuning on ImageNet1K dataset. The models significantly improve the training accuracy on ImageNet compared to training from scratch. They achieve state-of-the-art accuracy of 85.4% on ImageNet with the ResNext-101 32x48d model."}}
{"code": "{'Instruction': Could you suggest a GAN model to generate high-quality human faces images?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'PGAN', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Progressive Growing of GANs (PGAN) model from PyTorch Hub, which can generate high-quality human face images.', 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'PGAN', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorch_GAN_zoo:hub', model='PGAN', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Generative Adversarial Networks (GANs)", "api_name": "PGAN", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorch_GAN_zoo:hub', model='PGAN', pretrained=True)", "api_arguments": {"repo_or_dir": "facebookresearch/pytorch_GAN_zoo:hub", "model": "PGAN", "model_name": "celebAHQ-512", "pretrained": "True", "useGPU": "use_gpu"}, "python_environment_requirements": "Python 3", "example_code": {"import": "import torch", "use_gpu": "use_gpu = True if torch.cuda.is_available() else False", "load_model": "model = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'PGAN', model_name='celebAHQ-512', pretrained=True, useGPU=use_gpu)", "build_noise_data": "noise, _ = model.buildNoiseData(num_images)", "test": "generated_images = model.test(noise)", "plot_images": {"import_matplotlib": "import matplotlib.pyplot as plt", "import_torchvision": "import torchvision", "make_grid": "grid = torchvision.utils.make_grid(generated_images.clamp(min=-1, max=1), scale_each=True, normalize=True)", "imshow": "plt.imshow(grid.permute(1, 2, 0).cpu().numpy())", "show": "plt.show()"}}, "performance": {"dataset": "celebA", "accuracy": "High-quality celebrity faces"}, "description": "Progressive Growing of GANs (PGAN) is a method for generating high-resolution images using generative adversarial networks. The model is trained progressively, starting with low-resolution images and gradually increasing the resolution until the desired output is achieved. This implementation is based on the paper by Tero Karras et al., 'Progressive Growing of GANs for Improved Quality, Stability, and Variation'."}}
{"code": "{'Instruction': How can I use machine learning to identify different land use types from an aerial photograph or satellite imagery?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'alexnet', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained AlexNet model from PyTorch Hub, which can be fine-tuned for land use classification based on aerial photographs or satellite images.\", 'code': \"import torch\nmodel = torch.hub.load('pytorch/vision', 'alexnet', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='alexnet', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "AlexNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='alexnet', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": ">=1.9.0", "torchvision": ">=0.10.0"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"top-1_error": 43.45, "top-5_error": 20.91}}, "description": "AlexNet is a deep convolutional neural network that achieved a top-5 error of 15.3% in the 2012 ImageNet Large Scale Visual Recognition Challenge. The main contribution of the original paper was the depth of the model, which was computationally expensive but made feasible through the use of GPUs during training. The pretrained AlexNet model in PyTorch can be used for image classification tasks."}}
{"code": "{'Instruction': A photographer wants to classify the objects in his images. Provide an API that can be used for image classification., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resnet50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNet50 model from PyTorch Hub for image classification tasks, which can classify objects in images with high accuracy.', 'code': 'import torch\nmodel = torch.hub.load(\"NVIDIA/DeepLearningExamples:torchhub\", \"nvidia_resnet50\", pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_resnet50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNet50", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_resnet50', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["pip install validators matplotlib"], "example_code": ["import torch", "from PIL import Image", "import torchvision.transforms as transforms", "import numpy as np", "import json", "import requests", "import matplotlib.pyplot as plt", "import warnings", "warnings.filterwarnings('ignore')", "%matplotlib inline", "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')", "print(f'Using {device} for inference')", "resnet50 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resnet50', pretrained=True)", "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_convnets_processing_utils')", "resnet50.eval().to(device)", "uris = [...]", "batch = torch.cat([utils.prepare_input_from_uri(uri) for uri in uris]).to(device)", "with torch.no_grad():", " output = torch.nn.functional.softmax(resnet50(batch), dim=1)", "results = utils.pick_n_best(predictions=output, n=5)", "for uri, result in zip(uris, results):", " img = Image.open(requests.get(uri, stream=True).raw)", " img.thumbnail((256,256), Image.ANTIALIAS)", " plt.imshow(img)", " plt.show()", " print(result)"], "performance": {"dataset": "ImageNet", "accuracy": "~0.5% top1 improvement over ResNet50 v1"}, "description": "The ResNet50 v1.5 model is a modified version of the original ResNet50 v1 model. The difference between v1 and v1.5 is that, in the bottleneck blocks which requires downsampling, v1 has stride = 2 in the first 1x1 convolution, whereas v1.5 has stride = 2 in the 3x3 convolution. This difference makes ResNet50 v1.5 slightly more accurate (~0.5% top1) than v1, but comes with a small performance drawback (~5% imgs/sec). The model is initialized as described in Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification. This model is trained with mixed precision using Tensor Cores on Volta, Turing, and the NVIDIA Ampere GPU architectures."}}
{"code": "{'Instruction': Help the e-commerce company in developing an API for identifying and classifying images of products., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_se_resnext101_32x4d')\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained SE-ResNeXt101-32x4d model from the NVIDIA Deep Learning Examples repository, and utilize it for image classification of e-commerce products.', 'code': \"import torch\nmodel = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_se_resnext101_32x4d')\"}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_se_resnext101_32x4d', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SE-ResNeXt101", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_se_resnext101_32x4d', pretrained=True)", "api_arguments": "N/A", "python_environment_requirements": "validators, matplotlib", "example_code": "import torch\nfrom PIL import Image\nimport torchvision.transforms as transforms\nimport numpy as np\nimport json\nimport requests\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nprint(f'Using {device} for inference')\nresneXt = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_se_resnext101_32x4d')\nutils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_convnets_processing_utils')\nresneXt.eval().to(device)\nuris = ['http://images.cocodataset.org/test-stuff2017/000000024309.jpg','http://images.cocodataset.org/test-stuff2017/000000028117.jpg','http://images.cocodataset.org/test-stuff2017/000000006149.jpg','http://images.cocodataset.org/test-stuff2017/000000004954.jpg']\nbatch = torch.cat([utils.prepare_input_from_uri(uri) for uri in uris]).to(device)\nwith torch.no_grad():\n output = torch.nn.functional.softmax(resneXt(batch), dim=1)\nresults = utils.pick_n_best(predictions=output, n=5)\nfor uri, result in zip(uris, results):\n img = Image.open(requests.get(uri, stream=True).raw)\n img.thumbnail((256,256), Image.ANTIALIAS)\n plt.imshow(img)\n plt.show()\n print(result)", "performance": {"dataset": "IMAGENET", "accuracy": "N/A"}, "description": "The SE-ResNeXt101-32x4d is a ResNeXt101-32x4d model with added Squeeze-and-Excitation module. This model is trained with mixed precision using Tensor Cores on Volta, Turing, and the NVIDIA Ampere GPU architectures, which allows researchers to get results 3x faster than training without Tensor Cores while experiencing the benefits of mixed precision training."}}
{"code": "{'Instruction': I want a model that can classify text into positive, negative or neutral sentiment., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/fairseq', 'roberta.large', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained RoBERTa model from PyTorch Hub, then fine-tune it for sentiment classification, including positive, negative, and neutral categories.\", 'code': 'import torch\nmodel = torch.hub.load('pytorch/fairseq', 'roberta.large', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/fairseq', model='roberta.large', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "RoBERTa", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='pytorch/fairseq', model='roberta.large', pretrained=True)", "api_arguments": ["'pytorch/fairseq'", "'roberta.large'"], "python_environment_requirements": ["regex", "requests", "hydra-core", "omegaconf"], "example_code": ["import torch", "roberta = torch.hub.load('pytorch/fairseq', 'roberta.large')", "roberta.eval()", "tokens = roberta.encode('Hello world!')", "last_layer_features = roberta.extract_features(tokens)", "all_layers = roberta.extract_features(tokens, return_all_hiddens=True)", "roberta = torch.hub.load('pytorch/fairseq', 'roberta.large.mnli')", "roberta.eval()", "tokens = roberta.encode('Roberta is a heavily optimized version of BERT.', 'Roberta is not very optimized.')", "prediction = roberta.predict('mnli', tokens).argmax().item()", "tokens = roberta.encode('Roberta is a heavily optimized version of BERT.', 'Roberta is based on BERT.')", "prediction = roberta.predict('mnli', tokens).argmax().item()", "roberta.register_classification_head('new_task', num_classes=3)", "logprobs = roberta.predict('new_task', tokens)"], "performance": {"dataset": "MNLI", "accuracy": "N/A"}, "description": "RoBERTa is a robustly optimized version of BERT, a revolutionary self-supervised pretraining technique that learns to predict intentionally hidden (masked) sections of text. RoBERTa builds on BERT's language masking strategy and modifies key hyperparameters, including removing BERT's next-sentence pretraining objective, and training with much larger mini-batches and learning rates. RoBERTa was also trained on an order of magnitude more data than BERT, for a longer amount of time, allowing it to generalize even better to downstream tasks."}}
{"code": "{'Instruction': \"A Language teacher is teaching his class about pronunciation of English. They are looking for a TTS API that can help the students in the study process. Suggest an API for this purpose.\", 'Output': {'domain': 'Text-to-Speech', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_waveglow', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Use the pretrained WaveGlow model for text-to-speech, combined with Tacotron 2, to provide natural-sounding speech synthesis, helping students learn pronunciation.', 'code': 'import torch\nwaveglow = torch.hub.load(\"NVIDIA/DeepLearningExamples:torchhub\", \"nvidia_waveglow\", pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_waveglow', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Text-to-Speech", "framework": "PyTorch", "functionality": "Speech Synthesis", "api_name": "WaveGlow", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_waveglow', pretrained=True)", "api_arguments": {"repo_or_dir": "NVIDIA/DeepLearningExamples:torchhub", "model": "nvidia_waveglow", "model_math": "fp32"}, "python_environment_requirements": ["numpy", "scipy", "librosa", "unidecode", "inflect", "libsndfile1"], "example_code": {"load_waveglow_model": "waveglow = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_waveglow', model_math='fp32')", "prepare_waveglow_model": ["waveglow = waveglow.remove_weightnorm(waveglow)", "waveglow = waveglow.to('cuda')", "waveglow.eval()"], "load_tacotron2_model": "tacotron2 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tacotron2', model_math='fp32')", "prepare_tacotron2_model": ["tacotron2 = tacotron2.to('cuda')", "tacotron2.eval()"], "synthesize_speech": ["text = \"hello world, I missed you so much\"", "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tts_utils')", "sequences, lengths = utils.prepare_input_sequence([text])", "with torch.no_grad():", " mel, _, _ = tacotron2.infer(sequences, lengths)", " audio = waveglow.infer(mel)", "audio_numpy = audio[0].data.cpu().numpy()", "rate = 22050"], "save_audio": "write(\"audio.wav\", rate, audio_numpy)", "play_audio": "Audio(audio_numpy, rate=rate)"}, "performance": {"dataset": "LJ Speech", "accuracy": null}, "description": "The Tacotron 2 and WaveGlow model form a text-to-speech system that enables users to synthesize natural-sounding speech from raw transcripts without any additional prosody information. The Tacotron 2 model produces mel spectrograms from input text using encoder-decoder architecture. WaveGlow is a flow-based model that consumes the mel spectrograms to generate speech."}}
{"code": "{'Instruction': A marketing team needs to classify images for their advertisement targeting. Can you offer an API that can predict the class of an image?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resneXt', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Use a pre-trained ResNeXt101-32x4d model from NVIDIA for image classification. The model is trained on the ImageNet dataset and can predict object classes in images.', 'code': \"import torch\nmodel = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resneXt', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_resneXt', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNeXt101-32x4d", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_resneXt', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "validators", "matplotlib"], "example_code": ["import torch", "from PIL import Image", "import torchvision.transforms as transforms", "import numpy as np", "import json", "import requests", "import matplotlib.pyplot as plt", "import warnings", "warnings.filterwarnings('ignore')", "%matplotlib inline", "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")", "print(f'Using {device} for inference')", "resneXt = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resneXt')", "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_convnets_processing_utils')", "resneXt.eval().to(device)", "uris = [", " 'http://images.cocodataset.org/test-stuff2017/000000024309.jpg',", " 'http://images.cocodataset.org/test-stuff2017/000000028117.jpg',", " 'http://images.cocodataset.org/test-stuff2017/000000006149.jpg',", " 'http://images.cocodataset.org/test-stuff2017/000000004954.jpg',", "]", "batch = torch.cat(", " [utils.prepare_input_from_uri(uri) for uri in uris]", ").to(device)", "with torch.no_grad():", " output = torch.nn.functional.softmax(resneXt(batch), dim=1)", "results = utils.pick_n_best(predictions=output, n=5)", "for uri, result in zip(uris, results):", " img = Image.open(requests.get(uri, stream=True).raw)", " img.thumbnail((256,256), Image.ANTIALIAS)", " plt.imshow(img)", " plt.show()", " print(result)"], "performance": {"dataset": "IMAGENET", "accuracy": "Not provided"}, "description": "ResNeXt101-32x4d is a model introduced in the Aggregated Residual Transformations for Deep Neural Networks paper. It is based on regular ResNet model, substituting 3x3 convolutions inside the bottleneck block for 3x3 grouped convolutions. This model is trained with mixed precision using Tensor Cores on Volta, Turing, and the NVIDIA Ampere GPU architectures. It can be deployed for inference on the NVIDIA Triton Inference Server using TorchScript, ONNX Runtime or TensorRT as an execution backend."}}
{"code": "{'Instruction': I require a deep learning model to perform image classification on a dataset with multiple classes. Recommend an API that serves this purpose., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'densenet201', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained DenseNet-201 model from PyTorch Hub to perform image classification on a dataset with multiple classes.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'densenet201', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet201', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Dense Convolutional Network", "api_name": "Densenet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet201', pretrained=True)", "api_arguments": [{"name": "densenet201", "type": "str", "description": "Densenet-201 model"}], "python_environment_requirements": {"torch": "latest", "torchvision": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet201', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"densenet201": {"Top-1 error": 22.8, "Top-5 error": 6.43}}}, "description": "Dense Convolutional Network (DenseNet) connects each layer to every other layer in a feed-forward fashion. It alleviates the vanishing-gradient problem, strengthens feature propagation, encourages feature reuse, and substantially reduces the number of parameters."}}
{"code": "{'Instruction': Inform me about an API that can aid in image classification tasks and has a dense network architecture., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'densenet161', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained DenseNet-161 model from PyTorch Hub, which has a dense network architecture and can be used for image classification tasks.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'densenet161', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet161', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Dense Convolutional Network", "api_name": "Densenet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet161', pretrained=True)", "api_arguments": [{"name": "densenet161", "type": "str", "description": "Densenet-161 model"}], "python_environment_requirements": {"torch": "latest", "torchvision": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet161', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"densenet161": {"Top-1 error": 22.35, "Top-5 error": 6.2}}}, "description": "Dense Convolutional Network (DenseNet) connects each layer to every other layer in a feed-forward fashion. It alleviates the vanishing-gradient problem, strengthens feature propagation, encourages feature reuse, and substantially reduces the number of parameters."}}
{"code": "{'Instruction': An urban planning team needs to segment a satellite image to identify different landforms. Can you suggest an ML API that can provide segmentation maps?, 'Output': {'domain': 'Semantic Segmentation', 'api_call': \"model = torch.hub.load('pytorch/vision', 'fcn_resnet101', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained FCN-ResNet101 model from PyTorch Hub, which can be fine-tuned for semantic segmentation of satellite images to identify different landforms.', 'code': \"import torch\nmodel = torch.hub.load('pytorch/vision', 'fcn_resnet101', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='fcn_resnet101', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "Fully-Convolutional Network", "api_name": "fcn_resnet101", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='fcn_resnet101', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "boolean", "description": "If True, returns a model pre-trained on COCO train2017"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "matplotlib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'fcn_resnet101', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "input_image = input_image.convert(\"RGB\")", "preprocess = transforms.Compose([", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)['out'][0]", "output_predictions = output.argmax(0)"], "performance": {"dataset": "COCO val2017", "accuracy": {"fcn_resnet101": {"Mean IOU": 63.7, "Global Pixelwise Accuracy": 91.9}}}, "description": "FCN-ResNet is a Fully-Convolutional Network model using a ResNet-50 or a ResNet-101 backbone. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset."}}
{"code": "{'Instruction': Propose an API to process a satellite image and label different objects like buildings, roads, and vegetation., 'Output': {'domain': 'Semantic Segmentation', 'api_call': \"model = torch.hub.load('pytorch/vision', 'fcn_resnet50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained FCN-ResNet50 model from PyTorch Hub, a Fully-Convolutional Network designed for semantic segmentation, to process satellite images and label different objects such as buildings, roads, and vegetation.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'fcn_resnet50', pretrained=True)\nmodel.eval()'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='fcn_resnet50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "Fully-Convolutional Network", "api_name": "fcn_resnet50", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='fcn_resnet50', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "boolean", "description": "If True, returns a model pre-trained on COCO train2017"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "matplotlib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'fcn_resnet50', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "input_image = input_image.convert(\"RGB\")", "preprocess = transforms.Compose([", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)['out'][0]", "output_predictions = output.argmax(0)"], "performance": {"dataset": "COCO val2017", "accuracy": {"fcn_resnet50": {"Mean IOU": 60.5, "Global Pixelwise Accuracy": 91.4}}}, "description": "FCN-ResNet is a Fully-Convolutional Network model using a ResNet-50 or a ResNet-101 backbone. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset."}}
{"code": "{'Instruction': Find an API that can optimize a model for low memory usage and fast processing to differentiate between cats and dogs in images., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet39ds', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained HarDNet model from PyTorch Hub, an efficient and low-memory model that can be fine-tuned for differentiating between cats and dogs in images.\", 'code': 'import torch\nmodel = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet39ds', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet39ds', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "HarDNet", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet39ds', pretrained=True)", "api_arguments": [{"name": "hardnet39ds", "type": "str", "description": "HarDNet-39DS model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet39ds', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"hardnet39ds": {"Top-1 error": 27.92, "Top-5 error": 9.57}}}, "description": "Harmonic DenseNet (HarDNet) is a low memory traffic CNN model, which is fast and efficient. The basic concept is to minimize both computational cost and memory access cost at the same time, such that the HarDNet models are 35% faster than ResNet running on GPU comparing to models with the same accuracy (except the two DS models that were designed for comparing with MobileNet)."}}
{"code": "{'Instruction': Suggest an API that classifies images quickly, without sacrificing too much accuracy, as compared to traditional methods like ResNet., 'Output': {'domain': 'Image Classification', 'api_call': 'model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet85', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained HarDNet-85 model from PyTorch Hub, which is faster and more efficient than traditional methods like ResNet, without sacrificing much accuracy.', 'code': 'import torch\nmodel = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet85', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet85', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "HarDNet", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet85', pretrained=True)", "api_arguments": [{"name": "hardnet85", "type": "str", "description": "HarDNet-85 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet85', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"hardnet85": {"Top-1 error": 21.96, "Top-5 error": 6.11}}}, "description": "Harmonic DenseNet (HarDNet) is a low memory traffic CNN model, which is fast and efficient. The basic concept is to minimize both computational cost and memory access cost at the same time, such that the HarDNet models are 35% faster than ResNet running on GPU comparing to models with the same accuracy (except the two DS models that were designed for comparing with MobileNet)."}}
{"code": "{'Instruction': A machine learning engineer is working on a project to classify images of different dog breeds. Provide an example of an API that could be used for image classification., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet68', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained HarDNet-68 model from PyTorch Hub, which is efficient for image classification and can be fine-tuned for classifying dog breeds.', 'code': 'import torch\nmodel = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet68', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet68', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "HarDNet", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet68', pretrained=True)", "api_arguments": [{"name": "hardnet68", "type": "str", "description": "HarDNet-68 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet68', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"hardnet68": {"Top-1 error": 23.52, "Top-5 error": 6.99}}}, "description": "Harmonic DenseNet (HarDNet) is a low memory trafficCNN model, which is fast and efficient. The basic concept is to minimize both computational cost and memory access cost at the same time, such that the HarDNet models are 35% faster than ResNet running on GPU comparing to models with the same accuracy (except the two DS models that were designed for comparing with MobileNet)."}}
{"code": "{'Instruction': Recommend me a computer vision model that can determine the top 5 labels for an image., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'googlenet', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained GoogLeNet model from PyTorch Hub, which can be used to classify images by returning the top 5 labels.', 'code': 'import torch\nmodel = torch.hub.load(\"pytorch/vision\", \"googlenet\", pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='googlenet', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "GoogLeNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='googlenet', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": ">=1.0.0", "torchvision": ">=0.2.2"}, "example_code": {"import": ["import torch", "import urllib", "from PIL import Image", "from torchvision import transforms"], "load_model": "model = torch.hub.load('pytorch/vision:v0.10.0', 'googlenet', pretrained=True)", "model_eval": "model.eval()", "image_preprocessing": ["input_image = Image.open(filename)", "preprocess = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)"], "model_execution": ["if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)"], "output_processing": ["probabilities = torch.nn.functional.softmax(output[0], dim=0)", "top5_prob, top5_catid = torch.topk(probabilities, 5)"]}, "performance": {"dataset": "ImageNet", "accuracy": {"Top-1 error": "30.22", "Top-5 error": "10.47"}}, "description": "GoogLeNet is based on a deep convolutional neural network architecture codenamed 'Inception', which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014)."}}
{"code": "{'Instruction': Present me an API that efficiently classifies images with less computational cost compared to traditional methods., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('huawei-noah/ghostnet', 'ghostnet_1x', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained GhostNet model from PyTorch Hub for image classification. GhostNet is an efficient network architecture that generates more features using cheaper operations, offering superior speed and accuracy tradeoff.\", 'code': 'import torch\nmodel = torch.hub.load(\\'huawei-noah/ghostnet\\', \\'ghostnet_1x\\', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/ghostnet', model='ghostnet_1x', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Efficient networks by generating more features from cheap operations", "api_name": "GhostNet", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/ghostnet', model='ghostnet_1x', pretrained=True)", "api_arguments": ["pretrained"], "python_environment_requirements": ["torch", "torchvision", "PIL"], "example_code": ["import torch", "model = torch.hub.load('huawei-noah/ghostnet', 'ghostnet_1x', pretrained=True)", "model.eval()", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)", "print(probabilities)"], "performance": {"dataset": "ImageNet", "accuracy": {"Top-1 acc": "73.98", "Top-5 acc": "91.46"}}, "description": "The GhostNet architecture is based on an Ghost module structure which generates more features from cheap operations. Based on a set of intrinsic feature maps, a series of cheap operations are applied to generate many ghost feature maps that could fully reveal information underlying intrinsic features. Experiments conducted on benchmarks demonstrate the superiority of GhostNet in terms of speed and accuracy tradeoff."}}
{"code": "{'Instruction': We need to detect objects in images for a mobile platform with a low memory footprint CNN architecture. Could you suggest an appropriate API for this purpose?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet68ds', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained HarDNet-68DS model from PyTorch Hub, which is designed for efficient object detection in images with low memory and computational requirements.', 'code': 'import torch\nmodel = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet68ds', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet68ds', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "HarDNet", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet68ds', pretrained=True)", "api_arguments": [{"name": "hardnet68ds", "type": "str", "description": "HarDNet-68DS model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet68ds', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"hardnet68ds": {"Top-1 error": 25.71, "Top-5 error": 8.13}}}, "description": "Harmonic DenseNet (HarDNet) is a low memory traffic CNN model, which is fast and efficient. The basic concept is to minimize both computational cost and memory access cost at the same time, such that the HarDNet models are 35% faster than ResNet running on GPU comparing to models with the same accuracy (except the two DS models that were designed for comparing with MobileNet)."}}
{"code": "{'Instruction': Recommend an API for a vehicle recognition system which can identify models and types of vehicles from various visual angles., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('XingangPan/IBN-Net', 'resnet101_ibn_a', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNet-101-IBN-a model from PyTorch Hub, which can be fine-tuned for vehicle recognition and classification.', 'code': 'import torch\nmodel = torch.hub.load('XingangPan/IBN-Net', 'resnet101_ibn_a', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnet101_ibn_a', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "IBN-Net", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnet101_ibn_a', pretrained=True)", "api_arguments": [{"name": "resnet101_ibn_a", "type": "str", "description": "ResNet-101-IBN-a model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('XingangPan/IBN-Net', 'resnet101_ibn_a', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"resnet101_ibn_a": {"Top-1 acc": 78.61, "Top-5 acc": 94.41}}}, "description": "IBN-Net is a CNN model with domain/appearance invariance. Motivated by style transfer works, IBN-Net carefully unifies instance normalization and batch normalization in a single deep network. It provides a simple way to increase both modeling and generalization capacities without adding model complexity. IBN-Net is especially suitable for cross domain or person/vehicle re-identification tasks."}}
{"code": "{'Instruction': A live streaming platform needs a machine learning model for content moderation. Tell me how to implement an API that can classify an image for content moderation., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('XingangPan/IBN-Net', 'resnet50_ibn_a', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNet-50-IBN-a model from PyTorch Hub, which can be fine-tuned for content moderation by classifying an image into appropriate categories.', 'code': 'import torch\nmodel = torch.hub.load('XingangPan/IBN-Net', 'resnet50_ibn_a', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnet50_ibn_a', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "IBN-Net", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnet50_ibn_a', pretrained=True)", "api_arguments": [{"name": "resnet50_ibn_a", "type": "str", "description": "ResNet-50-IBN-a model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('XingangPan/IBN-Net', 'resnet50_ibn_a', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"resnet50_ibn_a": {"Top-1 acc": 77.46, "Top-5 acc": 93.68}}}, "description": "IBN-Net is a CNN model with domain/appearance invariance. Motivated by style transfer works, IBN-Net carefully unifies instance normalization and batch normalization in a single deep network. It provides a simple way to increase both modeling and generalization capacities without adding model complexity. IBN-Net is especially suitable for cross domain or person/vehicle re-identification tasks."}}
{"code": "{'Instruction': We have a dataset concerning pedestrian activities at crosswalks. Recommend an API capable of predicting activities like walking, running, or waiting given an image as input., 'Output': {'domain': 'Classification', 'api_call': 'model = torch.hub.load(\"XingangPan/IBN-Net\", \"resnext101_ibn_a\", pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNeXt-101-IBN-a model from PyTorch Hub and fine-tune it for activity classification, such as walking, running, or waiting from images.', 'code': 'import torch\nmodel = torch.hub.load(\"XingangPan/IBN-Net\", \"resnext101_ibn_a\", pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnext101_ibn_a', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "IBN-Net", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnext101_ibn_a', pretrained=True)", "api_arguments": [{"name": "resnext101_ibn_a", "type": "str", "description": "ResNeXt-101-IBN-a model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('XingangPan/IBN-Net', 'resnext101_ibn_a', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"resnext101_ibn_a": {"Top-1 acc": 79.12, "Top-5 acc": 94.58}}}, "description": "IBN-Net is a CNN model with domain/appearance invariance. Motivated by style transfer works, IBN-Net carefully unifies instance normalization and batch normalization in a single deep network. It provides a simple way to increase both modeling and generalization capacities without adding model complexity. IBN-Net is especially suitable for cross domain or person/vehicle re-identification tasks."}}
{"code": "{'Instruction': AML, a cybersecurity company is building a face identification engine for airports. Recommend an API that can output True if the two photos provided are the same person or False if not., 'Output': {'domain': 'Face Verification', 'api_call': 'model = torch.hub.load('nyoki-mtl/arcface', 'iresnet18', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Arcface model from PyTorch Hub that can be used to compute the embeddings for given photos and compare the similarity between the embeddings to determine if the two photos are of the same person.', 'code': 'import torch\nmodel = torch.hub.load('nyoki-mtl/arcface', 'iresnet18', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='se_resnet101_ibn_a', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "IBN-Net", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='se_resnet101_ibn_a', pretrained=True)", "api_arguments": [{"name": "se_resnet101_ibn_a", "type": "str", "description": "SE-ResNet-101-IBN-a model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('XingangPan/IBN-Net', 'se_resnet101_ibn_a', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"se_resnet101_ibn_a": {"Top-1 acc": 78.75, "Top-5 acc": 94.49}}}, "description": "IBN-Net is a CNN model with domain/appearance invariance. Motivated by style transfer works, IBN-Net carefully unifies instance normalization and batch normalization in a single deep network. It provides a simple way to increase both modeling and generalization capacities without adding model complexity. IBN-Net is especially suitable for cross domain or person/vehicle re-identification tasks."}}
{"code": "{'Instruction': We want to design an app to identify plants. Recommend an API that can be used to classify images of plants., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'inception_v3', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained Inception v3 model from PyTorch Hub and fine-tune it for plant image classification.\", 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'inception_v3', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='inception_v3', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "Inception_v3", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='inception_v3', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": "1.9.0", "torchvision": "0.10.0"}, "example_code": {"import_libraries": "import torch", "load_model": "model = torch.hub.load('pytorch/vision:v0.10.0', 'inception_v3', pretrained=True)", "model_evaluation": "model.eval()"}, "performance": {"dataset": "imagenet", "accuracy": {"top-1_error": 22.55, "top-5_error": 6.44}}, "description": "Inception v3, also called GoogleNetv3, is a famous Convolutional Neural Network trained on the ImageNet dataset from 2015. It is based on the exploration of ways to scale up networks to utilize the added computation as efficiently as possible by using suitably factorized convolutions and aggressive regularization. The model achieves a top-1 error of 22.55% and a top-5 error of 6.44% on the ImageNet dataset."}}
{"code": "{'Instruction': I am working at Pinterest and we want to build a classifier for the images in the platform. Give me an API that can classify images., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNeSt model for image classification from PyTorch Hub to classify various types of images for the Pinterest platform.', 'code': 'import torch\nmodel = torch.hub.load('zhanghang1989/ResNeSt', 'resnest50', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNeSt", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest50', pretrained=True)", "api_arguments": "resnest50", "python_environment_requirements": {"torch": "1.0.0", "torchvision": "0.2.2"}, "example_code": ["import torch", "model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest50', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeSt-50": 81.03}}, "description": "ResNeSt models are from the ResNeSt: Split-Attention Networks paper. They are a new ResNet variant that enables attention across feature-map groups. By stacking Split-Attention blocks ResNet-style, ResNeSt models outperform other networks with similar model complexities, and also help downstream tasks including object detection, instance segmentation, and semantic segmentation."}}
{"code": "{'Instruction': I want to create an app to recognize plants based on images. Suggest me an API that I can use for this purpose., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest200', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNeSt-200 model from PyTorch Hub, which can be fine-tuned for plant recognition from images.', 'code': 'import torch\nmodel = torch.hub.load('zhanghang1989/ResNeSt', 'resnest200', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest200', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNeSt", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest200', pretrained=True)", "api_arguments": "resnest200", "python_environment_requirements": {"torch": "1.0.0", "torchvision": "0.2.2"}, "example_code": ["import torch", "model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest200', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeSt-200": 83.84}}, "description": "ResNeSt models are from the ResNeSt: Split-Attention Networks paper. They are a new ResNet variant that enables attention across feature-map groups. By stacking Split-Attention blocks ResNet-style, ResNeSt models outperform other networks with similar model complexities, and also help downstream tasks including object detection, instance segmentation, and semantic segmentation."}}
{"code": "{'Instruction': A museum located in New York City wants to categorize their art collection by automatically identifying the content of the artwork. Which API can be used to perform image classification?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest101', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNeSt model for image classification from PyTorch Hub to automatically identify and categorize the content of artworks.', 'code': 'import torch\nmodel = torch.hub.load('zhanghang1989/ResNeSt', 'resnest101', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest101', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNeSt", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest101', pretrained=True)", "api_arguments": "resnest101", "python_environment_requirements": {"torch": "1.0.0", "torchvision": "0.2.2"}, "example_code": ["import torch", "model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest101', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeSt-101": 82.83}}, "description": "ResNeSt models are from the ResNeSt: Split-Attention Networks paper. They are a new ResNet variant that enables attention across feature-map groups. By stacking Split-Attention blocks ResNet-style, ResNeSt models outperform other networks with similar model complexities, and also help downstream tasks including object detection, instance segmentation, and semantic segmentation."}}
{"code": "{'Instruction': Let me know an API that can classify food from photos. It should be designed to run on low-power devices and offer good accuracy., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_cpu', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ProxylessNAS model from PyTorch Hub, which is optimized for low-power devices (CPU) and can be fine-tuned for food classification.', 'code': 'import torch\nmodel = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_cpu', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_cpu', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "ProxylessNAS", "api_name": "mit-han-lab/ProxylessNAS", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_cpu', pretrained=True)", "api_arguments": [{"name": "proxylessnas_cpu", "type": "str", "description": "ProxylessNAS optimized for CPU"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "target_platform = 'proxyless_cpu'", "model = torch.hub.load('mit-han-lab/ProxylessNAS', target_platform, pretrained=True)", "model.eval()"], "performance": {"dataset": [{"model_structure": "proxylessnas_cpu", "accuracy": 75.3}]}, "description": "ProxylessNAS models are from the ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware paper. They specialize CNN architectures for different hardware platforms, offering free yet significant performance boost on all three platforms (CPU, GPU, and Mobile) with similar accuracy."}}
{"code": "{'Instruction': I'm doing an image classification project, suggest an API that can help me classify images effectively., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest269', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNeSt-269 model from PyTorch Hub for effective image classification. ResNeSt models are ResNet variants that utilize Split-Attention Networks, providing improved performance in classification tasks.', 'code': 'import torch\nmodel = torch.hub.load('zhanghang1989/ResNeSt', 'resnest269', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest269', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNeSt", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest269', pretrained=True)", "api_arguments": "resnest269", "python_environment_requirements": {"torch": "1.0.0", "torchvision": "0.2.2"}, "example_code": ["import torch", "model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest269', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeSt-269": 84.54}}, "description": "ResNeSt models are from the ResNeSt: Split-Attention Networks paper. They are a new ResNet variant that enables attention across feature-map groups. By stacking Split-Attention blocks ResNet-style, ResNeSt models outperform other networks with similar model complexities, and also help downstream tasks including object detection, instance segmentation, and semantic segmentation."}}
{"code": "{'Instruction': Identify an API suitable for classifying images using a neural network optimized for high performance on GPUs., 'Output': {'domain': 'Classification', 'api_call': 'model = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_gpu', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the ProxylessNAS model optimized for GPUs from PyTorch Hub with pre-trained weights for image classification tasks.', 'code': 'import torch\nmodel = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_gpu', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_gpu', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "ProxylessNAS", "api_name": "mit-han-lab/ProxylessNAS", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_gpu', pretrained=True)", "api_arguments": [{"name": "proxylessnas_gpu", "type": "str", "description": "ProxylessNAS optimized for GPU"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "target_platform = 'proxyless_gpu'", "model = torch.hub.load('mit-han-lab/ProxylessNAS', target_platform, pretrained=True)", "model.eval()"], "performance": {"dataset": [{"model_structure": "proxylessnas_gpu", "accuracy": 75.1}]}, "description": "ProxylessNAS models are from the ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware paper. They specialize CNN architectures for different hardware platforms, offering free yet significant performance boost on all three platforms (CPU, GPU, and Mobile) with similar accuracy."}}
{"code": "{'Instruction': The Clubhouse App wants a function for image classification. Advise them an API that could be a potential solution for them., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'mobilenet_v2', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MobileNet v2 model for image classification from PyTorch Hub to classify images into various classes, suitable for the Clubhouse App.', 'code': 'import torch\nmodel = torch.hub.load(\"pytorch/vision\", \"mobilenet_v2\", pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='mobilenet_v2', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MobileNet v2", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='mobilenet_v2', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision', 'mobilenet_v2', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)"], "performance": {"dataset": "ImageNet", "accuracy": {"top-1_error": 28.12, "top-5_error": 9.71}}, "description": "The MobileNet v2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input. MobileNet v2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, non-linearities in the narrow layers were removed in order to maintain representational power."}}
{"code": "{'Instruction': I am from Google and I am looking for an API that can classify images into categories using neural networks optimized for mobiles., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_mobile', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the ProxylessNAS Mobile model from PyTorch Hub, which is optimized for mobile platforms and can perform image classification tasks.', 'code': 'import torch\nmodel = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_mobile', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_mobile', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "ProxylessNAS", "api_name": "mit-han-lab/ProxylessNAS", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_mobile', pretrained=True)", "api_arguments": [{"name": "proxylessnas_mobile", "type": "str", "description": "ProxylessNAS optimized for Mobile"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "target_platform = 'proxyless_mobile'", "model = torch.hub.load('mit-han-lab/ProxylessNAS', target_platform, pretrained=True)", "model.eval()"], "performance": {"dataset": [{"model_structure": "proxylessnas_mobile", "accuracy": 74.6}]}, "description": "ProxylessNAS models are from the ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware paper. They specialize CNN architectures for different hardware platforms, offering free yet significant performance boost on all three platforms (CPU, GPU, and Mobile) with similar accuracy."}}
{"code": "{'Instruction': Recommend an API to determine the type of objects in an image given a set of pre-trained labels., 'Output': {'domain': 'Image Classification', 'api_call': 'model = torch.hub.load('pytorch/vision', 'resnet34', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNet34 model from PyTorch Hub, which can be used for image classification tasks to recognize various objects in an image.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'resnet34', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet34', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Deep Residual Networks", "api_name": "ResNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet34', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "bool", "default": "False", "description": "If True, returns a model pre-trained on ImageNet"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet34', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "resnet34", "top-1_error": 26.7, "top-5_error": 8.58}}, "description": "ResNet models are deep residual networks pre-trained on ImageNet. They were proposed in the paper 'Deep Residual Learning for Image Recognition'. Available model variants include ResNet18, ResNet34, ResNet50, ResNet101, and ResNet152."}}
{"code": "{'Instruction': Recommend to me an API that can efficiently recognize faces from an image and classify them into particular groups or categories., 'Output': {'domain': 'Classification', 'api_call': 'model = torch.hub.load('pytorch/vision', 'resnet18', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNet18 model from PyTorch Hub, which can be fine-tuned for face recognition and classification into specific groups or categories.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'resnet18', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet18', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Deep Residual Networks", "api_name": "ResNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet18', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "bool", "default": "False", "description": "If True, returns a model pre-trained on ImageNet"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "resnet18", "top-1_error": 30.24, "top-5_error": 10.92}}, "description": "ResNet models are deep residual networks pre-trained on ImageNet. They were proposed in the paper 'Deep Residual Learning for Image Recognition'. Available model variants include ResNet18, ResNet34, ResNet50, ResNet101, and ResNet152."}}
{"code": "{'Instruction': Recommend me a model that has a high accuracy in recognizing the emotion of people displayed in images., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'resnet50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNet-50 model from PyTorch Hub, which can be fine-tuned for emotion recognition in facial images with high accuracy.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'resnet50', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Deep Residual Networks", "api_name": "ResNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet50', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "bool", "default": "False", "description": "If True, returns a model pre-trained on ImageNet"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "resnet50", "top-1_error": 23.85, "top-5_error": 7.13}}, "description": "ResNet models are deep residual networks pre-trained on ImageNet. They were proposed in the paper 'Deep Residual Learning for Image Recognition'. Available model variants include ResNet18, ResNet34, ResNet50, ResNet101, and ResNet152."}}
{"code": "{'Instruction': Looking for an API that can identify different breeds of dogs in images. It must provide high accuracy in identifying the breed., 'Output': {'domain': 'Classification', 'api_call': 'model = torch.hub.load(\\\"pytorch/vision\\\", \\\"resnet101\\\", pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the ResNet101 model from PyTorch Hub. The model is pretrained on ImageNet and can be fine-tuned to classify dog breeds with high accuracy.', 'code': 'import torch\nmodel = torch.hub.load(\"pytorch/vision\", \"resnet101\", pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet101', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Deep Residual Networks", "api_name": "ResNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet101', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "bool", "default": "False", "description": "If True, returns a model pre-trained on ImageNet"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet101', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "resnet101", "top-1_error": 22.63, "top-5_error": 6.44}}, "description": "ResNet models are deep residual networks pre-trained on ImageNet. They were proposed in the paper 'Deep Residual Learning for Image Recognition'. Available model variants include ResNet18, ResNet34, ResNet50, ResNet101, and ResNet152."}}
{"code": "{'Instruction': I noticed that some of the bird species are mistakenly tagged in the bird photo collection. Provide me an API to auto fix the tags., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'resnet152', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNet152 model from PyTorch Hub, which can be fine-tuned for bird species identification and classification to automatically fix tags.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'resnet152', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet152', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Deep Residual Networks", "api_name": "ResNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet152', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "bool", "default": "False", "description": "If True, returns a model pre-trained on ImageNet"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet152', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "resnet152", "top-1_error": 21.69, "top-5_error": 5.94}}, "description": "ResNet models are deep residual networks pre-trained on ImageNet. They were proposed in the paper 'Deep Residual Learning for Image Recognition'. Available model variants include ResNet18, ResNet34, ResNet50, ResNet101, and ResNet152."}}
{"code": "{'Instruction': How can I build an AI system that can recognize dogs and cats? Provide me an API and pretrained model that will help., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'resnext50_32x4d', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load a pre-trained ResNext model from the PyTorch Hub for image classification. It can be fine-tuned to distinguish between dogs and cats.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'resnext50_32x4d', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnext50_32x4d', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNext", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnext50_32x4d', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "PIL"], "example_code": ["import torch", "from PIL import Image", "from torchvision import transforms", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnext50_32x4d', pretrained=True)", "model.eval()", "input_image = Image.open('dog.jpg')", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)"], "performance": {"dataset": "ImageNet", "accuracy": {"resnext50_32x4d": {"top-1": 22.38, "top-5": 6.3}}}, "description": "ResNext is a next-generation ResNet architecture for image classification. It is more efficient and accurate than the original ResNet. This implementation includes two versions of the model, resnext50_32x4d and resnext101_32x8d, with 50 and 101 layers respectively."}}
{"code": "{'Instruction': Analyze a given image for its content and classify it into its respective category. Suggest an API that can help with this., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'resnext101_32x4d', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNext model from PyTorch Hub for image classification. The model will process a given image and classify it into the respective category.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'resnext101_32x4d', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnext101_32x4d', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNext", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnext101_32x4d', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "PIL"], "example_code": ["import torch", "from PIL import Image", "from torchvision import transforms", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnext101_32x4d', pretrained=True)", "model.eval()", "input_image = Image.open('dog.jpg')", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)"], "performance": {"dataset": "ImageNet", "resnext101_32x8d": {"top-1": 20.69, "top-5": 5.47}}, "description": "ResNext is a next-generation ResNet architecture for image classification. It is more efficient and accurate than the original ResNet. This implementation includes two versions of the model, resnext50_32x4d and resnext101_32x8d, with 50 and 101 layers respectively."}}
{"code": "{'Instruction': Find me an API that can classify images into different categories., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_b', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the SNNMLP model from PyTorch Hub for image classification tasks, it achieves high accuracy with low computational cost.', 'code': \"import torch\nmodel = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_b', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_b', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SNNMLP", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_b', pretrained=True)", "api_arguments": [{"name": "snnmlp_b", "type": "str", "description": "SNNMLP Base model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_b', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "SNNMLP Base", "top-1": 85.59}}, "description": "SNNMLP incorporates the mechanism of LIF neurons into the MLP models, to achieve better accuracy without extra FLOPs. We propose a full-precision LIF operation to communicate between patches, including horizontal LIF and vertical LIF in different directions. We also propose to use group LIF to extract better local features. With LIF modules, our SNNMLP model achieves 81.9%, 83.3% and 83.6% top-1 accuracy on ImageNet dataset with only 4.4G, 8.5G and 15.2G FLOPs, respectively."}}
{"code": "{'Instruction': I need to classify a set of random images. What pre-trained machine learning API would you recommend for this purpose?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_t', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the SNNMLP Tiny model from PyTorch Hub, which is optimized for image classification with high accuracy and low FLOPs, making it suitable for classifying random images.\", 'code': \"import torch\nmodel = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_t', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_t', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SNNMLP", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_t', pretrained=True)", "api_arguments": [{"name": "snnmlp_t", "type": "str", "description": "SNNMLP Tiny model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_t', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "SNNMLP Tiny", "top-1": 81.88}}, "description": "SNNMLP incorporates the mechanism of LIF neurons into the MLP models, to achieve better accuracy without extra FLOPs. We propose a full-precision LIF operation to communicate between patches, including horizontal LIF and vertical LIF in different directions. We also propose to use group LIF to extract better local features. With LIF modules, our SNNMLP model achieves 81.9%, 83.3% and 83.6% top-1 accuracy on ImageNet dataset with only 4.4G, 8.5G and 15.2G FLOPs, respectively."}}
{"code": "{'Instruction': A company is looking to build an app that automatically scans items in the grocery store and categorizes them in real time. What API should they use?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'squeezenet1_0', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Use the pretrained SqueezeNet model from PyTorch Hub for efficient image classification, as it achieves high accuracy with fewer parameters, making it ideal for real-time classification in a grocery store app.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'squeezenet1_0', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='squeezenet1_0', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SqueezeNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='squeezenet1_0', pretrained=True)", "api_arguments": {"version": "v0.10.0", "model": ["squeezenet1_0"], "pretrained": "True"}, "python_environment_requirements": {"torch": ">=1.9.0", "torchvision": ">=0.10.0"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'squeezenet1_0', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)", "print(probabilities)"], "performance": {"dataset": "ImageNet", "accuracy": {"squeezenet1_0": {"Top-1 error": 41.9, "Top-5 error": 19.58}}}, "description": "SqueezeNet is an image classification model that achieves AlexNet-level accuracy with 50x fewer parameters. It has two versions: squeezenet1_0 and squeezenet1_1, with squeezenet1_1 having 2.4x less computation and slightly fewer parameters than squeezenet1_0, without sacrificing accuracy."}}
{"code": "{'Instruction': Our company, New York Pillow, is developing an inventory system for our products, and we need a machine learning model to classify images of pillows by their type. Can you give me an API which can classify pillow types?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_s', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the SNNMLP Small model from PyTorch Hub, which can be fine-tuned for pillow classification.', 'code': 'import torch\nmodel = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_s', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_s', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SNNMLP", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_s', pretrained=True)", "api_arguments": [{"name": "snnmlp_s", "type": "str", "description": "SNNMLP Small model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_s', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "SNNMLP Small", "top-1": 83.3}}, "description": "SNNMLP incorporates the mechanism of LIF neurons into the MLP models, to achieve better accuracy without extra FLOPs. We propose a full-precision LIF operation to communicate between patches, including horizontal LIF and vertical LIF in different directions. We also propose to use group LIF to extract better local features. With LIF modules, our SNNMLP model achieves 81.9%, 83.3% and 83.6% top-1 accuracy on ImageNet dataset with only 4.4G, 8.5G and 15.2G FLOPs, respectively."}}
{"code": "{'Instruction': I need an API that can identify dog breeds in a given photo., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg11', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained VGG11 model from the PyTorch Hub, which can be fine-tuned for dog breed identification tasks.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg11', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg11', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg11', pretrained=True)", "api_arguments": [{"name": "vgg11", "type": "str", "description": "VGG11 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg11', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg11": {"Top-1 error": 30.98, "Top-5 error": 11.37}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': Find an API that can classify a given image, while maintaining a high efficiency for mobile phone usage., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'shufflenet_v2_x1_0', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ShuffleNet v2 model from PyTorch Hub, which is designed for efficient image classification on mobile devices.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'shufflenet_v2_x1_0', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='shufflenet_v2_x1_0', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ShuffleNet v2", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='shufflenet_v2_x1_0', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": "torch", "torchvision": "torchvision", "PIL": "Image", "urllib": "urllib"}, "example_code": {"import_libraries": ["import torch", "from PIL import Image", "from torchvision import transforms", "import urllib"], "load_model": ["model = torch.hub.load('pytorch/vision:v0.10.0', 'shufflenet_v2_x1_0', pretrained=True)", "model.eval()"], "load_image": ["url, filename = ('https://github.com/pytorch/hub/raw/master/images/dog.jpg', 'dog.jpg')", "try: urllib.URLopener().retrieve(url, filename)", "except: urllib.request.urlretrieve(url, filename)", "input_image = Image.open(filename)"], "preprocess_image": ["preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)"], "run_inference": ["if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)"], "get_probabilities": ["probabilities = torch.nn.functional.softmax(output[0], dim=0)"], "top_categories": ["top5_prob, top5_catid = torch.topk(probabilities, 5)", "for i in range(top5_prob.size(0)):", " print(categories[top5_catid[i]], top5_prob[i].item())"]}, "performance": {"dataset": "Imagenet", "accuracy": {"top-1_error": 30.64, "top-5_error": 11.68}}, "description": "ShuffleNet V2 is an efficient ConvNet optimized for speed and memory, pre-trained on Imagenet. It is designed based on practical guidelines for efficient network design, including speed and accuracy tradeoff."}}
{"code": "{'Instruction': I'm working on building an app that would let users know what's in a photograph. Suggest an efficient image recognition API appropriate for my purpose., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg11_bn', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained VGG11 model with batch normalization (vgg11_bn) from PyTorch Hub for efficient image recognition tasks.', 'code': \"import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg11_bn', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg11_bn', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg11_bn', pretrained=True)", "api_arguments": [{"name": "vgg11_bn", "type": "str", "description": "VGG11 model with batch normalization"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg11_bn', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg11_bn": {"Top-1 error": 26.7, "Top-5 error": 8.58}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': Propose an API that can identify the type of car in a photo taken at a car dealership., 'Output': {'domain': 'Image Recognition', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg16', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained VGG16 model from PyTorch Hub, which can be fine-tuned to identify the type of car in a photo taken at a car dealership.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg16', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg16', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg16', pretrained=True)", "api_arguments": [{"name": "vgg16", "type": "str", "description": "VGG16 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg16": {"Top-1 error": 28.41, "Top-5 error": 9.62}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': I am running a website on animal rescue and I have lots of animal photos during adoption process. Recommend me an API that can recognize animal breeds from images, so I can improve my website's user experience., 'Output': {'domain': 'Classification', 'api_call': 'model = torch.hub.load('pytorch/vision', 'vgg13', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained VGG-13 model from PyTorch Hub for image recognition tasks, and fine-tune it for recognizing animal breeds from images.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg13', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg13', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg13', pretrained=True)", "api_arguments": [{"name": "vgg13", "type": "str", "description": "VGG13 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg13', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg13": {"Top-1 error": 30.07, "Top-5 error": 10.75}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn,vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': What pre-trained API can I use to recognize different objects from an image?, 'Output': {'domain': 'Image Recognition', 'api_call': 'model = torch.hub.load('pytorch/vision', 'vgg16_bn', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Use the pre-trained VGG16 model with batch normalization from PyTorch Vision to recognize various objects in an image.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg16_bn', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg16_bn', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg16_bn', pretrained=True)", "api_arguments": [{"name": "vgg16_bn", "type": "str", "description": "VGG16 model with batch normalization"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16_bn', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg16_bn": {"Top-1 error": 26.63, "Top-5 error": 8.5}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': A company is building an AI-powered image categorization tool to help label images in their database with greater accuracy. Recommend an API to identify and classify images with high accuracy., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg13_bn', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained VGG13 with batch normalization (vgg13_bn) model from PyTorch Hub, which can accurately identify and classify images for the image categorization tool.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg13_bn', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg13_bn', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg13_bn', pretrained=True)", "api_arguments": [{"name": "vgg13_bn", "type": "str", "description": "VGG13 model with batch normalization"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg13_bn', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg13_bn": {"Top-1 error": 28.45, "Top-5 error": 9.63}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': I run a pet adoption center and want to identify dog breeds for our customers. How can I use a pre-trained model to recognize dog breeds from images?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg19_bn', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pre-trained VGG19-bn model from PyTorch Hub, and fine-tune it to recognize dog breeds from images.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg19_bn', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg19_bn', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg19_bn', pretrained=True)", "api_arguments": [{"name": "vgg19_bn", "type": "str", "description": "VGG19 model with batch normalization"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg19_bn', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg19_bn": {"Top-1 error": 25.76, "Top-5 error": 8.15}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': A client of mine wants to classify photographs into relevant categories for their website. Can you recommend a suitable API to do the job?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'wide_resnet50_2', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Wide Residual Network 50 (Wide ResNet-50) from PyTorch Hub for robust image classification and organization.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'wide_resnet50_2', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='wide_resnet50_2', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Wide Residual Networks", "api_name": "wide_resnet50_2", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='wide_resnet50_2', pretrained=True)", "api_arguments": "pretrained", "python_environment_requirements": "torch, torchvision", "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'wide_resnet50_2', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"wide_resnet50_2": {"Top-1 error": 21.49, "Top-5 error": 5.91}}}, "description": "Wide Residual networks simply have increased number of channels compared to ResNet. Otherwise the architecture is the same. Deeper ImageNet models with bottleneck block have increased number of channels in the inner 3x3 convolution. The wide_resnet50_2 and wide_resnet101_2 models were trained in FP16 with mixed precision training using SGD with warm restarts. Checkpoints have weights in half precision (except batch norm) for smaller size, and can be used in FP32 models too."}}
{"code": "{'Instruction': A fitness app is trying to categorize workout videos based on the activity. Can you propose an API that can do this?, 'Output': {'domain': 'Video Classification', 'api_call': \"model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)\", 'api_provider': 'PyTorchVideo', 'explanation': \"Load the pretrained SlowFast model from PyTorchVideo to classify workout videos into different action categories based on the activity.\", 'code': \"import torch\nmodel = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='slowfast_r50', pretrained=True)", "provider": "PyTorchVideo", "api_data": {"domain": "Video Classification", "framework": "PyTorchVideo", "functionality": "SlowFast Networks", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='slowfast_r50', pretrained=True)", "api_arguments": {"repository": "facebookresearch/pytorchvideo", "model": "slowfast_r50", "pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "pytorchvideo"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)", "device = 'cpu'", "model = model.eval()", "model = model.to(device)"], "performance": {"dataset": "Kinetics 400", "accuracy": {"top1": 76.94, "top5": 92.69}, "flops": 65.71, "params": 34.57}, "description": "Slow Fast model architectures are based on the paper 'SlowFast Networks for Video Recognition' by Christoph Feichtenhofer et al. They are pretrained on the Kinetics 400 dataset using the 8x8 setting. This model is capable of classifying video clips into different action categories. It is provided by the FAIR PyTorchVideo library."}}
{"code": "{'Instruction': I am building an app that can detect which animal is in a given image. Give me an API for image classification that would be suitable for this task., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'wide_resnet101_2', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Wide ResNet-101 model from PyTorch Hub, which can be fine-tuned for animal detection and classification in a given image.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'wide_resnet101_2', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='wide_resnet101_2', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Wide Residual Networks", "api_name": "wide_resnet101_2", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='wide_resnet101_2', pretrained=True)", "api_arguments": "pretrained", "python_environment_requirements": "torch, torchvision", "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'wide_resnet101_2', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"wide_resnet101_2": {"Top-1 error": 21.16, "Top-5 error": 5.72}}}, "description": "Wide Residual networks simply have increased number of channels compared to ResNet. Otherwise the architecture is the same. Deeper ImageNet models with bottleneck block have increased number of channels in the inner 3x3 convolution. The wide_resnet50_2 and wide_resnet101_2 models were trained in FP16 with mixed precision training using SGD with warm restarts. Checkpoints have weights in half precision (except batch norm) for smaller size, and can be used in FP32 models too."}}
{"code": "{'Instruction': Recommend a video classification API that can identify the netball activity from a short video clip., 'Output': {'domain': 'Video Classification', 'api_call': \"model = torch.hub.load('facebookresearch/pytorchvideo', 'x3d_s', pretrained=True)\", 'api_provider': 'PyTorchVideo', 'explanation': 'Use the pretrained X3D model from the PyTorchVideo library, which is capable of classifying video clips into different action categories, including netball activity.', 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/pytorchvideo', 'x3d_s', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='x3d_s', pretrained=True)", "provider": "PyTorchVideo", "api_data": {"domain": "Video Classification", "framework": "PyTorchVideo", "functionality": "X3D Networks", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='x3d_s', pretrained=True)", "api_arguments": {"repository": "facebookresearch/pytorchvideo", "model": "x3d_s", "pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "pytorchvideo"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/pytorchvideo', 'x3d_s', pretrained=True)", "device = 'cpu'", "model = model.eval()", "model = model.to(device)"], "performance": {"dataset": "Kinetics 400", "accuracy": {"top1": 73.33, "top5": 91.27}, "flops": 2.96, "params": 3.79}, "description": "X3D model architectures are based on the paper 'X3D: Expanding Architectures for Efficient Video Recognition' by Christoph Feichtenhofer. They are pretrained on the Kinetics 400 dataset. This model is capable of classifying video clips into different action categories. It is provided by the FAIR PyTorchVideo library."}}
{"code": "{'Instruction': Recommend an API for classifying images that is efficient on edge devices., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('mit-han-lab/once-for-all', 'ofa_supernet_mbv3_w10', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Once-for-all (OFA) network with OFA_MobileNetV3 model for efficient image classification on edge devices.', 'code': 'import torch\nmodel = torch.hub.load('mit-han-lab/once-for-all', 'ofa_supernet_mbv3_w10', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/once-for-all', model='ofa_supernet_mbv3_w10', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Once-for-all (OFA) Networks", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/once-for-all', model='ofa_supernet_mbv3_w10', pretrained=True)", "api_arguments": {"repository": "mit-han-lab/once-for-all", "model": "ofa_supernet_mbv3_w10", "pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "super_net_name = 'ofa_supernet_mbv3_w10'", "super_net = torch.hub.load('mit-han-lab/once-for-all', super_net_name, pretrained=True).eval()"], "performance": {"description": "OFA networks outperform state-of-the-art NAS methods (up to 4.0% ImageNet top1 accuracy improvement over MobileNetV3, or same accuracy but 1.5x faster than MobileNetV3, 2.6x faster than EfficientNet w.r.t measured latency) while reducing many orders of magnitude GPU hours and CO2 emission."}, "description": "Once-for-all (OFA) networks are a family of neural networks designed by MIT Han Lab. They decouple training and search, achieving efficient inference across various edge devices and resource constraints. OFA networks are pretrained on the IMAGENET dataset and are capable of classifying images into different categories."}}
{"code": "{'Instruction': I am developing an app that would automatically tag uploaded images. Suggest an API that can perform image classification efficiently on a GPU., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_gpunet', pretrained=True, model_type=model_type, model_math=precision)\", 'api_provider': 'PyTorch', 'explanation': 'Use the pretrained GPUNet model from NVIDIA, which is optimized for GPU performance and can classify images into different categories.', 'code': 'import torch\nmodel_type = 'GPUNet-0'\nprecision = 'fp32'\nmodel = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_gpunet', pretrained=True, model_type=model_type, model_math=precision)'}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_gpunet', pretrained=True, model_type=model_type, model_math=precision)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "GPUNet Networks", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_gpunet', pretrained=True, model_type=model_type, model_math=precision)", "api_arguments": {"repository": "NVIDIA/DeepLearningExamples:torchhub", "model": "nvidia_gpunet", "pretrained": "True", "model_type": "GPUNet-0", "model_math": "fp32"}, "python_environment_requirements": ["torch", "validators", "matplotlib", "timm==0.5.4"], "example_code": ["import torch", "model_type = 'GPUNet-0'", "precision = 'fp32'", "gpunet = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_gpunet', pretrained=True, model_type=model_type, model_math=precision)", "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')", "gpunet.to(device)", "gpunet.eval()"], "performance": {"dataset": "IMAGENET", "description": "GPUNet demonstrates state-of-the-art inference performance up to 2x faster than EfficientNet-X and FBNet-V3."}, "description": "GPUNet is a family of Convolutional Neural Networks designed by NVIDIA using novel Neural Architecture Search (NAS) methods. They are optimized for NVIDIA GPU and TensorRT performance. GPUNet models are pretrained on the IMAGENET dataset and are capable of classifying images into different categories. The models are provided by the NVIDIA Deep Learning Examples library."}}
{"code": "{'Instruction': I need a pre-trained model to translate English text to French, which API can help perform that task?, 'Output': {'domain': 'Neural Machine Translation', 'api_call': \"model = torch.hub.load('pytorch/fairseq', 'transformer.wmt14.en-fr', tokenizer='moses', bpe='subword_nmt')\", 'api_provider': 'PyTorch', 'explanation': \"Load the Transformer model from PyTorch Hub for English to French translation, using the WMT'14 dataset.\", 'code': \"import torch\nen2fr = torch.hub.load('pytorch/fairseq', 'transformer.wmt14.en-fr', tokenizer='moses', bpe='subword_nmt')\"}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/fairseq')", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "Neural Machine Translation", "api_name": "Transformer (NMT)", "api_call": "torch.hub.load(repo_or_dir='pytorch/fairseq')", "api_arguments": ["model_name", "tokenizer", "bpe", "beam", "sampling", "sampling_topk"], "python_environment_requirements": ["bitarray", "fastBPE", "hydra-core", "omegaconf", "regex", "requests", "sacremoses", "subword_nmt"], "example_code": "import torch\n\nen2fr = torch.hub.load('pytorch/fairseq', 'transformer.wmt14.en-fr', tokenizer='moses', bpe='subword_nmt')\n\nen2fr.cuda()\n\nfr = en2fr.translate('Hello world!', beam=5)\nassert fr == 'Bonjour \u00e0 tous !'", "performance": {"dataset": [{"name": "WMT'14", "accuracy": "Not provided"}, {"name": "WMT'18", "accuracy": "Not provided"}, {"name": "WMT'19", "accuracy": "Not provided"}]}, "description": "Transformer (NMT) is a powerful sequence-to-sequence modeling architecture that produces state-of-the-art neural machine translation systems. It is based on the paper 'Attention Is All You Need' and has been improved using techniques such as large-scale semi-supervised training, back-translation, and noisy-channel reranking. It supports English-French and English-German translation as well as round-trip translation for paraphrasing."}}
{"code": "{'Instruction': I need to develop an image classifier for an app that can recognize pictures of objects. Any recommended API that can do this?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models', 'resnet18_swsl', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNet-18 model from Facebook Research Semi-supervised ImageNet Models library to classify images into different categories.', 'code': 'import torch\nmodel = torch.hub.load(\"facebookresearch/semi-supervised-ImageNet1K-models\", \"resnet18_swsl\", pretrained=True)\nmodel.eval()'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/semi-supervised-ImageNet1K-models', model='resnet18_swsl', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Semi-supervised and semi-weakly supervised ImageNet Models", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/semi-supervised-ImageNet1K-models', model='resnet18_swsl', pretrained=True)", "api_arguments": {"repository": "facebookresearch/semi-supervised-ImageNet1K-models", "model": "resnet18_swsl", "pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models', 'resnet18_swsl', pretrained=True)", "model.eval()"], "performance": {"description": "Semi-supervised and semi-weakly supervised ImageNet models achieve state-of-the-art accuracy of 81.2% on ImageNet for the widely used/adopted ResNet-50 model architecture."}, "description": "Semi-supervised and semi-weakly supervised ImageNet Models are introduced in the 'Billion scale semi-supervised learning for image classification' paper. These models are pretrained on a subset of unlabeled YFCC100M public image dataset and fine-tuned with the ImageNet1K training dataset. They are capable of classifying images into different categories and are provided by the Facebook Research library."}}
{"code": "{'Instruction': Can you recommend an NLP API for an AI chatbot that understands user's message and generates human-like responses?, 'Output': {'domain': 'Natural Language Processing', 'api_call': \"model = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-cased')\", 'api_provider': 'Hugging Face/PyTorch', 'explanation': 'You can use the \"BERT\" model from the \"PyTorch-Transformers\" library by Hugging Face to better understand user messages and generate human-like responses in your AI chatbot.', 'code': 'import torch\ntokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-cased')\nmodel = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-cased')'}}", "api_call": "torch.hub.load(repo_or_dir='huggingface/pytorch-transformers')", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Natural Language Processing", "api_name": "PyTorch-Transformers", "api_call": "torch.hub.load(repo_or_dir='huggingface/pytorch-transformers')", "api_arguments": ["pretrained_model_or_path", "output_attention", "output_hidden_states", "config", "from_tf"], "python_environment_requirements": ["tqdm", "boto3", "requests", "regex", "sentencepiece", "sacremoses"], "example_code": "import torch\ntokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-cased')\n\ntext_1 = \"Jim Henson was a puppeteer\"\ntext_2 = \"Who was Jim Henson ?\"\n\nindexed_tokens = tokenizer.encode(text_1, text_2, add_special_tokens=True)\n\nmodel = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-cased')\n\nwith torch.no_grad():\n encoded_layers, _ = model(tokens_tensor, token_type_ids=segments_tensors)", "performance": {"dataset": [{"name": "MRPC", "accuracy": "Not provided"}]}, "description": "PyTorch-Transformers is a library of state-of-the-art pre-trained models for Natural Language Processing (NLP) including BERT, GPT, GPT-2, Transformer-XL, XLNet, XLM, RoBERTa, and DistilBERT. The library provides functionality for tokenization, configuration, and various model architectures for different tasks such as causal language modeling, sequence classification, question answering, and masked language modeling."}}
{"code": "{'Instruction': Develop a model to automatically identify plant species in images., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_efficientnet_b0', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained EfficientNet-B0 model from PyTorch Hub, which can be fine-tuned for plant species identification from images.', 'code': \"import torch\nefficientnet = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_efficientnet_b0', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_efficientnet_b0', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "EfficientNet", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_efficientnet_b0', pretrained=True)", "api_arguments": ["model_name", "pretrained"], "python_environment_requirements": ["validators", "matplotlib"], "example_code": "import torch\n\nefficientnet = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_efficientnet_b0', pretrained=True)\nutils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_convnets_processing_utils')\n\nefficientnet.eval().to(device)\n\nbatch = torch.cat([utils.prepare_input_from_uri(uri) for uri in uris]).to(device)\n\nwith torch.no_grad():\n output = torch.nn.functional.softmax(efficientnet(batch), dim=1)\n \nresults = utils.pick_n_best(predictions=output, n=5)", "performance": {"dataset": {"name": "IMAGENET", "accuracy": "Not provided"}}, "description": "EfficientNet is a family of image classification models that achieve state-of-the-art accuracy while being smaller and faster. The models are trained with mixed precision using Tensor Cores on the NVIDIA Volta and Ampere GPU architectures. The EfficientNet models include EfficientNet-B0, EfficientNet-B4, EfficientNet-WideSE-B0, and EfficientNet-WideSE-B4. The WideSE models use wider Squeeze-and-Excitation layers than the original EfficientNet models, resulting in slightly better accuracy."}}
{"code": "{'Instruction': Recommend a PyTorch API for converting text to speech with high-quality audio output., 'Output': {'domain': 'Text-to-Speech', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tacotron2', model_math='fp16')\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Tacotron 2 model from NVIDIA, which is capable of converting text to high-quality speech using an encoder-decoder architecture.', 'code': 'import torch\nmodel = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tacotron2', model_math=\"fp16\")'}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_tacotron2', model_math='fp16')", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Text-to-Speech", "api_name": "Tacotron 2", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_tacotron2', model_math='fp16')", "api_arguments": {"model_math": "fp16"}, "python_environment_requirements": ["numpy", "scipy", "librosa", "unidecode", "inflect", "libsndfile1"], "example_code": ["import torch", "tacotron2 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tacotron2', model_math='fp16')", "tacotron2 = tacotron2.to('cuda')", "tacotron2.eval()", "text = 'Hello world, I missed you so much.'", "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tts_utils')", "sequences, lengths = utils.prepare_input_sequence([text])", "with torch.no_grad():", " mel, _, _ = tacotron2.infer(sequences, lengths)", " audio = waveglow.infer(mel)", "audio_numpy = audio[0].data.cpu().numpy()", "rate = 22050"], "performance": {"dataset": "LJ Speech", "accuracy": "Not specified"}, "description": "The Tacotron 2 model generates mel spectrograms from input text using an encoder-decoder architecture, and it is designed for generating natural-sounding speech from raw transcripts without any additional prosody information. This implementation uses Dropout instead of Zoneout to regularize the LSTM layers. The WaveGlow model (also available via torch.hub) is a flow-based model that consumes the mel spectrograms to generate speech."}}
{"code": "{'Instruction': I am building a security system for a shopping mall, and I need to identify objects in real-time from CCTV footage. Can you suggest an API?, 'Output': {'domain': 'Object Detection', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Use the pre-trained Single Shot MultiBox Detector (SSD) model from PyTorch Hub, which is highly efficient and accurate for real-time object detection in images.', 'code': \"import torch\nmodel = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_ssd', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Object Detection", "framework": "PyTorch", "functionality": "Single Shot MultiBox Detector", "api_name": "SSD", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_ssd', pretrained=True)", "api_arguments": ["model_name"], "python_environment_requirements": ["numpy", "scipy", "scikit-image", "matplotlib"], "example_code": "import torch\n\nssd_model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd')\nutils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd_processing_utils')\n\nssd_model.to('cuda')\nssd_model.eval()\n\ninputs = [utils.prepare_input(uri) for uri in uris]\ntensor = utils.prepare_tensor(inputs)\n\nwith torch.no_grad():\n detections_batch = ssd_model(tensor)\n\nresults_per_input = utils.decode_results(detections_batch)\nbest_results_per_input = [utils.pick_best(results, 0.40) for results in results_per_input]", "performance": {"dataset": {"name": "COCO", "accuracy": "Not provided"}}, "description": "The SSD (Single Shot MultiBox Detector) model is an object detection model based on the paper 'SSD: Single Shot MultiBox Detector'. It uses a deep neural network for detecting objects in images. This implementation replaces the obsolete VGG model backbone with the more modern ResNet-50 model. The SSD model is trained on the COCO dataset and can be used to detect objects in images with high accuracy and efficiency."}}
{"code": "{'Instruction': I need an API capable of classifying images with state-of-the-art accuracy using no tricks, for a call center to use., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', 'mealv2_resnest50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MEAL V2 model with ResNet50, which achieves state-of-the-art image classification accuracy without using any tricks or modifications.', 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', 'mealv2_resnest50', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50', pretrained=True)", "api_arguments": {"model_name": "mealv2_resnest50"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ ResNet50", "resolution": "224", "parameters": "25.6M", "top1": "80.67", "top5": "95.09"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': What API can classify different types of dogs from an image?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50_cutmix', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained MEAL V2 model from PyTorch Hub, which can be used for dog breed classification from an image.\", 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50_cutmix', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50_cutmix', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50_cutmix', pretrained=True)", "api_arguments": {"model_name": "mealv2_resnest50_cutmix"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 + CutMix w/ ResNet50", "resolution": "224", "parameters": "25.6M", "top1": "80.98", "top5": "95.35"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': I want to classify common objects in an image. Recommend an API that can do this for me., 'Output': {'domain': 'Image Classification', 'api_call': 'model = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_mobilenetv3_small_075', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MEAL V2 model with MobileNet V3 Small architecture from PyTorch Hub, which can classify common objects in an image.', 'code': 'import torch\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_mobilenetv3_small_075', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenetv3_small_075', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenetv3_small_075', pretrained=True)", "api_arguments": {"model_name": "mealv2_mobilenetv3_small_075"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ MobileNet V3-Small 0.75", "resolution": "224", "parameters": "2.04M", "top1": "67.60", "top5": "87.23"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': ABC fashion company wishes to use an API to classify the images of clothing styles. Suggest an API for this purpose., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MEAL V2 model for image classification from PyTorch Hub. This model can be used to classify images of different clothing styles.', 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50_380x380', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50_380x380', pretrained=True)", "api_arguments": {"model_name": "mealv2_resnest50_380x380"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ ResNet50", "resolution": "380", "parameters": "25.6M", "top1": "81.72", "top5": "95.81"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': Rankz wants to develop a custom video suggestion feature by classifying user's uploaded videos. Recommend a suitable API for this task., 'Output': {'domain': 'Video Classification', 'api_call': \"model = torch.hub.load('facebookresearch/pytorchvideo', 'slow_r50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained 3D ResNet model for video classification from PyTorch Hub to classify user-uploaded video clips into different categories.', 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/pytorchvideo', 'slow_r50', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='slow_r50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Video Classification", "framework": "PyTorch", "functionality": "3D ResNet", "api_name": "slow_r50", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='slow_r50', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["torch", "json", "urllib", "pytorchvideo", "torchvision", "torchaudio", "torchtext", "torcharrow", "TorchData", "TorchRec", "TorchServe", "PyTorch on XLA Devices"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/pytorchvideo', 'slow_r50', pretrained=True)", "device = 'cpu'", "model = model.eval()", "model = model.to(device)"], "performance": {"dataset": "Kinetics 400", "accuracy": {"top_1": 74.58, "top_5": 91.63}, "Flops (G)": 54.52, "Params (M)": 32.45}, "description": "The 3D ResNet model is a Resnet-style video classification network pretrained on the Kinetics 400 dataset. It is based on the architecture from the paper 'SlowFast Networks for Video Recognition' by Christoph Feichtenhofer et al."}}
{"code": "{'Instruction': Can you tell me an API for classifying the content of an image?, 'Output': {'domain': 'Image Classification', 'api_call': 'torch.hub.load(repo_or_dir=\"szq0214/MEAL-V2\", \"meal_v2\", model=\"mealv2_efficientnet_b0\", pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'MEAL-V2 image classification API allows you to classify the content of an image using the pretrained EfficientNet-B0 model, which achieves high accuracy on ImageNet dataset.', 'code': 'import torch\nmodel = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model=\"mealv2_efficientnet_b0\", pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_efficientnet_b0', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_efficientnet_b0', pretrained=True)", "api_arguments": {"model_name": "mealv2_efficientnet_b0"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ EfficientNet-B0", "resolution": "224", "parameters": "5.29M", "top1": "78.29", "top5": "93.95"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': Present an API that boosts vanilla ResNet-50 to 80%+ top-1 accuracy on ImageNet for a farmer's crop identification project., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', 'mealv2_mobilenet_v3_large_100', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MEAL-V2 model, which boosts the accuracy of ResNet-50, from PyTorch Hub for crop identification.', 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', 'mealv2_mobilenet_v3_large_100', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenet_v3_large_100', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenet_v3_large_100', pretrained=True)", "api_arguments": {"model_name": "mealv2_mobilenet_v3_large_100"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ MobileNet V3-Large 1.0", "resolution": "224", "parameters": "5.48M", "top1": "76.92", "top5": "93.32"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': I want to build a self-driving car software that can detect traffic objects, drivable areas, and lanes. Which API can perform these tasks efficiently?, 'Output': {'domain': 'Object Detection and Scene Understanding', 'api_call': \"model = torch.hub.load('datvuthanh/hybridnets', 'hybridnets', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained HybridNets model from PyTorch Hub for efficient traffic object detection, drivable area segmentation, and lane detection in the context of self-driving cars.\", 'code': \"import torch\nmodel = torch.hub.load('datvuthanh/hybridnets', 'hybridnets', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='datvuthanh/hybridnets', model='hybridnets', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Object Detection", "framework": "PyTorch", "functionality": "Traffic Object Detection, Drivable Area Segmentation, Lane Detection", "api_name": "HybridNets", "api_call": "torch.hub.load(repo_or_dir='datvuthanh/hybridnets', model='hybridnets', pretrained=True)", "api_arguments": "pretrained", "python_environment_requirements": "Python>=3.7, PyTorch>=1.10", "example_code": "import torch\nmodel = torch.hub.load('datvuthanh/hybridnets', 'hybridnets', pretrained=True)\nimg = torch.randn(1,3,640,384)\nfeatures, regression, classification, anchors, segmentation = model(img)", "performance": {"dataset": [{"name": "BDD100K", "accuracy": {"Traffic Object Detection": {"Recall (%)": 92.8, "mAP@0.5 (%)": 77.3}, "Drivable Area Segmentation": {"Drivable mIoU (%)": 90.5}, "Lane Line Detection": {"Accuracy (%)": 85.4, "Lane Line IoU (%)": 31.6}}}]}, "description": "HybridNets is an end2end perception network for multi-tasks. Our work focused on traffic object detection, drivable area segmentation and lane detection. HybridNets can run real-time on embedded systems, and obtains SOTA Object Detection, Lane Detection on BDD100K Dataset."}}
{"code": "{'Instruction': Create a multi-task network for autonomous driving to handle object detection, drivable area segmentation, and lane detection in real-time. Which API would you suggest to achieve this?, 'Output': {'domain': 'Object Detection', 'api_call': \"model = torch.hub.load('hustvl/yolop', 'yolop', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained YOLOP model from PyTorch Hub, an efficient multi-task network that can simultaneously handle object detection, drivable area segmentation, and lane detection for autonomous driving.', 'code': \"import torch\nmodel = torch.hub.load('hustvl/yolop', 'yolop', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='hustvl/yolop', model='yolop', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Object Detection", "framework": "PyTorch", "functionality": "Object Detection, Drivable Area Segmentation, Lane Detection", "api_name": "YOLOP", "api_call": "torch.hub.load(repo_or_dir='hustvl/yolop', model='yolop', pretrained=True)", "api_arguments": "pretrained", "python_environment_requirements": "pip install -qr https://github.com/hustvl/YOLOP/blob/main/requirements.txt", "example_code": "import torch\nmodel = torch.hub.load('hustvl/yolop', 'yolop', pretrained=True)\nimg = torch.randn(1,3,640,640)\ndet_out, da_seg_out,ll_seg_out = model(img)", "performance": {"dataset": "BDD100K", "accuracy": {"Object Detection": {"Recall(%)": 89.2, "mAP50(%)": 76.5, "Speed(fps)": 41}, "Drivable Area Segmentation": {"mIOU(%)": 91.5, "Speed(fps)": 41}, "Lane Detection": {"mIOU(%)": 70.5, "IOU(%)": 26.2}}}, "description": "YOLOP is an efficient multi-task network that can jointly handle three crucial tasks in autonomous driving: object detection, drivable area segmentation and lane detection. And it is also the first to reach real-time on embedded devices while maintaining state-of-the-art level performance on the BDD100K dataset."}}
{"code": "{'Instruction': Design an intelligent robot. Tell me how to build a robot that can estimate objects' distance from its camera just by taking a photo., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('intel-isl/MiDaS', 'DPT_Hybrid', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Use the pretrained MiDaS model to estimate the relative depth of objects from a single image captured by the robot\\'s camera.', 'code': 'import torch\nmodel = torch.hub.load(\"intel-isl/MiDaS\", \"DPT_Hybrid\", pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='intel-isl/MiDaS', model='DPT_Hybrid', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Computing relative depth from a single image", "api_name": "MiDaS", "api_call": "torch.hub.load(repo_or_dir='intel-isl/MiDaS', model='DPT_Hybrid', pretrained=True)", "api_arguments": {"repo_or_dir": "intel-isl/MiDaS", "model": "model_type"}, "python_environment_requirements": "pip install timm", "example_code": ["import cv2", "import torch", "import urllib.request", "import matplotlib.pyplot as plt", "url, filename = ('https://github.com/pytorch/hub/raw/master/images/dog.jpg', 'dog.jpg')", "urllib.request.urlretrieve(url, filename)", "model_type = 'DPT_Large'", "midas = torch.hub.load('intel-isl/MiDaS', 'DPT_Hybrid')", "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')", "midas.to(device)", "midas.eval()", "midas_transforms = torch.hub.load('intel-isl/MiDaS', 'transforms')", "if model_type == 'DPT_Large' or model_type == 'DPT_Hybrid':", " transform = midas_transforms.dpt_transform", "else:", " transform = midas_transforms.small_transform", "img = cv2.imread(filename)", "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)", "input_batch = transform(img).to(device)", "with torch.no_grad():", " prediction = midas(input_batch)", "prediction = torch.nn.functional.interpolate(", " prediction.unsqueeze(1),", " size=img.shape[:2],", " mode='bicubic',", " align_corners=False,", ").squeeze()", "output = prediction.cpu().numpy()", "plt.imshow(output)", "plt.show()"], "performance": {"dataset": "10 distinct datasets", "accuracy": "Multi-objective optimization"}, "description": "MiDaS computes relative inverse depth from a single image. The repository provides multiple models that cover different use cases ranging from a small, high-speed model to a very large model that provide the highest accuracy. The models have been trained on 10 distinct datasets using multi-objective optimization to ensure high quality on a wide range of inputs."}}
{"code": "{'Instruction': I am an architect for a construction project and I need to compute the depth of objects in an image. What API will allow me to do this?, 'Output': {'domain': 'Image Depth Estimation', 'api_call': 'midas = torch.hub.load('intel-isl/MiDaS', 'DPT_Large')', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MiDaS model (DPT_Large variant) from PyTorch Hub to compute the relative depth of objects in an image.', 'code': 'import torch\nmidas = torch.hub.load('intel-isl/MiDaS', 'DPT_Large', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='intel-isl/MiDaS', model='DPT_Large', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Computing relative depth from a single image", "api_name": "MiDaS", "api_call": "torch.hub.load(repo_or_dir='intel-isl/MiDaS', model='DPT_Large', pretrained=True)", "api_arguments": {"repo_or_dir": "intel-isl/MiDaS", "model": "model_type"}, "python_environment_requirements": "pip install timm", "example_code": ["import cv2", "import torch", "import urllib.request", "import matplotlib.pyplot as plt", "url, filename = ('https://github.com/pytorch/hub/raw/master/images/dog.jpg', 'dog.jpg')", "urllib.request.urlretrieve(url, filename)", "model_type = 'DPT_Large'", "midas = torch.hub.load('intel-isl/MiDaS', 'DPT_Large')", "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')", "midas.to(device)", "midas.eval()", "midas_transforms = torch.hub.load('intel-isl/MiDaS', 'transforms')", "if model_type == 'DPT_Large' or model_type == 'DPT_Hybrid':", " transform = midas_transforms.dpt_transform", "else:", " transform = midas_transforms.small_transform", "img = cv2.imread(filename)", "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)", "input_batch = transform(img).to(device)", "with torch.no_grad():", " prediction = midas(input_batch)", "prediction = torch.nn.functional.interpolate(", " prediction.unsqueeze(1),", " size=img.shape[:2],", " mode='bicubic',", " align_corners=False,", ").squeeze()", "output = prediction.cpu().numpy()", "plt.imshow(output)", "plt.show()"], "performance": {"dataset": "10 distinct datasets", "accuracy": "Multi-objective optimization"}, "description": "MiDaS computes relative inverse depth from a single image. The repository provides multiple models that cover different use cases ranging from a small, high-speed model to a very large model that provide the highest accuracy. The models have been trained on 10 distinct datasets using multi-objective optimization to ensure high quality on a wide range of inputs."}}
{"code": "{'Instruction': I need a deep learning model to separate vocals from background music for a given audio clip. What API can I use for this?, 'Output': {'domain': 'Audio Separation', 'api_call': \"model = torch.hub.load('sigsep/open-unmix-pytorch', 'umxhq', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Use the pretrained Open-Unmix model to separate the audio clip into vocals and background music using PyTorch.', 'code': 'import torch\nseparator = torch.hub.load('sigsep/open-unmix-pytorch', 'umxhq')'}}", "api_call": "torch.hub.load(repo_or_dir='sigsep/open-unmix-pytorch', model='umxhq', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Audio Separation", "framework": "PyTorch", "functionality": "Music Source Separation", "api_name": "Open-Unmix", "api_call": "torch.hub.load(repo_or_dir='sigsep/open-unmix-pytorch', model='umxhq', pretrained=True)", "api_arguments": ["model_name"], "python_environment_requirements": ["PyTorch >=1.6.0", "torchaudio"], "example_code": ["import torch", "separator = torch.hub.load('sigsep/open-unmix-pytorch', 'umxhq')", "audio = torch.rand((1, 2, 100000))", "original_sample_rate = separator.sample_rate", "estimates = separator(audio)"], "performance": {"dataset": "MUSDB18", "accuracy": "N/A"}, "description": "Open-Unmix provides ready-to-use models that allow users to separate pop music into four stems: vocals, drums, bass and the remaining other instruments. The models were pre-trained on the freely available MUSDB18 dataset."}}
{"code": "{'Instruction': I am a photographer and I want to determine the relative depth of objects in my images. Suggest an API that can compute the depth information from a single image., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('intel-isl/MiDaS', 'MiDaS_small', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the MiDaS model from PyTorch Hub, which is designed to compute relative depth information from a single image.', 'code': 'import torch\nmodel = torch.hub.load('intel-isl/MiDaS', 'MiDaS_small', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='intel-isl/MiDaS', model='MiDaS_small', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Computing relative depth from a single image", "api_name": "MiDaS", "api_call": "torch.hub.load(repo_or_dir='intel-isl/MiDaS', model='MiDaS_small', pretrained=True)", "api_arguments": {"repo_or_dir": "intel-isl/MiDaS", "model": "model_type"}, "python_environment_requirements": "pip install timm", "example_code": ["import cv2", "import torch", "import urllib.request", "import matplotlib.pyplot as plt", "url, filename = ('https://github.com/pytorch/hub/raw/master/images/dog.jpg', 'dog.jpg')", "urllib.request.urlretrieve(url, filename)", "model_type = 'DPT_Large'", "midas = torch.hub.load('intel-isl/MiDaS', 'MiDaS_small')", "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')", "midas.to(device)", "midas.eval()", "midas_transforms = torch.hub.load('intel-isl/MiDaS', 'transforms')", "if model_type == 'DPT_Large' or model_type == 'DPT_Hybrid':", " transform = midas_transforms.dpt_transform", "else:", " transform = midas_transforms.small_transform", "img = cv2.imread(filename)", "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)", "input_batch = transform(img).to(device)", "with torch.no_grad():", " prediction = midas(input_batch)", "prediction = torch.nn.functional.interpolate(", " prediction.unsqueeze(1),", " size=img.shape[:2],", " mode='bicubic',", " align_corners=False,", ").squeeze()", "output = prediction.cpu().numpy()", "plt.imshow(output)", "plt.show()"], "performance": {"dataset": "10 distinct datasets", "accuracy": "Multi-objective optimization"}, "description": "MiDaS computes relative inverse depth from a single image. The repository provides multiple models that cover different use cases ranging from a small, high-speed model to a very large model that provide the highest accuracy. The models have been trained on 10 distinct datasets using multi-objective optimization to ensure high quality on a wide range of inputs."}}
{"code": "{'Instruction': An AI company wants to build a bird species identifier given an image of bird. Give me an API that can classify bird species from an input image., 'Output': {'domain': 'Classification', 'api_call': 'model = torch.hub.load('nicolalandro/ntsnet-cub200', 'ntsnet', pretrained=True, **{'topN': 6, 'device':'cpu', 'num_classes': 200})', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained NTS-Net model from PyTorch Hub, which is a fine-grained image classifier for bird species trained on the CUB200 2011 dataset.', 'code': \"import torch\nmodel = torch.hub.load('nicolalandro/ntsnet-cub200', 'ntsnet', pretrained=True, **{'topN': 6, 'device':'cpu', 'num_classes': 200})\"}}", "api_call": "torch.hub.load(repo_or_dir='nicolalandro/ntsnet-cub200', model='ntsnet', pretrained=True, **{'topN': 6, 'device':'cpu', 'num_classes': 200})", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Fine-grained image classifier", "api_name": "ntsnet", "api_call": "torch.hub.load(repo_or_dir='nicolalandro/ntsnet-cub200', model='ntsnet', pretrained=True, **{'topN': 6, 'device':'cpu', 'num_classes': 200})", "api_arguments": {"pretrained": "True", "topN": "6", "device": "cpu", "num_classes": "200"}, "python_environment_requirements": ["torch", "torchvision", "PIL"], "example_code": "from torchvision import transforms\nimport torch\nimport urllib\nfrom PIL import Image\n\ntransform_test = transforms.Compose([\n transforms.Resize((600, 600), Image.BILINEAR),\n transforms.CenterCrop((448, 448)),\n transforms.ToTensor(),\n transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n])\n\nmodel = torch.hub.load('nicolalandro/ntsnet-cub200', 'ntsnet', pretrained=True, **{'topN': 6, 'device':'cpu', 'num_classes': 200})\nmodel.eval()\n\nurl = 'https://raw.githubusercontent.com/nicolalandro/ntsnet-cub200/master/images/nts-net.png'\nimg = Image.open(urllib.request.urlopen(url))\nscaled_img = transform_test(img)\ntorch_images = scaled_img.unsqueeze(0)\n\nwith torch.no_grad():\n top_n_coordinates, concat_out, raw_logits, concat_logits, part_logits, top_n_index, top_n_prob = model(torch_images)\n\n_, predict = torch.max(concat_logits, 1)\npred_id = predict.item()\nprint('bird class:', model.bird_classes[pred_id])", "performance": {"dataset": "CUB200 2011", "accuracy": "Not provided"}, "description": "This is an nts-net pretrained with CUB200 2011 dataset, which is a fine-grained dataset of birds species."}}
{"code": "{'Instruction': I'm working on a project that involves transcribing voice recordings. Recommend an API that can convert speech to text., 'Output': {'domain': 'Speech-To-Text', 'api_call': 'model, decoder, utils = torch.hub.load(repo_or_dir=\\'snakers4/silero-models\\', model=\\'silero_stt\\', language=\\'en\\', device=device)', 'api_provider': 'PyTorch', 'explanation': 'Load the Silero Speech-To-Text (STT) model from PyTorch Hub for the English language. It will transcribe voice recordings into text.', 'code': 'import torch\ndevice = torch.device('cpu')\nmodel, decoder, utils = torch.hub.load(repo_or_dir=\\'snakers4/silero-models\\', model=\\'silero_stt\\', language=\\'en\\', device=device)'}}", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_stt', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Speech-To-Text", "api_name": "Silero Models", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_stt', pretrained=True)", "api_arguments": {"repo_or_dir": "snakers4/silero-models", "model": "silero_stt", "language": ["en", "de", "es"], "device": "device"}, "python_environment_requirements": ["pip install -q torchaudio omegaconf soundfile"], "example_code": ["import torch", "import zipfile", "import torchaudio", "from glob import glob", "device = torch.device('cpu')", "model, decoder, utils = torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_stt', language='en', device=device)", "(read_batch, split_into_batches, read_audio, prepare_model_input) = utils", "torch.hub.download_url_to_file('https://opus-codec.org/static/examples/samples/speech_orig.wav', dst ='speech_orig.wav', progress=True)", "test_files = glob('speech_orig.wav')", "batches = split_into_batches(test_files, batch_size=10)", "input = prepare_model_input(read_batch(batches[0]), device=device)", "output = model(input)", "for example in output:", " print(decoder(example.cpu()))"], "performance": {"dataset": "Open-STT", "accuracy": "See the wiki for quality and performance benchmarks"}, "description": "Silero Speech-To-Text models provide enterprise grade STT in a compact form-factor for several commonly spoken languages. The models are robust to a variety of dialects, codecs, domains, noises, and lower sampling rates. They consume a normalized audio in the form of samples and output frames with token probabilities. A decoder utility is provided for simplicity."}}
{"code": "{'Instruction': Based upon a traffic camera, San Francisco Traffic Authority wants to identify the segmented boundaries for everything the camera captures. Provide an API that can help them perform this task., 'Output': {'domain': 'Semantic Segmentation', 'api_call': \"model = torch.hub.load('pytorch/vision', 'deeplabv3_resnet50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained DeepLabV3 model with ResNet-50 backbone for performing semantic segmentation on the image captured by the traffic camera.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'deeplabv3_resnet50', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='deeplabv3_resnet50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "DeepLabV3", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='deeplabv3_resnet50', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": "latest", "torchvision": "latest", "PIL": "latest", "matplotlib": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_resnet50', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "input_image = input_image.convert('RGB')", "preprocess = transforms.Compose([", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)['out'][0]", "output_predictions = output.argmax(0)"], "performance": {"dataset": "COCO val2017", "accuracy": [{"model": "deeplabv3_resnet50", "Mean IOU": 66.4, "Global Pixelwise Accuracy": 92.4}]}, "description": "DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset."}}
{"code": "{'Instruction': Recommend an API that can detect if there is a speech or silence segments in an audio file., 'Output': {'domain': 'Voice Activity Detection', 'api_call': \"model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the Silero Voice Activity Detector from PyTorch Hub to detect speech and silence segments in audio files.\", 'code': \"import torch\nmodel, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Voice Activity Detection", "api_name": "Silero Voice Activity Detector", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=True)", "api_arguments": {"repo_or_dir": "snakers4/silero-vad", "model": "silero_vad", "force_reload": "True"}, "python_environment_requirements": {"torchaudio": "pip install -q torchaudio"}, "example_code": {"import": ["import torch", "torch.set_num_threads(1)", "from IPython.display import Audio", "from pprint import pprint"], "download_example": "torch.hub.download_url_to_file('https://models.silero.ai/vad_models/en.wav', 'en_example.wav')", "load_model": "model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=True)", "load_utils": "(get_speech_timestamps, _, read_audio, _) = utils", "set_sampling_rate": "sampling_rate = 16000", "read_audio": "wav = read_audio('en_example.wav', sampling_rate=sampling_rate)", "get_speech_timestamps": "speech_timestamps = get_speech_timestamps(wav, model, sampling_rate=sampling_rate)", "print_speech_timestamps": "pprint(speech_timestamps)"}, "performance": {"dataset": "", "accuracy": ""}, "description": "Silero VAD is a pre-trained enterprise-grade Voice Activity Detector (VAD) that aims to provide a high-quality and modern alternative to the WebRTC Voice Activity Detector. The model is optimized for performance on 1 CPU thread and is quantized."}}
{"code": "{'Instruction': Tell me about an API that is capable of segmenting objects in images based on their semantics, like identifying a car or a person., 'Output': {'domain': 'Semantic Segmentation', 'api_call': \"model = torch.hub.load('pytorch/vision', 'deeplabv3_resnet101', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the DeepLabV3 ResNet-101 model from PyTorch Hub. This model is pretrained for semantic segmentation and can identify various objects in images, such as cars and people.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'deeplabv3_resnet101', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='deeplabv3_resnet101', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "DeepLabV3", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='deeplabv3_resnet101', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": "latest", "torchvision": "latest", "PIL": "latest", "matplotlib": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_resnet101', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "input_image = input_image.convert('RGB')", "preprocess = transforms.Compose([", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)['out'][0]", "output_predictions = output.argmax(0)"], "performance": {"dataset": "COCO val2017", "accuracy": [{"model": "deeplabv3_resnet101", "Mean IOU": 67.4, "Global Pixelwise Accuracy": 92.4}]}, "description": "DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset."}}
{"code": "{'Instruction': Can you recommend an API that can detect different types of animals in a picture?, 'Output': {'domain': 'Object Detection', 'api_call': \"model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained YOLOv5 model from PyTorch Hub, which is capable of detecting various types of animals in an image.', 'code': 'import torch\nmodel = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\", pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='ultralytics/yolov5', model='yolov5s', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Object Detection", "framework": "PyTorch", "functionality": "YOLOv5", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='ultralytics/yolov5', model='yolov5s', pretrained=True)", "api_arguments": ["'ultralytics/yolov5'", "'yolov5s'", "pretrained=True"], "python_environment_requirements": "Python>=3.8, PyTorch>=1.7", "example_code": ["import torch", "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)", "imgs = ['https://ultralytics.com/images/zidane.jpg']", "results = model(imgs)", "results.print()", "results.save()", "results.xyxy[0]", "results.pandas().xyxy[0]"], "performance": {"dataset": "COCO", "accuracy": {"YOLOv5s6": {"mAPval0.5:0.95": 43.3, "mAPtest0.5:0.95": 43.3, "mAPval0.5": 61.9}, "YOLOv5m6": {"mAPval0.5:0.95": 50.5, "mAPtest0.5:0.95": 50.5, "mAPval0.5": 68.7}, "YOLOv5l6": {"mAPval0.5:0.95": 53.4, "mAPtest0.5:0.95": 53.4, "mAPval0.5": 71.1}, "YOLOv5x6": {"mAPval0.5:0.95": 54.4, "mAPtest0.5:0.95": 54.4, "mAPval0.5": 72.0}, "YOLOv5x6 TTA": {"mAPval0.5:0.95": 55.0, "mAPtest0.5:0.95": 55.0, "mAPval0.5": 72.0}}}, "description": "YOLOv5 is a family of compound-scaled object detection models trained on the COCO dataset, and includes simple functionality for Test Time Augmentation (TTA), model ensembling, hyperparameter evolution, and export to ONNX, CoreML and TFLite."}}
{"code": "{'Instruction': I want to perform semantic segmentation on an image to differentiate the foreground objects from the background. Recommend an API that can accomplish this., 'Output': {'domain': 'Semantic Segmentation', 'api_call': \"model = torch.hub.load('pytorch/vision', 'deeplabv3_mobilenet_v3_large', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained DeepLabV3 model with MobileNet-V3 backbone for semantic segmentation from PyTorch Hub to differentiate foreground objects from the background in an image.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'deeplabv3_mobilenet_v3_large', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='deeplabv3_mobilenet_v3_large', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "DeepLabV3", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='deeplabv3_mobilenet_v3_large', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": "latest", "torchvision": "latest", "PIL": "latest", "matplotlib": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_mobilenet_v3_large', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "input_image = input_image.convert('RGB')", "preprocess = transforms.Compose([", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)['out'][0]", "output_predictions = output.argmax(0)"], "performance": {"dataset": "COCO val2017", "accuracy": [{"model": "deeplabv3_mobilenet_v3_large", "Mean IOU": 60.3, "Global Pixelwise Accuracy": 91.4}]}, "description": "DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset."}}
{"code": "{'Instruction': Help me to convert this text to speech: \"The weather is great today\" with an API call., 'Input': 'The weather is great today', 'Output': {'domain': 'Text-To-Speech', 'api_call': \"model, symbols, sample_rate, example_text, apply_tts = torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_tts', language='en', speaker='lj_16khz')\", 'api_provider': 'PyTorch', 'explanation': 'Load the Silero Text-To-Speech model from PyTorch Hub to convert the text \"The weather is great today\" into speech.', 'code': 'import torch\nlanguage = \"en\"\nspeaker = \"lj_16khz\"\nexample_text = \"The weather is great today\"\ndevice = torch.device(\"cpu\")\nmodel, symbols, sample_rate, _, apply_tts = torch.hub.load(repo_or_dir=\"snakers4/silero-models\", model=\"silero_tts\", language=language, speaker=speaker)\nmodel = model.to(device)\naudio = apply_tts(texts=[example_text], model=model, sample_rate=sample_rate, symbols=symbols, device=device)'}}", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_tts', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Text-To-Speech", "framework": "PyTorch", "functionality": "Text-To-Speech", "api_name": "Silero Text-To-Speech Models", "api_call": "torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_tts', pretrained=True)", "api_arguments": {"repo_or_dir": "snakers4/silero-models", "model": "silero_tts", "language": "language", "speaker": "speaker"}, "python_environment_requirements": ["pip install -q torchaudio omegaconf"], "example_code": "import torch\nlanguage = 'en'\nspeaker = 'lj_16khz'\ndevice = torch.device('cpu')\nmodel, symbols, sample_rate, example_text, apply_tts = torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_tts', language=language, speaker=speaker)\nmodel = model.to(device)\naudio = apply_tts(texts=[example_text], model=model, sample_rate=sample_rate, symbols=symbols, device=device)", "performance": {"dataset": [{"language": "Russian", "speakers": 6}, {"language": "English", "speakers": 1}, {"language": "German", "speakers": 1}, {"language": "Spanish", "speakers": 1}, {"language": "French", "speakers": 1}], "accuracy": "High throughput on slow hardware. Decent performance on one CPU thread"}, "description": "Silero Text-To-Speech models provide enterprise grade TTS in a compact form-factor for several commonly spoken languages. They offer one-line usage, naturally sounding speech, no GPU or training required, minimalism and lack of dependencies, a library of voices in many languages, support for 16kHz and 8kHz out of the box."}}
{"code": "{'Instruction': I want to make a video game and I need randomly generated low resolution images for background. Can you suggest an API that can create images as per my requirement?, 'Output': {'domain': 'Image Generation', 'api_call': \"model = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'DCGAN', pretrained=True, useGPU=use_gpu)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained Deep Convolutional Generative Adversarial Networks (DCGAN) model from PyTorch Hub. This model can generate low-resolution images suitable for your video game's background.\", 'code': \"import torch\nuse_gpu = True if torch.cuda.is_available() else False\nmodel = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'DCGAN', pretrained=True, useGPU=use_gpu)\"}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorch_GAN_zoo:hub', model='DCGAN', pretrained=True, useGPU=use_gpu)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Generative Adversarial Networks", "api_name": "DCGAN", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorch_GAN_zoo:hub', model='DCGAN', pretrained=True, useGPU=use_gpu)", "api_arguments": {"pretrained": "True", "useGPU": "use_gpu"}, "python_environment_requirements": "Python 3", "example_code": {"import": ["import torch", "import matplotlib.pyplot as plt", "import torchvision"], "use_gpu": "use_gpu = True if torch.cuda.is_available() else False", "load_model": "model = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'DCGAN', pretrained=True, useGPU=use_gpu)", "build_noise_data": "noise, _ = model.buildNoiseData(num_images)", "generate_images": "with torch.no_grad(): generated_images = model.test(noise)", "plot_images": ["plt.imshow(torchvision.utils.make_grid(generated_images).permute(1, 2, 0).cpu().numpy())", "plt.show()"]}, "performance": {"dataset": "FashionGen", "accuracy": "N/A"}, "description": "DCGAN is a model designed in 2015 by Radford et. al. in the paper Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks. It is a GAN architecture both very simple and efficient for low resolution image generation (up to 64x64)."}}
{"code": "{'Instruction': I need an API to classify an image according to the ImageNet dataset. Tell me which API to use and how., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x8d_wsl', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Use the pretrained ResNext WSL model from PyTorch Hub for image classification according to the ImageNet dataset.', 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x8d_wsl', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x8d_wsl', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNext WSL", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x8d_wsl', pretrained=True)", "api_arguments": [{"name": "resnext101_32x8d_wsl", "type": "str", "description": "ResNeXt-101 32x8d WSL model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x8d_wsl')", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(output[0])", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeXt-101 32x8d": {"Top-1 Acc.": "82.2", "Top-5 Acc.": "96.4"}}}, "description": "The provided ResNeXt models are pre-trained in weakly-supervised fashion on 940 million public images with 1.5K hashtags matching with 1000 ImageNet1K synsets, followed by fine-tuning on ImageNet1K dataset. The models significantly improve the training accuracy on ImageNet compared to training from scratch. They achieve state-of-the-art accuracy of 85.4% on ImageNet with the ResNext-101 32x48d model."}}
{"code": "{'Instruction': Some friends and I want to organize a competition where people send us images of their pets and the photos are sorted into categories based on the type of pet in the image. Provide a deep learning API that can meet this goal., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNext-101 32x16d WSL model from PyTorch Hub, which can be fine-tuned for classifying pet images into different categories.', 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x16d_wsl', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNext WSL", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x16d_wsl', pretrained=True)", "api_arguments": [{"name": "resnext101_32x16d_wsl", "type": "str", "description": "ResNeXt-101 32x16d WSL model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x16d_wsl')", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(output[0])", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeXt-101 32x16d": {"Top-1 Acc.": "84.2", "Top-5 Acc.": "97.2"}}}, "description": "The provided ResNeXt models are pre-trained in weakly-supervised fashion on 940 million public images with 1.5K hashtags matching with 1000 ImageNet1K synsets, followed by fine-tuning on ImageNet1K dataset. The models significantly improve the training accuracy on ImageNet compared to training from scratch. They achieve state-of-the-art accuracy of 85.4% on ImageNet with the ResNext-101 32x48d model."}}
{"code": "{'Instruction': Help me discover an API suitable for classifying images in a custom dataset using Instagram users' preferences., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x32d_wsl', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNext WSL model from PyTorch Hub, which is trained on 940 million public images with 1.5K hashtags. This model can be fine-tuned for classifying images based on Instagram users\\' preferences.', 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x32d_wsl', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x32d_wsl', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNext WSL", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x32d_wsl', pretrained=True)", "api_arguments": [{"name": "resnext101_32x32d_wsl", "type": "str", "description": "ResNeXt-101 32x32d WSL model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x32d_wsl')", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(output[0])", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeXt-101 32x32d": {"Top-1 Acc.": "85.1", "Top-5 Acc.": "97.5"}}}, "description": "The provided ResNeXt models are pre-trained in weakly-supervised fashion on 940 million public images with 1.5K hashtags matching with 1000 ImageNet1K synsets, followed by fine-tuning on ImageNet1K dataset. The models significantly improve the training accuracy on ImageNet compared to training from scratch. They achieve state-of-the-art accuracy of 85.4% on ImageNet with the ResNext-101 32x48d model."}}
{"code": "{'Instruction': A marketing company needs to clasify images of different catagories for a client. Can you provide an API to achive this?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x48d_wsl')\", 'api_provider': 'PyTorch', 'explanation': 'Load the pre-trained ResNext WSL model from PyTorch Hub for image classification.', 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x48d_wsl')'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x48d_wsl', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNext WSL", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/WSL-Images', model='resnext101_32x48d_wsl', pretrained=True)", "api_arguments": [{"name": "resnext101_32x48d_wsl", "type": "str", "description": "ResNeXt-101 32x48d WSL model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x48d_wsl')", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(output[0])", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeXt-101 32x48d": {"Top-1 Acc.": "85.4", "Top-5 Acc.": "97.6"}}}, "description": "The provided ResNeXt models are pre-trained in weakly-supervised fashion on 940 million public images with 1.5K hashtags matching with 1000 ImageNet1K synsets, followed by fine-tuning on ImageNet1K dataset. The models significantly improve the training accuracy on ImageNet compared to training from scratch. They achieve state-of-the-art accuracy of 85.4% on ImageNet with the ResNext-101 32x48d model."}}
{"code": "{'Instruction': We are building a comic generator software that creates new comic images by mixing scenes of other comics. Suggest a relevant API for generating such new comic images., 'Output': {'domain': 'Image Generation', 'api_call': \"model = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'PGAN', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Progressive Growing of GANs (PGAN) model from PyTorch Hub, which can be fine-tuned for generating new comic images by mixing scenes from other comics.', 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'PGAN', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorch_GAN_zoo:hub', model='PGAN', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Generative Adversarial Networks (GANs)", "api_name": "PGAN", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorch_GAN_zoo:hub', model='PGAN', pretrained=True)", "api_arguments": {"repo_or_dir": "facebookresearch/pytorch_GAN_zoo:hub", "model": "PGAN", "model_name": "celebAHQ-512", "pretrained": "True", "useGPU": "use_gpu"}, "python_environment_requirements": "Python 3", "example_code": {"import": "import torch", "use_gpu": "use_gpu = True if torch.cuda.is_available() else False", "load_model": "model = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'PGAN', model_name='celebAHQ-512', pretrained=True, useGPU=use_gpu)", "build_noise_data": "noise, _ = model.buildNoiseData(num_images)", "test": "generated_images = model.test(noise)", "plot_images": {"import_matplotlib": "import matplotlib.pyplot as plt", "import_torchvision": "import torchvision", "make_grid": "grid = torchvision.utils.make_grid(generated_images.clamp(min=-1, max=1), scale_each=True, normalize=True)", "imshow": "plt.imshow(grid.permute(1, 2, 0).cpu().numpy())", "show": "plt.show()"}}, "performance": {"dataset": "celebA", "accuracy": "High-quality celebrity faces"}, "description": "Progressive Growing of GANs (PGAN) is a method for generating high-resolution images using generative adversarial networks. The model is trained progressively, starting with low-resolution images and gradually increasing the resolution until the desired output is achieved. This implementation is based on the paper by Tero Karras et al., 'Progressive Growing of GANs for Improved Quality, Stability, and Variation'."}}
{"code": "{'Instruction': Teach me how to operate an API that can segment abnormalities in brain MRI images., 'Output': {'domain': 'Semantic Segmentation', 'api_call': \"model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Use the pre-trained U-Net model and load it from PyTorch Hub for abnormality segmentation in brain MRI images.', 'code': 'import torch\nmodel = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='mateuszbuda/brain-segmentation-pytorch', model='unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "Biomedical Image Segmentation", "api_name": "U-Net for brain MRI", "api_call": "torch.hub.load(repo_or_dir='mateuszbuda/brain-segmentation-pytorch', model='unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)", "api_arguments": {"in_channels": 3, "out_channels": 1, "init_features": 32, "pretrained": true}, "python_environment_requirements": ["torch", "torchvision", "numpy", "PIL"], "example_code": ["import torch", "model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)", "import numpy as np", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "m, s = np.mean(input_image, axis=(0, 1)), np.std(input_image, axis=(0, 1))", "preprocess = transforms.Compose([transforms.ToTensor(),transforms.Normalize(mean=m, std=s)])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model = model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(torch.round(output[0]))"], "performance": {"dataset": "kaggle.com/mateuszbuda/lgg-mri-segmentation"}, "description": "U-Net with batch normalization for biomedical image segmentation with pretrained weights for abnormality segmentation in brain MRI. The model comprises four levels of blocks containing two convolutional layers with batch normalization and ReLU activation function, and one max pooling layer in the encoding part and up-convolutional layers instead in the decoding part. The number of convolutional filters in each block is 32, 64, 128, and 256. The bottleneck layer has 512 convolutional filters. From the encoding layers, skip connections are used to the corresponding layers in the decoding part. Input image is a 3-channel brain MRI slice from pre-contrast, FLAIR, and post-contrast sequences, respectively. Output is a one-channel probability map of abnormality regions with the same size as the input image."}}
{"code": "{'Instruction': A company is looking for an image recognition API to help them categorize their products. Suggest an API that could do this., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resnet50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNet50 v1.5 model from PyTorch Hub to help categorize products using image recognition.', 'code': \"import torch\nmodel = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resnet50', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_resnet50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNet50", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_resnet50', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["pip install validators matplotlib"], "example_code": ["import torch", "from PIL import Image", "import torchvision.transforms as transforms", "import numpy as np", "import json", "import requests", "import matplotlib.pyplot as plt", "import warnings", "warnings.filterwarnings('ignore')", "%matplotlib inline", "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')", "print(f'Using {device} for inference')", "resnet50 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resnet50', pretrained=True)", "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_convnets_processing_utils')", "resnet50.eval().to(device)", "uris = [...]", "batch = torch.cat([utils.prepare_input_from_uri(uri) for uri in uris]).to(device)", "with torch.no_grad():", " output = torch.nn.functional.softmax(resnet50(batch), dim=1)", "results = utils.pick_n_best(predictions=output, n=5)", "for uri, result in zip(uris, results):", " img = Image.open(requests.get(uri, stream=True).raw)", " img.thumbnail((256,256), Image.ANTIALIAS)", " plt.imshow(img)", " plt.show()", " print(result)"], "performance": {"dataset": "ImageNet", "accuracy": "~0.5% top1 improvement over ResNet50 v1"}, "description": "The ResNet50 v1.5 model is a modified version of the original ResNet50 v1 model. The difference between v1 and v1.5 is that, in the bottleneck blocks which requires downsampling, v1 has stride = 2 in the first 1x1 convolution, whereas v1.5 has stride = 2 in the 3x3 convolution. This difference makes ResNet50 v1.5 slightly more accurate (~0.5% top1) than v1, but comes with a small performance drawback (~5% imgs/sec). The model is initialized as described in Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification. This model is trained with mixed precision using Tensor Cores on Volta, Turing, and the NVIDIA Ampere GPU architectures."}}
{"code": "{'Instruction': Share an image classification API that can recognize objects in real-world images, including 1,000 categories., 'Output': {'domain': 'Classification', 'api_call': 'model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resneXt', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNeXt101-32x4d model from PyTorch Hub, capable of recognizing objects in real-world images across 1,000 categories.', 'code': 'import torch\nmodel = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resneXt', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_resneXt', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNeXt101-32x4d", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_resneXt', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "validators", "matplotlib"], "example_code": ["import torch", "from PIL import Image", "import torchvision.transforms as transforms", "import numpy as np", "import json", "import requests", "import matplotlib.pyplot as plt", "import warnings", "warnings.filterwarnings('ignore')", "%matplotlib inline", "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")", "print(f'Using {device} for inference')", "resneXt = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resneXt')", "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_convnets_processing_utils')", "resneXt.eval().to(device)", "uris = [", " 'http://images.cocodataset.org/test-stuff2017/000000024309.jpg',", " 'http://images.cocodataset.org/test-stuff2017/000000028117.jpg',", " 'http://images.cocodataset.org/test-stuff2017/000000006149.jpg',", " 'http://images.cocodataset.org/test-stuff2017/000000004954.jpg',", "]", "batch = torch.cat(", " [utils.prepare_input_from_uri(uri) for uri in uris]", ").to(device)", "with torch.no_grad():", " output = torch.nn.functional.softmax(resneXt(batch), dim=1)", "results = utils.pick_n_best(predictions=output, n=5)", "for uri, result in zip(uris, results):", " img = Image.open(requests.get(uri, stream=True).raw)", " img.thumbnail((256,256), Image.ANTIALIAS)", " plt.imshow(img)", " plt.show()", " print(result)"], "performance": {"dataset": "IMAGENET", "accuracy": "Not provided"}, "description": "ResNeXt101-32x4d is a model introduced in the Aggregated Residual Transformations for Deep Neural Networks paper. It is based on regular ResNet model, substituting 3x3 convolutions inside the bottleneck block for 3x3 grouped convolutions. This model is trained with mixed precision using Tensor Cores on Volta, Turing, and the NVIDIA Ampere GPU architectures. It can be deployed for inference on the NVIDIA Triton Inference Server using TorchScript, ONNX Runtime or TensorRT as an execution backend."}}
{"code": "{'Instruction': A Facebook user wants to classify images from their vacation album. Recommend an AI-based solution that can classify images into different categories., 'Output': {'domain': 'Classification', 'api_call': 'model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_se_resnext101_32x4d', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained SE-ResNeXt101 model, which is an image classification model that can classify vacation images into different categories.', 'code': 'import torch\nmodel = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_se_resnext101_32x4d', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_se_resnext101_32x4d', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SE-ResNeXt101", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_se_resnext101_32x4d', pretrained=True)", "api_arguments": "N/A", "python_environment_requirements": "validators, matplotlib", "example_code": "import torch\nfrom PIL import Image\nimport torchvision.transforms as transforms\nimport numpy as np\nimport json\nimport requests\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nprint(f'Using {device} for inference')\nresneXt = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_se_resnext101_32x4d')\nutils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_convnets_processing_utils')\nresneXt.eval().to(device)\nuris = ['http://images.cocodataset.org/test-stuff2017/000000024309.jpg','http://images.cocodataset.org/test-stuff2017/000000028117.jpg','http://images.cocodataset.org/test-stuff2017/000000006149.jpg','http://images.cocodataset.org/test-stuff2017/000000004954.jpg']\nbatch = torch.cat([utils.prepare_input_from_uri(uri) for uri in uris]).to(device)\nwith torch.no_grad():\n output = torch.nn.functional.softmax(resneXt(batch), dim=1)\nresults = utils.pick_n_best(predictions=output, n=5)\nfor uri, result in zip(uris, results):\n img = Image.open(requests.get(uri, stream=True).raw)\n img.thumbnail((256,256), Image.ANTIALIAS)\n plt.imshow(img)\n plt.show()\n print(result)", "performance": {"dataset": "IMAGENET", "accuracy": "N/A"}, "description": "The SE-ResNeXt101-32x4d is a ResNeXt101-32x4d model with added Squeeze-and-Excitation module. This model is trained with mixed precision using Tensor Cores on Volta, Turing, and the NVIDIA Ampere GPU architectures, which allows researchers to get results 3x faster than training without Tensor Cores while experiencing the benefits of mixed precision training."}}
{"code": "{'Instruction': I need a suggestion for an API that can classify images of birds and flowers., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'densenet121', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Densenet-121 model for image classification from PyTorch Hub to classify images of birds and flowers.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'densenet121', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet121', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Dense Convolutional Network", "api_name": "Densenet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet121', pretrained=True)", "api_arguments": [{"name": "densenet121", "type": "str", "description": "Densenet-121 model"}], "python_environment_requirements": {"torch": "latest", "torchvision": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet121', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"densenet121": {"Top-1 error": 25.35, "Top-5 error": 7.83}}}, "description": "Dense Convolutional Network (DenseNet) connects each layer to every other layer in a feed-forward fashion. It alleviates the vanishing-gradient problem, strengthens feature propagation, encourages feature reuse, and substantially reduces the number of parameters."}}
{"code": "{'Instruction': Identify the breed of a dog in a given image using an API., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'alexnet', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained AlexNet model from PyTorch Hub, which can be fine-tuned for dog breed classification given a dog's image.\", 'code': \"import torch\nmodel = torch.hub.load('pytorch/vision', 'alexnet', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='alexnet', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "AlexNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='alexnet', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": ">=1.9.0", "torchvision": ">=0.10.0"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"top-1_error": 43.45, "top-5_error": 20.91}}, "description": "AlexNet is a deep convolutional neural network that achieved a top-5 error of 15.3% in the 2012 ImageNet Large Scale Visual Recognition Challenge. The main contribution of the original paper was the depth of the model, which was computationally expensive but made feasible through the use of GPUs during training. The pretrained AlexNet model in PyTorch can be used for image classification tasks."}}
{"code": "{'Instruction': Recommend me an API that can create synthesized speech from text input., 'Output': {'domain': 'Text-to-Speech', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_waveglow', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained WaveGlow model from NVIDIA PyTorch Hub, which can be used in combination with the Tacotron 2 model to synthesize natural-sounding speech from text input.', 'code': 'import torch\nmodel = torch.hub.load(\"NVIDIA/DeepLearningExamples:torchhub\", \"nvidia_waveglow\", pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_waveglow', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Text-to-Speech", "framework": "PyTorch", "functionality": "Speech Synthesis", "api_name": "WaveGlow", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_waveglow', pretrained=True)", "api_arguments": {"repo_or_dir": "NVIDIA/DeepLearningExamples:torchhub", "model": "nvidia_waveglow", "model_math": "fp32"}, "python_environment_requirements": ["numpy", "scipy", "librosa", "unidecode", "inflect", "libsndfile1"], "example_code": {"load_waveglow_model": "waveglow = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_waveglow', model_math='fp32')", "prepare_waveglow_model": ["waveglow = waveglow.remove_weightnorm(waveglow)", "waveglow = waveglow.to('cuda')", "waveglow.eval()"], "load_tacotron2_model": "tacotron2 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tacotron2', model_math='fp32')", "prepare_tacotron2_model": ["tacotron2 = tacotron2.to('cuda')", "tacotron2.eval()"], "synthesize_speech": ["text = \"hello world, I missed you so much\"", "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tts_utils')", "sequences, lengths = utils.prepare_input_sequence([text])", "with torch.no_grad():", " mel, _, _ = tacotron2.infer(sequences, lengths)", " audio = waveglow.infer(mel)", "audio_numpy = audio[0].data.cpu().numpy()", "rate = 22050"], "save_audio": "write(\"audio.wav\", rate, audio_numpy)", "play_audio": "Audio(audio_numpy, rate=rate)"}, "performance": {"dataset": "LJ Speech", "accuracy": null}, "description": "The Tacotron 2 and WaveGlow model form a text-to-speech system that enables users to synthesize natural-sounding speech from raw transcripts without any additional prosody information. The Tacotron 2 model produces mel spectrograms from input text using encoder-decoder architecture. WaveGlow is a flow-based model that consumes the mel spectrograms to generate speech."}}
{"code": "{'Instruction': 'Suppose there is an English teaching website and the website wants to help students check the similarity between two given sentences. Suggest an API for that purpose.', 'Output': {'domain': 'NLP', 'api_call': \"model = torch.hub.load('pytorch/fairseq', 'roberta.large', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained RoBERTa model from PyTorch Hub. RoBERTa can be used to encode the sentences and compute the similarity by measuring the cosine distance between the encoded sentence vectors.\", 'code': 'import torch\nmodel = torch.hub.load(\"pytorch/fairseq\", \"roberta.large\", pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/fairseq', model='roberta.large', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "RoBERTa", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='pytorch/fairseq', model='roberta.large', pretrained=True)", "api_arguments": ["'pytorch/fairseq'", "'roberta.large'"], "python_environment_requirements": ["regex", "requests", "hydra-core", "omegaconf"], "example_code": ["import torch", "roberta = torch.hub.load('pytorch/fairseq', 'roberta.large')", "roberta.eval()", "tokens = roberta.encode('Hello world!')", "last_layer_features = roberta.extract_features(tokens)", "all_layers = roberta.extract_features(tokens, return_all_hiddens=True)", "roberta = torch.hub.load('pytorch/fairseq', 'roberta.large.mnli')", "roberta.eval()", "tokens = roberta.encode('Roberta is a heavily optimized version of BERT.', 'Roberta is not very optimized.')", "prediction = roberta.predict('mnli', tokens).argmax().item()", "tokens = roberta.encode('Roberta is a heavily optimized version of BERT.', 'Roberta is based on BERT.')", "prediction = roberta.predict('mnli', tokens).argmax().item()", "roberta.register_classification_head('new_task', num_classes=3)", "logprobs = roberta.predict('new_task', tokens)"], "performance": {"dataset": "MNLI", "accuracy": "N/A"}, "description": "RoBERTa is a robustly optimized version of BERT, a revolutionary self-supervised pretraining technique that learns to predict intentionally hidden (masked) sections of text. RoBERTa builds on BERT's language masking strategy and modifies key hyperparameters, including removing BERT's next-sentence pretraining objective, and training with much larger mini-batches and learning rates. RoBERTa was also trained on an order of magnitude more data than BERT, for a longer amount of time, allowing it to generalize even better to downstream tasks."}}
{"code": "{'Instruction': Recommend me an API that can classify images of different types of food., 'Output': {'domain': 'Classification', 'api_call': 'model = torch.hub.load('pytorch/vision', 'densenet169', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Densenet-169 model for image classification from PyTorch Hub, which can be fine-tuned for classifying different types of food.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'densenet169', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet169', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Dense Convolutional Network", "api_name": "Densenet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet169', pretrained=True)", "api_arguments": [{"name": "densenet169", "type": "str", "description": "Densenet-169 model"}], "python_environment_requirements": {"torch": "latest", "torchvision": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet169', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"densenet169": {"Top-1 error": 24.0, "Top-5 error": 7.0}}}, "description": "Dense Convolutional Network (DenseNet) connects each layer to every other layer in a feed-forward fashion. It alleviates the vanishing-gradient problem, strengthens feature propagation, encourages feature reuse, and substantially reduces the number of parameters."}}
{"code": "{'Instruction': Design a food recognition system that can identify different kinds of food given photos. Recommend an API that can help achieve this., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'densenet201', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Densenet-201 model from PyTorch Hub, which can be fine-tuned for food recognition and classification given images.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'densenet201', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet201', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Dense Convolutional Network", "api_name": "Densenet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet201', pretrained=True)", "api_arguments": [{"name": "densenet201", "type": "str", "description": "Densenet-201 model"}], "python_environment_requirements": {"torch": "latest", "torchvision": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet201', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"densenet201": {"Top-1 error": 22.8, "Top-5 error": 6.43}}}, "description": "Dense Convolutional Network (DenseNet) connects each layer to every other layer in a feed-forward fashion. It alleviates the vanishing-gradient problem, strengthens feature propagation, encourages feature reuse, and substantially reduces the number of parameters."}}
{"code": "{'Instruction': Recommend a neural network API for semantic segmentation of images containing objects like cars, airplanes, and people., 'Output': {'domain': 'Semantic Segmentation', 'api_call': 'model = torch.hub.load('pytorch/vision', 'fcn_resnet50', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained FCN-ResNet50 model from PyTorch Hub, which can perform semantic segmentation on images containing objects like cars, airplanes, and people.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'fcn_resnet50', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='fcn_resnet50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "Fully-Convolutional Network", "api_name": "fcn_resnet50", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='fcn_resnet50', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "boolean", "description": "If True, returns a model pre-trained on COCO train2017"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "matplotlib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'fcn_resnet50', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "input_image = input_image.convert(\"RGB\")", "preprocess = transforms.Compose([", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)['out'][0]", "output_predictions = output.argmax(0)"], "performance": {"dataset": "COCO val2017", "accuracy": {"fcn_resnet50": {"Mean IOU": 60.5, "Global Pixelwise Accuracy": 91.4}}}, "description": "FCN-ResNet is a Fully-Convolutional Network model using a ResNet-50 or a ResNet-101 backbone. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset."}}
{"code": "{'Instruction': A social media platform is building an image classifier to identify and tag the content of images. Suggest an API that can help with this task., 'Output': {'domain': 'Image Classification', 'api_call': 'torch.hub.load(\"pytorch/vision\", \"densenet161\", pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Utilize the pretrained DenseNet-161 model from the PyTorch library to classify and tag the content of images.', 'code': 'import torch\nmodel = torch.hub.load(\"pytorch/vision\", \"densenet161\", pretrained=True)\nmodel.eval()' }}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet161', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Dense Convolutional Network", "api_name": "Densenet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='densenet161', pretrained=True)", "api_arguments": [{"name": "densenet161", "type": "str", "description": "Densenet-161 model"}], "python_environment_requirements": {"torch": "latest", "torchvision": "latest"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet161', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"densenet161": {"Top-1 error": 22.35, "Top-5 error": 6.2}}}, "description": "Dense Convolutional Network (DenseNet) connects each layer to every other layer in a feed-forward fashion. It alleviates the vanishing-gradient problem, strengthens feature propagation, encourages feature reuse, and substantially reduces the number of parameters."}}
{"code": "{'Instruction': Help me find an API that can perform semantic segmentation on an image of San Francisco streets., 'Output': {'domain': 'Image Segmentation', 'api_call': 'model = torch.hub.load('pytorch/vision', 'fcn_resnet101', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained FCN-ResNet101 model from PyTorch Hub to perform semantic segmentation on an image of San Francisco streets.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'fcn_resnet101', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='fcn_resnet101', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "Fully-Convolutional Network", "api_name": "fcn_resnet101", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='fcn_resnet101', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "boolean", "description": "If True, returns a model pre-trained on COCO train2017"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "matplotlib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'fcn_resnet101', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "input_image = input_image.convert(\"RGB\")", "preprocess = transforms.Compose([", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)['out'][0]", "output_predictions = output.argmax(0)"], "performance": {"dataset": "COCO val2017", "accuracy": {"fcn_resnet101": {"Mean IOU": 63.7, "Global Pixelwise Accuracy": 91.9}}}, "description": "FCN-ResNet is a Fully-Convolutional Network model using a ResNet-50 or a ResNet-101 backbone. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset."}}
{"code": "{'Instruction': Find me an API that can efficiently classify an image with low computational cost and memory access., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet39ds', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained HarDNet model from PyTorch Hub for efficient image classification with low computational cost and memory access.', 'code': 'import torch\nmodel = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet39ds', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet39ds', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "HarDNet", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet39ds', pretrained=True)", "api_arguments": [{"name": "hardnet39ds", "type": "str", "description": "HarDNet-39DS model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet39ds', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"hardnet39ds": {"Top-1 error": 27.92, "Top-5 error": 9.57}}}, "description": "Harmonic DenseNet (HarDNet) is a low memory traffic CNN model, which is fast and efficient. The basic concept is to minimize both computational cost and memory access cost at the same time, such that the HarDNet models are 35% faster than ResNet running on GPU comparing to models with the same accuracy (except the two DS models that were designed for comparing with MobileNet)."}}
{"code": "{'Instruction': Design a recommendation system for your video upload platform. Suggest an API that can detect and classify the subject from the image of the first frame., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet68ds', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained HarDNet model for image classification from PyTorch Hub to detect and classify subjects from image frames.', 'code': 'import torch\nmodel = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet68ds', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet68ds', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "HarDNet", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet68ds', pretrained=True)", "api_arguments": [{"name": "hardnet68ds", "type": "str", "description": "HarDNet-68DS model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet68ds', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"hardnet68ds": {"Top-1 error": 25.71, "Top-5 error": 8.13}}}, "description": "Harmonic DenseNet (HarDNet) is a low memory traffic CNN model, which is fast and efficient. The basic concept is to minimize both computational cost and memory access cost at the same time, such that the HarDNet models are 35% faster than ResNet running on GPU comparing to models with the same accuracy (except the two DS models that were designed for comparing with MobileNet)."}}
{"code": "{'Instruction': Suggest an API specialized in image classification which can efficiently perform the task while consuming less memory and computational resources., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet68', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained HarDNet model from PyTorch Hub, which is efficient in terms of memory usage and computational resources while providing high-quality image classification.', 'code': 'import torch\nmodel = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet68', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet68', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "HarDNet", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet68', pretrained=True)", "api_arguments": [{"name": "hardnet68", "type": "str", "description": "HarDNet-68 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet68', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"hardnet68": {"Top-1 error": 23.52, "Top-5 error": 6.99}}}, "description": "Harmonic DenseNet (HarDNet) is a low memory trafficCNN model, which is fast and efficient. The basic concept is to minimize both computational cost and memory access cost at the same time, such that the HarDNet models are 35% faster than ResNet running on GPU comparing to models with the same accuracy (except the two DS models that were designed for comparing with MobileNet)."}}
{"code": "{'Instruction': Imagine you are creating an image recognition app. Select an API well-suited for this task, that is both fast and efficient., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet85', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained HarDNet-85 model from PyTorch Hub, an efficient and fast model for image classification, suitable for an image recognition app.', 'code': 'import torch\nmodel = torch.hub.load(\"PingoLH/Pytorch-HarDNet\", \"hardnet85\", pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet85', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "HarDNet", "api_call": "torch.hub.load(repo_or_dir='PingoLH/Pytorch-HarDNet', model='hardnet85', pretrained=True)", "api_arguments": [{"name": "hardnet85", "type": "str", "description": "HarDNet-85 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('PingoLH/Pytorch-HarDNet', 'hardnet85', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"hardnet85": {"Top-1 error": 21.96, "Top-5 error": 6.11}}}, "description": "Harmonic DenseNet (HarDNet) is a low memory traffic CNN model, which is fast and efficient. The basic concept is to minimize both computational cost and memory access cost at the same time, such that the HarDNet models are 35% faster than ResNet running on GPU comparing to models with the same accuracy (except the two DS models that were designed for comparing with MobileNet)."}}
{"code": "{'Instruction': A sports federation needs an AI-powered app that can recognize logos of soccer teams in a stadium. Give me an API that can output the name of the team, given a photo of its logo., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('huawei-noah/ghostnet', 'ghostnet_1x', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained GhostNet model from PyTorch Hub, which can be fine-tuned for soccer team logo recognition given a photo of the logo.', 'code': \"import torch\nmodel = torch.hub.load('huawei-noah/ghostnet', 'ghostnet_1x', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/ghostnet', model='ghostnet_1x', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Efficient networks by generating more features from cheap operations", "api_name": "GhostNet", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/ghostnet', model='ghostnet_1x', pretrained=True)", "api_arguments": ["pretrained"], "python_environment_requirements": ["torch", "torchvision", "PIL"], "example_code": ["import torch", "model = torch.hub.load('huawei-noah/ghostnet', 'ghostnet_1x', pretrained=True)", "model.eval()", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)", "print(probabilities)"], "performance": {"dataset": "ImageNet", "accuracy": {"Top-1 acc": "73.98", "Top-5 acc": "91.46"}}, "description": "The GhostNet architecture is based on an Ghost module structure which generates more features from cheap operations. Based on a set of intrinsic feature maps, a series of cheap operations are applied to generate many ghost feature maps that could fully reveal information underlying intrinsic features. Experiments conducted on benchmarks demonstrate the superiority of GhostNet in terms of speed and accuracy tradeoff."}}
{"code": "{'Instruction': I want to create a website that allows users to upload images and classify what's in the image. Tell me an API that can perform image classification., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'googlenet', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained GoogLeNet model for image classification from PyTorch Hub to identify and classify the contents of a user's uploaded image.\", 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'googlenet', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='googlenet', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "GoogLeNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='googlenet', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": ">=1.0.0", "torchvision": ">=0.2.2"}, "example_code": {"import": ["import torch", "import urllib", "from PIL import Image", "from torchvision import transforms"], "load_model": "model = torch.hub.load('pytorch/vision:v0.10.0', 'googlenet', pretrained=True)", "model_eval": "model.eval()", "image_preprocessing": ["input_image = Image.open(filename)", "preprocess = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)"], "model_execution": ["if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)"], "output_processing": ["probabilities = torch.nn.functional.softmax(output[0], dim=0)", "top5_prob, top5_catid = torch.topk(probabilities, 5)"]}, "performance": {"dataset": "ImageNet", "accuracy": {"Top-1 error": "30.22", "Top-5 error": "10.47"}}, "description": "GoogLeNet is based on a deep convolutional neural network architecture codenamed 'Inception', which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014)."}}
{"code": "{'Instruction': Advise an API for classifying images into their correct domain or appearance using PyTorch., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('XingangPan/IBN-Net', 'resnet101_ibn_a', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the ResNet-101-IBN-a model from PyTorch Hub for classifying images into their correct domain or appearance.', 'code': 'import torch\nmodel = torch.hub.load('XingangPan/IBN-Net', 'resnet101_ibn_a', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnet101_ibn_a', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "IBN-Net", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnet101_ibn_a', pretrained=True)", "api_arguments": [{"name": "resnet101_ibn_a", "type": "str", "description": "ResNet-101-IBN-a model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('XingangPan/IBN-Net', 'resnet101_ibn_a', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"resnet101_ibn_a": {"Top-1 acc": 78.61, "Top-5 acc": 94.41}}}, "description": "IBN-Net is a CNN model with domain/appearance invariance. Motivated by style transfer works, IBN-Net carefully unifies instance normalization and batch normalization in a single deep network. It provides a simple way to increase both modeling and generalization capacities without adding model complexity. IBN-Net is especially suitable for cross domain or person/vehicle re-identification tasks."}}
{"code": "{'Instruction': I want to implement real-time face identification and facial recognition using the ResNet-50 model with IBN-Net. Recommend an appropriate API for the task., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('XingangPan/IBN-Net', 'resnet50_ibn_a', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNet-50-IBN-a model from PyTorch Hub, which can be fine-tuned for real-time face identification and facial recognition.', 'code': 'import torch\nmodel = torch.hub.load('XingangPan/IBN-Net', 'resnet50_ibn_a', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnet50_ibn_a', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "IBN-Net", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='resnet50_ibn_a', pretrained=True)", "api_arguments": [{"name": "resnet50_ibn_a", "type": "str", "description": "ResNet-50-IBN-a model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('XingangPan/IBN-Net', 'resnet50_ibn_a', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"resnet50_ibn_a": {"Top-1 acc": 77.46, "Top-5 acc": 93.68}}}, "description": "IBN-Net is a CNN model with domain/appearance invariance. Motivated by style transfer works, IBN-Net carefully unifies instance normalization and batch normalization in a single deep network. It provides a simple way to increase both modeling and generalization capacities without adding model complexity. IBN-Net is especially suitable for cross domain or person/vehicle re-identification tasks."}}
{"code": "{'Instruction': I need an API that can classify pedestrian and vehicle images into their respective categories., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('XingangPan/IBN-Net', 'se_resnet101_ibn_a', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained IBN-Net model (SE-ResNet-101-IBN-a) from PyTorch Hub, a CNN model with domain/appearance invariance, suitable for person/vehicle re-identification tasks.', 'code': 'import torch\nmodel = torch.hub.load('XingangPan/IBN-Net', 'se_resnet101_ibn_a', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='se_resnet101_ibn_a', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "IBN-Net", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='XingangPan/IBN-Net', model='se_resnet101_ibn_a', pretrained=True)", "api_arguments": [{"name": "se_resnet101_ibn_a", "type": "str", "description": "SE-ResNet-101-IBN-a model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('XingangPan/IBN-Net', 'se_resnet101_ibn_a', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"se_resnet101_ibn_a": {"Top-1 acc": 78.75, "Top-5 acc": 94.49}}}, "description": "IBN-Net is a CNN model with domain/appearance invariance. Motivated by style transfer works, IBN-Net carefully unifies instance normalization and batch normalization in a single deep network. It provides a simple way to increase both modeling and generalization capacities without adding model complexity. IBN-Net is especially suitable for cross domain or person/vehicle re-identification tasks."}}
{"code": "{'Instruction': A company wants to classify images for their AI application, provide an API that can classify images based on the ImageNet dataset., 'Output': {'domain': 'Image Classification', 'api_call': 'model = torch.hub.load(repo_or_dir='pytorch/vision', model='inception_v3', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Inception_v3 model from PyTorch Hub, which is trained on the ImageNet dataset, to classify images.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'inception_v3', pretrained=True)\nmodel.eval()'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='inception_v3', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "Inception_v3", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='inception_v3', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": "1.9.0", "torchvision": "0.10.0"}, "example_code": {"import_libraries": "import torch", "load_model": "model = torch.hub.load('pytorch/vision:v0.10.0', 'inception_v3', pretrained=True)", "model_evaluation": "model.eval()"}, "performance": {"dataset": "imagenet", "accuracy": {"top-1_error": 22.55, "top-5_error": 6.44}}, "description": "Inception v3, also called GoogleNetv3, is a famous Convolutional Neural Network trained on the ImageNet dataset from 2015. It is based on the exploration of ways to scale up networks to utilize the added computation as efficiently as possible by using suitably factorized convolutions and aggressive regularization. The model achieves a top-1 error of 22.55% and a top-5 error of 6.44% on the ImageNet dataset."}}
{"code": "{'Instruction': A wildlife conservatory organization wants to classify different species of birds using image recognition. Suggest an API that can classify high-resolution bird images., 'Output': {'domain': 'Image Classification', 'api_call': 'model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest101', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNeSt-101 model from PyTorch Hub for high-resolution image classification, which can be fine-tuned to classify bird species in wildlife images.', 'code': 'import torch\nmodel = torch.hub.load('zhanghang1989/ResNeSt', 'resnest101', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest101', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNeSt", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest101', pretrained=True)", "api_arguments": "resnest101", "python_environment_requirements": {"torch": "1.0.0", "torchvision": "0.2.2"}, "example_code": ["import torch", "model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest101', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeSt-101": 82.83}}, "description": "ResNeSt models are from the ResNeSt: Split-Attention Networks paper. They are a new ResNet variant that enables attention across feature-map groups. By stacking Split-Attention blocks ResNet-style, ResNeSt models outperform other networks with similar model complexities, and also help downstream tasks including object detection, instance segmentation, and semantic segmentation."}}
{"code": "{'Instruction': Can you suggest an API that can classify images into different categories like animals, objects, etc.?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest269', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNeSt model from PyTorch Hub for image classification into diverse categories, such as animals and objects.', 'code': 'import torch\nmodel = torch.hub.load('zhanghang1989/ResNeSt', 'resnest269', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest269', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNeSt", "api_call": "torch.hub.load(repo_or_dir='zhanghang1989/ResNeSt', model='resnest269', pretrained=True)", "api_arguments": "resnest269", "python_environment_requirements": {"torch": "1.0.0", "torchvision": "0.2.2"}, "example_code": ["import torch", "model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest269', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"ResNeSt-269": 84.54}}, "description": "ResNeSt models are from the ResNeSt: Split-Attention Networks paper. They are a new ResNet variant that enables attention across feature-map groups. By stacking Split-Attention blocks ResNet-style, ResNeSt models outperform other networks with similar model complexities, and also help downstream tasks including object detection, instance segmentation, and semantic segmentation."}}
{"code": "{'Instruction': Recommend me an API that can optimize image classification for varying hardware platforms., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_cpu', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained ProxylessNAS model from PyTorch Hub, optimized for different hardware platforms to improve image classification performance.\", 'code': 'import torch\ntarget_platform = \"proxylessnas_cpu\"\nmodel = torch.hub.load(\"mit-han-lab/ProxylessNAS\", target_platform, pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_cpu', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "ProxylessNAS", "api_name": "mit-han-lab/ProxylessNAS", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_cpu', pretrained=True)", "api_arguments": [{"name": "proxylessnas_cpu", "type": "str", "description": "ProxylessNAS optimized for CPU"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "target_platform = 'proxyless_cpu'", "model = torch.hub.load('mit-han-lab/ProxylessNAS', target_platform, pretrained=True)", "model.eval()"], "performance": {"dataset": [{"model_structure": "proxylessnas_cpu", "accuracy": 75.3}]}, "description": "ProxylessNAS models are from the ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware paper. They specialize CNN architectures for different hardware platforms, offering free yet significant performance boost on all three platforms (CPU, GPU, and Mobile) with similar accuracy."}}
{"code": "{'Instruction': I work at a company that needs an optimized model for image classification on GPU with high accuracy. Can you suggest a suitable API for me?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_gpu', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'For an optimized image classification model on GPU with high accuracy, consider using the pre-trained ProxylessNAS model from the MIT-HAN lab.', 'code': 'import torch\nmodel = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_gpu', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_gpu', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "ProxylessNAS", "api_name": "mit-han-lab/ProxylessNAS", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_gpu', pretrained=True)", "api_arguments": [{"name": "proxylessnas_gpu", "type": "str", "description": "ProxylessNAS optimized for GPU"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "target_platform = 'proxyless_gpu'", "model = torch.hub.load('mit-han-lab/ProxylessNAS', target_platform, pretrained=True)", "model.eval()"], "performance": {"dataset": [{"model_structure": "proxylessnas_gpu", "accuracy": 75.1}]}, "description": "ProxylessNAS models are from the ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware paper. They specialize CNN architectures for different hardware platforms, offering free yet significant performance boost on all three platforms (CPU, GPU, and Mobile) with similar accuracy."}}
{"code": "{'Instruction': I want an API that can handle object detection on mobile devices. It should be light-weight and efficient., 'Output': {'domain': 'Object Detection', 'api_call': \"model = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_mobile', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ProxylessNAS Mobile model, optimized for object detection on mobile devices. It is lightweight and efficient.', 'code': 'import torch\nmodel = torch.hub.load('mit-han-lab/ProxylessNAS', 'proxylessnas_mobile', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_mobile', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "ProxylessNAS", "api_name": "mit-han-lab/ProxylessNAS", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/ProxylessNAS', model='proxylessnas_mobile', pretrained=True)", "api_arguments": [{"name": "proxylessnas_mobile", "type": "str", "description": "ProxylessNAS optimized for Mobile"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "target_platform = 'proxyless_mobile'", "model = torch.hub.load('mit-han-lab/ProxylessNAS', target_platform, pretrained=True)", "model.eval()"], "performance": {"dataset": [{"model_structure": "proxylessnas_mobile", "accuracy": 74.6}]}, "description": "ProxylessNAS models are from the ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware paper. They specialize CNN architectures for different hardware platforms, offering free yet significant performance boost on all three platforms (CPU, GPU, and Mobile) with similar accuracy."}}
{"code": "{'Instruction': Can you suggest me an API that can classify objects from images?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'resnet50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNet50 model from PyTorch Hub, designed for image classification tasks, including object recognition in images.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'resnet50', pretrained=True)\nmodel.eval()'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Deep Residual Networks", "api_name": "ResNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet50', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "bool", "default": "False", "description": "If True, returns a model pre-trained on ImageNet"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "resnet50", "top-1_error": 23.85, "top-5_error": 7.13}}, "description": "ResNet models are deep residual networks pre-trained on ImageNet. They were proposed in the paper 'Deep Residual Learning for Image Recognition'. Available model variants include ResNet18, ResNet34, ResNet50, ResNet101, and ResNet152."}}
{"code": "{'Instruction': A software engineer at TikTok is developing a recommendation system for videos. He needs an API that can recognize content of an image associated with a video., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'resnet18', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pre-trained ResNet18 model from PyTorch Hub for image classification, which can be used to recognize the content of an image associated with a video.\", 'code': 'import torch\nmodel = torch.hub.load(\"pytorch/vision\", \"resnet18\", pretrained=True)\nmodel.eval()'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet18', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Deep Residual Networks", "api_name": "ResNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet18', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "bool", "default": "False", "description": "If True, returns a model pre-trained on ImageNet"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "resnet18", "top-1_error": 30.24, "top-5_error": 10.92}}, "description": "ResNet models are deep residual networks pre-trained on ImageNet. They were proposed in the paper 'Deep Residual Learning for Image Recognition'. Available model variants include ResNet18, ResNet34, ResNet50, ResNet101, and ResNet152."}}
{"code": "{'Instruction': I am building an image identification service that recognizes objects in images. I need an API suitable for this task., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'resnet34', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pre-trained ResNet34 model from PyTorch Hub to perform object recognition in images.\", 'code': \"import torch\nmodel = torch.hub.load('pytorch/vision', 'resnet34', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet34', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Deep Residual Networks", "api_name": "ResNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet34', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "bool", "default": "False", "description": "If True, returns a model pre-trained on ImageNet"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet34', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "resnet34", "top-1_error": 26.7, "top-5_error": 8.58}}, "description": "ResNet models are deep residual networks pre-trained on ImageNet. They were proposed in the paper 'Deep Residual Learning for Image Recognition'. Available model variants include ResNet18, ResNet34, ResNet50, ResNet101, and ResNet152."}}
{"code": "{'Instruction': Build an API to identify the object in the image by its class., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'resnet152', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNet model from PyTorch Hub to classify objects in images and identify their classes.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'resnet152', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet152', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Deep Residual Networks", "api_name": "ResNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet152', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "bool", "default": "False", "description": "If True, returns a model pre-trained on ImageNet"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet152', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "resnet152", "top-1_error": 21.69, "top-5_error": 5.94}}, "description": "ResNet models are deep residual networks pre-trained on ImageNet. They were proposed in the paper 'Deep Residual Learning for Image Recognition'. Available model variants include ResNet18, ResNet34, ResNet50, ResNet101, and ResNet152."}}
{"code": "{'Instruction': I want an API to label which type of object is in a given picture: car, dog, tree or house. Provide an API that can do this assignment., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'resnet101', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNet-101 model from PyTorch Hub, which can be fine-tuned for object classification into car, dog, tree, or house.', 'code': \"import torch\nmodel = torch.hub.load('pytorch/vision', 'resnet101', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet101', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Deep Residual Networks", "api_name": "ResNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnet101', pretrained=True)", "api_arguments": [{"name": "pretrained", "type": "bool", "default": "False", "description": "If True, returns a model pre-trained on ImageNet"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet101', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "resnet101", "top-1_error": 22.63, "top-5_error": 6.44}}, "description": "ResNet models are deep residual networks pre-trained on ImageNet. They were proposed in the paper 'Deep Residual Learning for Image Recognition'. Available model variants include ResNet18, ResNet34, ResNet50, ResNet101, and ResNet152."}}
{"code": "{'Instruction': An image classification tool needs to be integrated into a real time webcam application that can detect different objects. Suggest me an API that can do this with low latency for a real time application., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'mobilenet_v2', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MobileNet v2 model for image classification from PyTorch Hub to perform object detection in real-time webcam applications with low latency.', 'code': 'import torch\nimport torchvision.transforms as transforms\nfrom PIL import Image\n\nmodel = torch.hub.load('pytorch/vision', 'mobilenet_v2', pretrained=True)\nmodel.eval()\n\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='mobilenet_v2', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MobileNet v2", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='mobilenet_v2', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision', 'mobilenet_v2', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)"], "performance": {"dataset": "ImageNet", "accuracy": {"top-1_error": 28.12, "top-5_error": 9.71}}, "description": "The MobileNet v2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input. MobileNet v2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, non-linearities in the narrow layers were removed in order to maintain representational power."}}
{"code": "{'Instruction': We are developing an image-based plant disease identification system. Give me an image classification API that yields high accuracy and can be fine-tuned to our task., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'resnext101_32x8d', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ResNext101_32x8d model from PyTorch Hub, which provides high accuracy and can be fine-tuned for plant disease identification.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'resnext101_32x8d', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnext101_32x4d', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ResNext", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='resnext101_32x4d', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "PIL"], "example_code": ["import torch", "from PIL import Image", "from torchvision import transforms", "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnext101_32x4d', pretrained=True)", "model.eval()", "input_image = Image.open('dog.jpg')", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)"], "performance": {"dataset": "ImageNet", "resnext101_32x8d": {"top-1": 20.69, "top-5": 5.47}}, "description": "ResNext is a next-generation ResNet architecture for image classification. It is more efficient and accurate than the original ResNet. This implementation includes two versions of the model, resnext50_32x4d and resnext101_32x8d, with 50 and 101 layers respectively."}}
{"code": "{'Instruction': Suggest me a powerful image classification API for efficient image classification with decent accuracy., 'Output': {'domain': 'Image Classification', 'api_call': 'model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_s', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the SNNMLP model from PyTorch Hub with pre-trained weights for efficient image classification with high accuracy.', 'code': 'import torch\nmodel = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_s', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_s', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SNNMLP", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_s', pretrained=True)", "api_arguments": [{"name": "snnmlp_s", "type": "str", "description": "SNNMLP Small model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_s', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "SNNMLP Small", "top-1": 83.3}}, "description": "SNNMLP incorporates the mechanism of LIF neurons into the MLP models, to achieve better accuracy without extra FLOPs. We propose a full-precision LIF operation to communicate between patches, including horizontal LIF and vertical LIF in different directions. We also propose to use group LIF to extract better local features. With LIF modules, our SNNMLP model achieves 81.9%, 83.3% and 83.6% top-1 accuracy on ImageNet dataset with only 4.4G, 8.5G and 15.2G FLOPs, respectively."}}
{"code": "{'Instruction': We're at Huawei Labs and building an image classification model that does not have extra floating-point operations. Suggest an API that does this task., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_b', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained SNNMLP Base model from PyTorch Hub, which incorporates LIF neurons and achieves better accuracy without additional FLOPs for image classification.\", 'code': 'import torch\nmodel = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_b', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_b', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SNNMLP", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_b', pretrained=True)", "api_arguments": [{"name": "snnmlp_b", "type": "str", "description": "SNNMLP Base model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_b', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "SNNMLP Base", "top-1": 85.59}}, "description": "SNNMLP incorporates the mechanism of LIF neurons into the MLP models, to achieve better accuracy without extra FLOPs. We propose a full-precision LIF operation to communicate between patches, including horizontal LIF and vertical LIF in different directions. We also propose to use group LIF to extract better local features. With LIF modules, our SNNMLP model achieves 81.9%, 83.3% and 83.6% top-1 accuracy on ImageNet dataset with only 4.4G, 8.5G and 15.2G FLOPs, respectively."}}
{"code": "{'Instruction': Recommend me a model that can classify objects within images without increasing computational complexity considerably., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_t', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the SNNMLP Tiny model from PyTorch Hub, which incorporates LIF neurons into MLP models for better accuracy without extra FLOPs, making it efficient for object classification.\", 'code': \"import torch\nmodel = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_t', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_t', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SNNMLP", "api_call": "torch.hub.load(repo_or_dir='huawei-noah/Efficient-AI-Backbones', model='snnmlp_t', pretrained=True)", "api_arguments": [{"name": "snnmlp_t", "type": "str", "description": "SNNMLP Tiny model"}], "python_environment_requirements": ["torch", "torchvision", "PIL", "urllib"], "example_code": ["import torch", "model = torch.hub.load('huawei-noah/Efficient-AI-Backbones', 'snnmlp_t', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "print(torch.nn.functional.softmax(output[0], dim=0))"], "performance": {"dataset": "ImageNet", "accuracy": {"model": "SNNMLP Tiny", "top-1": 81.88}}, "description": "SNNMLP incorporates the mechanism of LIF neurons into the MLP models, to achieve better accuracy without extra FLOPs. We propose a full-precision LIF operation to communicate between patches, including horizontal LIF and vertical LIF in different directions. We also propose to use group LIF to extract better local features. With LIF modules, our SNNMLP model achieves 81.9%, 83.3% and 83.6% top-1 accuracy on ImageNet dataset with only 4.4G, 8.5G and 15.2G FLOPs, respectively."}}
{"code": "{'Instruction': Find me an AI model capable of classifying images efficiently with fewer parameters but still maintaining high accuracy., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'squeezenet1_0', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained SqueezeNet model from PyTorch Hub. SqueezeNet is an image classification model that achieves competitive accuracy with fewer parameters, making it efficient for use.\", 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'squeezenet1_0', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='squeezenet1_0', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SqueezeNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='squeezenet1_0', pretrained=True)", "api_arguments": {"version": "v0.10.0", "model": ["squeezenet1_0"], "pretrained": "True"}, "python_environment_requirements": {"torch": ">=1.9.0", "torchvision": ">=0.10.0"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'squeezenet1_0', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)", "print(probabilities)"], "performance": {"dataset": "ImageNet", "accuracy": {"squeezenet1_0": {"Top-1 error": 41.9, "Top-5 error": 19.58}}}, "description": "SqueezeNet is an image classification model that achieves AlexNet-level accuracy with 50x fewer parameters. It has two versions: squeezenet1_0 and squeezenet1_1, with squeezenet1_1 having 2.4x less computation and slightly fewer parameters than squeezenet1_0, without sacrificing accuracy."}}
{"code": "{'Instruction': Can you recommend an API that can classify images based on their content and requires less computational resources compared to AlexNet?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'squeezenet1_1', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'SqueezeNet is an image classification model that has fewer parameters and requires less computation compared to AlexNet while maintaining similar accuracy.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'squeezenet1_1', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='squeezenet1_1', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "SqueezeNet", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='squeezenet1_1', pretrained=True)", "api_arguments": {"version": "v0.10.0", "model": ["squeezenet1_1"], "pretrained": "True"}, "python_environment_requirements": {"torch": ">=1.9.0", "torchvision": ">=0.10.0"}, "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'squeezenet1_1', pretrained=True)", "model.eval()", "from PIL import Image", "from torchvision import transforms", "input_image = Image.open(filename)", "preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)", "if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)", "probabilities = torch.nn.functional.softmax(output[0], dim=0)", "print(probabilities)"], "performance": {"dataset": "ImageNet", "accuracy": {"squeezenet1_1": {"Top-1 error": 41.81, "Top-5 error": 19.38}}}, "description": "SqueezeNet is an image classification model that achieves AlexNet-level accuracy with 50x fewer parameters. It has two versions: squeezenet1_0 and squeezenet1_1, with squeezenet1_1 having 2.4x less computation and slightly fewer parameters than squeezenet1_0, without sacrificing accuracy."}}
{"code": "{'Instruction': Provide me an API that can efficiently classify images into different categories., 'Output': {'domain': 'Image Classification', 'api_call': 'model = torch.hub.load('pytorch/vision', 'shufflenet_v2_x1_0', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained ShuffleNet v2 model from PyTorch Hub, optimized for speed and memory, to efficiently classify images into different categories.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'shufflenet_v2_x1_0', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='shufflenet_v2_x1_0', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "ShuffleNet v2", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='shufflenet_v2_x1_0', pretrained=True)", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"torch": "torch", "torchvision": "torchvision", "PIL": "Image", "urllib": "urllib"}, "example_code": {"import_libraries": ["import torch", "from PIL import Image", "from torchvision import transforms", "import urllib"], "load_model": ["model = torch.hub.load('pytorch/vision:v0.10.0', 'shufflenet_v2_x1_0', pretrained=True)", "model.eval()"], "load_image": ["url, filename = ('https://github.com/pytorch/hub/raw/master/images/dog.jpg', 'dog.jpg')", "try: urllib.URLopener().retrieve(url, filename)", "except: urllib.request.urlretrieve(url, filename)", "input_image = Image.open(filename)"], "preprocess_image": ["preprocess = transforms.Compose([", " transforms.Resize(256),", " transforms.CenterCrop(224),", " transforms.ToTensor(),", " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),", "])", "input_tensor = preprocess(input_image)", "input_batch = input_tensor.unsqueeze(0)"], "run_inference": ["if torch.cuda.is_available():", " input_batch = input_batch.to('cuda')", " model.to('cuda')", "with torch.no_grad():", " output = model(input_batch)"], "get_probabilities": ["probabilities = torch.nn.functional.softmax(output[0], dim=0)"], "top_categories": ["top5_prob, top5_catid = torch.topk(probabilities, 5)", "for i in range(top5_prob.size(0)):", " print(categories[top5_catid[i]], top5_prob[i].item())"]}, "performance": {"dataset": "Imagenet", "accuracy": {"top-1_error": 30.64, "top-5_error": 11.68}}, "description": "ShuffleNet V2 is an efficient ConvNet optimized for speed and memory, pre-trained on Imagenet. It is designed based on practical guidelines for efficient network design, including speed and accuracy tradeoff."}}
{"code": "{'Instruction': Recommend an API to classify artwork images into categories like Impressionism, Cubism, and Surrealism., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg11', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained VGG11 model from PyTorch Hub, which can be fine-tuned for artwork classification into categories like Impressionism, Cubism, and Surrealism.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg11', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg11', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg11', pretrained=True)", "api_arguments": [{"name": "vgg11", "type": "str", "description": "VGG11 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg11', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg11": {"Top-1 error": 30.98, "Top-5 error": 11.37}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': \"A client wants to recognize different types of plant diseases from an image. Suggest an appropriate machine learning API that can accomplish this.\", 'Output': {'domain': 'Image Recognition', 'api_call': 'model = torch.hub.load(\\'pytorch/vision\\', \\'vgg11_bn\\', pretrained=True)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained VGG11 model with batch normalization using PyTorch Hub, which can be fine-tuned for plant disease recognition from image data.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg11_bn', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg11_bn', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg11_bn', pretrained=True)", "api_arguments": [{"name": "vgg11_bn", "type": "str", "description": "VGG11 model with batch normalization"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg11_bn', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg11_bn": {"Top-1 error": 26.7, "Top-5 error": 8.58}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': What is an API that I can use to identify what kind of food is in an image?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg13_bn', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained VGG13 model with batch normalization from PyTorch Hub, which can be fine-tuned for food identification in images.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg13_bn', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg13_bn', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg13_bn', pretrained=True)", "api_arguments": [{"name": "vgg13_bn", "type": "str", "description": "VGG13 model with batch normalization"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg13_bn', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg13_bn": {"Top-1 error": 28.45, "Top-5 error": 9.63}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': Describe an API that can identify different breeds of dogs in a given image., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg19', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained VGG19 model from PyTorch Hub, which can be fine-tuned to identify different dog breeds in a given image.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg19', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg19', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg19', pretrained=True)", "api_arguments": [{"name": "vgg19", "type": "str", "description": "VGG19 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg19', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg19": {"Top-1 error": 27.62, "Top-5 error": 9.12}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': An art gallery is interested in recognizing artwork styles in photographs. Advise them an API to accomplish this task., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg19_bn', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained VGG19 with batch normalization from PyTorch Hub, which can be fine-tuned for artwork style recognition in photographs.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg19_bn', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg19_bn', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg19_bn', pretrained=True)", "api_arguments": [{"name": "vgg19_bn", "type": "str", "description": "VGG19 model with batch normalization"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg19_bn', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg19_bn": {"Top-1 error": 25.76, "Top-5 error": 8.15}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': I need to find an API to recognize objects in photos, what would you suggest?, 'Output': {'domain': 'Image Recognition', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg16', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained VGG-16 model from PyTorch Hub for object recognition in images.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg16', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg16', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg16', pretrained=True)", "api_arguments": [{"name": "vgg16", "type": "str", "description": "VGG16 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg16": {"Top-1 error": 28.41, "Top-5 error": 9.62}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': An online glasses store needs an AI tool to categorize thousands of glasses into their different styles. Provide a suitable image recognition API for this purpose., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg13', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained VGG13 model from PyTorch Hub, which can be fine-tuned for the categorization of different glasses styles in an online store.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'vgg13', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg13', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg13', pretrained=True)", "api_arguments": [{"name": "vgg13", "type": "str", "description": "VGG13 model"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg13', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg13": {"Top-1 error": 30.07, "Top-5 error": 10.75}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn,vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': Find me a deep learning API that can recognize images of cars and their types from large datasets like Imagenet., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'vgg16_bn', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained VGG16 model with batch normalization from PyTorch Hub, which is suitable for large-scale image recognition tasks like recognizing car types from the ImageNet dataset.\", 'code': 'import torch\nmodel = torch.hub.load(\"pytorch/vision\", \"vgg16_bn\", pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg16_bn', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Recognition", "api_name": "vgg-nets", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='vgg16_bn', pretrained=True)", "api_arguments": [{"name": "vgg16_bn", "type": "str", "description": "VGG16 model with batch normalization"}], "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16_bn', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"vgg16_bn": {"Top-1 error": 26.63, "Top-5 error": 8.5}}}, "description": "vgg-nets are award-winning ConvNets from the 2014 Imagenet ILSVRC challenge. They are used for large-scale image recognition tasks. The available models are vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, and vgg19_bn."}}
{"code": "{'Instruction': Suggest an optimal API for building an image recognition system for a company that demands high accuracy., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'wide_resnet50_2', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Wide ResNet-50 model for image recognition, offering high accuracy for various image classification tasks.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'wide_resnet50_2', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='wide_resnet50_2', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Wide Residual Networks", "api_name": "wide_resnet50_2", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='wide_resnet50_2', pretrained=True)", "api_arguments": "pretrained", "python_environment_requirements": "torch, torchvision", "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'wide_resnet50_2', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"wide_resnet50_2": {"Top-1 error": 21.49, "Top-5 error": 5.91}}}, "description": "Wide Residual networks simply have increased number of channels compared to ResNet. Otherwise the architecture is the same. Deeper ImageNet models with bottleneck block have increased number of channels in the inner 3x3 convolution. The wide_resnet50_2 and wide_resnet101_2 models were trained in FP16 with mixed precision training using SGD with warm restarts. Checkpoints have weights in half precision (except batch norm) for smaller size, and can be used in FP32 models too."}}
{"code": "{'Instruction': What API should I use for my drone's object detertion that classify objects from images using Wide Residual Networks?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('pytorch/vision', 'wide_resnet101_2', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained Wide ResNet-101-2 model from PyTorch Hub for image classification in object detection scenarios on drone-acquired images.', 'code': 'import torch\nmodel = torch.hub.load('pytorch/vision', 'wide_resnet101_2', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='wide_resnet101_2', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Wide Residual Networks", "api_name": "wide_resnet101_2", "api_call": "torch.hub.load(repo_or_dir='pytorch/vision', model='wide_resnet101_2', pretrained=True)", "api_arguments": "pretrained", "python_environment_requirements": "torch, torchvision", "example_code": ["import torch", "model = torch.hub.load('pytorch/vision:v0.10.0', 'wide_resnet101_2', pretrained=True)", "model.eval()"], "performance": {"dataset": "ImageNet", "accuracy": {"wide_resnet101_2": {"Top-1 error": 21.16, "Top-5 error": 5.72}}}, "description": "Wide Residual networks simply have increased number of channels compared to ResNet. Otherwise the architecture is the same. Deeper ImageNet models with bottleneck block have increased number of channels in the inner 3x3 convolution. The wide_resnet50_2 and wide_resnet101_2 models were trained in FP16 with mixed precision training using SGD with warm restarts. Checkpoints have weights in half precision (except batch norm) for smaller size, and can be used in FP32 models too."}}
{"code": "{'Instruction': A video analyst is working on recognizing sports clips to create a promotional video. Which API should they use to classify actions in the video?, 'Output': {'domain': 'Video Classification', 'api_call': \"model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)\", 'api_provider': 'PyTorchVideo', 'explanation': \"Load the SlowFast Networks model from PyTorchVideo, which is pretrained on the Kinetics 400 dataset and able to classify actions in a video.\", 'code': 'import torch\nmodel = torch.hub.load(\\'facebookresearch/pytorchvideo\\', \\'slowfast_r50\\', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='slowfast_r50', pretrained=True)", "provider": "PyTorchVideo", "api_data": {"domain": "Video Classification", "framework": "PyTorchVideo", "functionality": "SlowFast Networks", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='slowfast_r50', pretrained=True)", "api_arguments": {"repository": "facebookresearch/pytorchvideo", "model": "slowfast_r50", "pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "pytorchvideo"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)", "device = 'cpu'", "model = model.eval()", "model = model.to(device)"], "performance": {"dataset": "Kinetics 400", "accuracy": {"top1": 76.94, "top5": 92.69}, "flops": 65.71, "params": 34.57}, "description": "Slow Fast model architectures are based on the paper 'SlowFast Networks for Video Recognition' by Christoph Feichtenhofer et al. They are pretrained on the Kinetics 400 dataset using the 8x8 setting. This model is capable of classifying video clips into different action categories. It is provided by the FAIR PyTorchVideo library."}}
{"code": "{'Instruction': I want to create a program that analyzes CCTV footage and classifies suspicious activities. Help me with an API that can do this., 'Output': {'domain': 'Video Classification', 'api_call': \"model = torch.hub.load('facebookresearch/pytorchvideo', 'x3d_s', pretrained=True)\", 'api_provider': 'PyTorchVideo', 'explanation': \"Load the pretrained X3D model from PyTorchVideo Hub for video classification, which can help identify suspicious activities in CCTV footage.\", 'code': \"import torch\nmodel = torch.hub.load('facebookresearch/pytorchvideo', 'x3d_s', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='x3d_s', pretrained=True)", "provider": "PyTorchVideo", "api_data": {"domain": "Video Classification", "framework": "PyTorchVideo", "functionality": "X3D Networks", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/pytorchvideo', model='x3d_s', pretrained=True)", "api_arguments": {"repository": "facebookresearch/pytorchvideo", "model": "x3d_s", "pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision", "pytorchvideo"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/pytorchvideo', 'x3d_s', pretrained=True)", "device = 'cpu'", "model = model.eval()", "model = model.to(device)"], "performance": {"dataset": "Kinetics 400", "accuracy": {"top1": 73.33, "top5": 91.27}, "flops": 2.96, "params": 3.79}, "description": "X3D model architectures are based on the paper 'X3D: Expanding Architectures for Efficient Video Recognition' by Christoph Feichtenhofer. They are pretrained on the Kinetics 400 dataset. This model is capable of classifying video clips into different action categories. It is provided by the FAIR PyTorchVideo library."}}
{"code": "{'Instruction': Find me an API that helps in classifying images based on their content using models trained with limited labeled data., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models', 'resnet18_swsl', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained ResNet-18 model from Facebook Research's Semi-Supervised ImageNet1K Models' collection, designed for image classification using models trained with limited labeled data.\", 'code': 'import torch\nmodel = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models', 'resnet18_swsl', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/semi-supervised-ImageNet1K-models', model='resnet18_swsl', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Semi-supervised and semi-weakly supervised ImageNet Models", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='facebookresearch/semi-supervised-ImageNet1K-models', model='resnet18_swsl', pretrained=True)", "api_arguments": {"repository": "facebookresearch/semi-supervised-ImageNet1K-models", "model": "resnet18_swsl", "pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "model = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models', 'resnet18_swsl', pretrained=True)", "model.eval()"], "performance": {"description": "Semi-supervised and semi-weakly supervised ImageNet models achieve state-of-the-art accuracy of 81.2% on ImageNet for the widely used/adopted ResNet-50 model architecture."}, "description": "Semi-supervised and semi-weakly supervised ImageNet Models are introduced in the 'Billion scale semi-supervised learning for image classification' paper. These models are pretrained on a subset of unlabeled YFCC100M public image dataset and fine-tuned with the ImageNet1K training dataset. They are capable of classifying images into different categories and are provided by the Facebook Research library."}}
{"code": "{'Instruction': I am building an app for classifying pet images into dog breeds using neural networks. Can you recommend a suitable API to use?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('mit-han-lab/once-for-all', 'ofa_supernet_mbv3_w10', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained OFA SuperNet MobileNetV3 from PyTorch Hub, which can be fine-tuned for classifying pet images into different dog breeds.', 'code': 'import torch\nmodel = torch.hub.load('mit-han-lab/once-for-all', 'ofa_supernet_mbv3_w10', pretrained=True)'}}", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/once-for-all', model='ofa_supernet_mbv3_w10', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Once-for-all (OFA) Networks", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='mit-han-lab/once-for-all', model='ofa_supernet_mbv3_w10', pretrained=True)", "api_arguments": {"repository": "mit-han-lab/once-for-all", "model": "ofa_supernet_mbv3_w10", "pretrained": "True"}, "python_environment_requirements": ["torch", "torchvision"], "example_code": ["import torch", "super_net_name = 'ofa_supernet_mbv3_w10'", "super_net = torch.hub.load('mit-han-lab/once-for-all', super_net_name, pretrained=True).eval()"], "performance": {"description": "OFA networks outperform state-of-the-art NAS methods (up to 4.0% ImageNet top1 accuracy improvement over MobileNetV3, or same accuracy but 1.5x faster than MobileNetV3, 2.6x faster than EfficientNet w.r.t measured latency) while reducing many orders of magnitude GPU hours and CO2 emission."}, "description": "Once-for-all (OFA) networks are a family of neural networks designed by MIT Han Lab. They decouple training and search, achieving efficient inference across various edge devices and resource constraints. OFA networks are pretrained on the IMAGENET dataset and are capable of classifying images into different categories."}}
{"code": "{'Instruction': I work at NVIDIA and want to find an API which can help me classify images based on their content using a GPU-optimized neural network., 'Output': {'domain': 'Classification', 'api_call': 'gpunet = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_gpunet', pretrained=True, model_type=model_type, model_math=precision)', 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained GPUNet model from NVIDIA PyTorch Hub to classify images. This model is optimized for NVIDIA GPU and TensorRT performance.', 'code': 'import torch\nmodel_type = 'GPUNet-0'\nprecision = 'fp32'\ngpunet = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_gpunet', pretrained=True, model_type=model_type, model_math=precision)'}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_gpunet', pretrained=True, model_type=model_type, model_math=precision)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "GPUNet Networks", "api_name": "torch.hub.load", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_gpunet', pretrained=True, model_type=model_type, model_math=precision)", "api_arguments": {"repository": "NVIDIA/DeepLearningExamples:torchhub", "model": "nvidia_gpunet", "pretrained": "True", "model_type": "GPUNet-0", "model_math": "fp32"}, "python_environment_requirements": ["torch", "validators", "matplotlib", "timm==0.5.4"], "example_code": ["import torch", "model_type = 'GPUNet-0'", "precision = 'fp32'", "gpunet = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_gpunet', pretrained=True, model_type=model_type, model_math=precision)", "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')", "gpunet.to(device)", "gpunet.eval()"], "performance": {"dataset": "IMAGENET", "description": "GPUNet demonstrates state-of-the-art inference performance up to 2x faster than EfficientNet-X and FBNet-V3."}, "description": "GPUNet is a family of Convolutional Neural Networks designed by NVIDIA using novel Neural Architecture Search (NAS) methods. They are optimized for NVIDIA GPU and TensorRT performance. GPUNet models are pretrained on the IMAGENET dataset and are capable of classifying images into different categories. The models are provided by the NVIDIA Deep Learning Examples library."}}
{"code": "{'Instruction': Find me an API that can tokenize English text and perform NLP tasks like text summarization, translation, or question answering., 'Output': {'domain': 'Natural Language Processing', 'api_call': 'tokenizer = torch.hub.load(\\'huggingface/pytorch-transformers\\', \\'tokenizer\\', \\'bert-base-cased\\')\nmodel = torch.hub.load(\\'huggingface/pytorch-transformers\\', \\'model\\', \\'bert-base-cased\\')', 'api_provider': 'PyTorch', 'explanation': 'Use the PyTorch-Transformers library by Hugging Face, which provides pre-trained models like BERT, GPT, and RoBERTa for tokenization and NLP tasks such as summarization, translation, and question answering.', 'code': 'import torch\ntokenizer = torch.hub.load(\\'huggingface/pytorch-transformers\\', \\'tokenizer\\', \\'bert-base-cased\\')\nmodel = torch.hub.load(\\'huggingface/pytorch-transformers\\', \\'model\\', \\'bert-base-cased\\')'}}", "api_call": "torch.hub.load(repo_or_dir='huggingface/pytorch-transformers')", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Natural Language Processing", "api_name": "PyTorch-Transformers", "api_call": "torch.hub.load(repo_or_dir='huggingface/pytorch-transformers')", "api_arguments": ["pretrained_model_or_path", "output_attention", "output_hidden_states", "config", "from_tf"], "python_environment_requirements": ["tqdm", "boto3", "requests", "regex", "sentencepiece", "sacremoses"], "example_code": "import torch\ntokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-cased')\n\ntext_1 = \"Jim Henson was a puppeteer\"\ntext_2 = \"Who was Jim Henson ?\"\n\nindexed_tokens = tokenizer.encode(text_1, text_2, add_special_tokens=True)\n\nmodel = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-cased')\n\nwith torch.no_grad():\n encoded_layers, _ = model(tokens_tensor, token_type_ids=segments_tensors)", "performance": {"dataset": [{"name": "MRPC", "accuracy": "Not provided"}]}, "description": "PyTorch-Transformers is a library of state-of-the-art pre-trained models for Natural Language Processing (NLP) including BERT, GPT, GPT-2, Transformer-XL, XLNet, XLM, RoBERTa, and DistilBERT. The library provides functionality for tokenization, configuration, and various model architectures for different tasks such as causal language modeling, sequence classification, question answering, and masked language modeling."}}
{"code": "{'Instruction': A language teacher wants to check the grammar of some sentences by translating them back and forth between languages. Provide an API that can handle neural machine translation for this purpose., 'Output': {'domain': 'Natural Language Processing', 'api_call': \"model = torch.hub.load('pytorch/fairseq', 'transformer.wmt14.en-fr', tokenizer='moses', bpe='subword_nmt')\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained Transformer (NMT) model from PyTorch Hub, which can handle neural machine translation. This will help the language teacher to check grammar by translating sentences back and forth between languages.\", 'code': \"import torch\nmodel = torch.hub.load('pytorch/fairseq', 'transformer.wmt14.en-fr', tokenizer='moses', bpe='subword_nmt')\"}}", "api_call": "torch.hub.load(repo_or_dir='pytorch/fairseq')", "provider": "PyTorch", "api_data": {"domain": "Semantic Segmentation", "framework": "PyTorch", "functionality": "Neural Machine Translation", "api_name": "Transformer (NMT)", "api_call": "torch.hub.load(repo_or_dir='pytorch/fairseq')", "api_arguments": ["model_name", "tokenizer", "bpe", "beam", "sampling", "sampling_topk"], "python_environment_requirements": ["bitarray", "fastBPE", "hydra-core", "omegaconf", "regex", "requests", "sacremoses", "subword_nmt"], "example_code": "import torch\n\nen2fr = torch.hub.load('pytorch/fairseq', 'transformer.wmt14.en-fr', tokenizer='moses', bpe='subword_nmt')\n\nen2fr.cuda()\n\nfr = en2fr.translate('Hello world!', beam=5)\nassert fr == 'Bonjour \u00e0 tous !'", "performance": {"dataset": [{"name": "WMT'14", "accuracy": "Not provided"}, {"name": "WMT'18", "accuracy": "Not provided"}, {"name": "WMT'19", "accuracy": "Not provided"}]}, "description": "Transformer (NMT) is a powerful sequence-to-sequence modeling architecture that produces state-of-the-art neural machine translation systems. It is based on the paper 'Attention Is All You Need' and has been improved using techniques such as large-scale semi-supervised training, back-translation, and noisy-channel reranking. It supports English-French and English-German translation as well as round-trip translation for paraphrasing."}}
{"code": "{'Instruction': I'm looking for an API that can quickly classify images with high accuracy. What do you suggest?, 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_efficientnet_b0', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained EfficientNet model from PyTorch Hub, which provides state-of-the-art accuracy while being smaller and faster for image classification tasks.', 'code': \"import torch\nefficientnet = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_efficientnet_b0', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_efficientnet_b0', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "EfficientNet", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_efficientnet_b0', pretrained=True)", "api_arguments": ["model_name", "pretrained"], "python_environment_requirements": ["validators", "matplotlib"], "example_code": "import torch\n\nefficientnet = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_efficientnet_b0', pretrained=True)\nutils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_convnets_processing_utils')\n\nefficientnet.eval().to(device)\n\nbatch = torch.cat([utils.prepare_input_from_uri(uri) for uri in uris]).to(device)\n\nwith torch.no_grad():\n output = torch.nn.functional.softmax(efficientnet(batch), dim=1)\n \nresults = utils.pick_n_best(predictions=output, n=5)", "performance": {"dataset": {"name": "IMAGENET", "accuracy": "Not provided"}}, "description": "EfficientNet is a family of image classification models that achieve state-of-the-art accuracy while being smaller and faster. The models are trained with mixed precision using Tensor Cores on the NVIDIA Volta and Ampere GPU architectures. The EfficientNet models include EfficientNet-B0, EfficientNet-B4, EfficientNet-WideSE-B0, and EfficientNet-WideSE-B4. The WideSE models use wider Squeeze-and-Excitation layers than the original EfficientNet models, resulting in slightly better accuracy."}}
{"code": "{'Instruction': Can you recommend an API to ensure the safety of autonomous cars by detecting pedestrians, cars and other objects in a timely manner?, 'Output': {'domain': 'Object Detection', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained NVIDIA SSD (Single Shot MultiBox Detector) model from PyTorch Hub to detect pedestrians, cars, and other objects in images, ensuring autonomous vehicles' safety.\", 'code': \"import torch\nmodel = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_ssd', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Object Detection", "framework": "PyTorch", "functionality": "Single Shot MultiBox Detector", "api_name": "SSD", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_ssd', pretrained=True)", "api_arguments": ["model_name"], "python_environment_requirements": ["numpy", "scipy", "scikit-image", "matplotlib"], "example_code": "import torch\n\nssd_model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd')\nutils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd_processing_utils')\n\nssd_model.to('cuda')\nssd_model.eval()\n\ninputs = [utils.prepare_input(uri) for uri in uris]\ntensor = utils.prepare_tensor(inputs)\n\nwith torch.no_grad():\n detections_batch = ssd_model(tensor)\n\nresults_per_input = utils.decode_results(detections_batch)\nbest_results_per_input = [utils.pick_best(results, 0.40) for results in results_per_input]", "performance": {"dataset": {"name": "COCO", "accuracy": "Not provided"}}, "description": "The SSD (Single Shot MultiBox Detector) model is an object detection model based on the paper 'SSD: Single Shot MultiBox Detector'. It uses a deep neural network for detecting objects in images. This implementation replaces the obsolete VGG model backbone with the more modern ResNet-50 model. The SSD model is trained on the COCO dataset and can be used to detect objects in images with high accuracy and efficiency."}}
{"code": "{'Instruction': I'm looking for an API that can provide me a deep learning model to convert text to speech with human-like voice quality., 'Output': {'domain': 'Speech Synthesis', 'api_call': \"model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tacotron2', model_math='fp16')\", 'api_provider': 'PyTorch', 'explanation': \"Load NVIDIA's Tacotron 2 model from PyTorch Hub, which is designed to convert text to human-like speech.\", 'code': 'import torch\nmodel = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tacotron2', model_math=\"fp16\")'}}", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_tacotron2', model_math='fp16')", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Text-to-Speech", "api_name": "Tacotron 2", "api_call": "torch.hub.load(repo_or_dir='NVIDIA/DeepLearningExamples:torchhub', model='nvidia_tacotron2', model_math='fp16')", "api_arguments": {"model_math": "fp16"}, "python_environment_requirements": ["numpy", "scipy", "librosa", "unidecode", "inflect", "libsndfile1"], "example_code": ["import torch", "tacotron2 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tacotron2', model_math='fp16')", "tacotron2 = tacotron2.to('cuda')", "tacotron2.eval()", "text = 'Hello world, I missed you so much.'", "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tts_utils')", "sequences, lengths = utils.prepare_input_sequence([text])", "with torch.no_grad():", " mel, _, _ = tacotron2.infer(sequences, lengths)", " audio = waveglow.infer(mel)", "audio_numpy = audio[0].data.cpu().numpy()", "rate = 22050"], "performance": {"dataset": "LJ Speech", "accuracy": "Not specified"}, "description": "The Tacotron 2 model generates mel spectrograms from input text using an encoder-decoder architecture, and it is designed for generating natural-sounding speech from raw transcripts without any additional prosody information. This implementation uses Dropout instead of Zoneout to regularize the LSTM layers. The WaveGlow model (also available via torch.hub) is a flow-based model that consumes the mel spectrograms to generate speech."}}
{"code": "{'Instruction': I desire a robust image classifier for my online art school. Can you provide me with an API that can classify images?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MEAL V2 model for image classification from PyTorch Hub to classify images efficiently and accurately.', 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50', pretrained=True)", "api_arguments": {"model_name": "mealv2_resnest50"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ ResNet50", "resolution": "224", "parameters": "25.6M", "top1": "80.67", "top5": "95.09"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': I want to identify the main object in a given image. Which API would help me in doing this?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MEAL V2 model from PyTorch Hub, which is capable of identifying the main object in an image by classifying it into one of the ImageNet classes.', 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V1', model='mealv1_resnest50', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V1', model='mealv1_resnest50', pretrained=True)", "api_arguments": {"model_name": "mealv1_resnest50"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V1 w/ ResNet50", "resolution": "224", "parameters": "25.6M", "top1": "78.21", "top5": "94.01"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': I need an API to classify images on the ImageNet dataset without using any additional tricks. Please suggest a suitable one., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MEAL V2 model for image classification on the ImageNet dataset. It achieves state-of-the-art results without tricks like architecture modification or additional outside training data.', 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50_cutmix', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50_cutmix', pretrained=True)", "api_arguments": {"model_name": "mealv2_resnest50_cutmix"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 + CutMix w/ ResNet50", "resolution": "224", "parameters": "25.6M", "top1": "80.98", "top5": "95.35"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': My company is building a tool to automatically classify images using AI. Tell me about an API that can be used for this purpose., 'Output': {'domain': 'Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', 'mealv2_mobilenetv3_small_075', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'To automatically classify images, use the MEAL V2 pre-trained model for image classification from PyTorch Hub.', 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', 'mealv2_mobilenetv3_small_075', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenetv3_small_075', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenetv3_small_075', pretrained=True)", "api_arguments": {"model_name": "mealv2_mobilenetv3_small_075"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ MobileNet V3-Small 0.75", "resolution": "224", "parameters": "2.04M", "top1": "67.60", "top5": "87.23"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': Recommend an image classifier API to develop a phone app that can identify and tag objects in photos., 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenet_v3_large_100', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': 'Load the pretrained MEAL V2 model from PyTorch Hub, which can identify and tag objects in photos with high accuracy.', 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenet_v3_large_100', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenet_v3_large_100', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_mobilenet_v3_large_100', pretrained=True)", "api_arguments": {"model_name": "mealv2_mobilenet_v3_large_100"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ MobileNet V3-Large 1.0", "resolution": "224", "parameters": "5.48M", "top1": "76.92", "top5": "93.32"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
{"code": "{'Instruction': I want to classify an image of a dog for a contest. Can you recommend me an API for accurately doing this task?, 'Output': {'domain': 'Image Classification', 'api_call': \"model = torch.hub.load('szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50_380x380', pretrained=True)\", 'api_provider': 'PyTorch', 'explanation': \"Load the pretrained MEAL V2 model from PyTorch Hub for highly accurate image classification, which can be used to classify the dog's image for the contest.\", 'code': \"import torch\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_380x380', pretrained=True)\"}}", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50_380x380', pretrained=True)", "provider": "PyTorch", "api_data": {"domain": "Classification", "framework": "PyTorch", "functionality": "Image Classification", "api_name": "MEAL_V2", "api_call": "torch.hub.load(repo_or_dir='szq0214/MEAL-V2', 'meal_v2', model='mealv2_resnest50_380x380', pretrained=True)", "api_arguments": {"model_name": "mealv2_resnest50_380x380"}, "python_environment_requirements": "!pip install timm", "example_code": "import torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nmodel = torch.hub.load('szq0214/MEAL-V2','meal_v2', 'mealv2_resnest50_cutmix', pretrained=True)\nmodel.eval()\n\ninput_image = Image.open('dog.jpg')\npreprocess = transforms.Compose([\n transforms.Resize(256),\n transforms.CenterCrop(224),\n transforms.ToTensor(),\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\n\nif torch.cuda.is_available():\n input_batch = input_batch.to('cuda')\n model.to('cuda')\n\nwith torch.no_grad():\n output = model(input_batch)\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\nprint(probabilities)", "performance": [{"dataset": "ImageNet", "accuracy": {"model": "MEAL-V2 w/ ResNet50", "resolution": "380", "parameters": "25.6M", "top1": "81.72", "top5": "95.81"}}], "description": "MEAL V2 models are from the MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks paper. The method is based on ensemble knowledge distillation via discriminators, and it achieves state-of-the-art results without using common tricks such as architecture modification, outside training data, autoaug/randaug, cosine learning rate, mixup/cutmix training, or label smoothing."}}
