{"question_id": 1, "text": " Design a feature for a social media website to recommend articles to users based on how similar the articles are to their previously liked articles.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'csebuetnlp/mT5_multilingual_XLSum\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'csebuetnlp/mT5_multilingual_XLSum\\')\", \\'api_arguments\\': [\\'model_name\\'], \\'python_environment_requirements\\': [\\'transformers==4.11.0.dev0\\'], \\'example_code\\': \"import re\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nWHITESPACE_HANDLER = lambda k: re.sub(\\'\\\\\\\\s+\\', \\' \\', re.sub(\\'\\\\\\\\n+\\', \\' \\', k.strip()))\\\\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people\\'s scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We\\'re expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\\\\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\\\ninput_ids = tokenizer(\\\\n [WHITESPACE_HANDLER(article_text)],\\\\n return_tensors=pt,\\\\n padding=max_length,\\\\n truncation=True,\\\\n max_length=512\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n max_length=84,\\\\n no_repeat_ngram_size=2,\\\\n num_beams=4\\\\n)[0]\\\\nsummary = tokenizer.decode(\\\\n output_ids,\\\\n skip_special_tokens=True,\\\\n clean_up_tokenization_spaces=False\\\\n)\\\\nprint(summary)\", \\'performance\\': {\\'dataset\\': \\'xsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.5, \\'ROUGE-2\\': 13.934, \\'ROUGE-L\\': 28.988, \\'ROUGE-LSUM\\': 28.996, \\'loss\\': 2.067, \\'gen_len\\': 26.973}}, \\'description\\': \\'This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 2, "text": " The user is interested in a tool to find relationships between medical terms.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'promptcap-coco-vqa\\', \\'api_call\\': \"PromptCap(\\'vqascore/promptcap-coco-vqa\\')\", \\'api_arguments\\': {\\'prompt\\': \\'string\\', \\'image\\': \\'string\\'}, \\'python_environment_requirements\\': \\'pip install promptcap\\', \\'example_code\\': [\\'import torch\\', \\'from promptcap import PromptCap\\', \\'model = PromptCap(vqascore/promptcap-coco-vqa)\\', \\'if torch.cuda.is_available():\\', \\'  model.cuda()\\', \\'prompt = please describe this image according to the given question: what piece of clothing is this boy putting on?\\', \\'image = glove_boy.jpeg\\', \\'print(model.caption(prompt, image))\\'], \\'performance\\': {\\'dataset\\': {\\'coco\\': {\\'accuracy\\': \\'150 CIDEr\\'}, \\'OK-VQA\\': {\\'accuracy\\': \\'60.4%\\'}, \\'A-OKVQA\\': {\\'accuracy\\': \\'59.6%\\'}}}, \\'description\\': \\'PromptCap is a captioning model that can be controlled by natural language instruction. The instruction may contain a question that the user is interested in. It achieves SOTA performance on COCO captioning (150 CIDEr) and knowledge-based VQA tasks when paired with GPT-3 (60.4% on OK-VQA and 59.6% on A-OKVQA).\\'}', metadata={})]", "category": "generic"}
{"question_id": 3, "text": " As a journalist, I am curious about speech sentiment analysis in a group of people in a crowd. I want to extract features from the audio to run sentiment analysis.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentiment Analysis\\', \\'api_name\\': \\'michellejieli/NSFW_text_classifier\\', \\'api_call\\': \"pipeline(\\'sentiment-analysis\\', model=\\'michellejieli/NSFW_text_classification\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'classifier(I see you\u2019ve set aside this special time to humiliate yourself in public.)\\', \\'performance\\': {\\'dataset\\': \\'Reddit posts\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'DistilBERT is a transformer model that performs sentiment analysis. I fine-tuned the model on Reddit posts with the purpose of classifying not safe for work (NSFW) content, specifically text that is considered inappropriate and unprofessional. The model predicts 2 classes, which are NSFW or safe for work (SFW). The model is a fine-tuned version of DistilBERT. It was fine-tuned on 14317 Reddit posts pulled from the Reddit API.\\'}', metadata={})]", "category": "generic"}
{"question_id": 4, "text": " A chat service needs a way to compare and cluster similar sentences from users in different languages. Find a suitable feature extraction method to achieve this.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Classification\\', \\'api_name\\': \\'microsoft/swinv2-tiny-patch4-window8-256\\', \\'api_call\\': \"AutoModelForImageClassification.from_pretrained(\\'microsoft/swinv2-tiny-patch4-window8-256\\')\", \\'api_arguments\\': {\\'image\\': \\'http://images.cocodataset.org/val2017/000000039769.jpg\\'}, \\'python_environment_requirements\\': [\\'transformers\\', \\'PIL\\', \\'requests\\'], \\'example_code\\': \\'from transformers import AutoImageProcessor, AutoModelForImageClassification\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nprocessor = AutoImageProcessor.from_pretrained(microsoft/swinv2-tiny-patch4-window8-256)\\\\nmodel = AutoModelForImageClassification.from_pretrained(microsoft/swinv2-tiny-patch4-window8-256)\\\\ninputs = processor(images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'imagenet-1k\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Swin Transformer v2 model pre-trained on ImageNet-1k at resolution 256x256. It was introduced in the paper Swin Transformer V2: Scaling Up Capacity and Resolution by Liu et al. and first released in this repository. The Swin Transformer is a type of Vision Transformer. It builds hierarchical feature maps by merging image patches in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window. Swin Transformer v2 adds 3 main improvements: 1) a residual-post-norm method combined with cosine attention to improve training stability; 2) a log-spaced continuous position bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with high-resolution inputs; 3) a self-supervised pre-training method, SimMIM, to reduce the needs of vast labeled images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 5, "text": " I am an interior designer and want to showcase a modern living room with a fireplace and a large window overlooking a forest. Create an image according to this description.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Unconditional Image Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Denoising Diffusion Probabilistic Models (DDPM)\\', \\'api_name\\': \\'google/ddpm-bedroom-256\\', \\'api_call\\': \"DDPMPipeline.from_pretrained(\\'google/ddpm-bedroom-256\\')\", \\'api_arguments\\': \\'None\\', \\'python_environment_requirements\\': \\'diffusers\\', \\'example_code\\': \\'!pip install diffusers\\\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\\\nmodel_id = google/ddpm-bedroom-256\\\\nddpm = DDPMPipeline.from_pretrained(model_id)\\\\nimage = ddpm().images[0]\\\\nimage.save(ddpm_generated_image.png)\\', \\'performance\\': {\\'dataset\\': \\'CIFAR10\\', \\'accuracy\\': {\\'Inception score\\': 9.46, \\'FID score\\': 3.17}}, \\'description\\': \\'We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.\\'}', metadata={})]", "category": "generic"}
{"question_id": 6, "text": " We need a product description for an image-based online store platform that will help customers understand the specifics of the product.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 7, "text": " Create a program to generate a description for an image provided as input.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image Generation\\', \\'api_name\\': \\'stabilityai/stable-diffusion-2\\', \\'api_call\\': \"StableDiffusionPipeline.from_pretrained(\\'stabilityai/stable-diffusion-2\\', scheduler=EulerDiscreteScheduler.from_pretrained(\\'stabilityai/stable-diffusion-2\\', subfolder=scheduler), torch_dtype=torch.float16)\", \\'api_arguments\\': {\\'prompt\\': \\'a photo of an astronaut riding a horse on mars\\'}, \\'python_environment_requirements\\': [\\'diffusers\\', \\'transformers\\', \\'accelerate\\', \\'scipy\\', \\'safetensors\\'], \\'example_code\\': \\'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\\\nmodel_id = stabilityai/stable-diffusion-2\\\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = a photo of an astronaut riding a horse on mars\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(astronaut_rides_horse.png)\\', \\'performance\\': {\\'dataset\\': \\'COCO2017 validation set\\', \\'accuracy\\': \\'Not optimized for FID scores\\'}, \\'description\\': \\'Stable Diffusion v2 is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is primarily intended for research purposes, such as safe deployment of models with potential to generate harmful content, understanding limitations and biases of generative models, and generation of artworks for design and artistic processes.\\'}', metadata={})]", "category": "generic"}
{"question_id": 8, "text": " I am a financial analyst, and I receive report after report filled with charts helping to explain trends and data in my field. However, I also need to have this information in tabular format. Please help me extract a linearized table from this chart.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 9, "text": " We are building an automatic video generation platform based on user-provided text. We need a reliable model to convert text instructions into appropriate videos.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 10, "text": " How can I extract video content from a text file? Provide a code sample to generate the video based on the text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Video\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Video\\', \\'api_name\\': \\'camenduru/text2-video-zero\\', \\'api_call\\': \"pipeline(\\'text-to-video\\', model=\\'camenduru/text2-video-zero\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'This model is used for generating videos from text inputs. It is based on the Hugging Face framework and can be used with the transformers library. The model is trained on a variety of text and video datasets, and can be used for tasks such as video summarization, video generation from text prompts, and more.\\'}', metadata={})]", "category": "generic"}
{"question_id": 11, "text": " We are developing a mobile app to demonstrate the AI's ability to generate a short video from text. The app focuses on processing written stories into video.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 12, "text": " Hey, I want to analyze images in my phone gallery and answer questions about them.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Visual Question Answering\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Visual Question Answering\\', \\'api_name\\': \\'JosephusCheung/GuanacoVQAOnConsumerHardware\\', \\'api_call\\': \"pipeline(\\'visual-question-answering\\', model=\\'JosephusCheung/GuanacoVQAOnConsumerHardware\\')\", \\'api_arguments\\': {\\'model\\': \\'JosephusCheung/GuanacoVQAOnConsumerHardware\\', \\'tokenizer\\': \\'JosephusCheung/GuanacoVQAOnConsumerHardware\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'latest\\', \\'torch\\': \\'latest\\'}, \\'example_code\\': \\'vqa(image_path, question)\\', \\'performance\\': {\\'dataset\\': \\'JosephusCheung/GuanacoVQADataset\\', \\'accuracy\\': \\'unknown\\'}, \\'description\\': \\'A Visual Question Answering model trained on the GuanacoVQADataset, designed to work on consumer hardware like Colab Free T4 GPU. The model can be used to answer questions about images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 13, "text": " My company wants to develop an application that will analyze images in relation to food and answer questions about them. We want it to handle questions like \\\"what is in the dish\\\" and \\\"how many calories does it have\\\".\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'csebuetnlp/mT5_multilingual_XLSum\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'csebuetnlp/mT5_multilingual_XLSum\\')\", \\'api_arguments\\': [\\'model_name\\'], \\'python_environment_requirements\\': [\\'transformers==4.11.0.dev0\\'], \\'example_code\\': \"import re\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nWHITESPACE_HANDLER = lambda k: re.sub(\\'\\\\\\\\s+\\', \\' \\', re.sub(\\'\\\\\\\\n+\\', \\' \\', k.strip()))\\\\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people\\'s scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We\\'re expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\\\\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\\\ninput_ids = tokenizer(\\\\n [WHITESPACE_HANDLER(article_text)],\\\\n return_tensors=pt,\\\\n padding=max_length,\\\\n truncation=True,\\\\n max_length=512\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n max_length=84,\\\\n no_repeat_ngram_size=2,\\\\n num_beams=4\\\\n)[0]\\\\nsummary = tokenizer.decode(\\\\n output_ids,\\\\n skip_special_tokens=True,\\\\n clean_up_tokenization_spaces=False\\\\n)\\\\nprint(summary)\", \\'performance\\': {\\'dataset\\': \\'xsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.5, \\'ROUGE-2\\': 13.934, \\'ROUGE-L\\': 28.988, \\'ROUGE-LSUM\\': 28.996, \\'loss\\': 2.067, \\'gen_len\\': 26.973}}, \\'description\\': \\'This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 14, "text": " We have received an invoice document, and would like to extract the total amount from it.\\n###Input: {'question': 'What is the total amount?', 'context': 'Invoice information for order ABC_123\\\\nProduct: Widget A, Quantity: 10, Price: $5 each\\\\nProduct: Widget B, Quantity: 5, Price: $3 each\\\\nProduct: Widget C, Quantity: 15, Price: $2 each\\\\nSubtotal: $75, Tax: $6.38, Total Amount Due: $81.38'}\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Document Question Answering\\', \\'api_name\\': \\'CQI_Visual_Question_Awnser_PT_v0\\', \\'api_call\\': \"pipeline(\\'question-answering\\', model=LayoutLMForQuestionAnswering.from_pretrained(\\'microsoft/layoutlm-base-uncased\\'))\", \\'api_arguments\\': [\\'url\\', \\'question\\'], \\'python_environment_requirements\\': [\\'PIL\\', \\'pytesseract\\', \\'PyTorch\\', \\'transformers\\'], \\'example_code\\': [\"nlp(\\'https://templates.invoicehome.com/invoice-template-us-neat-750px.png\\', \\'What is the invoice number?\\')\", \"nlp(\\'https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg\\', \\'What is the purchase amount?\\')\", \"nlp(\\'https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png\\', \\'What are the 2020 net sales?\\')\"], \\'performance\\': {\\'dataset\\': [{\\'accuracy\\': 0.9943977}, {\\'accuracy\\': 0.9912159}, {\\'accuracy\\': 0.59147286}]}, \\'description\\': \\'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 15, "text": " As a clerk in a school, you want to extract information from some student enrollment forms. These forms contain students' details such as Name, age, and address.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Generation\\', \\'api_name\\': \\'pygmalion-6b\\', \\'api_call\\': \"AutoModelForCausalLM.from_pretrained(\\'waifu-workshop/pygmalion-6b\\')\", \\'api_arguments\\': [\\'input_ids\\', \\'max_length\\', \\'num_return_sequences\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': \"from transformers import AutoTokenizer, AutoModelForCausalLM\\\\n\\\\ntokenizer = AutoTokenizer.from_pretrained(\\'waifu-workshop/pygmalion-6b\\')\\\\nmodel = AutoModelForCausalLM.from_pretrained(\\'waifu-workshop/pygmalion-6b\\')\\\\n\\\\ninput_text = [CHARACTER]\\'s Persona: [A few sentences about the character you want the model to play]\\\\\\\\n<START>\\\\\\\\n[DIALOGUE HISTORY]\\\\\\\\nYou: [Your input message here]\\\\\\\\n[CHARACTER]:\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=\\'pt\\')\\\\n\\\\noutput = model.generate(input_ids, max_length=100, num_return_sequences=1)\\\\n\\\\noutput_text = tokenizer.decode(output[0], skip_special_tokens=True)\", \\'performance\\': {\\'dataset\\': \\'56MB of dialogue data gathered from multiple sources\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \"Pygmalion 6B is a proof-of-concept dialogue model based on EleutherAI\\'s GPT-J-6B. It is fine-tuned on 56MB of dialogue data gathered from multiple sources, which includes both real and partially machine-generated conversations. The model is intended for conversational text generation and can be used to play a character in a dialogue.\"}', metadata={})]", "category": "generic"}
{"question_id": 16, "text": " Find a model that can be used to predict the properties of molecules based on their graph representations.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Graph Machine Learning\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'graphormer-base-pcqm4mv1\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'graphormer-base-pcqm4mv1\\')\", \\'api_arguments\\': [\\'model_name\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'See the Graph Classification with Transformers tutorial\\', \\'performance\\': {\\'dataset\\': \\'PCQM4M-LSC\\', \\'accuracy\\': \\'1st place on the KDD CUP 2021 (quantum prediction track)\\'}, \\'description\\': \\'The Graphormer is a graph Transformer model, pretrained on PCQM4M-LSC, and which got 1st place on the KDD CUP 2021 (quantum prediction track). Developed by Microsoft, this model should be used for graph classification tasks or graph representation tasks; the most likely associated task is molecule modeling. It can either be used as such, or finetuned on downstream tasks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 17, "text": " Estimate the depth of a pool using computational depth estimation, given an underwater photo.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Zero-Shot Image Classification\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Zero-Shot Image Classification\\', \\'api_name\\': \\'laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup\\', \\'api_call\\': \"pipeline(\\'image-classification\\', model=\\'laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup\\')\", \\'api_arguments\\': \\'image_path, class_names\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"results = model(image_path, class_names=\\'cat, dog, bird\\')\", \\'performance\\': {\\'dataset\\': \\'ImageNet-1k\\', \\'accuracy\\': \\'76.9\\'}, \\'description\\': \\'A series of CLIP ConvNeXt-Large (w/ extra text depth, vision MLP head) models trained on the LAION-2B (english) subset of LAION-5B using OpenCLIP. The models utilize the timm ConvNeXt-Large model (convnext_large) as the image tower, a MLP (fc - gelu - drop - fc) head in vision tower instead of the single projection of other CLIP models, and a text tower with same width but 4 layers more depth than ViT-L / RN50x16 models (depth 16, embed dim 768).\\'}', metadata={})]", "category": "generic"}
{"question_id": 18, "text": " I need technology that can analyze images and estimate their depth in a single camera.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Conversational\\', \\'api_name\\': \\'facebook/blenderbot_small-90M\\', \\'api_call\\': \"BlenderbotForConditionalGeneration.from_pretrained(\\'facebook/blenderbot_small-90M\\')\", \\'api_arguments\\': [\\'message\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'Input a message to start chatting with facebook/blenderbot_small-90M.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Blenderbot is a chatbot model that provides engaging talking points and listens to their partners, both asking and answering questions, and displaying knowledge, empathy, and personality appropriately, depending on the situation.\\'}', metadata={})]", "category": "generic"}
{"question_id": 19, "text": " The client is a real estate company working on virtual tours. We need to help them estimate depth in images of houses.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'csebuetnlp/mT5_multilingual_XLSum\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'csebuetnlp/mT5_multilingual_XLSum\\')\", \\'api_arguments\\': [\\'model_name\\'], \\'python_environment_requirements\\': [\\'transformers==4.11.0.dev0\\'], \\'example_code\\': \"import re\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nWHITESPACE_HANDLER = lambda k: re.sub(\\'\\\\\\\\s+\\', \\' \\', re.sub(\\'\\\\\\\\n+\\', \\' \\', k.strip()))\\\\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people\\'s scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We\\'re expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\\\\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\\\ninput_ids = tokenizer(\\\\n [WHITESPACE_HANDLER(article_text)],\\\\n return_tensors=pt,\\\\n padding=max_length,\\\\n truncation=True,\\\\n max_length=512\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n max_length=84,\\\\n no_repeat_ngram_size=2,\\\\n num_beams=4\\\\n)[0]\\\\nsummary = tokenizer.decode(\\\\n output_ids,\\\\n skip_special_tokens=True,\\\\n clean_up_tokenization_spaces=False\\\\n)\\\\nprint(summary)\", \\'performance\\': {\\'dataset\\': \\'xsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.5, \\'ROUGE-2\\': 13.934, \\'ROUGE-L\\': 28.988, \\'ROUGE-LSUM\\': 28.996, \\'loss\\': 2.067, \\'gen_len\\': 26.973}}, \\'description\\': \\'This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 20, "text": " Assist me in setting up an image classifier that can recognize objects within an image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 21, "text": " Identify an object within an image based on textual description. For example, find a dog in the image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Text Classification\\', \\'api_name\\': \\'distilbert-base-uncased-finetuned-sst-2-english\\', \\'api_call\\': \"DistilBertForSequenceClassification.from_pretrained(\\'distilbert-base-uncased-finetuned-sst-2-english\\')\", \\'api_arguments\\': [\\'inputs\\'], \\'python_environment_requirements\\': [\\'torch\\', \\'transformers\\'], \\'example_code\\': \"import torch\\\\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification\\\\ntokenizer = DistilBertTokenizer.from_pretrained(\\'distilbert-base-uncased\\')\\\\nmodel = DistilBertForSequenceClassification.from_pretrained(\\'distilbert-base-uncased-finetuned-sst-2-english\\')\\\\ninputs = tokenizer(\\'Hello, my dog is cute\\', return_tensors=\\'pt\\')\\\\nwith torch.no_grad():\\\\n    logits = model(**inputs).logits\\\\npredicted_class_id = logits.argmax().item()\\\\nmodel.config.id2label[predicted_class_id]\", \\'performance\\': {\\'dataset\\': \\'glue\\', \\'accuracy\\': 0.911}, \\'description\\': \\'This model is a fine-tune checkpoint of DistilBERT-base-uncased, fine-tuned on SST-2. It reaches an accuracy of 91.3 on the dev set (for comparison, Bert bert-base-uncased version reaches an accuracy of 92.7). This model can be used for topic classification.\\'}', metadata={})]", "category": "generic"}
{"question_id": 22, "text": " Our client is an AI gaming company and we need to develop a bot for the game Valorant. The bot should detect objects like dropped spike, enemy, planted spike, and teammate within the game.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Object Detection\\', \\'api_name\\': \\'keremberke/yolov8m-valorant-detection\\', \\'api_call\\': \"YOLO(\\'keremberke/yolov8m-valorant-detection\\')\", \\'api_arguments\\': {\\'conf\\': 0.25, \\'iou\\': 0.45, \\'agnostic_nms\\': False, \\'max_det\\': 1000}, \\'python_environment_requirements\\': \\'pip install ultralyticsplus==0.0.23 ultralytics==8.0.21\\', \\'example_code\\': \"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO(\\'keremberke/yolov8m-valorant-detection\\')\\\\nmodel.overrides[\\'conf\\'] = 0.25\\\\nmodel.overrides[\\'iou\\'] = 0.45\\\\nmodel.overrides[\\'agnostic_nms\\'] = False\\\\nmodel.overrides[\\'max_det\\'] = 1000\\\\nimage = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\", \\'performance\\': {\\'dataset\\': \\'valorant-object-detection\\', \\'accuracy\\': 0.965}, \\'description\\': \\'A YOLOv8 model for object detection in Valorant game, trained on a custom dataset. It detects dropped spike, enemy, planted spike, and teammate objects.\\'}', metadata={})]", "category": "generic"}
{"question_id": 23, "text": " A client from real estate agency needs to get a list of objects present in a series of pictures to prepare their property listings.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'zero-shot-object-detection\\', \\'api_name\\': \\'google/owlvit-base-patch32\\', \\'api_call\\': \"OwlViTForObjectDetection.from_pretrained(\\'google/owlvit-base-patch32\\')\", \\'api_arguments\\': {\\'texts\\': \\'List of text queries\\', \\'images\\': \\'Image to be processed\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'import requests\\\\nfrom PIL import Image\\\\nimport torch\\\\nfrom transformers import OwlViTProcessor, OwlViTForObjectDetection\\\\nprocessor = OwlViTProcessor.from_pretrained(google/owlvit-base-patch32)\\\\nmodel = OwlViTForObjectDetection.from_pretrained(google/owlvit-base-patch32)\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ntexts = [[a photo of a cat, a photo of a dog]]\\\\ninputs = processor(text=texts, images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\ntarget_sizes = torch.Tensor([image.size[::-1]])\\\\nresults = processor.post_process(outputs=outputs, target_sizes=target_sizes)\\', \\'performance\\': {\\'dataset\\': \\'COCO and OpenImages\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'OWL-ViT is a zero-shot text-conditioned object detection model that uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. The model can be used to query an image with one or multiple text queries.\\'}', metadata={})]", "category": "generic"}
{"question_id": 24, "text": " We are developing an application for smartphones which automatically separates elements in a user's photo, and we need to implement this feature.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 25, "text": " I have a picture of a room demonstrating a mixture of objects. The model needs to seperate the objects and label them accordingly.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Text-to-Text Generation\\', \\'api_name\\': \\'optimum/t5-small\\', \\'api_call\\': \"ORTModelForSeq2SeqLM.from_pretrained(\\'optimum/t5-small\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'optimum.onnxruntime\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, pipeline\\\\nfrom optimum.onnxruntime import ORTModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained(optimum/t5-small)\\\\nmodel = ORTModelForSeq2SeqLM.from_pretrained(optimum/t5-small)\\\\ntranslator = pipeline(translation_en_to_fr, model=model, tokenizer=tokenizer)\\\\nresults = translator(My name is Eustache and I have a pet raccoon)\\\\nprint(results)\\', \\'performance\\': {\\'dataset\\': \\'c4\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'T5 is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks and for which each task is converted into a text-to-text format. It can be used for translation, text-to-text generation, and summarization.\\'}', metadata={})]", "category": "generic"}
{"question_id": 26, "text": " We want to randomly generate high-quality images of celebrity faces.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Unconditional Image Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Unconditional Image Generation\\', \\'api_name\\': \\'google/ddpm-cat-256\\', \\'api_call\\': \"DDPMPipeline.from_pretrained(\\'google/ddpm-cat-256\\')\", \\'api_arguments\\': [\\'model_id\\'], \\'python_environment_requirements\\': [\\'diffusers\\'], \\'example_code\\': \\'!pip install diffusers\\\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\\\nmodel_id = google/ddpm-cat-256\\\\nddpm = DDPMPipeline.from_pretrained(model_id)\\\\nimage = ddpm().images[0]\\\\nimage.save(ddpm_generated_image.png)\\', \\'performance\\': {\\'dataset\\': \\'CIFAR10\\', \\'accuracy\\': {\\'Inception_score\\': 9.46, \\'FID_score\\': 3.17}}, \\'description\\': \\'Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. It can generate high-quality images using discrete noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm. The model is trained on the unconditional CIFAR10 dataset and 256x256 LSUN, obtaining an Inception score of 9.46 and a state-of-the-art FID score of 3.17.\\'}', metadata={})]", "category": "generic"}
{"question_id": 27, "text": " Generate a new image based on the online database of bedroom art.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 28, "text": " I run an online store that sells butterfly-themed products. Please generate an image of a cute butterfly for our social media page.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 29, "text": " We need a video-based AI model for security purposes. We want the AI to check and categorize footage based on existing security guidelines.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 30, "text": " A new project demands to classify videos for a social media platform. Let us create a video classification pipeline.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'csebuetnlp/mT5_multilingual_XLSum\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'csebuetnlp/mT5_multilingual_XLSum\\')\", \\'api_arguments\\': [\\'model_name\\'], \\'python_environment_requirements\\': [\\'transformers==4.11.0.dev0\\'], \\'example_code\\': \"import re\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nWHITESPACE_HANDLER = lambda k: re.sub(\\'\\\\\\\\s+\\', \\' \\', re.sub(\\'\\\\\\\\n+\\', \\' \\', k.strip()))\\\\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people\\'s scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We\\'re expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\\\\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\\\ninput_ids = tokenizer(\\\\n [WHITESPACE_HANDLER(article_text)],\\\\n return_tensors=pt,\\\\n padding=max_length,\\\\n truncation=True,\\\\n max_length=512\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n max_length=84,\\\\n no_repeat_ngram_size=2,\\\\n num_beams=4\\\\n)[0]\\\\nsummary = tokenizer.decode(\\\\n output_ids,\\\\n skip_special_tokens=True,\\\\n clean_up_tokenization_spaces=False\\\\n)\\\\nprint(summary)\", \\'performance\\': {\\'dataset\\': \\'xsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.5, \\'ROUGE-2\\': 13.934, \\'ROUGE-L\\': 28.988, \\'ROUGE-LSUM\\': 28.996, \\'loss\\': 2.067, \\'gen_len\\': 26.973}}, \\'description\\': \\'This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 31, "text": " I am an insurance adjustor. I need a zero-shot image classifier that will tell me whether a car has been involved in a major accident or had minor damages.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 32, "text": " I want to analyze a medical image to find out if it's an X-ray, an MRI scan, or a CT scan.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 33, "text": " We are building a quiz application where the image will be shown, and we have to choose a dressings matching that image. Please help in classifying the image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 34, "text": " We're developing a chatbot that can quickly identify and describe images for our Chinese-speaking users.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 35, "text": " We would like to understand the sentiment of user's messages in a customer support chat system.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'Kirili4ik/mbart_ruDialogSum\\', \\'api_call\\': \"MBartForConditionalGeneration.from_pretrained(\\'Kirili4ik/mbart_ruDialogSum\\')\", \\'api_arguments\\': {\\'model_name\\': \\'Kirili4ik/mbart_ruDialogSum\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import MBartTokenizer, MBartForConditionalGeneration\\\\nmodel_name = Kirili4ik/mbart_ruDialogSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = MBartForConditionalGeneration.from_pretrained(model_name)\\\\nmodel.eval()\\\\narticle_text = ...\\\\ninput_ids = tokenizer(\\\\n [article_text],\\\\n max_length=600,\\\\n padding=max_length,\\\\n truncation=True,\\\\n return_tensors=pt,\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n top_k=0,\\\\n num_beams=3,\\\\n no_repeat_ngram_size=3\\\\n)[0]\\\\nsummary = tokenizer.decode(output_ids, skip_special_tokens=True)\\\\nprint(summary)\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'SAMSum Corpus (translated to Russian)\\', \\'accuracy\\': {\\'Validation ROGUE-1\\': 34.5, \\'Validation ROGUE-L\\': 33, \\'Test ROGUE-1\\': 31, \\'Test ROGUE-L\\': 28}}]}, \\'description\\': \\'MBart for Russian summarization fine-tuned for dialogues summarization. This model was firstly fine-tuned by Ilya Gusev on Gazeta dataset. We have fine tuned that model on SamSum dataset translated to Russian using GoogleTranslateAPI. Moreover! We have implemented a ! telegram bot @summarization_bot ! with the inference of this model. Add it to the chat and get summaries instead of dozens spam messages!\\'}', metadata={})]", "category": "generic"}
{"question_id": 36, "text": " As a book store owner, I want to classify customer reviews into positive and negative sentiments.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'results-yelp\\', \\'api_call\\': \"AutoTokenizer.from_pretrained(\\'bert-base-uncased\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"AutoTokenizer.from_pretrained(\\'bert-base-uncased\\')\", \\'config\\': \"AutoConfig.from_pretrained(\\'potatobunny/results-yelp\\')\"}, \\'python_environment_requirements\\': {\\'Transformers\\': \\'4.18.0\\', \\'Pytorch\\': \\'1.10.0+cu111\\', \\'Datasets\\': \\'2.0.0\\', \\'Tokenizers\\': \\'0.12.1\\'}, \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'Yelp\\', \\'accuracy\\': 0.9302}, \\'description\\': \\'This model is a fine-tuned version of textattack/bert-base-uncased-yelp-polarity on a filtered and manually reviewed Yelp dataset containing restaurant reviews only. It is intended to perform text classification, specifically sentiment analysis, on text data obtained from restaurant reviews to determine if the particular review is positive or negative.\\'}', metadata={})]", "category": "generic"}
{"question_id": 37, "text": " I am the owner of a news website. I have several consumers' comments about our publishing news. I want to analyze the sentiments of these comments.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \"PegasusForConditionalGeneration.from_pretrained(\\'tuner007/pegasus_summarizer\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 38, "text": " Our business is expanding to international markets. Analyze the sentiment of the following customer review to better understand their satisfaction with our product: \\\"\\u00a1Esto es maravilloso! Me encanta.\\\"\\n###Input: \\\"\\u00a1Esto es maravilloso! Me encanta.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Zero-Shot Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Zero-Shot Classification\\', \\'api_name\\': \\'Recognai/bert-base-spanish-wwm-cased-xnli\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'Recognai/bert-base-spanish-wwm-cased-xnli\\')\", \\'api_arguments\\': [\\'sequence\\', \\'candidate_labels\\', \\'hypothesis_template\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import pipeline\\\\nclassifier = pipeline(zero-shot-classification, model=Recognai/bert-base-spanish-wwm-cased-xnli)\\\\nclassifier(\\\\nEl autor se perfila, a los 50 a\u00f1os de su muerte, como uno de los grandes de su siglo,\\\\ncandidate_labels=[cultura, sociedad, economia, salud, deportes],\\\\nhypothesis_template=Este ejemplo es {}. \\\\n)\\', \\'performance\\': {\\'dataset\\': \\'XNLI-es\\', \\'accuracy\\': \\'79.9%\\'}, \\'description\\': \\'This model is a fine-tuned version of the spanish BERT model with the Spanish portion of the XNLI dataset. You can have a look at the training script for details of the training.\\'}', metadata={})]", "category": "generic"}
{"question_id": 39, "text": " We are a forum moderator team looking for a solution to classify comments into toxic or non-toxic categories.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'martin-ha/toxic-comment-model\\', \\'api_call\\': \"pipeline(model=\\'martin-ha/toxic-comment-model\\')\", \\'api_arguments\\': {\\'model_path\\': \\'martin-ha/toxic-comment-model\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import AutoModelForSequenceClassification, AutoTokenizer, TextClassificationPipeline\\\\nmodel_path = martin-ha/toxic-comment-model\\\\ntokenizer = AutoTokenizer.from_pretrained(model_path)\\\\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\\\\npipeline = TextClassificationPipeline(model=model, tokenizer=tokenizer)\\\\nprint(pipeline(\\'This is a test text.\\'))\", \\'performance\\': {\\'dataset\\': \\'held-out test set\\', \\'accuracy\\': 0.94, \\'f1-score\\': 0.59}, \\'description\\': \\'This model is a fine-tuned version of the DistilBERT model to classify toxic comments.\\'}', metadata={})]", "category": "generic"}
{"question_id": 40, "text": " My company is launching a social media campaign. We need an AI-based system that would automatically analyze the sentiment of any user-generated reviews or tweets concerning our product.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'csebuetnlp/mT5_multilingual_XLSum\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'csebuetnlp/mT5_multilingual_XLSum\\')\", \\'api_arguments\\': [\\'model_name\\'], \\'python_environment_requirements\\': [\\'transformers==4.11.0.dev0\\'], \\'example_code\\': \"import re\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nWHITESPACE_HANDLER = lambda k: re.sub(\\'\\\\\\\\s+\\', \\' \\', re.sub(\\'\\\\\\\\n+\\', \\' \\', k.strip()))\\\\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people\\'s scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We\\'re expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\\\\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\\\ninput_ids = tokenizer(\\\\n [WHITESPACE_HANDLER(article_text)],\\\\n return_tensors=pt,\\\\n padding=max_length,\\\\n truncation=True,\\\\n max_length=512\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n max_length=84,\\\\n no_repeat_ngram_size=2,\\\\n num_beams=4\\\\n)[0]\\\\nsummary = tokenizer.decode(\\\\n output_ids,\\\\n skip_special_tokens=True,\\\\n clean_up_tokenization_spaces=False\\\\n)\\\\nprint(summary)\", \\'performance\\': {\\'dataset\\': \\'xsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.5, \\'ROUGE-2\\': 13.934, \\'ROUGE-L\\': 28.988, \\'ROUGE-LSUM\\': 28.996, \\'loss\\': 2.067, \\'gen_len\\': 26.973}}, \\'description\\': \\'This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 41, "text": " I have jobs descriptions in French for sales manager, please highlight names of organizations or cities within the text.\\n###Input: \\\"La soci\\u00e9t\\u00e9 de Paris est sp\\u00e9cialis\\u00e9e dans la vente de v\\u00e9hicules \\u00e9lectriques. Responsable des ventes, vous travaillerez au sein d'une \\u00e9quipe dynamique dans l'agence de Lyon. Vous \\u00eates charg\\u00e9(e) de d\\u00e9velopper le portefeuille client et d'assurer la satisfaction des clients existants. Dans ce contexte, vous devrez travailler en lien \\u00e9troit avec le directeur commercial et les autres \\u00e9quipes de l'entreprise. Une exp\\u00e9rience pr\\u00e9alable chez Renault est un atout.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Token Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Named Entity Recognition\\', \\'api_name\\': \\'Jean-Baptiste/camembert-ner\\', \\'api_call\\': \"AutoModelForTokenClassification.from_pretrained(\\'Jean-Baptiste/camembert-ner\\')\", \\'api_arguments\\': {\\'model\\': \\'model\\', \\'tokenizer\\': \\'tokenizer\\', \\'aggregation_strategy\\': \\'simple\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'AutoTokenizer, AutoModelForTokenClassification, pipeline\\'}, \\'example_code\\': \"from transformers import AutoTokenizer, AutoModelForTokenClassification\\\\ntokenizer = AutoTokenizer.from_pretrained(Jean-Baptiste/camembert-ner)\\\\nmodel = AutoModelForTokenClassification.from_pretrained(Jean-Baptiste/camembert-ner)\\\\n\\\\nfrom transformers import pipeline\\\\nnlp = pipeline(\\'ner\\', model=model, tokenizer=tokenizer, aggregation_strategy=simple)\\\\nnlp(Apple est cre le 1er avril 1976 dans le garage de la maison d\\'enfance de Steve Jobs  Los Altos en Californie par Steve Jobs, Steve Wozniak et Ronald Wayne14, puis constitue sous forme de socit le 3 janvier 1977  l\\'origine sous le nom d\\'Apple Computer, mais pour ses 30 ans et pour reflter la diversification de ses produits, le mot \u00ab computer  est retir le 9 janvier 2015.)\", \\'performance\\': {\\'dataset\\': \\'wikiner-fr\\', \\'accuracy\\': {\\'overall_f1\\': 0.8914, \\'PER_f1\\': 0.9483, \\'ORG_f1\\': 0.8181, \\'LOC_f1\\': 0.8955, \\'MISC_f1\\': 0.8146}}, \\'description\\': \\'camembert-ner is a Named Entity Recognition (NER) model fine-tuned from camemBERT on the wikiner-fr dataset. It can recognize entities such as persons, organizations, locations, and miscellaneous entities.\\'}', metadata={})]", "category": "generic"}
{"question_id": 42, "text": " In order to have a better understanding of our clients, I'd like to identify the names of people and organizations mentioned in the following customer review.\\n###Input: \\\"I recently purchased a MacBook Pro from Apple Inc. and had a fantastic customer support experience. John from their tech support team was incredibly helpful and professional.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 43, "text": " I am building a social media app that requires people to write fascinating stories rather than boring sentences. Detect named entities in a sentence by using an NER model.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 44, "text": " We have a large dataset of customer orders in the form of a table. Help us answer questions about this data.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'PyTorch Transformers\\', \\'functionality\\': \\'Table Question Answering\\', \\'api_name\\': \\'table-question-answering-tapas\\', \\'api_call\\': \"pipeline(\\'table-question-answering\\', model=\\'Meena/table-question-answering-tapas\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'This model can be loaded on the Inference API on-demand.\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'SQA (Sequential Question Answering by Microsoft)\\', \\'accuracy\\': None}, {\\'name\\': \\'WTQ (Wiki Table Questions by Stanford University)\\', \\'accuracy\\': None}, {\\'name\\': \\'WikiSQL (by Salesforce)\\', \\'accuracy\\': None}]}, \\'description\\': \\'TAPAS, the model learns an inner representation of the English language used in tables and associated texts, which can then be used to extract features useful for downstream tasks such as answering questions about a table, or determining whether a sentence is entailed or refuted by the contents of a table. It is a BERT-based model specifically designed (and pre-trained) for answering questions about tabular data. TAPAS uses relative position embeddings and has 7 token types that encode tabular structure. It is pre-trained on the masked language modeling (MLM) objective on a large dataset comprising millions of tables from English Wikipedia and corresponding texts.\\'}', metadata={})]", "category": "generic"}
{"question_id": 45, "text": " Gather information about annual income and age demographics of employees to predict retirement patterns. Make sure to identify top employees for potential promotions.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Automatic Speech Recognition\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Speech Recognition\\', \\'api_name\\': \\'jonatasgrosman/wav2vec2-large-xlsr-53-japanese\\', \\'api_call\\': \"SpeechRecognitionModel(\\'jonatasgrosman/wav2vec2-large-xlsr-53-japanese\\')\", \\'api_arguments\\': [\\'audio_paths\\'], \\'python_environment_requirements\\': [\\'huggingsound\\', \\'torch\\', \\'librosa\\', \\'datasets\\', \\'transformers\\'], \\'example_code\\': \\'from huggingsound import SpeechRecognitionModel\\\\nmodel = SpeechRecognitionModel(jonatasgrosman/wav2vec2-large-xlsr-53-japanese)\\\\naudio_paths = [/path/to/file.mp3, /path/to/another_file.wav]\\\\ntranscriptions = model.transcribe(audio_paths)\\', \\'performance\\': {\\'dataset\\': \\'common_voice\\', \\'accuracy\\': {\\'WER\\': 81.8, \\'CER\\': 20.16}}, \\'description\\': \\'Fine-tuned XLSR-53 large model for speech recognition in Japanese. Trained on Common Voice 6.1, CSS10, and JSUT datasets. Make sure your speech input is sampled at 16kHz.\\'}', metadata={})]", "category": "generic"}
{"question_id": 46, "text": " To track our sales data, we need to find total sales of a specific product based on a table containing sales information per week.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'financial-sentiment-analysis\\', \\'api_name\\': \\'yiyanghkust/finbert-tone\\', \\'api_call\\': \"BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\", \\'api_arguments\\': [\\'sentences\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import BertTokenizer, BertForSequenceClassification\\\\nfrom transformers import pipeline\\\\nfinbert = BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\\\\ntokenizer = BertTokenizer.from_pretrained(\\'yiyanghkust/finbert-tone\\')\\\\nnlp = pipeline(sentiment-analysis, model=finbert, tokenizer=tokenizer)\\\\nsentences = [there is a shortage of capital, and we need extra financing,\\\\n growth is strong and we have plenty of liquidity,\\\\n there are doubts about our finances,\\\\n profits are flat]\\\\nresults = nlp(sentences)\\\\nprint(results)\", \\'performance\\': {\\'dataset\\': \\'10,000 manually annotated sentences from analyst reports\\', \\'accuracy\\': \\'superior performance on financial tone analysis task\\'}, \\'description\\': \\'FinBERT is a BERT model pre-trained on financial communication text. It is trained on the following three financial communication corpus: Corporate Reports 10-K & 10-Q, Earnings Call Transcripts, and Analyst Reports. This released finbert-tone model is the FinBERT model fine-tuned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports. This model achieves superior performance on financial tone analysis task.\\'}', metadata={})]", "category": "generic"}
{"question_id": 47, "text": " I have a table containing information about various animals and their important characteristics. I need the system to answer a query to provide information about the tallest animal in the table.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \"PegasusForConditionalGeneration.from_pretrained(\\'tuner007/pegasus_summarizer\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 48, "text": " You are building an app that allows users to find quick answers to textbook questions. Users will send a message with the question, and the answer should be detected directly from the textbook content.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'valhalla/t5-base-e2e-qg\\', \\'api_call\\': \"pipeline(\\'e2e-qg\\', model=\\'valhalla/t5-base-e2e-qg\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'Hugging Face Transformers\\'], \\'example_code\\': \"from pipelines import pipeline\\\\n\\\\ntext = Python is an interpreted, high-level, general-purpose programming language. Created by Guido van Rossum and first released in 1991, Python\\'s design philosophy emphasizes code readability with its notable use of significant whitespace.\\\\n\\\\nnlp = pipeline(e2e-qg, model=valhalla/t5-base-e2e-qg)\\\\n\\\\nnlp(text)\", \\'performance\\': {\\'dataset\\': \\'squad\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'This is a T5-base model trained for end-to-end question generation task. Simply input the text and the model will generate multiple questions. You can play with the model using the inference API, just put the text and see the results!\\'}', metadata={})]", "category": "generic"}
{"question_id": 49, "text": " We want to make sure clarify some questions about the legal implications of a new partnership contract for a real estate development project.\\n###Input: We hereby grant the Licensee the exclusive right to develop, construct, operate and promote the Project, as well as to manage the daily operations of the Licensed Facilities during the Term. In consideration for the grant of the License, the Licensee shall pay to the Licensor the full amount of Ten Million (10,000,000) Dollars within thirty (30) days after the execution hereof.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \"PegasusForConditionalGeneration.from_pretrained(\\'tuner007/pegasus_summarizer\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 50, "text": " Help me setup a tinyroberta model from deepset for Question and Answer. Provide a sample input and output.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Table Question Answering\\', \\'api_name\\': \\'google/tapas-medium-finetuned-sqa\\', \\'api_call\\': \"pipeline(\\'table-question-answering\\', model=\\'google/tapas-medium-finetuned-sqa\\')\", \\'api_arguments\\': \\'table, query\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\n\\\\n# Initialize the pipeline\\\\ntable_qa_pipeline = pipeline(\\'table-question-answering\\', model=\\'google/tapas-medium-finetuned-sqa\\')\\\\n\\\\n# Provide the table and query\\\\nresult = table_qa_pipeline(table=table, query=\\'What is the total revenue?\\')\", \\'performance\\': {\\'dataset\\': \\'msr_sqa\\', \\'accuracy\\': 0.6561}, \\'description\\': \\'TAPAS medium model fine-tuned on Sequential Question Answering (SQA). This model is pretrained on a large corpus of English data from Wikipedia and uses relative position embeddings. It can be used for answering questions related to a table in a conversational set-up.\\'}', metadata={})]", "category": "generic"}
{"question_id": 51, "text": " I want to build a tool to answer questions automatically from a given document. Which model do you recommend for this task?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'distilbert-base-uncased-distilled-squad\\', \\'api_call\\': \"pipeline(\\'question-answering\\', model=\\'distilbert-base-uncased-distilled-squad\\')\", \\'api_arguments\\': [\\'question\\', \\'context\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nquestion_answerer = pipeline(question-answering, model=\\'distilbert-base-uncased-distilled-squad\\')\\\\ncontext = r\\\\n... Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\\\\n... question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\\\\n... a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\\\\n... \\\\nresult = question_answerer(question=What is a good example of a question answering dataset?, context=context)\\\\nprint(\\\\n... fAnswer: \\'{result[\\'answer\\']}\\', score: {round(result[\\'score\\'], 4)}, start: {result[\\'start\\']}, end: {result[\\'end\\']}\\\\n...)\", \\'performance\\': {\\'dataset\\': \\'SQuAD v1.1\\', \\'accuracy\\': \\'86.9 F1 score\\'}, \\'description\\': \"DistilBERT base uncased distilled SQuAD is a fine-tuned version of DistilBERT-base-uncased for the task of question answering. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT\\'s performances as measured on the GLUE language understanding benchmark.\"}', metadata={})]", "category": "generic"}
{"question_id": 52, "text": " We have a French news agency and we want to categorize the news articles based on sports, politics, and science.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Feature Extraction\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'DeepPavlov/rubert-base-cased\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'DeepPavlov/rubert-base-cased\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'Russian part of Wikipedia and news data\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'RuBERT (Russian, cased, 12\u2011layer, 768\u2011hidden, 12\u2011heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT\u2011base as an initialization for RuBERT[1].\\'}', metadata={})]", "category": "generic"}
{"question_id": 53, "text": " I need a solution to detect whether a piece of news is talking about technology, sports, or politics.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 54, "text": " I want to build a chatbot that is used by language learners who want to communicate in French while they only know English. Generate a response for an English message.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \"PegasusForConditionalGeneration.from_pretrained(\\'tuner007/pegasus_summarizer\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 55, "text": " Translate the following text from French to English: \\u201cLe syst\\u00e8me \\u00e9ducatif fran\\u00e7ais est compos\\u00e9 d'\\u00e9coles maternelles, d'\\u00e9coles \\u00e9l\\u00e9mentaires, de coll\\u00e8ges et de lyc\\u00e9es.\\u201d\\n###Input: Le syst\\u00e8me \\u00e9ducatif fran\\u00e7ais est compos\\u00e9 d'\\u00e9coles maternelles, d'\\u00e9coles \\u00e9l\\u00e9mentaires, de coll\\u00e8ges et de lyc\\u00e9es.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Token Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Named Entity Recognition\\', \\'api_name\\': \\'Jean-Baptiste/camembert-ner\\', \\'api_call\\': \"AutoModelForTokenClassification.from_pretrained(\\'Jean-Baptiste/camembert-ner\\')\", \\'api_arguments\\': {\\'model\\': \\'model\\', \\'tokenizer\\': \\'tokenizer\\', \\'aggregation_strategy\\': \\'simple\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'AutoTokenizer, AutoModelForTokenClassification, pipeline\\'}, \\'example_code\\': \"from transformers import AutoTokenizer, AutoModelForTokenClassification\\\\ntokenizer = AutoTokenizer.from_pretrained(Jean-Baptiste/camembert-ner)\\\\nmodel = AutoModelForTokenClassification.from_pretrained(Jean-Baptiste/camembert-ner)\\\\n\\\\nfrom transformers import pipeline\\\\nnlp = pipeline(\\'ner\\', model=model, tokenizer=tokenizer, aggregation_strategy=simple)\\\\nnlp(Apple est cre le 1er avril 1976 dans le garage de la maison d\\'enfance de Steve Jobs  Los Altos en Californie par Steve Jobs, Steve Wozniak et Ronald Wayne14, puis constitue sous forme de socit le 3 janvier 1977  l\\'origine sous le nom d\\'Apple Computer, mais pour ses 30 ans et pour reflter la diversification de ses produits, le mot \u00ab computer  est retir le 9 janvier 2015.)\", \\'performance\\': {\\'dataset\\': \\'wikiner-fr\\', \\'accuracy\\': {\\'overall_f1\\': 0.8914, \\'PER_f1\\': 0.9483, \\'ORG_f1\\': 0.8181, \\'LOC_f1\\': 0.8955, \\'MISC_f1\\': 0.8146}}, \\'description\\': \\'camembert-ner is a Named Entity Recognition (NER) model fine-tuned from camemBERT on the wikiner-fr dataset. It can recognize entities such as persons, organizations, locations, and miscellaneous entities.\\'}', metadata={})]", "category": "generic"}
{"question_id": 56, "text": " I want to translate a text from one language to another.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 57, "text": " Our team member has written a long article that needs to be published on a company blog. Can you provide a shorter summary to be used as a snippet on the landing page?\\n###Input: \\\"Apple Inc. reported its quarterly earnings results yesterday. The company posted a record-breaking revenue of $123.9 billion for the first quarter of 2022, up by 11% from the same period last year. The increase was fueled by stronger demand for iPhones, iPads, and Macs, as well as continued growth in its services segment. Apple's operating profit for the quarter came in at $38.3 billion, up 17% from a year earlier. The results surpassed analysts' expectations, who had anticipated revenue of around $118 billion. This strong performance is largely attributed to the successful launch of the iPhone 13, which has enjoyed robust sales since its debut in September. Apple CEO Tim Cook said in a statement, \\\"Our record-breaking quarter reflects the strength of our entire ecosystem, from our innovative products and services to the unmatched dedication of our teams around the world.\\\" Despite the ongoing global supply chain disruptions, Apple has managed to maintain its growth trajectory, thanks in part to its vertically integrated operations and nimble supply chain management. The company is expected to face stiffer competition going forward, particularly in the smartphone market, as rivals introduce new devices and increased pricing pressures.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'csebuetnlp/mT5_multilingual_XLSum\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'csebuetnlp/mT5_multilingual_XLSum\\')\", \\'api_arguments\\': [\\'model_name\\'], \\'python_environment_requirements\\': [\\'transformers==4.11.0.dev0\\'], \\'example_code\\': \"import re\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nWHITESPACE_HANDLER = lambda k: re.sub(\\'\\\\\\\\s+\\', \\' \\', re.sub(\\'\\\\\\\\n+\\', \\' \\', k.strip()))\\\\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people\\'s scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We\\'re expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\\\\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\\\ninput_ids = tokenizer(\\\\n [WHITESPACE_HANDLER(article_text)],\\\\n return_tensors=pt,\\\\n padding=max_length,\\\\n truncation=True,\\\\n max_length=512\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n max_length=84,\\\\n no_repeat_ngram_size=2,\\\\n num_beams=4\\\\n)[0]\\\\nsummary = tokenizer.decode(\\\\n output_ids,\\\\n skip_special_tokens=True,\\\\n clean_up_tokenization_spaces=False\\\\n)\\\\nprint(summary)\", \\'performance\\': {\\'dataset\\': \\'xsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.5, \\'ROUGE-2\\': 13.934, \\'ROUGE-L\\': 28.988, \\'ROUGE-LSUM\\': 28.996, \\'loss\\': 2.067, \\'gen_len\\': 26.973}}, \\'description\\': \\'This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 58, "text": " Write a summary of a conference held by the World Health Organization discussing the impacts of climate change on human health.\\n###Input: Over the past week, the World Health Organization held a conference discussing the impacts of climate change on human health. The conference brought together leading experts from around the world to examine the current problems affecting people's health due to changing environmental conditions. The topics of discussion included increased occurrence of heat-related illnesses, heightened rates of vector-borne diseases, and the growing problem of air pollution. The conference concluded with a call to action for governments and organizations to invest in mitigating and adapting to the negative consequences of climate change for the sake of public health.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \"PegasusForConditionalGeneration.from_pretrained(\\'tuner007/pegasus_summarizer\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 59, "text": " Please provide a brief overview of a news article.\\n###Input: A new study suggests that eating chocolate at least once a week can lead to better cognition. The study, published in the journal Appetite, analyzed data from over 900 adults and found that individuals who consumed chocolate at least once a week performed better on cognitive tests than those who consumed chocolate less frequently. Researchers believe that the beneficial effects of chocolate on cognition may be due to the presence of flavonoids, which have been shown to be antioxidant-rich and to improve brain blood flow.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \"PegasusForConditionalGeneration.from_pretrained(\\'tuner007/pegasus_summarizer\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 60, "text": " I developed a document generation app, I need to create a summary of a long article given as input to provide to my users before they read the full article.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 61, "text": " We need a quick summary of a news article we found online. Can you help us with that?\\n###Input: Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people's scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We're expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'csebuetnlp/mT5_multilingual_XLSum\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'csebuetnlp/mT5_multilingual_XLSum\\')\", \\'api_arguments\\': [\\'model_name\\'], \\'python_environment_requirements\\': [\\'transformers==4.11.0.dev0\\'], \\'example_code\\': \"import re\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nWHITESPACE_HANDLER = lambda k: re.sub(\\'\\\\\\\\s+\\', \\' \\', re.sub(\\'\\\\\\\\n+\\', \\' \\', k.strip()))\\\\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people\\'s scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We\\'re expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\\\\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\\\ninput_ids = tokenizer(\\\\n [WHITESPACE_HANDLER(article_text)],\\\\n return_tensors=pt,\\\\n padding=max_length,\\\\n truncation=True,\\\\n max_length=512\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n max_length=84,\\\\n no_repeat_ngram_size=2,\\\\n num_beams=4\\\\n)[0]\\\\nsummary = tokenizer.decode(\\\\n output_ids,\\\\n skip_special_tokens=True,\\\\n clean_up_tokenization_spaces=False\\\\n)\\\\nprint(summary)\", \\'performance\\': {\\'dataset\\': \\'xsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.5, \\'ROUGE-2\\': 13.934, \\'ROUGE-L\\': 28.988, \\'ROUGE-LSUM\\': 28.996, \\'loss\\': 2.067, \\'gen_len\\': 26.973}}, \\'description\\': \\'This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 62, "text": " We'd like our chatbot to act as a fictional character for engaging with our users.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 63, "text": " Write a story about a spaceship journey to a distant planet in search of a new home for humanity.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 64, "text": " I want to write a story about a brave knight and a dragon but I'm unable to come up with a good start. Help me with that.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \"PegasusForConditionalGeneration.from_pretrained(\\'tuner007/pegasus_summarizer\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 65, "text": " I need a text analysis tool that can automatically predict the most plausible missing text in a given sentence.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Conversational\\', \\'api_name\\': \\'microsoft/GODEL-v1_1-large-seq2seq\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'microsoft/GODEL-v1_1-large-seq2seq\\')\", \\'api_arguments\\': {\\'instruction\\': \\'Instruction: given a dialog context, you need to response empathically.\\', \\'knowledge\\': \\'\\', \\'dialog\\': [\\'Does money buy happiness?\\', \\'It is a question. Money buys you a lot of things, but not enough to buy happiness.\\', \\'What is the best way to buy happiness ?\\']}, \\'python_environment_requirements\\': {\\'transformers\\': \\'AutoTokenizer, AutoModelForSeq2SeqLM\\'}, \\'example_code\\': \"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained(microsoft/GODEL-v1_1-large-seq2seq)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(microsoft/GODEL-v1_1-large-seq2seq)\\\\ndef generate(instruction, knowledge, dialog):\\\\n if knowledge != \\'\\':\\\\n knowledge = \\'[KNOWLEDGE] \\' + knowledge\\\\n dialog = \\' EOS \\'.join(dialog)\\\\n query = f{instruction} [CONTEXT] {dialog} {knowledge}\\\\n input_ids = tokenizer(f{query}, return_tensors=pt).input_ids\\\\n outputs = model.generate(input_ids, max_length=128, min_length=8, top_p=0.9, do_sample=True)\\\\n output = tokenizer.decode(outputs[0], skip_special_tokens=True)\\\\n return output\\\\n\\\\ninstruction = f\\'Instruction: given a dialog context, you need to response empathically.\\'\\\\nknowledge = \\'\\'\\\\ndialog = [\\\\n \\'Does money buy happiness?\\',\\\\n \\'It is a question. Money buys you a lot of things, but not enough to buy happiness.\\',\\\\n \\'What is the best way to buy happiness ?\\'\\\\n]\\\\nresponse = generate(instruction, knowledge, dialog)\\\\nprint(response)\", \\'performance\\': {\\'dataset\\': \\'Reddit discussion thread, instruction and knowledge grounded dialogs\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'GODEL is a large-scale pre-trained model for goal-directed dialogs. It is parameterized with a Transformer-based encoder-decoder model and trained for response generation grounded in external text, which allows more effective fine-tuning on dialog tasks that require conditioning the response on information that is external to the current conversation (e.g., a retrieved document). The pre-trained model can be efficiently fine-tuned and adapted to accomplish a new dialog task with a handful of task-specific dialogs. The v1.1 model is trained on 551M multi-turn dialogs from Reddit discussion thread, and 5M instruction and knowledge grounded dialogs.\\'}', metadata={})]", "category": "generic"}
{"question_id": 66, "text": " Help me fill in the blanks in the following Chinese sentence: \\\"\\u4e0a\\u6d77\\u662f\\u4e2d\\u56fd\\u7684[MASK]\\u5927\\u57ce\\u5e02\\u3002\\\"\\n###Input: \\u4e0a\\u6d77\\u662f\\u4e2d\\u56fd\\u7684[MASK]\\u5927\\u57ce\\u5e02\\u3002\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 67, "text": " We are building a source code autocompletion tool which will complete the code snippet containing a masked token.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 68, "text": " I work for a Japanese company, and my manager needs me to take a look at a request from a client. I can understand fluent Japanese, but I need a little help filling in missing words from the text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 69, "text": " We are building a platform to compare and contrast user input sentences with existing sentences in our database. It should return similar results.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 70, "text": " I need a method to compare the similarity between two sentences to be used within a meme generator, so we can produce a meme with a similar caption.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Unconditional Image Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Denoising Diffusion Probabilistic Models (DDPM)\\', \\'api_name\\': \\'google/ddpm-bedroom-256\\', \\'api_call\\': \"DDPMPipeline.from_pretrained(\\'google/ddpm-bedroom-256\\')\", \\'api_arguments\\': \\'None\\', \\'python_environment_requirements\\': \\'diffusers\\', \\'example_code\\': \\'!pip install diffusers\\\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\\\nmodel_id = google/ddpm-bedroom-256\\\\nddpm = DDPMPipeline.from_pretrained(model_id)\\\\nimage = ddpm().images[0]\\\\nimage.save(ddpm_generated_image.png)\\', \\'performance\\': {\\'dataset\\': \\'CIFAR10\\', \\'accuracy\\': {\\'Inception score\\': 9.46, \\'FID score\\': 3.17}}, \\'description\\': \\'We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.\\'}', metadata={})]", "category": "generic"}
{"question_id": 71, "text": " A student is writing a research paper and needs help with finding similar articles in order to include them in the literature review section.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Zero-Shot Image Classification\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Zero-Shot Image Classification\\', \\'api_name\\': \\'microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224\\', \\'api_call\\': \"pipeline(\\'zero-shot-image-classification\\', model=\\'microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224\\')\", \\'api_arguments\\': \\'image, possible_class_names\\', \\'python_environment_requirements\\': \\'transformers, torch, torchvision\\', \\'example_code\\': \"from transformers import pipeline\\\\nclip = pipeline(\\'zero-shot-image-classification\\', model=\\'microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224\\')\\\\nimage = \\'path/to/image.png\\'\\\\npossible_class_names = [\\'class1\\', \\'class2\\', \\'class3\\']\\\\nresult = clip(image, possible_class_names)\", \\'performance\\': {\\'dataset\\': \\'PMC-15M\\', \\'accuracy\\': \\'State of the art\\'}, \\'description\\': \\'BiomedCLIP is a biomedical vision-language foundation model pretrained on PMC-15M, a dataset of 15 million figure-caption pairs extracted from biomedical research articles in PubMed Central, using contrastive learning. It uses PubMedBERT as the text encoder and Vision Transformer as the image encoder, with domain-specific adaptations. It can perform various vision-language processing (VLP) tasks such as cross-modal retrieval, image classification, and visual question answering.\\'}', metadata={})]", "category": "generic"}
{"question_id": 72, "text": " Create a solution to convert a given Japanese sentence into a speech audio file.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'ESPnet\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'mio/Artoria\\', \\'api_call\\': \"pipeline(\\'text-to-speech\\', model=\\'mio/Artoria\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline; tts = pipeline(\\'text-to-speech\\', model=\\'mio/Artoria\\'); tts(\\'s\\')\", \\'performance\\': {\\'dataset\\': \\'fate\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'This model was trained by mio using fate recipe in espnet. It is a text-to-speech model that can convert text input into speech output.\\'}', metadata={})]", "category": "generic"}
{"question_id": 73, "text": " We are working on a transcription service for our customers. We need a way to convert audio files into text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 74, "text": " We are creating an online video conference service, and we need to detect when two or more speakers are speaking at the same time in the audio.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Automatic Speech Recognition\\', \\'framework\\': \\'pyannote.audio\\', \\'functionality\\': \\'overlapped-speech-detection\\', \\'api_name\\': \\'pyannote/overlapped-speech-detection\\', \\'api_call\\': \"pipeline.from_pretrained(\\'pyannote/overlapped-speech-detection\\', use_auth_token=\\'ACCESS_TOKEN_GOES_HERE\\')\", \\'api_arguments\\': [\\'audio.wav\\'], \\'python_environment_requirements\\': [\\'pyannote.audio 2.1\\'], \\'example_code\\': \\'from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained(pyannote/overlapped-speech-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\\\noutput = pipeline(audio.wav)\\\\nfor speech in output.get_timeline().support():\\\\n  # two or more speakers are active between speech.start and speech.end\\\\n  ...\\', \\'performance\\': {\\'dataset\\': \\'ami\\', \\'accuracy\\': None}, \\'description\\': \\'Automatic overlapped speech detection using pyannote.audio framework. The model detects when two or more speakers are active in an audio file.\\'}', metadata={})]", "category": "generic"}
{"question_id": 75, "text": " Our company develops smart speaker devices that involve interaction with the user. We need to transcribe the input from the users with the maintained accent or language.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'csebuetnlp/mT5_multilingual_XLSum\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'csebuetnlp/mT5_multilingual_XLSum\\')\", \\'api_arguments\\': [\\'model_name\\'], \\'python_environment_requirements\\': [\\'transformers==4.11.0.dev0\\'], \\'example_code\\': \"import re\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nWHITESPACE_HANDLER = lambda k: re.sub(\\'\\\\\\\\s+\\', \\' \\', re.sub(\\'\\\\\\\\n+\\', \\' \\', k.strip()))\\\\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people\\'s scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We\\'re expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\\\\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\\\ninput_ids = tokenizer(\\\\n [WHITESPACE_HANDLER(article_text)],\\\\n return_tensors=pt,\\\\n padding=max_length,\\\\n truncation=True,\\\\n max_length=512\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n max_length=84,\\\\n no_repeat_ngram_size=2,\\\\n num_beams=4\\\\n)[0]\\\\nsummary = tokenizer.decode(\\\\n output_ids,\\\\n skip_special_tokens=True,\\\\n clean_up_tokenization_spaces=False\\\\n)\\\\nprint(summary)\", \\'performance\\': {\\'dataset\\': \\'xsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.5, \\'ROUGE-2\\': 13.934, \\'ROUGE-L\\': 28.988, \\'ROUGE-LSUM\\': 28.996, \\'loss\\': 2.067, \\'gen_len\\': 26.973}}, \\'description\\': \\'This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 76, "text": " One of our clients is facing noise issues on their audio recordings. Can you help them to remove the noise from the audio?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 77, "text": " We are a media company and we have a large volume of Chinese language audio files. We want to transcribe the audios into chinese text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 78, "text": " Help us improve the listener experience from our customers by enhancing the audio of noisy recordings.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 79, "text": " Our company is working on a project to automatically translate spoken English audio to spoken Hokkien audio. We need a speech-to-speech translation model.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Automatic Speech Recognition\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'microsoft/wavlm-large\\', \\'api_call\\': \"Wav2Vec2Model.from_pretrained(\\'microsoft/wavlm-large\\')\", \\'api_arguments\\': \\'speech input\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'To fine-tune the model for speech recognition, see the official speech recognition example. To fine-tune the model for speech classification, see the official audio classification example.\\', \\'performance\\': {\\'dataset\\': \\'SUPERB benchmark\\', \\'accuracy\\': \\'state-of-the-art performance\\'}, \\'description\\': \\'WavLM-Large is a large model pretrained on 16kHz sampled speech audio. It is built based on the HuBERT framework, with an emphasis on both spoken content modeling and speaker identity preservation. WavLM is pretrained on 60,000 hours of Libri-Light, 10,000 hours of GigaSpeech, and 24,000 hours of VoxPopuli. It achieves state-of-the-art performance on the SUPERB benchmark and brings significant improvements for various speech processing tasks on their representative benchmarks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 80, "text": " We are a startup developing voice assistants. We need a keyword spotting system that can recognize user commands.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 81, "text": " The model needs to have speech recognition capability to identify languages in a given audio file.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Automatic Speech Recognition\\', \\'framework\\': \\'pyannote.audio\\', \\'functionality\\': \\'overlapped-speech-detection\\', \\'api_name\\': \\'pyannote/overlapped-speech-detection\\', \\'api_call\\': \"pipeline.from_pretrained(\\'pyannote/overlapped-speech-detection\\', use_auth_token=\\'ACCESS_TOKEN_GOES_HERE\\')\", \\'api_arguments\\': [\\'audio.wav\\'], \\'python_environment_requirements\\': [\\'pyannote.audio 2.1\\'], \\'example_code\\': \\'from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained(pyannote/overlapped-speech-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\\\noutput = pipeline(audio.wav)\\\\nfor speech in output.get_timeline().support():\\\\n  # two or more speakers are active between speech.start and speech.end\\\\n  ...\\', \\'performance\\': {\\'dataset\\': \\'ami\\', \\'accuracy\\': None}, \\'description\\': \\'Automatic overlapped speech detection using pyannote.audio framework. The model detects when two or more speakers are active in an audio file.\\'}', metadata={})]", "category": "generic"}
{"question_id": 82, "text": " I have just recorded a meeting, I want to find the best segments from the audio where people are speaking, and construct a summary.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'distilbart-cnn-12-6-samsum\\', \\'api_call\\': \"pipeline(\\'summarization\\', model=\\'philschmid/distilbart-cnn-12-6-samsum\\')\", \\'api_arguments\\': {\\'model\\': \\'philschmid/distilbart-cnn-12-6-samsum\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 41.09, \\'ROUGE-2\\': 20.746, \\'ROUGE-L\\': 31.595, \\'ROUGE-LSUM\\': 38.339}}, \\'description\\': \\'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 83, "text": " I am running a wine store, and I am looking for a machine learning model that can help me classify the quality of wine based on some given features.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 84, "text": " Build a simple application to predict the survival status of passengers on the Titanic based on their age, gender, and passenger class.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Classification\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Binary Classification\\', \\'api_name\\': \\'harithapliyal/autotrain-tatanic-survival-51030121311\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'harithapliyal/autotrain-tatanic-survival-51030121311\\')\", \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'harithapliyal/autotrain-data-tatanic-survival\\', \\'accuracy\\': 0.872}, \\'description\\': \\'A tabular classification model trained on the Titanic survival dataset using Hugging Face AutoTrain. The model predicts whether a passenger survived or not based on features such as age, gender, and passenger class.\\'}', metadata={})]", "category": "generic"}
{"question_id": 85, "text": " I need to estimate CO2 emissions from vehicles based on their characteristics, such as engine size, transmission type, and miles traveled.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Classification\\', \\'framework\\': \\'Joblib\\', \\'functionality\\': \\'Carbon Emissions\\', \\'api_name\\': \\'tejas23/autotrain-amx2-1702259728\\', \\'api_call\\': \"joblib.load(\\'model.joblib\\')\", \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'Validation Metrics\\', \\'accuracy\\': 0.831}, \\'description\\': \\'A multi-class classification model trained using AutoTrain to predict CO2 emissions based on tabular data.\\'}', metadata={})]", "category": "generic"}
{"question_id": 86, "text": " We have been asked to predict future criminal re-offense from a given dataset. What model should we adopt and how do we proceed?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'financial-sentiment-analysis\\', \\'api_name\\': \\'yiyanghkust/finbert-tone\\', \\'api_call\\': \"BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\", \\'api_arguments\\': [\\'sentences\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import BertTokenizer, BertForSequenceClassification\\\\nfrom transformers import pipeline\\\\nfinbert = BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\\\\ntokenizer = BertTokenizer.from_pretrained(\\'yiyanghkust/finbert-tone\\')\\\\nnlp = pipeline(sentiment-analysis, model=finbert, tokenizer=tokenizer)\\\\nsentences = [there is a shortage of capital, and we need extra financing,\\\\n growth is strong and we have plenty of liquidity,\\\\n there are doubts about our finances,\\\\n profits are flat]\\\\nresults = nlp(sentences)\\\\nprint(results)\", \\'performance\\': {\\'dataset\\': \\'10,000 manually annotated sentences from analyst reports\\', \\'accuracy\\': \\'superior performance on financial tone analysis task\\'}, \\'description\\': \\'FinBERT is a BERT model pre-trained on financial communication text. It is trained on the following three financial communication corpus: Corporate Reports 10-K & 10-Q, Earnings Call Transcripts, and Analyst Reports. This released finbert-tone model is the FinBERT model fine-tuned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports. This model achieves superior performance on financial tone analysis task.\\'}', metadata={})]", "category": "generic"}
{"question_id": 87, "text": " Our company's goal is to predict carbon emissions based on the given features of the compound.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Regression\\', \\'framework\\': \\'Joblib\\', \\'functionality\\': \\'Carbon Emissions\\', \\'api_name\\': \\'pcoloc/autotrain-600-dragino-1839063122\\', \\'api_call\\': \"joblib.load(\\'model.joblib\\')\", \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'pcoloc/autotrain-data-600-dragino\\', \\'accuracy\\': {\\'Loss\\': 93.595, \\'R2\\': 0.502, \\'MSE\\': 8760.052, \\'MAE\\': 77.527, \\'RMSLE\\': 0.445}}, \\'description\\': \\'This model is trained to perform single column regression on carbon emissions data using the AutoTrain framework. It predicts CO2 emissions in grams given the input data.\\'}', metadata={})]", "category": "generic"}
{"question_id": 88, "text": " The factory wants to make its production process more eco-friendly. Calculate the carbon emissions for given data.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Regression\\', \\'framework\\': \\'Joblib\\', \\'functionality\\': \\'Carbon Emissions\\', \\'api_name\\': \\'pcoloc/autotrain-600-dragino-1839063122\\', \\'api_call\\': \"joblib.load(\\'model.joblib\\')\", \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'pcoloc/autotrain-data-600-dragino\\', \\'accuracy\\': {\\'Loss\\': 93.595, \\'R2\\': 0.502, \\'MSE\\': 8760.052, \\'MAE\\': 77.527, \\'RMSLE\\': 0.445}}, \\'description\\': \\'This model is trained to perform single column regression on carbon emissions data using the AutoTrain framework. It predicts CO2 emissions in grams given the input data.\\'}', metadata={})]", "category": "generic"}
{"question_id": 89, "text": " We want to predict the carbon emissions of a new line of electric vehicles for an annual report. Automate the process of loading a regression model, then calculate the forecast of emissions for this year.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 90, "text": " We are planning to launch a website which provides tips to people for their daily lives. Can you please build a model to predict the appropriate amount of tips?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 91, "text": " We have a robotic arm in our warehouse that needs to be trained to optimize loading and unloading tasks. The robotic arm is based on the CartPole environment.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 92, "text": " There is an upcoming event called \\\"Space Party\\\" and we need a representative image for the event. Can you assist us in creating an image containing a party in space with astronauts and aliens having fun together?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Visual Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'git-large-textvqa\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'microsoft/git-large-textvqa\\')\", \\'api_arguments\\': \\'image, question\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'For code examples, we refer to the documentation.\\', \\'performance\\': {\\'dataset\\': \\'TextVQA\\', \\'accuracy\\': \\'See table 11 in the paper for more details.\\'}, \\'description\\': \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextVQA. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using \\'teacher forcing\\' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like: image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\"}', metadata={})]", "category": "generic"}
{"question_id": 93, "text": " We're creating a promotional image for a wildlife-themed event. We need to display two tigers in a natural setting.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Feature Extraction\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'facebook/dragon-plus-context-encoder\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'facebook/dragon-plus-context-encoder\\')\", \\'api_arguments\\': [\\'pretrained\\'], \\'python_environment_requirements\\': [\\'torch\\', \\'transformers\\'], \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModel\\\\ntokenizer = AutoTokenizer.from_pretrained(\\'facebook/dragon-plus-query-encoder\\')\\\\nquery_encoder = AutoModel.from_pretrained(\\'facebook/dragon-plus-query-encoder\\')\\\\ncontext_encoder = AutoModel.from_pretrained(\\'facebook/dragon-plus-context-encoder\\')\\\\nquery = \\'Where was Marie Curie born?\\'\\\\ncontexts = [\\\\n  \\'Maria Sklodowska, later known as Marie Curie, was born on November 7, 1867.\\',\\\\n  \\'Born in Paris on 15 May 1859, Pierre Curie was the son of Eug\u00e8ne Curie, a doctor of French Catholic origin from Alsace.\\'\\\\n]\\\\nquery_input = tokenizer(query, return_tensors=\\'pt\\')\\\\nctx_input = tokenizer(contexts, padding=True, truncation=True, return_tensors=\\'pt\\')\\\\nquery_emb = query_encoder(query_input).last_hidden_state[:, 0, :]\\\\nctx_emb = context_encoder(ctx_input).last_hidden_state[:, 0, :]\\\\nscore1 = query_emb @ ctx_emb[0]\\\\nscore2 = query_emb @ ctx_emb[1]\", \\'performance\\': {\\'dataset\\': \\'MS MARCO\\', \\'accuracy\\': 39.0}, \\'description\\': \\'DRAGON+ is a BERT-base sized dense retriever initialized from RetroMAE and further trained on the data augmented from MS MARCO corpus, following the approach described in How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval. The associated GitHub repository is available here https://github.com/facebookresearch/dpr-scale/tree/main/dragon. We use asymmetric dual encoder, with two distinctly parameterized encoders.\\'}', metadata={})]", "category": "generic"}
{"question_id": 94, "text": " We have a collection of low-resolution images of movie characters, and we need to upscale those images to get a more detailed high-resolution image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'Kirili4ik/mbart_ruDialogSum\\', \\'api_call\\': \"MBartForConditionalGeneration.from_pretrained(\\'Kirili4ik/mbart_ruDialogSum\\')\", \\'api_arguments\\': {\\'model_name\\': \\'Kirili4ik/mbart_ruDialogSum\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import MBartTokenizer, MBartForConditionalGeneration\\\\nmodel_name = Kirili4ik/mbart_ruDialogSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = MBartForConditionalGeneration.from_pretrained(model_name)\\\\nmodel.eval()\\\\narticle_text = ...\\\\ninput_ids = tokenizer(\\\\n [article_text],\\\\n max_length=600,\\\\n padding=max_length,\\\\n truncation=True,\\\\n return_tensors=pt,\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n top_k=0,\\\\n num_beams=3,\\\\n no_repeat_ngram_size=3\\\\n)[0]\\\\nsummary = tokenizer.decode(output_ids, skip_special_tokens=True)\\\\nprint(summary)\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'SAMSum Corpus (translated to Russian)\\', \\'accuracy\\': {\\'Validation ROGUE-1\\': 34.5, \\'Validation ROGUE-L\\': 33, \\'Test ROGUE-1\\': 31, \\'Test ROGUE-L\\': 28}}]}, \\'description\\': \\'MBart for Russian summarization fine-tuned for dialogues summarization. This model was firstly fine-tuned by Ilya Gusev on Gazeta dataset. We have fine tuned that model on SamSum dataset translated to Russian using GoogleTranslateAPI. Moreover! We have implemented a ! telegram bot @summarization_bot ! with the inference of this model. Add it to the chat and get summaries instead of dozens spam messages!\\'}', metadata={})]", "category": "generic"}
{"question_id": 95, "text": " I want you to create a function that generates captions for a list of images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 96, "text": " We need a tool to help us generate textual descriptions for images and videos related to our product.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 97, "text": " We need to build an AI-powered tool to assist visually impaired users in understanding their surroundings by answering questions about images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 98, "text": " I'm a nutritionist and want to help my clients by answering questions about their meals. They will send me an image of their food and ask me a question about it, like \\\"Is this vegan?\\\" or \\\"How many calories do you think it contains?\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'PyTorch Transformers\\', \\'functionality\\': \\'Table Question Answering\\', \\'api_name\\': \\'table-question-answering-tapas\\', \\'api_call\\': \"pipeline(\\'table-question-answering\\', model=\\'Meena/table-question-answering-tapas\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'This model can be loaded on the Inference API on-demand.\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'SQA (Sequential Question Answering by Microsoft)\\', \\'accuracy\\': None}, {\\'name\\': \\'WTQ (Wiki Table Questions by Stanford University)\\', \\'accuracy\\': None}, {\\'name\\': \\'WikiSQL (by Salesforce)\\', \\'accuracy\\': None}]}, \\'description\\': \\'TAPAS, the model learns an inner representation of the English language used in tables and associated texts, which can then be used to extract features useful for downstream tasks such as answering questions about a table, or determining whether a sentence is entailed or refuted by the contents of a table. It is a BERT-based model specifically designed (and pre-trained) for answering questions about tabular data. TAPAS uses relative position embeddings and has 7 token types that encode tabular structure. It is pre-trained on the masked language modeling (MLM) objective on a large dataset comprising millions of tables from English Wikipedia and corresponding texts.\\'}', metadata={})]", "category": "generic"}
{"question_id": 99, "text": " Our client is a legal firm that needs assistance in extracting specific information from a large number of legal documents. Automate the process of answering questions related to these documents.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Fill-Mask\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Fill-Mask\\', \\'api_name\\': \\'nlpaueb/legal-bert-small-uncased\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'nlpaueb/legal-bert-small-uncased\\')\", \\'api_arguments\\': {\\'pretrained_model_name_or_path\\': \\'nlpaueb/legal-bert-small-uncased\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'AutoTokenizer, AutoModel\\'}, \\'example_code\\': \"from transformers import AutoTokenizer, AutoModel\\\\ntokenizer = AutoTokenizer.from_pretrained(\\'nlpaueb/legal-bert-small-uncased\\')\\\\nmodel = AutoModel.from_pretrained(\\'nlpaueb/legal-bert-small-uncased\\')\", \\'performance\\': {\\'dataset\\': \\'Legal Corpora\\', \\'accuracy\\': \\'Comparable to larger models\\'}, \\'description\\': \\'LEGAL-BERT is a family of BERT models for the legal domain, intended to assist legal NLP research, computational law, and legal technology applications. This is the light-weight version of BERT-BASE (33% the size of BERT-BASE) pre-trained from scratch on legal data, which achieves comparable performance to larger models, while being much more efficient (approximately 4 times faster) with a smaller environmental footprint.\\'}', metadata={})]", "category": "generic"}
{"question_id": 100, "text": " In a healthcare company, we are trying to create an automated system for answering patient-related questions based on their medical documents. We need a solution using NLP.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 101, "text": " Develop a program which can answer questions related to a scanned document.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Table Question Answering\\', \\'api_name\\': \\'google/tapas-small-finetuned-wikisql-supervised\\', \\'api_call\\': \"TapasForQuestionAnswering.from_pretrained(\\'google/tapas-small-finetuned-wikisql-supervised\\')\", \\'api_arguments\\': \"model = TapasForQuestionAnswering.from_pretrained(\\'google/tapas-small-finetuned-wikisql-supervised\\')\", \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import TapasTokenizer, TapasForQuestionAnswering\\\\ntokenizer = TapasTokenizer.from_pretrained(\\'google/tapas-small-finetuned-wikisql-supervised\\')\\\\nmodel = TapasForQuestionAnswering.from_pretrained(\\'google/tapas-small-finetuned-wikisql-supervised\\')\", \\'performance\\': {\\'dataset\\': \\'wikisql\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'TAPAS is a BERT-like transformers model pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion. This model is fine-tuned on WikiSQL and can be used for answering questions related to a table.\\'}', metadata={})]", "category": "generic"}
{"question_id": 102, "text": " I have received a PDF document and a question. My task is to find the answer part in the document.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'distilbert-base-uncased-distilled-squad\\', \\'api_call\\': \"pipeline(\\'question-answering\\', model=\\'distilbert-base-uncased-distilled-squad\\')\", \\'api_arguments\\': [\\'question\\', \\'context\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nquestion_answerer = pipeline(question-answering, model=\\'distilbert-base-uncased-distilled-squad\\')\\\\ncontext = r\\\\n... Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\\\\n... question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\\\\n... a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\\\\n... \\\\nresult = question_answerer(question=What is a good example of a question answering dataset?, context=context)\\\\nprint(\\\\n... fAnswer: \\'{result[\\'answer\\']}\\', score: {round(result[\\'score\\'], 4)}, start: {result[\\'start\\']}, end: {result[\\'end\\']}\\\\n...)\", \\'performance\\': {\\'dataset\\': \\'SQuAD v1.1\\', \\'accuracy\\': \\'86.9 F1 score\\'}, \\'description\\': \"DistilBERT base uncased distilled SQuAD is a fine-tuned version of DistilBERT-base-uncased for the task of question answering. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT\\'s performances as measured on the GLUE language understanding benchmark.\"}', metadata={})]", "category": "generic"}
{"question_id": 103, "text": " An interior design firm builds a software to understand the depth of rooms captured in photographs for remodeling activities.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text2Text Generation\\', \\'api_name\\': \\'DialogLED-base-16384\\', \\'api_call\\': \"LEDForConditionalGeneration.from_pretrained(\\'MingZhong/DialogLED-base-16384\\')\", \\'api_arguments\\': \\'input_text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'arxiv\\', \\'accuracy\\': \\'2109.02492\\'}, \\'description\\': \\'DialogLED is a pre-trained model for long dialogue understanding and summarization. It builds on the Longformer-Encoder-Decoder (LED) architecture and uses window-based denoising as the pre-training task on a large amount of long dialogue data for further training. Here is a base version of DialogLED, the input length is limited to 16,384 in the pre-training phase.\\'}', metadata={})]", "category": "generic"}
{"question_id": 104, "text": " We are running an autonomous vehicle company and want to implement a depth estimation module for the real-time video feed captured by our camera.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 105, "text": " Our team wants to create a new app for autonomous vehicles. For that, we need to estimate the depth of the field from images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'dreamlike-art/dreamlike-photoreal-2.0\\', \\'api_call\\': \"StableDiffusionPipeline.from_pretrained(\\'dreamlike-art/dreamlike-photoreal-2.0\\', torch_dtype=torch.float16)(prompt).images[0]\", \\'api_arguments\\': {\\'prompt\\': \\'photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens\\'}, \\'python_environment_requirements\\': {\\'torch\\': \\'torch.float16\\', \\'diffusers\\': \\'StableDiffusionPipeline\\'}, \\'example_code\\': \\'from diffusers import StableDiffusionPipeline\\\\nimport torch\\\\nmodel_id = dreamlike-art/dreamlike-photoreal-2.0\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(./result.jpg)\\', \\'performance\\': {\\'dataset\\': \\'Stable Diffusion 1.5\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Dreamlike Photoreal 2.0 is a photorealistic model based on Stable Diffusion 1.5, made by dreamlike.art. It can be used to generate photorealistic images from text prompts.\\'}', metadata={})]", "category": "generic"}
{"question_id": 106, "text": " As a city planner, I need to measure the depth of spaces in a series of images taken from streets.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Zero-Shot Image Classification\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Zero-Shot Image Classification\\', \\'api_name\\': \\'laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup\\', \\'api_call\\': \"pipeline(\\'image-classification\\', model=\\'laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup\\')\", \\'api_arguments\\': \\'image_path, class_names\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"results = model(image_path, class_names=\\'cat, dog, bird\\')\", \\'performance\\': {\\'dataset\\': \\'ImageNet-1k\\', \\'accuracy\\': \\'76.9\\'}, \\'description\\': \\'A series of CLIP ConvNeXt-Large (w/ extra text depth, vision MLP head) models trained on the LAION-2B (english) subset of LAION-5B using OpenCLIP. The models utilize the timm ConvNeXt-Large model (convnext_large) as the image tower, a MLP (fc - gelu - drop - fc) head in vision tower instead of the single projection of other CLIP models, and a text tower with same width but 4 layers more depth than ViT-L / RN50x16 models (depth 16, embed dim 768).\\'}', metadata={})]", "category": "generic"}
{"question_id": 107, "text": " In our online ecommerce platform, we want to build an AI app to automatically recognize the type of products. It should be able to identify common items like clothing, electronics, furniture, and more.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 108, "text": " We need to recognize the breed of dog in the given image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 109, "text": " Develop a solution that can categorize an image of a cell phone, laptop, or smartwatch as one of these respective device types.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'zero-shot-object-detection\\', \\'api_name\\': \\'google/owlvit-base-patch16\\', \\'api_call\\': \"OwlViTForObjectDetection.from_pretrained(\\'google/owlvit-base-patch16\\')\", \\'api_arguments\\': [\\'texts\\', \\'images\\'], \\'python_environment_requirements\\': [\\'requests\\', \\'PIL\\', \\'torch\\', \\'transformers\\'], \\'example_code\\': \\'processor = OwlViTProcessor.from_pretrained(google/owlvit-base-patch16)\\\\nmodel = OwlViTForObjectDetection.from_pretrained(google/owlvit-base-patch16)\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ntexts = [[a photo of a cat, a photo of a dog]]\\\\ninputs = processor(text=texts, images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\ntarget_sizes = torch.Tensor([image.size[::-1]])\\\\nresults = processor.post_process(outputs=outputs, target_sizes=target_sizes)\\', \\'performance\\': {\\'dataset\\': \\'COCO\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. OWL-ViT uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features.\\'}', metadata={})]", "category": "generic"}
{"question_id": 110, "text": " Build a system to help companies identify logos from a collection of images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 111, "text": " Develop a pipeline that detects objects present in an image using computer vision.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Voice Activity Detection\\', \\'framework\\': \\'pyannote.audio\\', \\'functionality\\': \\'Automatic Speech Recognition\\', \\'api_name\\': \\'pyannote/voice-activity-detection\\', \\'api_call\\': \"Pipeline.from_pretrained(\\'pyannote/voice-activity-detection\\')\", \\'api_arguments\\': [\\'audio.wav\\'], \\'python_environment_requirements\\': [\\'pyannote.audio 2.1\\'], \\'example_code\\': \\'from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained(pyannote/voice-activity-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\\\noutput = pipeline(audio.wav)\\\\nfor speech in output.get_timeline().support():\\\\n  # active speech between speech.start and speech.end\\', \\'performance\\': {\\'dataset\\': \\'ami\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'A pretrained voice activity detection pipeline that detects active speech in audio files.\\'}', metadata={})]", "category": "generic"}
{"question_id": 112, "text": " Assit me to process and segment an image for further analysis.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Unconditional Image Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Unconditional Image Generation\\', \\'api_name\\': \\'CompVis/ldm-celebahq-256\\', \\'api_call\\': \"DiffusionPipeline.from_pretrained(\\'CompVis/ldm-celebahq-256\\')\", \\'api_arguments\\': [\\'model_id\\'], \\'python_environment_requirements\\': [\\'diffusers\\'], \\'example_code\\': \\'!pip install diffusers\\\\nfrom diffusers import DiffusionPipeline\\\\nmodel_id = CompVis/ldm-celebahq-256\\\\npipeline = DiffusionPipeline.from_pretrained(model_id)\\\\nimage = pipeline(num_inference_steps=200)[sample]\\\\nimage[0].save(ldm_generated_image.png)\\', \\'performance\\': {\\'dataset\\': \\'CelebA-HQ\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'Latent Diffusion Models (LDMs) achieve state-of-the-art synthesis results on image data and beyond by decomposing the image formation process into a sequential application of denoising autoencoders. LDMs enable high-resolution synthesis, semantic scene synthesis, super-resolution, and image inpainting while significantly reducing computational requirements compared to pixel-based DMs.\\'}', metadata={})]", "category": "generic"}
{"question_id": 113, "text": " We need to analyze satellite images to categorize the types of land use. For this purpose, I need to segment the images and identify different objects.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 114, "text": " We are a city planning department and want to evaluate the city layout. Analyze the image we provide to segment and understand the various urban elements.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 115, "text": " My company develops drones for agriculture purposes, and we need a model to segment aerial images accurately.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image Generation\\', \\'api_name\\': \\'stabilityai/stable-diffusion-2\\', \\'api_call\\': \"StableDiffusionPipeline.from_pretrained(\\'stabilityai/stable-diffusion-2\\', scheduler=EulerDiscreteScheduler.from_pretrained(\\'stabilityai/stable-diffusion-2\\', subfolder=scheduler), torch_dtype=torch.float16)\", \\'api_arguments\\': {\\'prompt\\': \\'a photo of an astronaut riding a horse on mars\\'}, \\'python_environment_requirements\\': [\\'diffusers\\', \\'transformers\\', \\'accelerate\\', \\'scipy\\', \\'safetensors\\'], \\'example_code\\': \\'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\\\nmodel_id = stabilityai/stable-diffusion-2\\\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = a photo of an astronaut riding a horse on mars\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(astronaut_rides_horse.png)\\', \\'performance\\': {\\'dataset\\': \\'COCO2017 validation set\\', \\'accuracy\\': \\'Not optimized for FID scores\\'}, \\'description\\': \\'Stable Diffusion v2 is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is primarily intended for research purposes, such as safe deployment of models with potential to generate harmful content, understanding limitations and biases of generative models, and generation of artworks for design and artistic processes.\\'}', metadata={})]", "category": "generic"}
{"question_id": 116, "text": " I want to generate images from text descriptions and use the scribble images as control inputs for my project.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Image-to-Image\\', \\'api_name\\': \\'GreeneryScenery/SheepsControlV3\\', \\'api_call\\': \"pipeline(\\'image-to-image\\', model=\\'GreeneryScenery/SheepsControlV3\\')\", \\'api_arguments\\': {\\'image\\': \\'Path to image file\\', \\'text_guidance\\': \\'Optional text guidance for the model\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'latest\\', \\'torch\\': \\'latest\\'}, \\'example_code\\': [\\'from transformers import pipeline\\', \"model = pipeline(\\'image-to-image\\', model=\\'GreeneryScenery/SheepsControlV3\\')\", \"result = model({\\'image\\': \\'path/to/image.jpg\\', \\'text_guidance\\': \\'Optional text guidance\\'})\"], \\'performance\\': {\\'dataset\\': \\'GreeneryScenery/SheepsControlV3\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\\'}', metadata={})]", "category": "generic"}
{"question_id": 118, "text": " We want to recommend workouts to our users, based on the type of sports they enjoy. Help us classify sports videos.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 119, "text": " We need to classify videos showing different actions for our new video moderation system.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 120, "text": " I need the AI to tell if an image is of a cat or a dog, regardless of its breed or species.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 121, "text": " We have a set of pictures for pets (dogs and cats). We need to offer an AI-based solution to classify the pictures given the pet name.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 122, "text": " Our startup team is now building an app for diagnosing plant diseases based on images. We need to get the diagnosis for different types of plant issues.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Generative Commonsense Reasoning\\', \\'api_name\\': \\'mrm8488/t5-base-finetuned-common_gen\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'mrm8488/t5-base-finetuned-common_gen\\')\", \\'api_arguments\\': [\\'words\\', \\'max_length\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import AutoModelWithLMHead, AutoTokenizer\\\\ntokenizer = AutoTokenizer.from_pretrained(mrm8488/t5-base-finetuned-common_gen)\\\\nmodel = AutoModelWithLMHead.from_pretrained(mrm8488/t5-base-finetuned-common_gen)\\\\ndef gen_sentence(words, max_length=32):\\\\n input_text = words\\\\n features = tokenizer([input_text], return_tensors=\\'pt\\')\\\\noutput = model.generate(input_ids=features[\\'input_ids\\'], attention_mask=features[\\'attention_mask\\'], max_length=max_length)\\\\nreturn tokenizer.decode(output[0], skip_special_tokens=True)\\\\nwords = tree plant ground hole dig\\\\ngen_sentence(words)\", \\'performance\\': {\\'dataset\\': \\'common_gen\\', \\'accuracy\\': {\\'ROUGE-2\\': 17.1, \\'ROUGE-L\\': 39.47}}, \\'description\\': \"Google\\'s T5 fine-tuned on CommonGen for Generative Commonsense Reasoning. CommonGen is a constrained text generation task, associated with a benchmark dataset, to explicitly test machines for the ability of generative commonsense reasoning. Given a set of common concepts; the task is to generate a coherent sentence describing an everyday scenario using these concepts.\"}', metadata={})]", "category": "generic"}
{"question_id": 123, "text": " We need to analyze customer reviews and find out how well our new product is doing in the market.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 124, "text": " A new tutoring company is founded, and they want a tutoring AI. To do so, they need help in creating better explanations for a chemistry concept.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'financial-summarization-pegasus\\', \\'api_call\\': \"PegasusForConditionalGeneration.from_pretrained(\\'human-centered-summarization/financial-summarization-pegasus\\')\", \\'api_arguments\\': [\\'model_name\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import PegasusTokenizer, PegasusForConditionalGeneration, TFPegasusForConditionalGeneration\\\\nmodel_name = human-centered-summarization/financial-summarization-pegasus\\\\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\\\\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name)\\\\ntext_to_summarize = National Commercial Bank (NCB), Saudi Arabia\u2019s largest lender by assets, agreed to buy rival Samba Financial Group for $15 billion in the biggest banking takeover this year.NCB will pay 28.45 riyals ($7.58) for each Samba share, according to a statement on Sunday, valuing it at about 55.7 billion riyals. NCB will offer 0.739 new shares for each Samba share, at the lower end of the 0.736-0.787 ratio the banks set when they signed an initial framework agreement in June.The offer is a 3.5% premium to Samba\u2019s Oct. 8 closing price of 27.50 riyals and about 24% higher than the level the shares traded at before the talks were made public. Bloomberg News first reported the merger discussions.The new bank will have total assets of more than $220 billion, creating the Gulf region\u2019s third-largest lender. The entity\u2019s $46 billion market capitalization nearly matches that of Qatar National Bank QPSC, which is still the Middle East\u2019s biggest lender with about $268 billion of assets.\\\\ninput_ids = tokenizer(text_to_summarize, return_tensors=pt).input_ids\\\\noutput = model.generate(input_ids, max_length=32, num_beams=5, early_stopping=True)\\\\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\\', \\'performance\\': {\\'dataset\\': \\'xsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 35.206, \\'ROUGE-2\\': 16.569, \\'ROUGE-L\\': 30.128, \\'ROUGE-LSUM\\': 30.171}}, \\'description\\': \\'This model was fine-tuned on a novel financial news dataset, which consists of 2K articles from Bloomberg, on topics such as stock, markets, currencies, rate and cryptocurrencies. It is based on the PEGASUS model and in particular PEGASUS fine-tuned on the Extreme Summarization (XSum) dataset: google/pegasus-xsum model. PEGASUS was originally proposed by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu in PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization.\\'}', metadata={})]", "category": "generic"}
{"question_id": 125, "text": " Create a function that can determine if a given text is a question or a statement.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'results-yelp\\', \\'api_call\\': \"AutoTokenizer.from_pretrained(\\'bert-base-uncased\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"AutoTokenizer.from_pretrained(\\'bert-base-uncased\\')\", \\'config\\': \"AutoConfig.from_pretrained(\\'potatobunny/results-yelp\\')\"}, \\'python_environment_requirements\\': {\\'Transformers\\': \\'4.18.0\\', \\'Pytorch\\': \\'1.10.0+cu111\\', \\'Datasets\\': \\'2.0.0\\', \\'Tokenizers\\': \\'0.12.1\\'}, \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'Yelp\\', \\'accuracy\\': 0.9302}, \\'description\\': \\'This model is a fine-tuned version of textattack/bert-base-uncased-yelp-polarity on a filtered and manually reviewed Yelp dataset containing restaurant reviews only. It is intended to perform text classification, specifically sentiment analysis, on text data obtained from restaurant reviews to determine if the particular review is positive or negative.\\'}', metadata={})]", "category": "generic"}
{"question_id": 126, "text": " I want to create a system that can answer questions by sorting out possible answers to a question.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'deepset/roberta-large-squad2\\', \\'api_call\\': \"pipeline(\\'question-answering\\', model=\\'deepset/roberta-large-squad2\\')\", \\'api_arguments\\': [\\'question\\', \\'context\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline; nlp = pipeline(\\'question-answering\\', model=\\'deepset/roberta-large-squad2\\'); nlp({\\'question\\': \\'What is the capital of Germany?\\', \\'context\\': \\'Berlin is the capital of Germany.\\'})\", \\'performance\\': {\\'dataset\\': \\'squad_v2\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'A pre-trained RoBERTa model for question answering tasks, specifically trained on the SQuAD v2 dataset. It can be used to answer questions based on a given context.\\'}', metadata={})]", "category": "generic"}
{"question_id": 127, "text": " We have a news article and we need to extract all the entities like the names of people, organizations, and locations.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Feature Extraction\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'DeepPavlov/rubert-base-cased\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'DeepPavlov/rubert-base-cased\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'Russian part of Wikipedia and news data\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'RuBERT (Russian, cased, 12\u2011layer, 768\u2011hidden, 12\u2011heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT\u2011base as an initialization for RuBERT[1].\\'}', metadata={})]", "category": "generic"}
{"question_id": 128, "text": " We are purchasing a CRM system to keep track of our customers and their organizations. We want to extract useful entities from customer emails automatically.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 129, "text": " As a researcher, I am trying to find an answer to my question in a table containing information about animals and their characteristics.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 130, "text": " A teacher wants to create a quiz for her students. We are now working on the questions and answers for the quiz that be arranged in a table format.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Table Question Answering\\', \\'api_name\\': \\'google/tapas-base-finetuned-sqa\\', \\'api_call\\': \"TapasTokenizer.from_pretrained(\\'google/tapas-base-finetuned-sqa\\')\", \\'api_arguments\\': [\\'question\\', \\'table\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\n\\\\n# Initialize the pipeline\\\\ntapas_pipeline = pipeline(\\'table-question-answering\\', model=\\'google/tapas-base-finetuned-sqa\\')\\\\n\\\\n# Define the table and question\\\\nquestion = \\'How many goals did player A score?\\'\\\\ntable = [[\\'Player\\', \\'Goals\\'], [\\'Player A\\', 5], [\\'Player B\\', 3]]\\\\n\\\\n# Get the answer\\\\nresult = tapas_pipeline(question=question, table=table)\\\\nprint(result)\", \\'performance\\': {\\'dataset\\': \\'msr_sqa\\', \\'accuracy\\': 0.6874}, \\'description\\': \\'TAPAS base model fine-tuned on Sequential Question Answering (SQA). It is a BERT-like transformers model pretrained on a large corpus of English data from Wikipedia and fine-tuned on SQA. It can be used for answering questions related to a table in a conversational set-up.\\'}', metadata={})]", "category": "generic"}
{"question_id": 131, "text": " I work for a financial company that stores all of its data in tables. We need a way to extract key information efficiently by asking natural language questions.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 132, "text": " We have a dataset related to coffee and tea prices. We need to answer a question on who sells hot chocolate and their prices.\\n###Input: {\\\"table\\\": [[\\\"Shop\\\", \\\"Drink\\\", \\\"Price\\\"], [\\\"Cafe A\\\", \\\"Coffee\\\", \\\"3.00\\\"], [\\\"Cafe B\\\", \\\"Tea\\\", \\\"2.50\\\"], [\\\"Cafe C\\\", \\\"Hot Chocolate\\\", \\\"4.50\\\"], [\\\"Cafe D\\\", \\\"Hot Chocolate\\\", \\\"3.75\\\"]], \\\"queries\\\": [\\\"Which shops sell hot chocolate and what are their prices?\\\"]}\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 133, "text": " A company is running a survey and they want to know how many respondents have given a specific answer for each question of the survey.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \"PegasusForConditionalGeneration.from_pretrained(\\'tuner007/pegasus_summarizer\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 134, "text": " Extract information about a non-compete clause from a legal document with a context related to data protection.\\n###Input: \\\"The data protection provisions set forth in this agreement shall be in effect for a period of 2 years after the termination of services. The non-compete clause states that the service provider is prohibited from providing similar services to any competitor within a 50-mile radius and during the 1-year period following termination of services.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 135, "text": " Tell me the day of the game when it was played given the following context: \\\"The game was played on February 7, 2016 at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\\\"\\n###Input: {'context': \\\"The game was played on February 7, 2016 at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\\\", 'question': \\\"What day was the game played on?\\\"}\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'csarron/bert-base-uncased-squad-v1\\', \\'api_call\\': \"pipeline(\\'question-answering\\', model=\\'csarron/bert-base-uncased-squad-v1\\', tokenizer=\\'csarron/bert-base-uncased-squad-v1\\')\", \\'api_arguments\\': {\\'model\\': \\'csarron/bert-base-uncased-squad-v1\\', \\'tokenizer\\': \\'csarron/bert-base-uncased-squad-v1\\'}, \\'python_environment_requirements\\': \\'Python 3.7.5\\', \\'example_code\\': \"from transformers import pipeline\\\\nqa_pipeline = pipeline(\\\\n question-answering,\\\\n model=csarron/bert-base-uncased-squad-v1,\\\\n tokenizer=csarron/bert-base-uncased-squad-v1\\\\n)\\\\npredictions = qa_pipeline({\\\\n \\'context\\': The game was played on February 7, 2016 at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California.,\\\\n \\'question\\': What day was the game played on?\\\\n})\\\\nprint(predictions)\", \\'performance\\': {\\'dataset\\': \\'SQuAD1.1\\', \\'accuracy\\': {\\'EM\\': 80.9, \\'F1\\': 88.2}}, \\'description\\': \\'BERT-base uncased model fine-tuned on SQuAD v1. This model is case-insensitive and does not make a difference between english and English.\\'}', metadata={})]", "category": "generic"}
{"question_id": 136, "text": " We need to identify the relationship between two sentences whether they are contradictory, entail each other, or neutral.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Automatic Speech Recognition\\', \\'framework\\': \\'pyannote.audio\\', \\'functionality\\': \\'overlapped-speech-detection\\', \\'api_name\\': \\'pyannote/overlapped-speech-detection\\', \\'api_call\\': \"pipeline.from_pretrained(\\'pyannote/overlapped-speech-detection\\', use_auth_token=\\'ACCESS_TOKEN_GOES_HERE\\')\", \\'api_arguments\\': [\\'audio.wav\\'], \\'python_environment_requirements\\': [\\'pyannote.audio 2.1\\'], \\'example_code\\': \\'from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained(pyannote/overlapped-speech-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\\\noutput = pipeline(audio.wav)\\\\nfor speech in output.get_timeline().support():\\\\n  # two or more speakers are active between speech.start and speech.end\\\\n  ...\\', \\'performance\\': {\\'dataset\\': \\'ami\\', \\'accuracy\\': None}, \\'description\\': \\'Automatic overlapped speech detection using pyannote.audio framework. The model detects when two or more speakers are active in an audio file.\\'}', metadata={})]", "category": "generic"}
{"question_id": 137, "text": " I am a climate change agency, looking to have my research summaries translated into Chinese for international audiences.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 138, "text": " We are building an app to summarize long articles for users. We need a solution to create a condensed summary of the given text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 139, "text": " A news agency wants to summaries their international news articles daily.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Feature Extraction\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'DeepPavlov/rubert-base-cased\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'DeepPavlov/rubert-base-cased\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'Russian part of Wikipedia and news data\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'RuBERT (Russian, cased, 12\u2011layer, 768\u2011hidden, 12\u2011heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT\u2011base as an initialization for RuBERT[1].\\'}', metadata={})]", "category": "generic"}
{"question_id": 140, "text": " Design a smart home system that can have conversations with the user for controlling the home appliances.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 141, "text": " An educational publishing company is developing a language comprehension program for elementary school students. They want a system that can generate a short story based on a given prompt.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Program Synthesis\\', \\'api_name\\': \\'Salesforce/codegen-350M-multi\\', \\'api_call\\': \"AutoTokenizer.from_pretrained(\\'Salesforce/codegen-350M-multi\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelForCausalLM\\\\ntokenizer = AutoTokenizer.from_pretrained(Salesforce/codegen-350M-multi)\\\\nmodel = AutoModelForCausalLM.from_pretrained(Salesforce/codegen-350M-multi)\\\\ntext = def hello_world():\\\\ninput_ids = tokenizer(text, return_tensors=pt).input_ids\\\\ngenerated_ids = model.generate(input_ids, max_length=128)\\\\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\\', \\'performance\\': {\\'dataset\\': \\'HumanEval and MTPB\\', \\'accuracy\\': \\'Refer to the paper for accuracy details\\'}, \\'description\\': \\'CodeGen is a family of autoregressive language models for program synthesis. The checkpoint included in this repository is denoted as CodeGen-Multi 350M, where Multi means the model is initialized with CodeGen-NL 350M and further pre-trained on a dataset of multiple programming languages, and 350M refers to the number of trainable parameters. The model is capable of extracting features from given natural language and programming language texts, and calculating the likelihood of them. It is best at program synthesis, generating executable code given English prompts, and can complete partially-generated code as well.\\'}', metadata={})]", "category": "generic"}
{"question_id": 142, "text": " Can you initiate a conversation with an AI model that plays the role of a friend who just got back from a conference?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 143, "text": " As a software developer, I want a program that can automatically generate code snippets for me based on natural language descriptions.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Program Synthesis\\', \\'api_name\\': \\'Salesforce/codegen-350M-multi\\', \\'api_call\\': \"AutoTokenizer.from_pretrained(\\'Salesforce/codegen-350M-multi\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelForCausalLM\\\\ntokenizer = AutoTokenizer.from_pretrained(Salesforce/codegen-350M-multi)\\\\nmodel = AutoModelForCausalLM.from_pretrained(Salesforce/codegen-350M-multi)\\\\ntext = def hello_world():\\\\ninput_ids = tokenizer(text, return_tensors=pt).input_ids\\\\ngenerated_ids = model.generate(input_ids, max_length=128)\\\\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\\', \\'performance\\': {\\'dataset\\': \\'HumanEval and MTPB\\', \\'accuracy\\': \\'Refer to the paper for accuracy details\\'}, \\'description\\': \\'CodeGen is a family of autoregressive language models for program synthesis. The checkpoint included in this repository is denoted as CodeGen-Multi 350M, where Multi means the model is initialized with CodeGen-NL 350M and further pre-trained on a dataset of multiple programming languages, and 350M refers to the number of trainable parameters. The model is capable of extracting features from given natural language and programming language texts, and calculating the likelihood of them. It is best at program synthesis, generating executable code given English prompts, and can complete partially-generated code as well.\\'}', metadata={})]", "category": "generic"}
{"question_id": 144, "text": " Provide a short summary of an article about cryptocurrency investment risks.\\n###Input: Cryptocurrencies have become exceedingly popular among investors seeking higher returns and diversification in their portfolios. However, investing in these digital currencies carries several inherent risks. Market volatility is a major factor \\u2013 cryptocurrencies can experience wild price swings, sometimes even within hours or minutes. This high volatility makes it difficult to predict the future value of the investments and can result in significant losses. Furthermore, the lack of regulatory oversight and security concerns may also lead to potential frauds and hacks, exposing investors to additional risk. Lastly, the environmental impact of mining digital currencies like Bitcoin has come under scrutiny, questioning the long-term sustainability of the cryptocurrency market.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \"PegasusForConditionalGeneration.from_pretrained(\\'tuner007/pegasus_summarizer\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 145, "text": " You have just met a person that speaks French. As a hotel manager, you need to tell them, \\\"Welcome to our hotel, we hope you enjoy your stay.\\\" in French.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Conversational\\', \\'api_name\\': \\'microsoft/GODEL-v1_1-large-seq2seq\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'microsoft/GODEL-v1_1-large-seq2seq\\')\", \\'api_arguments\\': {\\'instruction\\': \\'Instruction: given a dialog context, you need to response empathically.\\', \\'knowledge\\': \\'\\', \\'dialog\\': [\\'Does money buy happiness?\\', \\'It is a question. Money buys you a lot of things, but not enough to buy happiness.\\', \\'What is the best way to buy happiness ?\\']}, \\'python_environment_requirements\\': {\\'transformers\\': \\'AutoTokenizer, AutoModelForSeq2SeqLM\\'}, \\'example_code\\': \"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained(microsoft/GODEL-v1_1-large-seq2seq)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(microsoft/GODEL-v1_1-large-seq2seq)\\\\ndef generate(instruction, knowledge, dialog):\\\\n if knowledge != \\'\\':\\\\n knowledge = \\'[KNOWLEDGE] \\' + knowledge\\\\n dialog = \\' EOS \\'.join(dialog)\\\\n query = f{instruction} [CONTEXT] {dialog} {knowledge}\\\\n input_ids = tokenizer(f{query}, return_tensors=pt).input_ids\\\\n outputs = model.generate(input_ids, max_length=128, min_length=8, top_p=0.9, do_sample=True)\\\\n output = tokenizer.decode(outputs[0], skip_special_tokens=True)\\\\n return output\\\\n\\\\ninstruction = f\\'Instruction: given a dialog context, you need to response empathically.\\'\\\\nknowledge = \\'\\'\\\\ndialog = [\\\\n \\'Does money buy happiness?\\',\\\\n \\'It is a question. Money buys you a lot of things, but not enough to buy happiness.\\',\\\\n \\'What is the best way to buy happiness ?\\'\\\\n]\\\\nresponse = generate(instruction, knowledge, dialog)\\\\nprint(response)\", \\'performance\\': {\\'dataset\\': \\'Reddit discussion thread, instruction and knowledge grounded dialogs\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'GODEL is a large-scale pre-trained model for goal-directed dialogs. It is parameterized with a Transformer-based encoder-decoder model and trained for response generation grounded in external text, which allows more effective fine-tuning on dialog tasks that require conditioning the response on information that is external to the current conversation (e.g., a retrieved document). The pre-trained model can be efficiently fine-tuned and adapted to accomplish a new dialog task with a handful of task-specific dialogs. The v1.1 model is trained on 551M multi-turn dialogs from Reddit discussion thread, and 5M instruction and knowledge grounded dialogs.\\'}', metadata={})]", "category": "generic"}
{"question_id": 146, "text": " They are planning a trip to Germany and want to spend some leisure time in the parks of Munich, find out how to ask a question about the location of parks in Munich in German.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \"PegasusForConditionalGeneration.from_pretrained(\\'tuner007/pegasus_summarizer\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 147, "text": " We are a company offering speech to text services. We need to summarize the conversion and make it open-ended question.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 148, "text": " To help me with my writing, I need an AI tool that can fill in the gaps for me when I get stuck. It should be able to complete phrases or sentences.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 149, "text": " A writer needs help with generating the next word in the phrase \\\"The dog jumped over the\\\" __.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'git-large-textcaps\\', \\'api_call\\': \"AutoModelForCausalLM.from_pretrained(\\'microsoft/git-large-textcaps\\')\", \\'api_arguments\\': \\'image, text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'N/A\\', \\'performance\\': {\\'dataset\\': \\'TextCaps\\', \\'accuracy\\': \\'Refer to the paper\\'}, \\'description\\': \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using \\'teacher forcing\\' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\"}', metadata={})]", "category": "generic"}
{"question_id": 151, "text": " Help me find similarity scores for different restaurant reviews.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Sentence Similarity\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Sentence Transformers\\', \\'api_name\\': \\'flax-sentence-embeddings/all_datasets_v4_MiniLM-L6\\', \\'api_call\\': \"SentenceTransformer(\\'flax-sentence-embeddings/all_datasets_v4_MiniLM-L6\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'sentence-transformers library\\', \\'example_code\\': \"from sentence_transformers import SentenceTransformer\\\\nmodel = SentenceTransformer(\\'flax-sentence-embeddings/all_datasets_v4_MiniLM-L6\\')\\\\ntext = Replace me by any text you\\'d like.\\\\ntext_embbedding = model.encode(text)\", \\'performance\\': {\\'dataset\\': \\'1,097,953,922\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'The model is trained on very large sentence level datasets using a self-supervised contrastive learning objective. It is fine-tuned on a 1B sentence pairs dataset, and it aims to capture the semantic information of input sentences. The sentence vector can be used for information retrieval, clustering, or sentence similarity tasks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 152, "text": " Our company is building an automated assistance system for the visually impaired. We need a tool to convert text into spoken instructions.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'ESPnet\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'kan-bayashi_jvs_tts_finetune_jvs001_jsut_vits_raw_phn_jaconv_pyopenjta-truncated-178804\\', \\'api_call\\': \"AutoModelForCausalLM.from_pretrained(\\'espnet/kan-bayashi_jvs_tts_finetune_jvs001_jsut_vits_raw_phn_jaconv_pyopenjta-truncated-178804\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A Japanese text-to-speech model trained using the ESPnet framework. It is designed to convert text input into natural-sounding speech.\\'}', metadata={})]", "category": "generic"}
{"question_id": 153, "text": " I would like to create an application where users may listen to translations of English sentences. I need a Text-to-Speech model to support this functionality.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 154, "text": " We are a language learning app platform. We need a text-to-speech tool to read a sentence in multiple languages for our users.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 155, "text": " I need to create an audio output that translates the given text to speech for a French audiobook assistant.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Translation\\', \\'api_name\\': \\'Helsinki-NLP/opus-mt-en-fr\\', \\'api_call\\': \"translate(\\'input_text\\', model=\\'Helsinki-NLP/opus-mt-en-fr\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'opus\\', \\'accuracy\\': {\\'BLEU\\': {\\'newsdiscussdev2015-enfr.en.fr\\': 33.8, \\'newsdiscusstest2015-enfr.en.fr\\': 40.0, \\'newssyscomb2009.en.fr\\': 29.8, \\'news-test2008.en.fr\\': 27.5, \\'newstest2009.en.fr\\': 29.4, \\'newstest2010.en.fr\\': 32.7, \\'newstest2011.en.fr\\': 34.3, \\'newstest2012.en.fr\\': 31.8, \\'newstest2013.en.fr\\': 33.2, \\'Tatoeba.en.fr\\': 50.5}}}, \\'description\\': \\'Helsinki-NLP/opus-mt-en-fr is a translation model that translates English text to French using the Hugging Face Transformers library. It is based on the OPUS dataset and uses a transformer-align architecture with normalization and SentencePiece pre-processing.\\'}', metadata={})]", "category": "generic"}
{"question_id": 156, "text": " We are opening a platform where users can record their own podcast and host it on our platform, can you help us to convert the audio into text automatically?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 157, "text": " We are currently working on the development of a speech-to-text application for transcription purposes. Can you help generate the required transcription code?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 158, "text": " We are building an app for tour guides to transcribe what they say during the tour. Then the transcript will be translated into sign language.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 159, "text": " I am an English teacher. I have recently met a Hokkien speaker. I am looking to translate a conversational sentence from English to Hokkien to facilitate communication.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 160, "text": " Determine the keyword spoken in a recorded audio file.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Automatic Speech Recognition\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'microsoft/wavlm-large\\', \\'api_call\\': \"Wav2Vec2Model.from_pretrained(\\'microsoft/wavlm-large\\')\", \\'api_arguments\\': \\'speech input\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'To fine-tune the model for speech recognition, see the official speech recognition example. To fine-tune the model for speech classification, see the official audio classification example.\\', \\'performance\\': {\\'dataset\\': \\'SUPERB benchmark\\', \\'accuracy\\': \\'state-of-the-art performance\\'}, \\'description\\': \\'WavLM-Large is a large model pretrained on 16kHz sampled speech audio. It is built based on the HuBERT framework, with an emphasis on both spoken content modeling and speaker identity preservation. WavLM is pretrained on 60,000 hours of Libri-Light, 10,000 hours of GigaSpeech, and 24,000 hours of VoxPopuli. It achieves state-of-the-art performance on the SUPERB benchmark and brings significant improvements for various speech processing tasks on their representative benchmarks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 161, "text": " Determine which speaker an audio segment belongs to using the provided audio file.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Automatic Speech Recognition\\', \\'framework\\': \\'pyannote.audio\\', \\'functionality\\': \\'Speaker Diarization\\', \\'api_name\\': \\'pyannote/speaker-diarization\\', \\'api_call\\': \"Pipeline.from_pretrained(\\'pyannote/speaker-diarization@2.1\\', use_auth_token=\\'ACCESS_TOKEN_GOES_HERE\\')\", \\'api_arguments\\': {\\'num_speakers\\': \\'int (optional)\\', \\'min_speakers\\': \\'int (optional)\\', \\'max_speakers\\': \\'int (optional)\\'}, \\'python_environment_requirements\\': \\'pyannote.audio 2.1.1\\', \\'example_code\\': [\\'from pyannote.audio import Pipeline\\', \\'pipeline = Pipeline.from_pretrained(pyannote/speaker-diarization@2.1, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\', \\'diarization = pipeline(audio.wav)\\', \\'with open(audio.rttm, w) as rttm:\\', \\'  diarization.write_rttm(rttm)\\'], \\'performance\\': {\\'dataset\\': \\'ami\\', \\'accuracy\\': {\\'DER%\\': \\'18.91\\', \\'FA%\\': \\'4.48\\', \\'Miss%\\': \\'9.51\\', \\'Conf%\\': \\'4.91\\'}}, \\'description\\': \\'This API provides an automatic speaker diarization pipeline using the pyannote.audio framework. It can process audio files and output speaker diarization results in RTTM format. The pipeline can also handle cases where the number of speakers is known in advance or when providing lower and/or upper bounds on the number of speakers.\\'}', metadata={})]", "category": "generic"}
{"question_id": 162, "text": " We have a database of customer voices and are trying to build a voice recognition product so we can recognize customer voices when they call. How should we process and classify?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 163, "text": " We are developing a voice-controlled drone. Please identify the spoken command in the audio clip provided.\\n###Input: \\\"audio_clip.wav\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 164, "text": " Make a summary video for our last team meeting. The audio from the video must identify who is speaking and when.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Video\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Video\\', \\'api_name\\': \\'camenduru/text2-video-zero\\', \\'api_call\\': \"pipeline(\\'text-to-video\\', model=\\'camenduru/text2-video-zero\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'This model is used for generating videos from text inputs. It is based on the Hugging Face framework and can be used with the transformers library. The model is trained on a variety of text and video datasets, and can be used for tasks such as video summarization, video generation from text prompts, and more.\\'}', metadata={})]", "category": "generic"}
{"question_id": 165, "text": " I want to estimate the price of a house based on its features using this API. Please provide the code.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 166, "text": " Our company wants to predict housing prices in the US based on given features. Help us use the trained model to predict the prices.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Regression\\', \\'framework\\': \\'Joblib\\', \\'functionality\\': \\'Single Column Regression\\', \\'api_name\\': \\'jwan2021/autotrain-us-housing-prices-1771761512\\', \\'api_call\\': \"joblib.load(\\'model.joblib\\')\", \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\', \\'json\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'jwan2021/autotrain-data-us-housing-prices\\', \\'accuracy\\': {\\'Loss\\': 122809.223, \\'R2\\': 0.884, \\'MSE\\': 15082105200.447, \\'MAE\\': 95586.887, \\'RMSLE\\': 0.13}}, \\'description\\': \\'A model trained using AutoTrain for predicting US housing prices with single column regression. The model is based on the jwan2021/autotrain-data-us-housing-prices dataset and has a CO2 Emissions of 50.5369 grams.\\'}', metadata={})]", "category": "generic"}
{"question_id": 167, "text": " An environmental organization would like to use our Carbon Emissions prediction model to estimate CO2 emissions of different configurations of vehicles.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'distilbert-base-uncased-distilled-squad\\', \\'api_call\\': \"pipeline(\\'question-answering\\', model=\\'distilbert-base-uncased-distilled-squad\\')\", \\'api_arguments\\': [\\'question\\', \\'context\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nquestion_answerer = pipeline(question-answering, model=\\'distilbert-base-uncased-distilled-squad\\')\\\\ncontext = r\\\\n... Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\\\\n... question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\\\\n... a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\\\\n... \\\\nresult = question_answerer(question=What is a good example of a question answering dataset?, context=context)\\\\nprint(\\\\n... fAnswer: \\'{result[\\'answer\\']}\\', score: {round(result[\\'score\\'], 4)}, start: {result[\\'start\\']}, end: {result[\\'end\\']}\\\\n...)\", \\'performance\\': {\\'dataset\\': \\'SQuAD v1.1\\', \\'accuracy\\': \\'86.9 F1 score\\'}, \\'description\\': \"DistilBERT base uncased distilled SQuAD is a fine-tuned version of DistilBERT-base-uncased for the task of question answering. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT\\'s performances as measured on the GLUE language understanding benchmark.\"}', metadata={})]", "category": "generic"}
{"question_id": 168, "text": " I am a data analyst working in pollution detection, find a model and develop a piece of code for me for environment monitoring.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text-to-Text Generation\\', \\'api_name\\': \\'philschmid/bart-large-cnn-samsum\\', \\'api_call\\': \"pipeline(\\'summarization\\', model=\\'philschmid/bart-large-cnn-samsum\\')\", \\'api_arguments\\': {\\'model\\': \\'philschmid/bart-large-cnn-samsum\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'latest\\'}, \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/bart-large-cnn-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'eval_rouge1\\': 42.621, \\'eval_rouge2\\': 21.9825, \\'eval_rougeL\\': 33.034, \\'eval_rougeLsum\\': 39.6783, \\'test_rouge1\\': 41.3174, \\'test_rouge2\\': 20.8716, \\'test_rougeL\\': 32.1337, \\'test_rougeLsum\\': 38.4149}}, \\'description\\': \\'philschmid/bart-large-cnn-samsum is a BART-based model trained for text summarization on the SAMSum dataset. It can be used to generate abstractive summaries of conversations.\\'}', metadata={})]", "category": "generic"}
{"question_id": 169, "text": " We want to develop an intelligent prosthetic leg that can improve walking. Use a decision transformer to predict actions to be taken.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 170, "text": " You want to create a bot that can play the Pong No Frameskip-v4 game with exceptional skill.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'valhalla/t5-base-e2e-qg\\', \\'api_call\\': \"pipeline(\\'e2e-qg\\', model=\\'valhalla/t5-base-e2e-qg\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'Hugging Face Transformers\\'], \\'example_code\\': \"from pipelines import pipeline\\\\n\\\\ntext = Python is an interpreted, high-level, general-purpose programming language. Created by Guido van Rossum and first released in 1991, Python\\'s design philosophy emphasizes code readability with its notable use of significant whitespace.\\\\n\\\\nnlp = pipeline(e2e-qg, model=valhalla/t5-base-e2e-qg)\\\\n\\\\nnlp(text)\", \\'performance\\': {\\'dataset\\': \\'squad\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'This is a T5-base model trained for end-to-end question generation task. Simply input the text and the model will generate multiple questions. You can play with the model using the inference API, just put the text and see the results!\\'}', metadata={})]", "category": "generic"}
{"question_id": 171, "text": " I am a game developer working on a game project involving moving carts. I need to use reinforcement learning to improve the game experience.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'csarron/bert-base-uncased-squad-v1\\', \\'api_call\\': \"pipeline(\\'question-answering\\', model=\\'csarron/bert-base-uncased-squad-v1\\', tokenizer=\\'csarron/bert-base-uncased-squad-v1\\')\", \\'api_arguments\\': {\\'model\\': \\'csarron/bert-base-uncased-squad-v1\\', \\'tokenizer\\': \\'csarron/bert-base-uncased-squad-v1\\'}, \\'python_environment_requirements\\': \\'Python 3.7.5\\', \\'example_code\\': \"from transformers import pipeline\\\\nqa_pipeline = pipeline(\\\\n question-answering,\\\\n model=csarron/bert-base-uncased-squad-v1,\\\\n tokenizer=csarron/bert-base-uncased-squad-v1\\\\n)\\\\npredictions = qa_pipeline({\\\\n \\'context\\': The game was played on February 7, 2016 at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California.,\\\\n \\'question\\': What day was the game played on?\\\\n})\\\\nprint(predictions)\", \\'performance\\': {\\'dataset\\': \\'SQuAD1.1\\', \\'accuracy\\': {\\'EM\\': 80.9, \\'F1\\': 88.2}}, \\'description\\': \\'BERT-base uncased model fine-tuned on SQuAD v1. This model is case-insensitive and does not make a difference between english and English.\\'}', metadata={})]", "category": "generic"}
{"question_id": 172, "text": " Develop a soccer playing agent that can outperform its opponents in a 2v2 environment.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Classification\\', \\'api_name\\': \\'facebook/convnext-large-224\\', \\'api_call\\': \"ConvNextForImageClassification.from_pretrained(\\'facebook/convnext-large-224\\')\", \\'api_arguments\\': {\\'pretrained_model_name_or_path\\': \\'facebook/convnext-large-224\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'Hugging Face Transformers\\', \\'torch\\': \\'PyTorch\\', \\'datasets\\': \\'Hugging Face Datasets\\'}, \\'example_code\\': {\\'import\\': [\\'from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification\\', \\'import torch\\', \\'from datasets import load_dataset\\'], \\'load_dataset\\': \"dataset = load_dataset(\\'huggingface/cats-image\\')\", \\'image\\': \"image = dataset[\\'test\\'][\\'image\\'][0]\", \\'feature_extractor\\': \"feature_extractor = ConvNextFeatureExtractor.from_pretrained(\\'facebook/convnext-large-224\\')\", \\'model\\': \"model = ConvNextForImageClassification.from_pretrained(\\'facebook/convnext-large-224\\')\", \\'inputs\\': \"inputs = feature_extractor(image, return_tensors=\\'pt\\')\", \\'logits\\': \\'with torch.no_grad():\\\\n  logits = model(**inputs).logits\\', \\'predicted_label\\': \\'predicted_label = logits.argmax(-1).item()\\', \\'print\\': \\'print(model.config.id2label[predicted_label])\\'}, \\'performance\\': {\\'dataset\\': \\'imagenet-1k\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \"ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. The authors started from a ResNet and \\'modernized\\' its design by taking the Swin Transformer as inspiration.\"}', metadata={})]", "category": "generic"}
{"question_id": 173, "text": " We are tasked to analyze text for a Russian newspaper to help understand general sentiment and trends in the text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'financial-sentiment-analysis\\', \\'api_name\\': \\'ProsusAI/finbert\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'ProsusAI/finbert\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline; classifier = pipeline(\\'sentiment-analysis\\', model=\\'ProsusAI/finbert\\'); classifier(\\'your_text_here\\')\", \\'performance\\': {\\'dataset\\': \\'Financial PhraseBank\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'FinBERT is a pre-trained NLP model to analyze sentiment of financial text. It is built by further training the BERT language model in the finance domain, using a large financial corpus and thereby fine-tuning it for financial sentiment classification. Financial PhraseBank by Malo et al. (2014) is used for fine-tuning.\\'}', metadata={})]", "category": "generic"}
{"question_id": 174, "text": " We want to generate an image from a textual description for our PowerPoint presentation.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 175, "text": " A new manga has been released and we would like to provide a manga reader app with translations. Get the text from the manga image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Visual Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'git-large-textvqa\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'microsoft/git-large-textvqa\\')\", \\'api_arguments\\': \\'image, question\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'For code examples, we refer to the documentation.\\', \\'performance\\': {\\'dataset\\': \\'TextVQA\\', \\'accuracy\\': \\'See table 11 in the paper for more details.\\'}, \\'description\\': \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextVQA. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using \\'teacher forcing\\' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like: image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\"}', metadata={})]", "category": "generic"}
{"question_id": 176, "text": " We are building a social media site which creates automatic captions for users when they post a picture\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 177, "text": " There is robot in our factory which reads the image from the production line and then generate a text output based on the image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Image-to-Image\\', \\'api_name\\': \\'GreeneryScenery/SheepsControlV3\\', \\'api_call\\': \"pipeline(\\'image-to-image\\', model=\\'GreeneryScenery/SheepsControlV3\\')\", \\'api_arguments\\': {\\'image\\': \\'Path to image file\\', \\'text_guidance\\': \\'Optional text guidance for the model\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'latest\\', \\'torch\\': \\'latest\\'}, \\'example_code\\': [\\'from transformers import pipeline\\', \"model = pipeline(\\'image-to-image\\', model=\\'GreeneryScenery/SheepsControlV3\\')\", \"result = model({\\'image\\': \\'path/to/image.jpg\\', \\'text_guidance\\': \\'Optional text guidance\\'})\"], \\'performance\\': {\\'dataset\\': \\'GreeneryScenery/SheepsControlV3\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\\'}', metadata={})]", "category": "generic"}
{"question_id": 178, "text": " I am a filmmaker, and I need to make a short video based on a scene description from a script.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Video\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Video Synthesis\\', \\'api_name\\': \\'modelscope-damo-text-to-video-synthesis\\', \\'api_call\\': \"pipeline(\\'text-to-video-synthesis\\', model_dir.as_posix())\", \\'api_arguments\\': {\\'text\\': \\'A short text description in English\\'}, \\'python_environment_requirements\\': [\\'modelscope==1.4.2\\', \\'open_clip_torch\\', \\'pytorch-lightning\\'], \\'example_code\\': \"from huggingface_hub import snapshot_download\\\\nfrom modelscope.pipelines import pipeline\\\\nfrom modelscope.outputs import OutputKeys\\\\nimport pathlib\\\\n\\\\nmodel_dir = pathlib.Path(\\'weights\\')\\\\nsnapshot_download(\\'damo-vilab/modelscope-damo-text-to-video-synthesis\\',\\\\n repo_type=\\'model\\', local_dir=model_dir)\\\\n\\\\npipe = pipeline(\\'text-to-video-synthesis\\', model_dir.as_posix())\\\\n\\\\ntest_text = {\\\\n \\'text\\': \\'A panda eating bamboo on a rock.\\',\\\\n}\\\\n\\\\noutput_video_path = pipe(test_text,)[OutputKeys.OUTPUT_VIDEO]\\\\nprint(\\'output_video_path:\\', output_video_path)\", \\'performance\\': {\\'dataset\\': \\'Webvid, ImageNet, LAION5B\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported.\\'}', metadata={})]", "category": "generic"}
{"question_id": 179, "text": " I'm an author and want to create a short video based on a brief passage from my book. Can you generate a video based on this text?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Video\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Video\\', \\'api_name\\': \\'camenduru/text2-video-zero\\', \\'api_call\\': \"pipeline(\\'text-to-video\\', model=\\'camenduru/text2-video-zero\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'This model is used for generating videos from text inputs. It is based on the Hugging Face framework and can be used with the transformers library. The model is trained on a variety of text and video datasets, and can be used for tasks such as video summarization, video generation from text prompts, and more.\\'}', metadata={})]", "category": "generic"}
{"question_id": 180, "text": " I want to build an AI model that can analyze images and answer questions about the content of the image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Visual Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ivelin/donut-refexp-combined-v1\\', \\'api_call\\': \"pipeline(\\'visual-question-answering\\', model=\\'ivelin/donut-refexp-combined-v1\\')\", \\'api_arguments\\': \\'image, question\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"vqa(image=\\'path/to/image.jpg\\', question=\\'What is the color of the object?\\')\", \\'performance\\': {\\'dataset\\': \\'ivelin/donut-refexp-combined-v1\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'A visual question answering model that takes an image and a question as input and provides an answer based on the visual content of the image and the context of the question.\\'}', metadata={})]", "category": "generic"}
{"question_id": 181, "text": " We are trying to develop an application that helps tourists get information about attractions by analyzing images they take and responding to questions.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 182, "text": " We have the financial documents of a company and we want to extract information about the cash flow. Modify the model so it can answer the questions related to the cash flow.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \"PegasusForConditionalGeneration.from_pretrained(\\'tuner007/pegasus_summarizer\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 183, "text": " Please generate a correct building plan leveraging the data given.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'sberbank-ai/sbert_large_mt_nlu_ru\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'sberbank-ai/sbert_large_mt_nlu_ru\\')\", \\'api_arguments\\': [\\'sentences\\', \\'padding\\', \\'truncation\\', \\'max_length\\', \\'return_tensors\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': \"from transformers import AutoTokenizer, AutoModel\\\\nimport torch\\\\n\\\\n\\\\n# Mean Pooling - Take attention mask into account for correct averaging\\\\ndef mean_pooling(model_output, attention_mask):\\\\n    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\\\\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\\\\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\\\\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\\\\n    return sum_embeddings / sum_mask\\\\n\\\\n\\\\n# Sentences we want sentence embeddings for sentences = [\\'?\\']\\\\n\\\\n# Load AutoModel from huggingface model repository\\\\ntokenizer = AutoTokenizer.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\\\nmodel = AutoModel.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\\\n\\\\n# Tokenize sentences\\\\nencoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=24, return_tensors=\\'pt\\')\\\\n\\\\n# Compute token embeddings\\\\nwith torch.no_grad():\\\\n    model_output = model(**encoded_input)\\\\n\\\\n# Perform pooling. In this case, mean pooling\\\\nsentence_embeddings = mean_pooling(model_output, encoded_input[\\'attention_mask\\'])\", \\'performance\\': {\\'dataset\\': \\'Russian SuperGLUE\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'BERT large model multitask (cased) for Sentence Embeddings in Russian language.\\'}', metadata={})]", "category": "generic"}
{"question_id": 184, "text": " Help our drone video analyzing app estimate the depth in drone footage.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 185, "text": " As a salesperson, I need to analyze customer invoices to answer questions about the total amount, tax, and due date from an image file.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'layoutlm-invoices\\', \\'api_call\\': \"AutoModelForDocumentQuestionAnswering.from_pretrained(\\'impira/layoutlm-invoices\\')\", \\'api_arguments\\': \\'question, context\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"nlp(question=\\'What is the total amount?\\', context=\\'your_invoice_text\\')\", \\'performance\\': {\\'dataset\\': \\'proprietary dataset of invoices, SQuAD2.0, and DocVQA\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens, this model can predict longer-range, non-consecutive sequences with an additional classifier head.\\'}', metadata={})]", "category": "generic"}
{"question_id": 186, "text": " We need to find out the depth information of a room for monitoring purposes.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 187, "text": " We are creating an autonomous car and need to estimate the depth of objects in a given scene.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 188, "text": " Help us create an AI solution to automatically label images taken by a security camera.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Automatic Speech Recognition\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'facebook/wav2vec2-xlsr-53-espeak-cv-ft\\', \\'api_call\\': \"Wav2Vec2ForCTC.from_pretrained(\\'facebook/wav2vec2-xlsr-53-espeak-cv-ft\\')\", \\'api_arguments\\': {\\'model_name\\': \\'facebook/wav2vec2-xlsr-53-espeak-cv-ft\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'4.13.0\\', \\'torch\\': \\'1.10.0\\', \\'datasets\\': \\'1.14.0\\'}, \\'example_code\\': \\'processor = Wav2Vec2Processor.from_pretrained(facebook/wav2vec2-xlsr-53-espeak-cv-ft)\\\\nmodel = Wav2Vec2ForCTC.from_pretrained(facebook/wav2vec2-xlsr-53-espeak-cv-ft)\\\\nds = load_dataset(patrickvonplaten/librispeech_asr_dummy, clean, split=validation)\\\\ninput_values = processor(ds[0][audio][array], return_tensors=pt).input_values\\\\nwith torch.no_grad():\\\\n    logits = model(input_values).logits\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.batch_decode(predicted_ids)\\', \\'performance\\': {\\'dataset\\': \\'common_voice\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Wav2Vec2-Large-XLSR-53 finetuned on multi-lingual Common Voice for phonetic label recognition in multiple languages. The model outputs a string of phonetic labels, and a dictionary mapping phonetic labels to words has to be used to map the phonetic output labels to output words.\\'}', metadata={})]", "category": "generic"}
{"question_id": 189, "text": " Develop a software to classify an image from a URL into a thousand categories.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'naver-clova-ix/donut-base\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'naver-clova-ix/donut-base\\')\", \\'api_arguments\\': \\'image\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'result = donut(image_path)\\', \\'performance\\': {\\'dataset\\': \\'arxiv:2111.15664\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.\\'}', metadata={})]", "category": "generic"}
{"question_id": 190, "text": " Our delivery drones need to detect and avoid obstacles while flying. Develop a solution for them to detect objects in their path.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Object Detection\\', \\'api_name\\': \\'fcakyon/yolov5s-v7.0\\', \\'api_call\\': \"yolov5.load(\\'fcakyon/yolov5s-v7.0\\')\", \\'api_arguments\\': {\\'conf\\': 0.25, \\'iou\\': 0.45, \\'agnostic\\': False, \\'multi_label\\': False, \\'max_det\\': 1000, \\'img\\': \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\', \\'size\\': 640, \\'augment\\': True}, \\'python_environment_requirements\\': \\'pip install -U yolov5\\', \\'example_code\\': \"import yolov5\\\\nmodel = yolov5.load(\\'fcakyon/yolov5s-v7.0\\')\\\\nmodel.conf = 0.25\\\\nmodel.iou = 0.45\\\\nmodel.agnostic = False\\\\nmodel.multi_label = False\\\\nmodel.max_det = 1000\\\\nimg = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\\\\nresults = model(img)\\\\nresults = model(img, size=640)\\\\nresults = model(img, augment=True)\\\\npredictions = results.pred[0]\\\\nboxes = predictions[:, :4]\\\\nscores = predictions[:, 4]\\\\ncategories = predictions[:, 5]\\\\nresults.show()\\\\nresults.save(save_dir=\\'results/\\')\", \\'performance\\': {\\'dataset\\': \\'detection-datasets/coco\\', \\'accuracy\\': None}, \\'description\\': \\'Yolov5s-v7.0 is an object detection model trained on the COCO dataset. It can detect objects in images and return their bounding boxes, scores, and categories.\\'}', metadata={})]", "category": "generic"}
{"question_id": 191, "text": " Develop a code to recognize objects in images using deformable-detr model.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 192, "text": " I need to extract tables from a set of scanned document images to simplify data analysis.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023\\', \\'api_call\\': \"AutoModelForDocumentQuestionAnswering.from_pretrained(\\'L-oenai/LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023\\')\", \\'api_arguments\\': {}, \\'python_environment_requirements\\': {\\'transformers\\': \\'>=4.11.0\\'}, \\'example_code\\': {}, \\'performance\\': {\\'dataset\\': {}, \\'accuracy\\': {}}, \\'description\\': \\'A document question answering model based on LayoutLMv2, which can be used to extract answers from images with text and layout information.\\'}', metadata={})]", "category": "generic"}
{"question_id": 193, "text": " Our customers wish to automatically detect shoplifters in the store using a surveillance camera. Help them to implement object detection and identify potential thieves.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Object Detection\\', \\'api_name\\': \\'keremberke/yolov8m-csgo-player-detection\\', \\'api_call\\': \"YOLO(\\'keremberke/yolov8m-csgo-player-detection\\')\", \\'api_arguments\\': {\\'image\\': \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'}, \\'python_environment_requirements\\': \\'ultralyticsplus==0.0.23 ultralytics==8.0.21\\', \\'example_code\\': \"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO(\\'keremberke/yolov8m-csgo-player-detection\\')\\\\nmodel.overrides[\\'conf\\'] = 0.25\\\\nmodel.overrides[\\'iou\\'] = 0.45\\\\nmodel.overrides[\\'agnostic_nms\\'] = False\\\\nmodel.overrides[\\'max_det\\'] = 1000\\\\nimage = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\", \\'performance\\': {\\'dataset\\': \\'csgo-object-detection\\', \\'accuracy\\': 0.892}, \\'description\\': \"An object detection model trained to detect Counter-Strike: Global Offensive (CS:GO) players. The model is based on the YOLOv8 architecture and can identify \\'ct\\', \\'cthead\\', \\'t\\', and \\'thead\\' labels.\"}', metadata={})]", "category": "generic"}
{"question_id": 194, "text": " Create an object detector that can detect blood cells in an image, such as platelets, red blood cells, and white blood cells.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Blood Cell Detection\\', \\'api_name\\': \\'keremberke/yolov8n-blood-cell-detection\\', \\'api_call\\': \"YOLO(\\'keremberke/yolov8n-blood-cell-detection\\')\", \\'api_arguments\\': {\\'conf\\': 0.25, \\'iou\\': 0.45, \\'agnostic_nms\\': False, \\'max_det\\': 1000}, \\'python_environment_requirements\\': \\'ultralyticsplus==0.0.23 ultralytics==8.0.21\\', \\'example_code\\': \"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO(\\'keremberke/yolov8n-blood-cell-detection\\')\\\\nmodel.overrides[\\'conf\\'] = 0.25\\\\nmodel.overrides[\\'iou\\'] = 0.45\\\\nmodel.overrides[\\'agnostic_nms\\'] = False\\\\nmodel.overrides[\\'max_det\\'] = 1000\\\\nimage = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\", \\'performance\\': {\\'dataset\\': \\'blood-cell-object-detection\\', \\'accuracy\\': 0.893}, \\'description\\': \\'This model detects blood cells in images, specifically Platelets, RBC, and WBC. It is based on the YOLOv8 architecture and trained on the blood-cell-object-detection dataset.\\'}', metadata={})]", "category": "generic"}
{"question_id": 195, "text": " I am a real-estate agent working on a project where I need to convert images of room plans to a better visual representation.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 196, "text": " We recently received low resolution images of newly released products and need to upscale them for better quality.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 197, "text": " As a toy company, we are designing a new toy line. We'd like you to create an image of a toy robot using relevant text prompts as control input.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 198, "text": " I'm developing a game that needs more Minecraft skins for some characters. How can I generete them with this model?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Unconditional Image Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Diffusers\\', \\'api_name\\': \\'Minecraft-Skin-Diffusion\\', \\'api_call\\': \"DDPMPipeline.from_pretrained(\\'WiNE-iNEFF/Minecraft-Skin-Diffusion\\')\", \\'api_arguments\\': {}, \\'python_environment_requirements\\': [\\'diffusers\\'], \\'example_code\\': \"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained(\\'WiNE-iNEFF/Minecraft-Skin-Diffusion\\')\\\\nimage = pipeline().images[0].convert(\\'RGBA\\')\\\\nimage\", \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Unconditional Image Generation model for generating Minecraft skins using diffusion-based methods.\\'}', metadata={})]", "category": "generic"}
{"question_id": 199, "text": " Find me a generative model to create cat images in 256x256 resolution.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image Generation\\', \\'api_name\\': \\'stabilityai/stable-diffusion-2-1-base\\', \\'api_call\\': \"StableDiffusionPipeline.from_pretrained(\\'stabilityai/stable-diffusion-2-1-base\\', scheduler=EulerDiscreteScheduler.from_pretrained(stabilityai/stable-diffusion-2-1-base, subfolder=scheduler), torch_dtype=torch.float16)\", \\'api_arguments\\': {\\'prompt\\': \\'a photo of an astronaut riding a horse on mars\\'}, \\'python_environment_requirements\\': [\\'diffusers\\', \\'transformers\\', \\'accelerate\\', \\'scipy\\', \\'safetensors\\'], \\'example_code\\': {\\'install_dependencies\\': \\'pip install diffusers transformers accelerate scipy safetensors\\', \\'code\\': \\'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\\\nimport torch\\\\nmodel_id = stabilityai/stable-diffusion-2-1-base\\\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = a photo of an astronaut riding a horse on mars\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(astronaut_rides_horse.png)\\'}, \\'performance\\': {\\'dataset\\': \\'COCO2017 validation set\\', \\'accuracy\\': \\'Not optimized for FID scores\\'}, \\'description\\': \\'Stable Diffusion v2-1-base is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H). It is intended for research purposes only and can be used in areas such as safe deployment of models, understanding limitations and biases of generative models, generation of artworks, and research on generative models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 200, "text": " Our organization works with video surveillance. We need a system to analyze the videos and classify various events happening inside the video.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Video\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Video\\', \\'api_name\\': \\'camenduru/text2-video-zero\\', \\'api_call\\': \"pipeline(\\'text-to-video\\', model=\\'camenduru/text2-video-zero\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'This model is used for generating videos from text inputs. It is based on the Hugging Face framework and can be used with the transformers library. The model is trained on a variety of text and video datasets, and can be used for tasks such as video summarization, video generation from text prompts, and more.\\'}', metadata={})]", "category": "generic"}
{"question_id": 201, "text": " People in my company need an automatic solution to categorize videos based on their content. The system should be able to recognize the main theme of a video with high accuracy.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Video\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Video\\', \\'api_name\\': \\'camenduru/text2-video-zero\\', \\'api_call\\': \"pipeline(\\'text-to-video\\', model=\\'camenduru/text2-video-zero\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'This model is used for generating videos from text inputs. It is based on the Hugging Face framework and can be used with the transformers library. The model is trained on a variety of text and video datasets, and can be used for tasks such as video summarization, video generation from text prompts, and more.\\'}', metadata={})]", "category": "generic"}
{"question_id": 202, "text": " We want to build a product to classify images of pets into different categories.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Feature Extraction\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'DeepPavlov/rubert-base-cased\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'DeepPavlov/rubert-base-cased\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'Russian part of Wikipedia and news data\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'RuBERT (Russian, cased, 12\u2011layer, 768\u2011hidden, 12\u2011heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT\u2011base as an initialization for RuBERT[1].\\'}', metadata={})]", "category": "generic"}
{"question_id": 203, "text": " We are developing an app to classify food images. We have a set of images and want to use a pre-trained model for classification.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 205, "text": " The company wants to use geolocalization techniques to identify the location of a given image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Token Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Entity Extraction\\', \\'api_name\\': \\'903429548\\', \\'api_call\\': \"AutoModelForTokenClassification.from_pretrained(\\'ismail-lucifer011/autotrain-company_all-903429548\\', use_auth_token=True)\", \\'api_arguments\\': {\\'inputs\\': \\'I love AutoTrain\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'AutoModelForTokenClassification, AutoTokenizer\\'}, \\'example_code\\': \\'from transformers import AutoModelForTokenClassification, AutoTokenizer\\\\nmodel = AutoModelForTokenClassification.from_pretrained(ismail-lucifer011/autotrain-company_all-903429548, use_auth_token=True)\\\\ntokenizer = AutoTokenizer.from_pretrained(ismail-lucifer011/autotrain-company_all-903429548, use_auth_token=True)\\\\ninputs = tokenizer(I love AutoTrain, return_tensors=pt)\\\\noutputs = model(**inputs)\\', \\'performance\\': {\\'dataset\\': \\'ismail-lucifer011/autotrain-data-company_all\\', \\'accuracy\\': 0.9979930566588805}, \\'description\\': \\'A token classification model trained using AutoTrain for entity extraction. The model is based on the distilbert architecture and trained on the ismail-lucifer011/autotrain-data-company_all dataset. It can be used to identify and extract company names from text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 206, "text": " To improve customer service, the company is developing an AI-based chatbot that can help generate multiple paraphrases of a given customer query. We want to know if the generated paraphrases are adequate.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 207, "text": " We need to analyze user reviews for our app to determine whether they are positive, negative, or neutral.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 208, "text": " Give me a tool to detect named entities in multiple languages for my news portal analysis.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Automatic Speech Recognition\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'facebook/wav2vec2-xlsr-53-espeak-cv-ft\\', \\'api_call\\': \"Wav2Vec2ForCTC.from_pretrained(\\'facebook/wav2vec2-xlsr-53-espeak-cv-ft\\')\", \\'api_arguments\\': {\\'model_name\\': \\'facebook/wav2vec2-xlsr-53-espeak-cv-ft\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'4.13.0\\', \\'torch\\': \\'1.10.0\\', \\'datasets\\': \\'1.14.0\\'}, \\'example_code\\': \\'processor = Wav2Vec2Processor.from_pretrained(facebook/wav2vec2-xlsr-53-espeak-cv-ft)\\\\nmodel = Wav2Vec2ForCTC.from_pretrained(facebook/wav2vec2-xlsr-53-espeak-cv-ft)\\\\nds = load_dataset(patrickvonplaten/librispeech_asr_dummy, clean, split=validation)\\\\ninput_values = processor(ds[0][audio][array], return_tensors=pt).input_values\\\\nwith torch.no_grad():\\\\n    logits = model(input_values).logits\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.batch_decode(predicted_ids)\\', \\'performance\\': {\\'dataset\\': \\'common_voice\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Wav2Vec2-Large-XLSR-53 finetuned on multi-lingual Common Voice for phonetic label recognition in multiple languages. The model outputs a string of phonetic labels, and a dictionary mapping phonetic labels to words has to be used to map the phonetic output labels to output words.\\'}', metadata={})]", "category": "generic"}
{"question_id": 209, "text": " We are building a fiction-sharing platform for multiple languages. Extract named entities from the given text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 210, "text": " Someone asked the question \\\"What is the capital of Sweden?\\\" in the context \\\"Stockholm is the beautiful capital of Sweden, which is known for its high living standards and great attractions.\\\"\\n###Input: {\\\"question\\\": \\\"What is the capital of Sweden?\\\", \\\"context\\\": \\\"Stockholm is the beautiful capital of Sweden, which is known for its high living standards and great attractions.\\\"}\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'deepset/roberta-large-squad2\\', \\'api_call\\': \"pipeline(\\'question-answering\\', model=\\'deepset/roberta-large-squad2\\')\", \\'api_arguments\\': [\\'question\\', \\'context\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline; nlp = pipeline(\\'question-answering\\', model=\\'deepset/roberta-large-squad2\\'); nlp({\\'question\\': \\'What is the capital of Germany?\\', \\'context\\': \\'Berlin is the capital of Germany.\\'})\", \\'performance\\': {\\'dataset\\': \\'squad_v2\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'A pre-trained RoBERTa model for question answering tasks, specifically trained on the SQuAD v2 dataset. It can be used to answer questions based on a given context.\\'}', metadata={})]", "category": "generic"}
{"question_id": 211, "text": " Write an overview for astronomers about how Jupiter became the largest planet in our solar system.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'financial-summarization-pegasus\\', \\'api_call\\': \"PegasusForConditionalGeneration.from_pretrained(\\'human-centered-summarization/financial-summarization-pegasus\\')\", \\'api_arguments\\': [\\'model_name\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import PegasusTokenizer, PegasusForConditionalGeneration, TFPegasusForConditionalGeneration\\\\nmodel_name = human-centered-summarization/financial-summarization-pegasus\\\\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\\\\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name)\\\\ntext_to_summarize = National Commercial Bank (NCB), Saudi Arabia\u2019s largest lender by assets, agreed to buy rival Samba Financial Group for $15 billion in the biggest banking takeover this year.NCB will pay 28.45 riyals ($7.58) for each Samba share, according to a statement on Sunday, valuing it at about 55.7 billion riyals. NCB will offer 0.739 new shares for each Samba share, at the lower end of the 0.736-0.787 ratio the banks set when they signed an initial framework agreement in June.The offer is a 3.5% premium to Samba\u2019s Oct. 8 closing price of 27.50 riyals and about 24% higher than the level the shares traded at before the talks were made public. Bloomberg News first reported the merger discussions.The new bank will have total assets of more than $220 billion, creating the Gulf region\u2019s third-largest lender. The entity\u2019s $46 billion market capitalization nearly matches that of Qatar National Bank QPSC, which is still the Middle East\u2019s biggest lender with about $268 billion of assets.\\\\ninput_ids = tokenizer(text_to_summarize, return_tensors=pt).input_ids\\\\noutput = model.generate(input_ids, max_length=32, num_beams=5, early_stopping=True)\\\\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\\', \\'performance\\': {\\'dataset\\': \\'xsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 35.206, \\'ROUGE-2\\': 16.569, \\'ROUGE-L\\': 30.128, \\'ROUGE-LSUM\\': 30.171}}, \\'description\\': \\'This model was fine-tuned on a novel financial news dataset, which consists of 2K articles from Bloomberg, on topics such as stock, markets, currencies, rate and cryptocurrencies. It is based on the PEGASUS model and in particular PEGASUS fine-tuned on the Extreme Summarization (XSum) dataset: google/pegasus-xsum model. PEGASUS was originally proposed by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu in PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization.\\'}', metadata={})]", "category": "generic"}
{"question_id": 212, "text": " Create a virtual assistant that can provide answers to trivia questions about history.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'PyTorch Transformers\\', \\'functionality\\': \\'Table Question Answering\\', \\'api_name\\': \\'table-question-answering-tapas\\', \\'api_call\\': \"pipeline(\\'table-question-answering\\', model=\\'Meena/table-question-answering-tapas\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'This model can be loaded on the Inference API on-demand.\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'SQA (Sequential Question Answering by Microsoft)\\', \\'accuracy\\': None}, {\\'name\\': \\'WTQ (Wiki Table Questions by Stanford University)\\', \\'accuracy\\': None}, {\\'name\\': \\'WikiSQL (by Salesforce)\\', \\'accuracy\\': None}]}, \\'description\\': \\'TAPAS, the model learns an inner representation of the English language used in tables and associated texts, which can then be used to extract features useful for downstream tasks such as answering questions about a table, or determining whether a sentence is entailed or refuted by the contents of a table. It is a BERT-based model specifically designed (and pre-trained) for answering questions about tabular data. TAPAS uses relative position embeddings and has 7 token types that encode tabular structure. It is pre-trained on the masked language modeling (MLM) objective on a large dataset comprising millions of tables from English Wikipedia and corresponding texts.\\'}', metadata={})]", "category": "generic"}
{"question_id": 213, "text": " I want to build a system that can answer questions from users reading a book.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'luhua/chinese_pretrain_mrc_roberta_wwm_ext_large\\', \\'api_call\\': \"pipeline(\\'question-answering\\', model=\\'luhua/chinese_pretrain_mrc_roberta_wwm_ext_large\\')\", \\'api_arguments\\': \\'context, question\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nqa_pipeline = pipeline(\\'question-answering\\', model=\\'luhua/chinese_pretrain_mrc_roberta_wwm_ext_large\\')\\\\nresult = qa_pipeline({\\'context\\': \\'your_context_here\\', \\'question\\': \\'your_question_here\\'})\", \\'performance\\': {\\'dataset\\': \\'Dureader-2021\\', \\'accuracy\\': \\'83.1\\'}, \\'description\\': \\'A Chinese MRC roberta_wwm_ext_large model trained on a large amount of Chinese MRC data. This model has significantly improved performance on reading comprehension and classification tasks. It has helped multiple users achieve top 5 results in the Dureader-2021 competition.\\'}', metadata={})]", "category": "generic"}
{"question_id": 214, "text": " Our customer support team needs an AI assistant to handle customer inquiries. It should be able to understand and evaluate if the answer provided by the assistant is contradictory, neutral or entails the customer's question.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 215, "text": " Use a zero-shot classifier to classify news headlines into three categories: sports, technology, and politics.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Zero-Shot Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Cross-Encoder for Natural Language Inference\\', \\'api_name\\': \\'cross-encoder/nli-distilroberta-base\\', \\'api_call\\': \"CrossEncoder(\\'cross-encoder/nli-distilroberta-base\\')\", \\'api_arguments\\': \"(\\'A man is eating pizza\\', \\'A man eats something\\')\", \\'python_environment_requirements\\': [\\'sentence_transformers\\', \\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nclassifier = pipeline(zero-shot-classification, model=\\'cross-encoder/nli-distilroberta-base\\')\\\\nsent = Apple just announced the newest iPhone X\\\\ncandidate_labels = [technology, sports, politics]\\\\nres = classifier(sent, candidate_labels)\\\\nprint(res)\", \\'performance\\': {\\'dataset\\': \\'SNLI and MultiNLI\\', \\'accuracy\\': \\'See SBERT.net - Pretrained Cross-Encoder for evaluation results\\'}, \\'description\\': \\'This model was trained using SentenceTransformers Cross-Encoder class on the SNLI and MultiNLI datasets. For a given sentence pair, it will output three scores corresponding to the labels: contradiction, entailment, neutral.\\'}', metadata={})]", "category": "generic"}
{"question_id": 216, "text": " Analyze a German text and classify it into different categories like crime, tragedy, or theft.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Zero-Shot Image Classification\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Zero-Shot Image Classification\\', \\'api_name\\': \\'laion/CLIP-ViT-g-14-laion2B-s34B-b88K\\', \\'api_call\\': \"pipeline(\\'zero-shot-image-classification\\', model=\\'laion/CLIP-ViT-g-14-laion2B-s34B-b88K\\')\", \\'api_arguments\\': {\\'image\\': \\'path/to/image/file\\', \\'class_names\\': \\'list_of_class_names\\'}, \\'python_environment_requirements\\': {\\'huggingface_hub\\': \\'0.0.17\\', \\'transformers\\': \\'4.11.3\\', \\'torch\\': \\'1.9.0\\', \\'torchvision\\': \\'0.10.0\\'}, \\'example_code\\': None, \\'performance\\': {\\'dataset\\': None, \\'accuracy\\': None}, \\'description\\': \\'A zero-shot image classification model based on OpenCLIP, which can classify images into various categories without requiring any training data for those categories.\\'}', metadata={})]", "category": "generic"}
{"question_id": 217, "text": " Create a tool to determine the logical relationship between two given sentences.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Zero-Shot Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Natural Language Inference\\', \\'api_name\\': \\'cointegrated/rubert-base-cased-nli-threeway\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'cointegrated/rubert-base-cased-nli-threeway\\')\", \\'api_arguments\\': [\\'text1\\', \\'text2\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'sentencepiece\\'], \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\\\\nmodel_checkpoint = \\'cointegrated/rubert-base-cased-nli-threeway\\'\\\\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\\\\nmodel = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\\\\nif torch.cuda.is_available():\\\\n model.cuda()\\\\ntext1 = \\'.\\'\\\\ntext2 = \\'.\\'\\\\nwith torch.inference_mode():\\\\n out = model(**tokenizer(text1, text2, return_tensors=\\'pt\\').to(model.device))\\\\n proba = torch.softmax(out.logits, -1).cpu().numpy()[0]\\\\nprint({v: proba[k] for k, v in model.config.id2label.items()})\", \\'performance\\': {\\'dataset\\': [\\'JOCI\\', \\'MNLI\\', \\'MPE\\', \\'SICK\\', \\'SNLI\\', \\'ANLI\\', \\'NLI-style FEVER\\', \\'IMPPRES\\'], \\'accuracy\\': {\\'ROC AUC\\': {\\'entailment\\': 0.91, \\'contradiction\\': 0.71, \\'neutral\\': 0.79}}}, \\'description\\': \\'This is the DeepPavlov/rubert-base-cased fine-tuned to predict the logical relationship between two short texts: entailment, contradiction, or neutral.\\'}', metadata={})]", "category": "generic"}
{"question_id": 218, "text": " A media company needs to summarize a news article in order to make it easy for their audience to understand the main points quickly.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'csebuetnlp/mT5_multilingual_XLSum\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'csebuetnlp/mT5_multilingual_XLSum\\')\", \\'api_arguments\\': [\\'model_name\\'], \\'python_environment_requirements\\': [\\'transformers==4.11.0.dev0\\'], \\'example_code\\': \"import re\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nWHITESPACE_HANDLER = lambda k: re.sub(\\'\\\\\\\\s+\\', \\' \\', re.sub(\\'\\\\\\\\n+\\', \\' \\', k.strip()))\\\\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people\\'s scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We\\'re expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\\\\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\\\ninput_ids = tokenizer(\\\\n [WHITESPACE_HANDLER(article_text)],\\\\n return_tensors=pt,\\\\n padding=max_length,\\\\n truncation=True,\\\\n max_length=512\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n max_length=84,\\\\n no_repeat_ngram_size=2,\\\\n num_beams=4\\\\n)[0]\\\\nsummary = tokenizer.decode(\\\\n output_ids,\\\\n skip_special_tokens=True,\\\\n clean_up_tokenization_spaces=False\\\\n)\\\\nprint(summary)\", \\'performance\\': {\\'dataset\\': \\'xsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.5, \\'ROUGE-2\\': 13.934, \\'ROUGE-L\\': 28.988, \\'ROUGE-LSUM\\': 28.996, \\'loss\\': 2.067, \\'gen_len\\': 26.973}}, \\'description\\': \\'This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 219, "text": " We want to implement a customer service chatbot to engage with website visitors and provide support.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Conversational\\', \\'api_name\\': \\'facebook/blenderbot-3B\\', \\'api_call\\': \"BlenderbotForConditionalGeneration.from_pretrained(\\'facebook/blenderbot-3B\\')\", \\'api_arguments\\': [\\'model_name\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'Input a message to start chatting with facebook/blenderbot-3B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'BlenderBot-3B is a large-scale neural model designed for open-domain chatbot applications. It is trained on the blended_skill_talk dataset and can engage in multi-turn conversations, providing engaging talking points, asking and answering questions, and displaying knowledge, empathy, and personality. The model is available through the Hugging Face Transformers library.\\'}', metadata={})]", "category": "generic"}
{"question_id": 220, "text": " As the project manager of a company who receives long emails, my goal is to summarize them and extract the most important information.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text2Text Generation\\', \\'api_name\\': \\'DialogLED-base-16384\\', \\'api_call\\': \"LEDForConditionalGeneration.from_pretrained(\\'MingZhong/DialogLED-base-16384\\')\", \\'api_arguments\\': \\'input_text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'arxiv\\', \\'accuracy\\': \\'2109.02492\\'}, \\'description\\': \\'DialogLED is a pre-trained model for long dialogue understanding and summarization. It builds on the Longformer-Encoder-Decoder (LED) architecture and uses window-based denoising as the pre-training task on a large amount of long dialogue data for further training. Here is a base version of DialogLED, the input length is limited to 16,384 in the pre-training phase.\\'}', metadata={})]", "category": "generic"}
{"question_id": 221, "text": " I am the user and I want to start a multi-turn conversation with this model.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 222, "text": " Create a Python function named \\\"summarize_russian_text\\\" that accepts a Russian text as input and returns a brief summary of the text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Video\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-video-synthesis\\', \\'api_name\\': \\'damo-vilab/text-to-video-ms-1.7b\\', \\'api_call\\': \"DiffusionPipeline.from_pretrained(\\'damo-vilab/text-to-video-ms-1.7b\\', torch_dtype=torch.float16, variant=fp16)\", \\'api_arguments\\': [\\'prompt\\', \\'num_inference_steps\\', \\'num_frames\\'], \\'python_environment_requirements\\': [\\'pip install git+https://github.com/huggingface/diffusers transformers accelerate\\'], \\'example_code\\': \\'pipe = DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b, torch_dtype=torch.float16, variant=fp16)\\\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_model_cpu_offload()\\\\nprompt = Spiderman is surfing\\\\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\\\\nvideo_path = export_to_video(video_frames)\\', \\'performance\\': {\\'dataset\\': \\'Webvid\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'A multi-stage text-to-video generation diffusion model that inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. It supports English input only and has a wide range of applications.\\'}', metadata={})]", "category": "generic"}
{"question_id": 223, "text": " We want to implement a chatbot on our website to provide quick responses to customer inquiries.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 224, "text": " We want the AI to have an interesting conversation and ask it general knowledge questions.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'Kirili4ik/mbart_ruDialogSum\\', \\'api_call\\': \"MBartForConditionalGeneration.from_pretrained(\\'Kirili4ik/mbart_ruDialogSum\\')\", \\'api_arguments\\': {\\'model_name\\': \\'Kirili4ik/mbart_ruDialogSum\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import MBartTokenizer, MBartForConditionalGeneration\\\\nmodel_name = Kirili4ik/mbart_ruDialogSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = MBartForConditionalGeneration.from_pretrained(model_name)\\\\nmodel.eval()\\\\narticle_text = ...\\\\ninput_ids = tokenizer(\\\\n [article_text],\\\\n max_length=600,\\\\n padding=max_length,\\\\n truncation=True,\\\\n return_tensors=pt,\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n top_k=0,\\\\n num_beams=3,\\\\n no_repeat_ngram_size=3\\\\n)[0]\\\\nsummary = tokenizer.decode(output_ids, skip_special_tokens=True)\\\\nprint(summary)\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'SAMSum Corpus (translated to Russian)\\', \\'accuracy\\': {\\'Validation ROGUE-1\\': 34.5, \\'Validation ROGUE-L\\': 33, \\'Test ROGUE-1\\': 31, \\'Test ROGUE-L\\': 28}}]}, \\'description\\': \\'MBart for Russian summarization fine-tuned for dialogues summarization. This model was firstly fine-tuned by Ilya Gusev on Gazeta dataset. We have fine tuned that model on SamSum dataset translated to Russian using GoogleTranslateAPI. Moreover! We have implemented a ! telegram bot @summarization_bot ! with the inference of this model. Add it to the chat and get summaries instead of dozens spam messages!\\'}', metadata={})]", "category": "generic"}
{"question_id": 225, "text": " We need to create a dialogue in Russian for our educational app. It should cover a general greeting and asking about the users' well-being.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 226, "text": " The company wants to create a chatbot to help answer customer questions regarding the chatbot's consciousness. We need to be able to generate sensible responses.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Conversational\\', \\'api_name\\': \\'facebook/blenderbot-400M-distill\\', \\'api_call\\': \"BlenderbotForConditionalGeneration.from_pretrained(\\'facebook/blenderbot-400M-distill\\')\", \\'api_arguments\\': \"[\\'message\\']\", \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': \\'Input a message to start chatting with facebook/blenderbot-400M-distill.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'BlenderBot-400M-distill is a distilled version of the BlenderBot model, trained on the Blended Skill Talk dataset. It is designed for open-domain chatbot tasks and can generate text-to-text responses in a conversational manner. The model is based on the Transformers library and can be used with PyTorch, TensorFlow, and JAX.\\'}', metadata={})]", "category": "generic"}
{"question_id": 227, "text": " Create interesting variations of the given phrase 'How can I improve my time management skills?'.\\n###Input: How can I improve my time management skills?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 228, "text": " A customer wants to convert an input Korean text into a summary. Provide a solution for it.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'ESPnet\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'mio/Artoria\\', \\'api_call\\': \"pipeline(\\'text-to-speech\\', model=\\'mio/Artoria\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline; tts = pipeline(\\'text-to-speech\\', model=\\'mio/Artoria\\'); tts(\\'s\\')\", \\'performance\\': {\\'dataset\\': \\'fate\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'This model was trained by mio using fate recipe in espnet. It is a text-to-speech model that can convert text input into speech output.\\'}', metadata={})]", "category": "generic"}
{"question_id": 229, "text": " Our company is providing translation services for business clients. We need to translate English contracts to French.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 230, "text": " We are an online platform that offers various Chinese language courses for students. Now we need to autoplay a fill-in-the-blank video game for our users.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 231, "text": " We want to determine the similarity between two sentences, \\\"I love going to the park\\\" and \\\"My favorite activity is visiting the park\\\".\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Feature Extraction\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'facebook/dragon-plus-context-encoder\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'facebook/dragon-plus-context-encoder\\')\", \\'api_arguments\\': [\\'pretrained\\'], \\'python_environment_requirements\\': [\\'torch\\', \\'transformers\\'], \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModel\\\\ntokenizer = AutoTokenizer.from_pretrained(\\'facebook/dragon-plus-query-encoder\\')\\\\nquery_encoder = AutoModel.from_pretrained(\\'facebook/dragon-plus-query-encoder\\')\\\\ncontext_encoder = AutoModel.from_pretrained(\\'facebook/dragon-plus-context-encoder\\')\\\\nquery = \\'Where was Marie Curie born?\\'\\\\ncontexts = [\\\\n  \\'Maria Sklodowska, later known as Marie Curie, was born on November 7, 1867.\\',\\\\n  \\'Born in Paris on 15 May 1859, Pierre Curie was the son of Eug\u00e8ne Curie, a doctor of French Catholic origin from Alsace.\\'\\\\n]\\\\nquery_input = tokenizer(query, return_tensors=\\'pt\\')\\\\nctx_input = tokenizer(contexts, padding=True, truncation=True, return_tensors=\\'pt\\')\\\\nquery_emb = query_encoder(query_input).last_hidden_state[:, 0, :]\\\\nctx_emb = context_encoder(ctx_input).last_hidden_state[:, 0, :]\\\\nscore1 = query_emb @ ctx_emb[0]\\\\nscore2 = query_emb @ ctx_emb[1]\", \\'performance\\': {\\'dataset\\': \\'MS MARCO\\', \\'accuracy\\': 39.0}, \\'description\\': \\'DRAGON+ is a BERT-base sized dense retriever initialized from RetroMAE and further trained on the data augmented from MS MARCO corpus, following the approach described in How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval. The associated GitHub repository is available here https://github.com/facebookresearch/dpr-scale/tree/main/dragon. We use asymmetric dual encoder, with two distinctly parameterized encoders.\\'}', metadata={})]", "category": "generic"}
{"question_id": 232, "text": " We have an audiobook app in the Chinese language, and we want to convert the Chinese text to speech.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 233, "text": " We want to build an audiobook service. We need to convert text to speech for the user to listen.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Feature Extraction\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'DeepPavlov/rubert-base-cased\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'DeepPavlov/rubert-base-cased\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'Russian part of Wikipedia and news data\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'RuBERT (Russian, cased, 12\u2011layer, 768\u2011hidden, 12\u2011heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT\u2011base as an initialization for RuBERT[1].\\'}', metadata={})]", "category": "generic"}
{"question_id": 234, "text": " I want to convert English speech into different language speech. I have some audio in English language and I would like to convert that into Spanish.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'ESPnet\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best\\', \\'api_call\\': \"Text2Speech.from_pretrained(\\'espnet/kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'torch\\', \\'espnet_model_zoo\\'], \\'example_code\\': \\'import soundfile\\\\nfrom espnet2.bin.tts_inference import Text2Speech\\\\ntext2speech = Text2Speech.from_pretrained(espnet/kan-bayashi_csmsc_tts_train_tacotron2_raw_phn_pypinyin_g2p_phone_train.loss.best)\\\\ntext = \u6625\u6c5f\u6f6e\u6c34\u8fde\u6d77\u5e73\uff0c\u6d77\u4e0a\u660e\u6708\u5171\u6f6e\u751f\\\\nspeech = text2speech(text)[wav]\\\\nsoundfile.write(out.wav, speech.numpy(), text2speech.fs, PCM_16)\\', \\'performance\\': {\\'dataset\\': \\'csmsc\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'A pre-trained Text-to-Speech model for Chinese language using ESPnet framework. It can be used to convert text input into speech output in Chinese.\\'}', metadata={})]", "category": "generic"}
{"question_id": 235, "text": " We are trying to transcribe audios. Can you help with giving us an ASR model?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 236, "text": " Our blog platform accepts voice notes, and we want to transcribe these notes to text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'sberbank-ai/sbert_large_mt_nlu_ru\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'sberbank-ai/sbert_large_mt_nlu_ru\\')\", \\'api_arguments\\': [\\'sentences\\', \\'padding\\', \\'truncation\\', \\'max_length\\', \\'return_tensors\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': \"from transformers import AutoTokenizer, AutoModel\\\\nimport torch\\\\n\\\\n\\\\n# Mean Pooling - Take attention mask into account for correct averaging\\\\ndef mean_pooling(model_output, attention_mask):\\\\n    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\\\\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\\\\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\\\\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\\\\n    return sum_embeddings / sum_mask\\\\n\\\\n\\\\n# Sentences we want sentence embeddings for sentences = [\\'?\\']\\\\n\\\\n# Load AutoModel from huggingface model repository\\\\ntokenizer = AutoTokenizer.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\\\nmodel = AutoModel.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\\\n\\\\n# Tokenize sentences\\\\nencoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=24, return_tensors=\\'pt\\')\\\\n\\\\n# Compute token embeddings\\\\nwith torch.no_grad():\\\\n    model_output = model(**encoded_input)\\\\n\\\\n# Perform pooling. In this case, mean pooling\\\\nsentence_embeddings = mean_pooling(model_output, encoded_input[\\'attention_mask\\'])\", \\'performance\\': {\\'dataset\\': \\'Russian SuperGLUE\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'BERT large model multitask (cased) for Sentence Embeddings in Russian language.\\'}', metadata={})]", "category": "generic"}
{"question_id": 237, "text": " Create a program to separate music and vocals from an audio file using a pretrained model.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 238, "text": " \\nCreate an application that automatically separates vocals from a song for karaoke nights.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 239, "text": " One of our clients need high recognition capability of denoising audio streams for improving sound quality.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 240, "text": " A podcast company reached out to us wanting to change their speaker's voice in a recorded podcast.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 241, "text": " Our research team is focused on the analysis and separation of complex audio recordings. We need a model to be able to separate speaker voices from mixed sound.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'financial-sentiment-analysis\\', \\'api_name\\': \\'yiyanghkust/finbert-tone\\', \\'api_call\\': \"BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\", \\'api_arguments\\': [\\'sentences\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import BertTokenizer, BertForSequenceClassification\\\\nfrom transformers import pipeline\\\\nfinbert = BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\\\\ntokenizer = BertTokenizer.from_pretrained(\\'yiyanghkust/finbert-tone\\')\\\\nnlp = pipeline(sentiment-analysis, model=finbert, tokenizer=tokenizer)\\\\nsentences = [there is a shortage of capital, and we need extra financing,\\\\n growth is strong and we have plenty of liquidity,\\\\n there are doubts about our finances,\\\\n profits are flat]\\\\nresults = nlp(sentences)\\\\nprint(results)\", \\'performance\\': {\\'dataset\\': \\'10,000 manually annotated sentences from analyst reports\\', \\'accuracy\\': \\'superior performance on financial tone analysis task\\'}, \\'description\\': \\'FinBERT is a BERT model pre-trained on financial communication text. It is trained on the following three financial communication corpus: Corporate Reports 10-K & 10-Q, Earnings Call Transcripts, and Analyst Reports. This released finbert-tone model is the FinBERT model fine-tuned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports. This model achieves superior performance on financial tone analysis task.\\'}', metadata={})]", "category": "generic"}
{"question_id": 242, "text": " A Romanian-speaking person wants to communicate with an English-speaking friend over the phone using our platform. Please provide a method to translate their speech in real-time.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Classification\\', \\'framework\\': \\'Keras\\', \\'functionality\\': \\'Binary Classification\\', \\'api_name\\': \\'TF_Decision_Trees\\', \\'api_call\\': \\'TF_Decision_Trees(input_features, target)\\', \\'api_arguments\\': [\\'input_features\\', \\'target\\'], \\'python_environment_requirements\\': [\\'tensorflow >= 7.0\\'], \\'example_code\\': \\'https://github.com/tdubon/TF-GB-Forest/blob/c0cf4c7e3e29d819b996cfe4eecc1f2728115e52/TFDecisionTrees_Final.ipynb\\', \\'performance\\': {\\'dataset\\': \\'Census-Income Data Set\\', \\'accuracy\\': 96.57}, \\'description\\': \"Use TensorFlow\\'s Gradient Boosted Trees model in binary classification of structured data. Build a decision forests model by specifying the input feature usage. Implement a custom Binary Target encoder as a Keras Preprocessing layer to encode the categorical features with respect to their target value co-occurrences, and then use the encoded features to build a decision forests model. The model is trained on the US Census Income Dataset containing approximately 300k instances with 41 numerical and categorical variables. The task is to determine whether a person makes over 50k a year.\"}', metadata={})]", "category": "generic"}
{"question_id": 243, "text": " Implement an application that recognizes languages by analyzing an online audio file.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Automatic Speech Recognition\\', \\'framework\\': \\'pyannote.audio\\', \\'functionality\\': \\'overlapped-speech-detection\\', \\'api_name\\': \\'pyannote/overlapped-speech-detection\\', \\'api_call\\': \"pipeline.from_pretrained(\\'pyannote/overlapped-speech-detection\\', use_auth_token=\\'ACCESS_TOKEN_GOES_HERE\\')\", \\'api_arguments\\': [\\'audio.wav\\'], \\'python_environment_requirements\\': [\\'pyannote.audio 2.1\\'], \\'example_code\\': \\'from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained(pyannote/overlapped-speech-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\\\noutput = pipeline(audio.wav)\\\\nfor speech in output.get_timeline().support():\\\\n  # two or more speakers are active between speech.start and speech.end\\\\n  ...\\', \\'performance\\': {\\'dataset\\': \\'ami\\', \\'accuracy\\': None}, \\'description\\': \\'Automatic overlapped speech detection using pyannote.audio framework. The model detects when two or more speakers are active in an audio file.\\'}', metadata={})]", "category": "generic"}
{"question_id": 244, "text": " A toy company wants to use your software to recognize spoken numbers (0-9) in English by young children for an interactive game. Please create an identifier.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 245, "text": " Develop an automated podcast recommender system to suggest content that matches user's taste based on their favorite podcast speakers.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Automatic Speech Recognition\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'microsoft/wavlm-large\\', \\'api_call\\': \"Wav2Vec2Model.from_pretrained(\\'microsoft/wavlm-large\\')\", \\'api_arguments\\': \\'speech input\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'To fine-tune the model for speech recognition, see the official speech recognition example. To fine-tune the model for speech classification, see the official audio classification example.\\', \\'performance\\': {\\'dataset\\': \\'SUPERB benchmark\\', \\'accuracy\\': \\'state-of-the-art performance\\'}, \\'description\\': \\'WavLM-Large is a large model pretrained on 16kHz sampled speech audio. It is built based on the HuBERT framework, with an emphasis on both spoken content modeling and speaker identity preservation. WavLM is pretrained on 60,000 hours of Libri-Light, 10,000 hours of GigaSpeech, and 24,000 hours of VoxPopuli. It achieves state-of-the-art performance on the SUPERB benchmark and brings significant improvements for various speech processing tasks on their representative benchmarks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 246, "text": " We are developing an app that can help mediate virtual therapy sessions. We need a feature that can detect emotions in the voice of the user.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 247, "text": " We are engineering a green consulting firm. We need to predict carbon emissions with input features from customer's dataset.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 248, "text": " Develop a system that can detect voices in a podcast to find out if guests are speaking or not. \\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 249, "text": " Develop a machine learning product to help home buyers predict house prices in the US.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Token Classification\\', \\'framework\\': \\'Flair\\', \\'functionality\\': \\'Named Entity Recognition\\', \\'api_name\\': \\'flair/ner-english-ontonotes-large\\', \\'api_call\\': \"SequenceTagger.load(\\'flair/ner-english-ontonotes-large\\')\", \\'api_arguments\\': [\\'sentence\\'], \\'python_environment_requirements\\': [\\'flair\\'], \\'example_code\\': \"from flair.data import Sentence\\\\nfrom flair.models import SequenceTagger\\\\n\\\\n# load tagger\\\\ntagger = SequenceTagger.load(\\'flair/ner-english-ontonotes-large\\')\\\\n\\\\n# make example sentence\\\\nsentence = Sentence(\\'On September 1st George won 1 dollar while watching Game of Thrones.\\')\\\\n\\\\n# predict NER tags\\\\ntagger.predict(sentence)\\\\n\\\\n# print sentence\\\\nprint(sentence)\\\\n\\\\n# print predicted NER spans\\\\nprint(\\'The following NER tags are found:\\')\\\\n\\\\n# iterate over entities and print\\\\nfor entity in sentence.get_spans(\\'ner\\'):\\\\n    print(entity)\", \\'performance\\': {\\'dataset\\': \\'Ontonotes\\', \\'accuracy\\': 90.93}, \\'description\\': \\'English NER in Flair (Ontonotes large model). This is the large 18-class NER model for English that ships with Flair. It predicts 18 tags such as cardinal value, date value, event name, building name, geo-political entity, language name, law name, location name, money name, affiliation, ordinal value, organization name, percent value, person name, product name, quantity value, time value, and name of work of art. The model is based on document-level XLM-R embeddings and FLERT.\\'}', metadata={})]", "category": "generic"}
{"question_id": 250, "text": " A game developer needs to predict the HP of a new Pokemon character based on several attributes. Provide the code to make that prediction.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Regression\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Predicting Pokemon HP\\', \\'api_name\\': \\'julien-c/pokemon-predict-hp\\', \\'api_call\\': \"pipeline(\\'regression\\', model=\\'julien-c/pokemon-predict-hp\\')\", \\'api_arguments\\': \\'input_data\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'julien-c/kaggle-rounakbanik-pokemon\\', \\'accuracy\\': {\\'mean_absolute_error\\': 15.909, \\'model_loss\\': 647.605}}, \\'description\\': \\'A tabular regression model trained on the julien-c/kaggle-rounakbanik-pokemon dataset to predict the HP of Pokemon.\\'}', metadata={})]", "category": "generic"}
{"question_id": 251, "text": " Let's create a smart agent that can learn to play CartPole-v0 using reinforcement learning.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Reinforcement Learning\\', \\'framework\\': \\'ML-Agents\\', \\'functionality\\': \\'SoccerTwos\\', \\'api_name\\': \\'Raiden-1001/poca-Soccerv7\\', \\'api_call\\': \"mlagents-load-from-hf --repo-id=\\'Raiden-1001/poca-Soccerv7.1\\' --local-dir=\\'./downloads\\'\", \\'api_arguments\\': [\\'your_configuration_file_path.yaml\\', \\'run_id\\'], \\'python_environment_requirements\\': [\\'unity-ml-agents\\', \\'deep-reinforcement-learning\\', \\'ML-Agents-SoccerTwos\\'], \\'example_code\\': \\'Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\\\\nStep 2: Select your .nn /.onnx file\\\\nClick on Watch the agent play \ud83d\udc40\\', \\'performance\\': {\\'dataset\\': \\'SoccerTwos\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library.\\'}', metadata={})]", "category": "generic"}
{"question_id": 252, "text": " The sports equipment company I work for wants to build a virtual soccer training game. The agent must be able to effectively play soccer against another team.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentiment Analysis\\', \\'api_name\\': \\'michellejieli/NSFW_text_classifier\\', \\'api_call\\': \"pipeline(\\'sentiment-analysis\\', model=\\'michellejieli/NSFW_text_classification\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'classifier(I see you\u2019ve set aside this special time to humiliate yourself in public.)\\', \\'performance\\': {\\'dataset\\': \\'Reddit posts\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'DistilBERT is a transformer model that performs sentiment analysis. I fine-tuned the model on Reddit posts with the purpose of classifying not safe for work (NSFW) content, specifically text that is considered inappropriate and unprofessional. The model predicts 2 classes, which are NSFW or safe for work (SFW). The model is a fine-tuned version of DistilBERT. It was fine-tuned on 14317 Reddit posts pulled from the Reddit API.\\'}', metadata={})]", "category": "generic"}
{"question_id": 253, "text": " We want to build a smart-cars training system for our team. I need you to solve the gym environment 'MountainCar-v0'.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 254, "text": " We are a group of teachers looking to create an automated summary based on the student's essays.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 255, "text": " An important project is under development, and we need to know the positional relations between various keywords of a given medical text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 256, "text": " Build a system for detecting hate speech from social media comments in Korean.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'csebuetnlp/mT5_multilingual_XLSum\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'csebuetnlp/mT5_multilingual_XLSum\\')\", \\'api_arguments\\': [\\'model_name\\'], \\'python_environment_requirements\\': [\\'transformers==4.11.0.dev0\\'], \\'example_code\\': \"import re\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nWHITESPACE_HANDLER = lambda k: re.sub(\\'\\\\\\\\s+\\', \\' \\', re.sub(\\'\\\\\\\\n+\\', \\' \\', k.strip()))\\\\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people\\'s scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We\\'re expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\\\\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\\\ninput_ids = tokenizer(\\\\n [WHITESPACE_HANDLER(article_text)],\\\\n return_tensors=pt,\\\\n padding=max_length,\\\\n truncation=True,\\\\n max_length=512\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n max_length=84,\\\\n no_repeat_ngram_size=2,\\\\n num_beams=4\\\\n)[0]\\\\nsummary = tokenizer.decode(\\\\n output_ids,\\\\n skip_special_tokens=True,\\\\n clean_up_tokenization_spaces=False\\\\n)\\\\nprint(summary)\", \\'performance\\': {\\'dataset\\': \\'xsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.5, \\'ROUGE-2\\': 13.934, \\'ROUGE-L\\': 28.988, \\'ROUGE-LSUM\\': 28.996, \\'loss\\': 2.067, \\'gen_len\\': 26.973}}, \\'description\\': \\'This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 257, "text": " Our team is working on a recommendation system for a news article app. We should understand the semantic similarity of various texts.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Feature Extraction\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'DeepPavlov/rubert-base-cased\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'DeepPavlov/rubert-base-cased\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'Russian part of Wikipedia and news data\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'RuBERT (Russian, cased, 12\u2011layer, 768\u2011hidden, 12\u2011heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT\u2011base as an initialization for RuBERT[1].\\'}', metadata={})]", "category": "generic"}
{"question_id": 258, "text": " We are developing an AI chatbot for a Russian company. I want to extract features from the clients' text messages in Russian language.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Feature Extraction\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'DeepPavlov/rubert-base-cased\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'DeepPavlov/rubert-base-cased\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'Russian part of Wikipedia and news data\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'RuBERT (Russian, cased, 12\u2011layer, 768\u2011hidden, 12\u2011heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT\u2011base as an initialization for RuBERT[1].\\'}', metadata={})]", "category": "generic"}
{"question_id": 259, "text": " I am an artist who wants to create a new piece of artwork based on the prompt \\\"A futuristic city under the ocean\\\".\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'wavymulder/Analog-Diffusion\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'wavymulder/Analog-Diffusion\\')\", \\'api_arguments\\': [\\'prompt\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"text_to_image(\\'analog style landscape\\')\", \\'performance\\': {\\'dataset\\': \\'analog photographs\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \"Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token \\'analog style\\' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library.\"}', metadata={})]", "category": "generic"}
{"question_id": 260, "text": " Create a character-like artwork image based on the phrase 'anime-style girl with a guitar'.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Paraphrasing\\', \\'api_name\\': \\'prithivida/parrot_paraphraser_on_T5\\', \\'api_call\\': \"Parrot(model_tag=\\'prithivida/parrot_paraphraser_on_T5\\', use_gpu=False)\", \\'api_arguments\\': [\\'input_phrase\\', \\'diversity_ranker\\', \\'do_diverse\\', \\'max_return_phrases\\', \\'max_length\\', \\'adequacy_threshold\\', \\'fluency_threshold\\'], \\'python_environment_requirements\\': [\\'torch\\', \\'transformers\\'], \\'example_code\\': \\'from parrot import Parrot\\\\nimport torch\\\\nimport warnings\\\\nwarnings.filterwarnings(ignore)\\\\n\\\\nparrot = Parrot(model_tag=prithivida/parrot_paraphraser_on_T5, use_gpu=False)\\\\nphrases = [Can you recommed some upscale restaurants in Newyork?,\\\\n What are the famous places we should not miss in Russia?\\\\n]\\\\nfor phrase in phrases:\\\\n print(-*100)\\\\n print(Input_phrase: , phrase)\\\\n print(-*100)\\\\n para_phrases = parrot.augment(input_phrase=phrase)\\\\n for para_phrase in para_phrases:\\\\n  print(para_phrase)\\', \\'performance\\': {\\'dataset\\': \\'Not mentioned\\', \\'accuracy\\': \\'Not mentioned\\'}, \\'description\\': \\'Parrot is a paraphrase based utterance augmentation framework purpose built to accelerate training NLU models. It offers knobs to control Adequacy, Fluency, and Diversity as per your needs. It mainly focuses on augmenting texts typed-into or spoken-to conversational interfaces for building robust NLU models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 261, "text": " We are designing an application that can generate images based on textual descriptions. We need to load a model capable of performing text-to-image conversion.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 262, "text": " A food delivery app wants to help users understand ingredients in the food item by analyzing the images. We need a solution to process the food images and give textual information about the items.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 263, "text": " The company wants to create a social media application like Pinterest that generates captions for users' images. Show us how to create this functionality.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'gsdf/Counterfeit-V2.5\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'gsdf/Counterfeit-V2.5\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'((masterpiece,best quality)),1girl, solo, animal ears, rabbit, barefoot, knees up, dress, sitting, rabbit ears, short sleeves, looking at viewer, grass, short hair, smile, white hair, puffy sleeves, outdoors, puffy short sleeves, bangs, on ground, full body, animal, white dress, sunlight, brown eyes, dappled sunlight, day, depth of field\\', \\'performance\\': {\\'dataset\\': \\'EasyNegative\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 264, "text": " We are an analytics company that needs help in extracting data tables from plots and charts efficiently.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 265, "text": " Our marketing team wants a video commercial for our product. They are looking for a video with a couple sitting in a cafe and laughing while using our product.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 266, "text": " A company is having an ad campaign with a superhero theme. They want to generate a video of Spiderman surfing to showcase as a social media ad.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Video\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-video-synthesis\\', \\'api_name\\': \\'damo-vilab/text-to-video-ms-1.7b\\', \\'api_call\\': \"DiffusionPipeline.from_pretrained(\\'damo-vilab/text-to-video-ms-1.7b\\', torch_dtype=torch.float16, variant=fp16)\", \\'api_arguments\\': [\\'prompt\\', \\'num_inference_steps\\', \\'num_frames\\'], \\'python_environment_requirements\\': [\\'pip install git+https://github.com/huggingface/diffusers transformers accelerate\\'], \\'example_code\\': \\'pipe = DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b, torch_dtype=torch.float16, variant=fp16)\\\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe.enable_model_cpu_offload()\\\\nprompt = Spiderman is surfing\\\\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\\\\nvideo_path = export_to_video(video_frames)\\', \\'performance\\': {\\'dataset\\': \\'Webvid\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'A multi-stage text-to-video generation diffusion model that inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. It supports English input only and has a wide range of applications.\\'}', metadata={})]", "category": "generic"}
{"question_id": 267, "text": " We are developing a solution where we can visually detect the medical conditions of patients. Can you write code to determine the condition based on the image provided?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 268, "text": " Help me to build a visual-question-answering model that takes a question and an image as input and returns an answer based on the image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Visual Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ivelin/donut-refexp-combined-v1\\', \\'api_call\\': \"pipeline(\\'visual-question-answering\\', model=\\'ivelin/donut-refexp-combined-v1\\')\", \\'api_arguments\\': \\'image, question\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"vqa(image=\\'path/to/image.jpg\\', question=\\'What is the color of the object?\\')\", \\'performance\\': {\\'dataset\\': \\'ivelin/donut-refexp-combined-v1\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'A visual question answering model that takes an image and a question as input and provides an answer based on the visual content of the image and the context of the question.\\'}', metadata={})]", "category": "generic"}
{"question_id": 269, "text": " Implement an optical text recognition system for documents that can answer a simple question about the document's content.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Visual Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ivelin/donut-refexp-combined-v1\\', \\'api_call\\': \"pipeline(\\'visual-question-answering\\', model=\\'ivelin/donut-refexp-combined-v1\\')\", \\'api_arguments\\': \\'image, question\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"vqa(image=\\'path/to/image.jpg\\', question=\\'What is the color of the object?\\')\", \\'performance\\': {\\'dataset\\': \\'ivelin/donut-refexp-combined-v1\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'A visual question answering model that takes an image and a question as input and provides an answer based on the visual content of the image and the context of the question.\\'}', metadata={})]", "category": "generic"}
{"question_id": 270, "text": " Our company deals with insurance claims. We need a smart assistant who can retrieve information from documents, especially invoices, such as total amount, date of invoice, and name of the service provider.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'layoutlm-invoices\\', \\'api_call\\': \"AutoModelForDocumentQuestionAnswering.from_pretrained(\\'impira/layoutlm-invoices\\')\", \\'api_arguments\\': \\'question, context\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"nlp(question=\\'What is the total amount?\\', context=\\'your_invoice_text\\')\", \\'performance\\': {\\'dataset\\': \\'proprietary dataset of invoices, SQuAD2.0, and DocVQA\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens, this model can predict longer-range, non-consecutive sequences with an additional classifier head.\\'}', metadata={})]", "category": "generic"}
{"question_id": 271, "text": " We have an AI-powered document management system that needs to answer questions based on the content of a given document.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Visual Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'vilt-finetuned-vqasi\\', \\'api_call\\': \"ViltModel.from_pretrained(\\'tufa15nik/vilt-finetuned-vqasi\\')\", \\'api_arguments\\': {\\'model\\': \\'tufa15nik/vilt-finetuned-vqasi\\', \\'tokenizer\\': \\'tufa15nik/vilt-finetuned-vqasi\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'>=4.11.3\\'}, \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A Visual Question Answering model fine-tuned on the VQASI dataset by tufa15nik using the ViLT architecture. The model is designed to answer questions based on the content of an input image.\\'}', metadata={})]", "category": "generic"}
{"question_id": 272, "text": " Take a look at this document image and tell me the answer to my question: \\\"What is the total amount due?\\\".\\n###Input: {\\\"image_url\\\": \\\"https://example.com/document_invoice.jpg\\\", \\\"question\\\": \\\"What is the total amount due?\\\"}\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text2Text Generation\\', \\'api_name\\': \\'DialogLED-base-16384\\', \\'api_call\\': \"LEDForConditionalGeneration.from_pretrained(\\'MingZhong/DialogLED-base-16384\\')\", \\'api_arguments\\': \\'input_text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'arxiv\\', \\'accuracy\\': \\'2109.02492\\'}, \\'description\\': \\'DialogLED is a pre-trained model for long dialogue understanding and summarization. It builds on the Longformer-Encoder-Decoder (LED) architecture and uses window-based denoising as the pre-training task on a large amount of long dialogue data for further training. Here is a base version of DialogLED, the input length is limited to 16,384 in the pre-training phase.\\'}', metadata={})]", "category": "generic"}
{"question_id": 273, "text": " I am designing a parking spot detector for a car parking management solution. I want the model to estimage the depth of the car park.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 274, "text": " We are researching multi-view 3D scanning, and we would like to improve our depth estimation with a pretrained model.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 275, "text": " I am building an app to detect objects in an image from a URL. How can I experiment with object detection models using transformers?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Object Detection\\', \\'api_name\\': \\'fcakyon/yolov5s-v7.0\\', \\'api_call\\': \"yolov5.load(\\'fcakyon/yolov5s-v7.0\\')\", \\'api_arguments\\': {\\'conf\\': 0.25, \\'iou\\': 0.45, \\'agnostic\\': False, \\'multi_label\\': False, \\'max_det\\': 1000, \\'img\\': \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\', \\'size\\': 640, \\'augment\\': True}, \\'python_environment_requirements\\': \\'pip install -U yolov5\\', \\'example_code\\': \"import yolov5\\\\nmodel = yolov5.load(\\'fcakyon/yolov5s-v7.0\\')\\\\nmodel.conf = 0.25\\\\nmodel.iou = 0.45\\\\nmodel.agnostic = False\\\\nmodel.multi_label = False\\\\nmodel.max_det = 1000\\\\nimg = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\\\\nresults = model(img)\\\\nresults = model(img, size=640)\\\\nresults = model(img, augment=True)\\\\npredictions = results.pred[0]\\\\nboxes = predictions[:, :4]\\\\nscores = predictions[:, 4]\\\\ncategories = predictions[:, 5]\\\\nresults.show()\\\\nresults.save(save_dir=\\'results/\\')\", \\'performance\\': {\\'dataset\\': \\'detection-datasets/coco\\', \\'accuracy\\': None}, \\'description\\': \\'Yolov5s-v7.0 is an object detection model trained on the COCO dataset. It can detect objects in images and return their bounding boxes, scores, and categories.\\'}', metadata={})]", "category": "generic"}
{"question_id": 276, "text": " I have a picture of my backyard, but I am not sure if a cat sneaked in. Can you help me find out if it is a cat in the picture?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'distilbart-cnn-12-6-samsum\\', \\'api_call\\': \"pipeline(\\'summarization\\', model=\\'philschmid/distilbart-cnn-12-6-samsum\\')\", \\'api_arguments\\': {\\'model\\': \\'philschmid/distilbart-cnn-12-6-samsum\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 41.09, \\'ROUGE-2\\': 20.746, \\'ROUGE-L\\': 31.595, \\'ROUGE-LSUM\\': 38.339}}, \\'description\\': \\'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 277, "text": " Our hospital needs to analyze digital blood samples in order to detect and count platelets, red blood cells, and white blood cells.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Blood Cell Detection\\', \\'api_name\\': \\'keremberke/yolov8n-blood-cell-detection\\', \\'api_call\\': \"YOLO(\\'keremberke/yolov8n-blood-cell-detection\\')\", \\'api_arguments\\': {\\'conf\\': 0.25, \\'iou\\': 0.45, \\'agnostic_nms\\': False, \\'max_det\\': 1000}, \\'python_environment_requirements\\': \\'ultralyticsplus==0.0.23 ultralytics==8.0.21\\', \\'example_code\\': \"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO(\\'keremberke/yolov8n-blood-cell-detection\\')\\\\nmodel.overrides[\\'conf\\'] = 0.25\\\\nmodel.overrides[\\'iou\\'] = 0.45\\\\nmodel.overrides[\\'agnostic_nms\\'] = False\\\\nmodel.overrides[\\'max_det\\'] = 1000\\\\nimage = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\", \\'performance\\': {\\'dataset\\': \\'blood-cell-object-detection\\', \\'accuracy\\': 0.893}, \\'description\\': \\'This model detects blood cells in images, specifically Platelets, RBC, and WBC. It is based on the YOLOv8 architecture and trained on the blood-cell-object-detection dataset.\\'}', metadata={})]", "category": "generic"}
{"question_id": 278, "text": " Develop a script that extracts a table from a given document and then visualize the results.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 279, "text": " We are now working on an architectural image generation project to generate different images of architecture.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 280, "text": " We are designing a book cover where the book is about love and roses. We have the idea of \\\"A head full of roses.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 281, "text": " Help me generate a realistic bedroom interior image that can be used as reference for a 3D model being created for a virtual reality game.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'microsoft/git-base-coco\\', \\'api_call\\': \"pipeline(\\'text-generation\\', model=\\'microsoft/git-base-coco\\')\", \\'api_arguments\\': \\'image\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'See the model hub for fine-tuned versions on a task that interests you.\\', \\'performance\\': {\\'dataset\\': \\'COCO\\', \\'accuracy\\': \\'Refer to the paper for evaluation results.\\'}, \\'description\\': \\'GIT (short for GenerativeImage2Text) model, base-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is a Transformer decoder conditioned on both CLIP image tokens and text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\\'}', metadata={})]", "category": "generic"}
{"question_id": 282, "text": " You feel lonely and need to generate a picture of a butterfly. How would you proceed?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Conversational\\', \\'api_name\\': \\'microsoft/GODEL-v1_1-large-seq2seq\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'microsoft/GODEL-v1_1-large-seq2seq\\')\", \\'api_arguments\\': {\\'instruction\\': \\'Instruction: given a dialog context, you need to response empathically.\\', \\'knowledge\\': \\'\\', \\'dialog\\': [\\'Does money buy happiness?\\', \\'It is a question. Money buys you a lot of things, but not enough to buy happiness.\\', \\'What is the best way to buy happiness ?\\']}, \\'python_environment_requirements\\': {\\'transformers\\': \\'AutoTokenizer, AutoModelForSeq2SeqLM\\'}, \\'example_code\\': \"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained(microsoft/GODEL-v1_1-large-seq2seq)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(microsoft/GODEL-v1_1-large-seq2seq)\\\\ndef generate(instruction, knowledge, dialog):\\\\n if knowledge != \\'\\':\\\\n knowledge = \\'[KNOWLEDGE] \\' + knowledge\\\\n dialog = \\' EOS \\'.join(dialog)\\\\n query = f{instruction} [CONTEXT] {dialog} {knowledge}\\\\n input_ids = tokenizer(f{query}, return_tensors=pt).input_ids\\\\n outputs = model.generate(input_ids, max_length=128, min_length=8, top_p=0.9, do_sample=True)\\\\n output = tokenizer.decode(outputs[0], skip_special_tokens=True)\\\\n return output\\\\n\\\\ninstruction = f\\'Instruction: given a dialog context, you need to response empathically.\\'\\\\nknowledge = \\'\\'\\\\ndialog = [\\\\n \\'Does money buy happiness?\\',\\\\n \\'It is a question. Money buys you a lot of things, but not enough to buy happiness.\\',\\\\n \\'What is the best way to buy happiness ?\\'\\\\n]\\\\nresponse = generate(instruction, knowledge, dialog)\\\\nprint(response)\", \\'performance\\': {\\'dataset\\': \\'Reddit discussion thread, instruction and knowledge grounded dialogs\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'GODEL is a large-scale pre-trained model for goal-directed dialogs. It is parameterized with a Transformer-based encoder-decoder model and trained for response generation grounded in external text, which allows more effective fine-tuning on dialog tasks that require conditioning the response on information that is external to the current conversation (e.g., a retrieved document). The pre-trained model can be efficiently fine-tuned and adapted to accomplish a new dialog task with a handful of task-specific dialogs. The v1.1 model is trained on 551M multi-turn dialogs from Reddit discussion thread, and 5M instruction and knowledge grounded dialogs.\\'}', metadata={})]", "category": "generic"}
{"question_id": 283, "text": " We need an AI-generated insect image for our biology article about African habitats.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 284, "text": " Our client is involved in a sports organization, and they require a solution to classify sports videos efficiently.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 285, "text": " We need to create a content filter for images submitted by users in our online community. We want to detect and filter out adult content and offensive images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 286, "text": " We are an animal rescue organization, and we are looking for a way to automatically identify if images uploaded to our site contain cats, dogs, or birds.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 287, "text": " Create a machine learning-based image recognition tool that can identify whether an animal in an image is a cat or a dog.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'zero-shot-object-detection\\', \\'api_name\\': \\'google/owlvit-base-patch16\\', \\'api_call\\': \"OwlViTForObjectDetection.from_pretrained(\\'google/owlvit-base-patch16\\')\", \\'api_arguments\\': [\\'texts\\', \\'images\\'], \\'python_environment_requirements\\': [\\'requests\\', \\'PIL\\', \\'torch\\', \\'transformers\\'], \\'example_code\\': \\'processor = OwlViTProcessor.from_pretrained(google/owlvit-base-patch16)\\\\nmodel = OwlViTForObjectDetection.from_pretrained(google/owlvit-base-patch16)\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ntexts = [[a photo of a cat, a photo of a dog]]\\\\ninputs = processor(text=texts, images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\ntarget_sizes = torch.Tensor([image.size[::-1]])\\\\nresults = processor.post_process(outputs=outputs, target_sizes=target_sizes)\\', \\'performance\\': {\\'dataset\\': \\'COCO\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. OWL-ViT uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features.\\'}', metadata={})]", "category": "generic"}
{"question_id": 288, "text": " I want to find out whether a given review is positive or negative. I need an API that can help me with sentiment analysis.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'distilbart-cnn-12-6-samsum\\', \\'api_call\\': \"pipeline(\\'summarization\\', model=\\'philschmid/distilbart-cnn-12-6-samsum\\')\", \\'api_arguments\\': {\\'model\\': \\'philschmid/distilbart-cnn-12-6-samsum\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 41.09, \\'ROUGE-2\\': 20.746, \\'ROUGE-L\\': 31.595, \\'ROUGE-LSUM\\': 38.339}}, \\'description\\': \\'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 289, "text": " I am a traveler and I want to know the best time to visit a Chinese historical site. Take a look at an image and tell me whether it is a good time to visit the site or not.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 290, "text": " Organizing a special event and want to identify the place where street pictures were taken for invitations.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Graph Machine Learning\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'graphormer-base-pcqm4mv1\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'graphormer-base-pcqm4mv1\\')\", \\'api_arguments\\': [\\'model_name\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'See the Graph Classification with Transformers tutorial\\', \\'performance\\': {\\'dataset\\': \\'PCQM4M-LSC\\', \\'accuracy\\': \\'1st place on the KDD CUP 2021 (quantum prediction track)\\'}, \\'description\\': \\'The Graphormer is a graph Transformer model, pretrained on PCQM4M-LSC, and which got 1st place on the KDD CUP 2021 (quantum prediction track). Developed by Microsoft, this model should be used for graph classification tasks or graph representation tasks; the most likely associated task is molecule modeling. It can either be used as such, or finetuned on downstream tasks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 291, "text": " Our client wants us to monitor consumer sentiment on their e-commerce platform. We must identify low-rated product reviews in six languages: English, Dutch, German, French, Italian, and Spanish.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Sentiment Analysis\\', \\'api_name\\': \\'bert-base-multilingual-uncased-sentiment\\', \\'api_call\\': \"pipeline(\\'sentiment-analysis\\', model=\\'nlptown/bert-base-multilingual-uncased-sentiment\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"result = sentiment_pipeline(\\'I love this product!\\')\", \\'performance\\': {\\'dataset\\': [{\\'language\\': \\'English\\', \\'accuracy\\': {\\'exact\\': \\'67%\\', \\'off-by-1\\': \\'95%\\'}}, {\\'language\\': \\'Dutch\\', \\'accuracy\\': {\\'exact\\': \\'57%\\', \\'off-by-1\\': \\'93%\\'}}, {\\'language\\': \\'German\\', \\'accuracy\\': {\\'exact\\': \\'61%\\', \\'off-by-1\\': \\'94%\\'}}, {\\'language\\': \\'French\\', \\'accuracy\\': {\\'exact\\': \\'59%\\', \\'off-by-1\\': \\'94%\\'}}, {\\'language\\': \\'Italian\\', \\'accuracy\\': {\\'exact\\': \\'59%\\', \\'off-by-1\\': \\'95%\\'}}, {\\'language\\': \\'Spanish\\', \\'accuracy\\': {\\'exact\\': \\'58%\\', \\'off-by-1\\': \\'95%\\'}}]}, \\'description\\': \\'This a bert-base-multilingual-uncased model finetuned for sentiment analysis on product reviews in six languages: English, Dutch, German, French, Spanish and Italian. It predicts the sentiment of the review as a number of stars (between 1 and 5).\\'}', metadata={})]", "category": "generic"}
{"question_id": 292, "text": " Write a script for an online forum moderator that will help them to detect gibberish text in a post.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Regression\\', \\'framework\\': \\'Keras\\', \\'functionality\\': \\'anomaly-detection\\', \\'api_name\\': \\'keras-io/timeseries-anomaly-detection\\', \\'api_call\\': \"TFAutoModelForSequenceClassification.from_pretrained(\\'keras-io/timeseries-anomaly-detection\\')\", \\'api_arguments\\': {\\'optimizer\\': {\\'name\\': \\'Adam\\', \\'learning_rate\\': 0.001, \\'decay\\': 0.0, \\'beta_1\\': 0.9, \\'beta_2\\': 0.999, \\'epsilon\\': 1e-07, \\'amsgrad\\': False}, \\'training_precision\\': \\'float32\\'}, \\'python_environment_requirements\\': [\\'tensorflow\\', \\'keras\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'Numenta Anomaly Benchmark(NAB)\\', \\'accuracy\\': {\\'Train Loss\\': 0.006, \\'Validation Loss\\': 0.008}}, \\'description\\': \\'This script demonstrates how you can use a reconstruction convolutional autoencoder model to detect anomalies in timeseries data. We will use the Numenta Anomaly Benchmark(NAB) dataset. It provides artifical timeseries data containing labeled anomalous periods of behavior. Data are ordered, timestamped, single-valued metrics.\\'}', metadata={})]", "category": "generic"}
{"question_id": 293, "text": " As a restaurant owner, I want to know if the total revenue for last week met our target revenue.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 294, "text": " We are a sports analysis company that processes data from spreadsheets with game statistics. We need to identify the player who has scored the maximum goals in a given match.\\n###Input: \\\"What player scored the most goals?\\\", \\\"Player,Goals\\\\nA,2\\\\nB,3\\\\nC,1\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 295, "text": " A manager in our company requires an aggregated answer of the highest and lowest sales numbers for a given period to evaluate the perfomance.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 296, "text": " A doctor needs to find medical information in a large document. We are building an AI model to help them extract answers to their questions. \\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 297, "text": " We are building a news classification system, and for Spanish articles, we have to categorize them into different sections.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Feature Extraction\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'DeepPavlov/rubert-base-cased\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'DeepPavlov/rubert-base-cased\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'Russian part of Wikipedia and news data\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'RuBERT (Russian, cased, 12\u2011layer, 768\u2011hidden, 12\u2011heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT\u2011base as an initialization for RuBERT[1].\\'}', metadata={})]", "category": "generic"}
{"question_id": 298, "text": " In my project, I need to classify news articles into their respective categories, such as technology, sports, and politics, without providing any labeled training data.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Zero-Shot Image Classification\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Zero-Shot Image Classification\\', \\'api_name\\': \\'laion/CLIP-ViT-g-14-laion2B-s34B-b88K\\', \\'api_call\\': \"pipeline(\\'zero-shot-image-classification\\', model=\\'laion/CLIP-ViT-g-14-laion2B-s34B-b88K\\')\", \\'api_arguments\\': {\\'image\\': \\'path/to/image/file\\', \\'class_names\\': \\'list_of_class_names\\'}, \\'python_environment_requirements\\': {\\'huggingface_hub\\': \\'0.0.17\\', \\'transformers\\': \\'4.11.3\\', \\'torch\\': \\'1.9.0\\', \\'torchvision\\': \\'0.10.0\\'}, \\'example_code\\': None, \\'performance\\': {\\'dataset\\': None, \\'accuracy\\': None}, \\'description\\': \\'A zero-shot image classification model based on OpenCLIP, which can classify images into various categories without requiring any training data for those categories.\\'}', metadata={})]", "category": "generic"}
{"question_id": 299, "text": " I have a Catalan language text and I need to publish it as a blog article in Spanish, so I want to translate my Catalan text into Spanish.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 300, "text": " Generate an automatic summarization of an executive's meeting notes without sharing too much details.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Visual Question Answering\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Visual Question Answering\\', \\'api_name\\': \\'sheldonxxxx/OFA_model_weights\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'sheldonxxxx/OFA_model_weights\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'This is an unoffical mirror of the model weights for use with https://github.com/OFA-Sys/OFA. The original link is too slow when downloading from outside of China.\\'}', metadata={})]", "category": "generic"}
{"question_id": 301, "text": " A customer needs to translate a French email they received into Spanish.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Translation\\', \\'api_name\\': \\'opus-mt-fr-en\\', \\'api_call\\': \"pipeline(\\'translation_fr_to_en\\', model=\\'Helsinki-NLP/opus-mt-fr-en\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': \"translation_pipeline(\\'Bonjour, comment \u00e7a va?\\')\", \\'performance\\': {\\'dataset\\': \\'opus\\', \\'accuracy\\': {\\'BLEU\\': {\\'newsdiscussdev2015-enfr.fr.en\\': 33.1, \\'newsdiscusstest2015-enfr.fr.en\\': 38.7, \\'newssyscomb2009.fr.en\\': 30.3, \\'news-test2008.fr.en\\': 26.2, \\'newstest2009.fr.en\\': 30.2, \\'newstest2010.fr.en\\': 32.2, \\'newstest2011.fr.en\\': 33.0, \\'newstest2012.fr.en\\': 32.8, \\'newstest2013.fr.en\\': 33.9, \\'newstest2014-fren.fr.en\\': 37.8, \\'Tatoeba.fr.en\\': 57.5}}}, \\'description\\': \\'Helsinki-NLP/opus-mt-fr-en is a machine translation model trained to translate from French to English. It is based on the Marian NMT framework and trained on the OPUS dataset.\\'}', metadata={})]", "category": "generic"}
{"question_id": 302, "text": " I am a writer, and I have written a long article, but I need to come up with a summary for my article to publish.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 303, "text": " Your company has launched a new chatbot service. We need to create conversations with the customers to provide information about our products and answer their queries.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'csebuetnlp/mT5_multilingual_XLSum\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'csebuetnlp/mT5_multilingual_XLSum\\')\", \\'api_arguments\\': [\\'model_name\\'], \\'python_environment_requirements\\': [\\'transformers==4.11.0.dev0\\'], \\'example_code\\': \"import re\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nWHITESPACE_HANDLER = lambda k: re.sub(\\'\\\\\\\\s+\\', \\' \\', re.sub(\\'\\\\\\\\n+\\', \\' \\', k.strip()))\\\\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people\\'s scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We\\'re expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\\\\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\\\ninput_ids = tokenizer(\\\\n [WHITESPACE_HANDLER(article_text)],\\\\n return_tensors=pt,\\\\n padding=max_length,\\\\n truncation=True,\\\\n max_length=512\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n max_length=84,\\\\n no_repeat_ngram_size=2,\\\\n num_beams=4\\\\n)[0]\\\\nsummary = tokenizer.decode(\\\\n output_ids,\\\\n skip_special_tokens=True,\\\\n clean_up_tokenization_spaces=False\\\\n)\\\\nprint(summary)\", \\'performance\\': {\\'dataset\\': \\'xsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.5, \\'ROUGE-2\\': 13.934, \\'ROUGE-L\\': 28.988, \\'ROUGE-LSUM\\': 28.996, \\'loss\\': 2.067, \\'gen_len\\': 26.973}}, \\'description\\': \\'This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 304, "text": " We are a travel company trying to offer personalized experiences to our customers. We want to implement a conversational AI to help users get information about travel destinations.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 305, "text": " Write a tweet on the topic of \\\"The Future of AI in Education\\\".\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Text Classification\\', \\'api_name\\': \\'distilbert-base-uncased-finetuned-sst-2-english\\', \\'api_call\\': \"DistilBertForSequenceClassification.from_pretrained(\\'distilbert-base-uncased-finetuned-sst-2-english\\')\", \\'api_arguments\\': [\\'inputs\\'], \\'python_environment_requirements\\': [\\'torch\\', \\'transformers\\'], \\'example_code\\': \"import torch\\\\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification\\\\ntokenizer = DistilBertTokenizer.from_pretrained(\\'distilbert-base-uncased\\')\\\\nmodel = DistilBertForSequenceClassification.from_pretrained(\\'distilbert-base-uncased-finetuned-sst-2-english\\')\\\\ninputs = tokenizer(\\'Hello, my dog is cute\\', return_tensors=\\'pt\\')\\\\nwith torch.no_grad():\\\\n    logits = model(**inputs).logits\\\\npredicted_class_id = logits.argmax().item()\\\\nmodel.config.id2label[predicted_class_id]\", \\'performance\\': {\\'dataset\\': \\'glue\\', \\'accuracy\\': 0.911}, \\'description\\': \\'This model is a fine-tune checkpoint of DistilBERT-base-uncased, fine-tuned on SST-2. It reaches an accuracy of 91.3 on the dev set (for comparison, Bert bert-base-uncased version reaches an accuracy of 92.7). This model can be used for topic classification.\\'}', metadata={})]", "category": "generic"}
{"question_id": 306, "text": " Create a poem about nature for a school assignment.\\n###Input: Once upon a time, in a land of greenery and beauty,\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'PyTorch Transformers\\', \\'functionality\\': \\'Table Question Answering\\', \\'api_name\\': \\'table-question-answering-tapas\\', \\'api_call\\': \"pipeline(\\'table-question-answering\\', model=\\'Meena/table-question-answering-tapas\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'This model can be loaded on the Inference API on-demand.\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'SQA (Sequential Question Answering by Microsoft)\\', \\'accuracy\\': None}, {\\'name\\': \\'WTQ (Wiki Table Questions by Stanford University)\\', \\'accuracy\\': None}, {\\'name\\': \\'WikiSQL (by Salesforce)\\', \\'accuracy\\': None}]}, \\'description\\': \\'TAPAS, the model learns an inner representation of the English language used in tables and associated texts, which can then be used to extract features useful for downstream tasks such as answering questions about a table, or determining whether a sentence is entailed or refuted by the contents of a table. It is a BERT-based model specifically designed (and pre-trained) for answering questions about tabular data. TAPAS uses relative position embeddings and has 7 token types that encode tabular structure. It is pre-trained on the masked language modeling (MLM) objective on a large dataset comprising millions of tables from English Wikipedia and corresponding texts.\\'}', metadata={})]", "category": "generic"}
{"question_id": 307, "text": " We want to code a loading spinner to display when our system is undergoing maintenance. Can you generate the code for us?\\n###Input: <noinput>\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 308, "text": " I am an English teacher. What kind of creative writing exercise could I give to my students that they could write a story using the words \\\"moon\\\", \\\"rabbit\\\", \\\"forest\\\", and \\\"magic\\\"?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 309, "text": " Our team needs to develop a tool that can translate articles from English to German in real-time.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Unconditional Image Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Denoising Diffusion Probabilistic Models (DDPM)\\', \\'api_name\\': \\'google/ddpm-bedroom-256\\', \\'api_call\\': \"DDPMPipeline.from_pretrained(\\'google/ddpm-bedroom-256\\')\", \\'api_arguments\\': \\'None\\', \\'python_environment_requirements\\': \\'diffusers\\', \\'example_code\\': \\'!pip install diffusers\\\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\\\nmodel_id = google/ddpm-bedroom-256\\\\nddpm = DDPMPipeline.from_pretrained(model_id)\\\\nimage = ddpm().images[0]\\\\nimage.save(ddpm_generated_image.png)\\', \\'performance\\': {\\'dataset\\': \\'CIFAR10\\', \\'accuracy\\': {\\'Inception score\\': 9.46, \\'FID score\\': 3.17}}, \\'description\\': \\'We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.\\'}', metadata={})]", "category": "generic"}
{"question_id": 310, "text": " Design an efficient search engine that retrieves most relevant articles based on a pool of long text documents.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'google/bigbird-pegasus-large-arxiv\\', \\'api_call\\': \"BigBirdPegasusForConditionalGeneration.from_pretrained(\\'google/bigbird-pegasus-large-arxiv\\')\", \\'api_arguments\\': [\\'attention_type\\', \\'block_size\\', \\'num_random_blocks\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import BigBirdPegasusForConditionalGeneration, AutoTokenizer\\\\ntokenizer = AutoTokenizer.from_pretrained(google/bigbird-pegasus-large-arxiv)\\\\nmodel = BigBirdPegasusForConditionalGeneration.from_pretrained(google/bigbird-pegasus-large-arxiv)\\\\ntext = Replace me by any text you\\'d like.\\\\ninputs = tokenizer(text, return_tensors=\\'pt\\')\\\\nprediction = model.generate(**inputs)\\\\nprediction = tokenizer.batch_decode(prediction)\", \\'performance\\': {\\'dataset\\': \\'scientific_papers\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.028, \\'ROUGE-2\\': 13.417, \\'ROUGE-L\\': 21.961, \\'ROUGE-LSUM\\': 29.648}}, \\'description\\': \"BigBird, is a sparse-attention based transformer which extends Transformer based models, such as BERT to much longer sequences. Moreover, BigBird comes along with a theoretical understanding of the capabilities of a complete transformer that the sparse model can handle. BigBird was introduced in this paper and first released in this repository. BigBird relies on block sparse attention instead of normal attention (i.e. BERT\\'s attention) and can handle sequences up to a length of 4096 at a much lower compute cost compared to BERT. It has achieved SOTA on various tasks involving very long sequences such as long documents summarization, question-answering with long contexts.\"}', metadata={})]", "category": "generic"}
{"question_id": 311, "text": " Our designer hired me to create an application to automatically translate colors from a design. Usually, our designer likes to work with English, to get the color in Italian.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 312, "text": " I am building an AI to create in work report, this model should be able to complete the sentence for the report.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentiment Analysis\\', \\'api_name\\': \\'michellejieli/NSFW_text_classifier\\', \\'api_call\\': \"pipeline(\\'sentiment-analysis\\', model=\\'michellejieli/NSFW_text_classification\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'classifier(I see you\u2019ve set aside this special time to humiliate yourself in public.)\\', \\'performance\\': {\\'dataset\\': \\'Reddit posts\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'DistilBERT is a transformer model that performs sentiment analysis. I fine-tuned the model on Reddit posts with the purpose of classifying not safe for work (NSFW) content, specifically text that is considered inappropriate and unprofessional. The model predicts 2 classes, which are NSFW or safe for work (SFW). The model is a fine-tuned version of DistilBERT. It was fine-tuned on 14317 Reddit posts pulled from the Reddit API.\\'}', metadata={})]", "category": "generic"}
{"question_id": 313, "text": " A language learning application wants to generate fill-in-the-blank questions for learners. We need to mask sentences with a keyword to create these questions.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 314, "text": " Our client has a medical report and we are trying to assist him in finding relevant information.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Unconditional Image Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Denoising Diffusion Probabilistic Models (DDPM)\\', \\'api_name\\': \\'google/ddpm-bedroom-256\\', \\'api_call\\': \"DDPMPipeline.from_pretrained(\\'google/ddpm-bedroom-256\\')\", \\'api_arguments\\': \\'None\\', \\'python_environment_requirements\\': \\'diffusers\\', \\'example_code\\': \\'!pip install diffusers\\\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\\\nmodel_id = google/ddpm-bedroom-256\\\\nddpm = DDPMPipeline.from_pretrained(model_id)\\\\nimage = ddpm().images[0]\\\\nimage.save(ddpm_generated_image.png)\\', \\'performance\\': {\\'dataset\\': \\'CIFAR10\\', \\'accuracy\\': {\\'Inception score\\': 9.46, \\'FID score\\': 3.17}}, \\'description\\': \\'We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.\\'}', metadata={})]", "category": "generic"}
{"question_id": 315, "text": " I am a data scientist and need to find which among given documents is similar, provide me the example code.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'distilbart-cnn-12-6-samsum\\', \\'api_call\\': \"pipeline(\\'summarization\\', model=\\'philschmid/distilbart-cnn-12-6-samsum\\')\", \\'api_arguments\\': {\\'model\\': \\'philschmid/distilbart-cnn-12-6-samsum\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 41.09, \\'ROUGE-2\\': 20.746, \\'ROUGE-L\\': 31.595, \\'ROUGE-LSUM\\': 38.339}}, \\'description\\': \\'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 316, "text": " My company wants to analyze and cluster different movie scripts. I need a suggestion to embed the movie dialogue text into dense vector representations.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Zero-Shot Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Zero-Shot Classification\\', \\'api_name\\': \\'MoritzLaurer/DeBERTa-v3-xsmall-mnli-fever-anli-ling-binary\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'MoritzLaurer/DeBERTa-v3-xsmall-mnli-fever-anli-ling-binary\\')\", \\'api_arguments\\': {\\'premise\\': \\'string\\', \\'hypothesis\\': \\'string\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'>=4.13\\', \\'torch\\': \\'any\\'}, \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\\\nimport torch\\\\ndevice = torch.device(cuda) if torch.cuda.is_available() else torch.device(cpu)\\\\nmodel_name = MoritzLaurer/DeBERTa-v3-xsmall-mnli-fever-anli-ling-binary\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\\\\npremise = I first thought that I liked the movie, but upon second thought it was actually disappointing.\\\\nhypothesis = The movie was good.\\\\ninput = tokenizer(premise, hypothesis, truncation=True, return_tensors=pt)\\\\noutput = model(input[input_ids].to(device))\\\\nprediction = torch.softmax(output[logits][0], -1).tolist()\\\\nlabel_names = [entailment, not_entailment]\\\\nprediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\\\\nprint(prediction)\\', \\'performance\\': {\\'dataset\\': {\\'mnli-m-2c\\': {\\'accuracy\\': 0.925}, \\'mnli-mm-2c\\': {\\'accuracy\\': 0.922}, \\'fever-nli-2c\\': {\\'accuracy\\': 0.892}, \\'anli-all-2c\\': {\\'accuracy\\': 0.676}, \\'anli-r3-2c\\': {\\'accuracy\\': 0.665}, \\'lingnli-2c\\': {\\'accuracy\\': 0.888}}}, \\'description\\': \\'This model was trained on 782 357 hypothesis-premise pairs from 4 NLI datasets: MultiNLI, Fever-NLI, LingNLI and ANLI. The base model is DeBERTa-v3-xsmall from Microsoft. The v3 variant of DeBERTa substantially outperforms previous versions of the model by including a different pre-training objective.\\'}', metadata={})]", "category": "generic"}
{"question_id": 317, "text": " Analyze how characters in a book are connected and if they share any similarity based on their conversation.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Segmentation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'facebook/mask2former-swin-large-coco-panoptic\\', \\'api_call\\': \"Mask2FormerForUniversalSegmentation.from_pretrained(\\'facebook/mask2former-swin-large-coco-panoptic\\')\", \\'api_arguments\\': [\\'image\\'], \\'python_environment_requirements\\': [\\'requests\\', \\'torch\\', \\'PIL\\', \\'transformers\\'], \\'example_code\\': \\'processor = AutoImageProcessor.from_pretrained(facebook/mask2former-swin-large-coco-panoptic)\\\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained(facebook/mask2former-swin-large-coco-panoptic)\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = processor(images=image, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\nclass_queries_logits = outputs.class_queries_logits\\\\nmasks_queries_logits = outputs.masks_queries_logits\\\\nresult = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\\\npredicted_panoptic_map = result[segmentation]\\', \\'performance\\': {\\'dataset\\': \\'COCO\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Mask2Former model trained on COCO panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.\\'}', metadata={})]", "category": "generic"}
{"question_id": 318, "text": " We are a company specializing in building automation systems for homes. We'd like to convert our written rules into an audio format for user interaction purposes.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 319, "text": " We are developing a virtual assistant. One of its major functionalities is to convert written text to speech.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 320, "text": " I have a recorded meeting between multiple participants. Identify when two or more people are speaking simultaneously.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Automatic Speech Recognition\\', \\'framework\\': \\'pyannote.audio\\', \\'functionality\\': \\'overlapped-speech-detection\\', \\'api_name\\': \\'pyannote/overlapped-speech-detection\\', \\'api_call\\': \"pipeline.from_pretrained(\\'pyannote/overlapped-speech-detection\\', use_auth_token=\\'ACCESS_TOKEN_GOES_HERE\\')\", \\'api_arguments\\': [\\'audio.wav\\'], \\'python_environment_requirements\\': [\\'pyannote.audio 2.1\\'], \\'example_code\\': \\'from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained(pyannote/overlapped-speech-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\\\noutput = pipeline(audio.wav)\\\\nfor speech in output.get_timeline().support():\\\\n  # two or more speakers are active between speech.start and speech.end\\\\n  ...\\', \\'performance\\': {\\'dataset\\': \\'ami\\', \\'accuracy\\': None}, \\'description\\': \\'Automatic overlapped speech detection using pyannote.audio framework. The model detects when two or more speakers are active in an audio file.\\'}', metadata={})]", "category": "generic"}
{"question_id": 321, "text": " I have a collection of audio recordings from interviews that I need transcribed into text. How can I achieve this using automatic speech recognition?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Text-to-Text Generation\\', \\'api_name\\': \\'optimum/t5-small\\', \\'api_call\\': \"ORTModelForSeq2SeqLM.from_pretrained(\\'optimum/t5-small\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'optimum.onnxruntime\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, pipeline\\\\nfrom optimum.onnxruntime import ORTModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained(optimum/t5-small)\\\\nmodel = ORTModelForSeq2SeqLM.from_pretrained(optimum/t5-small)\\\\ntranslator = pipeline(translation_en_to_fr, model=model, tokenizer=tokenizer)\\\\nresults = translator(My name is Eustache and I have a pet raccoon)\\\\nprint(results)\\', \\'performance\\': {\\'dataset\\': \\'c4\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'T5 is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks and for which each task is converted into a text-to-text format. It can be used for translation, text-to-text generation, and summarization.\\'}', metadata={})]", "category": "generic"}
{"question_id": 322, "text": " A Russia-based online lesson website needs an English subtitle for the recorded lessons of the students.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Information Retrieval\\', \\'api_name\\': \\'cross-encoder/ms-marco-TinyBERT-L-2-v2\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'cross-encoder/ms-marco-TinyBERT-L-2-v2\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\"}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': {\\'import\\': \\'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\', \\'model\\': \"model = AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\", \\'scores\\': \\'with torch.no_grad():\\\\n    scores = model(**features).logits\\\\n    print(scores)\\'}, \\'performance\\': {\\'dataset\\': \\'TREC Deep Learning 2019\\', \\'accuracy\\': \\'69.84 (NDCG@10)\\'}, \\'description\\': \\'This model was trained on the MS Marco Passage Ranking task. It can be used for Information Retrieval: Given a query, encode the query with all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. The training code is available here: SBERT.net Training MS Marco.\\'}', metadata={})]", "category": "generic"}
{"question_id": 323, "text": " My lecture has been recorded, and I would like to transcribe the audio to create a transcript of my presentation.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Token Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Named Entity Recognition\\', \\'api_name\\': \\'dslim/bert-base-NER\\', \\'api_call\\': \"AutoModelForTokenClassification.from_pretrained(\\'dslim/bert-base-NER\\')\", \\'api_arguments\\': {\\'model\\': \\'AutoModelForTokenClassification.from_pretrained(dslim/bert-base-NER)\\', \\'tokenizer\\': \\'AutoTokenizer.from_pretrained(dslim/bert-base-NER)\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelForTokenClassification\\\\nfrom transformers import pipeline\\\\ntokenizer = AutoTokenizer.from_pretrained(dslim/bert-base-NER)\\\\nmodel = AutoModelForTokenClassification.from_pretrained(dslim/bert-base-NER)\\\\nnlp = pipeline(ner, model=model, tokenizer=tokenizer)\\\\nexample = My name is Wolfgang and I live in Berlin\\\\nner_results = nlp(example)\\\\nprint(ner_results)\\', \\'performance\\': {\\'dataset\\': \\'conll2003\\', \\'accuracy\\': {\\'f1\\': 91.3, \\'precision\\': 90.7, \\'recall\\': 91.9}}, \\'description\\': \\'bert-base-NER is a fine-tuned BERT model that is ready to use for Named Entity Recognition and achieves state-of-the-art performance for the NER task. It has been trained to recognize four types of entities: location (LOC), organizations (ORG), person (PER) and Miscellaneous (MISC). Specifically, this model is a bert-base-cased model that was fine-tuned on the English version of the standard CoNLL-2003 Named Entity Recognition dataset.\\'}', metadata={})]", "category": "generic"}
{"question_id": 324, "text": " Provide the code to enhance a single audio track, possibly containing dialogue, music and background noise, extracted from a video game.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Table Question Answering\\', \\'api_name\\': \\'google/tapas-medium-finetuned-sqa\\', \\'api_call\\': \"pipeline(\\'table-question-answering\\', model=\\'google/tapas-medium-finetuned-sqa\\')\", \\'api_arguments\\': \\'table, query\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\n\\\\n# Initialize the pipeline\\\\ntable_qa_pipeline = pipeline(\\'table-question-answering\\', model=\\'google/tapas-medium-finetuned-sqa\\')\\\\n\\\\n# Provide the table and query\\\\nresult = table_qa_pipeline(table=table, query=\\'What is the total revenue?\\')\", \\'performance\\': {\\'dataset\\': \\'msr_sqa\\', \\'accuracy\\': 0.6561}, \\'description\\': \\'TAPAS medium model fine-tuned on Sequential Question Answering (SQA). This model is pretrained on a large corpus of English data from Wikipedia and uses relative position embeddings. It can be used for answering questions related to a table in a conversational set-up.\\'}', metadata={})]", "category": "generic"}
{"question_id": 325, "text": " We're working with a voiceover company, and they're looking for a tool to help them change the voice style of voiceovers while keeping the same content.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'nitrosocke/nitro-diffusion\\', \\'api_call\\': \"StableDiffusionPipeline.from_pretrained(\\'nitrosocke/nitro-diffusion\\', torch_dtype=torch.float16)\", \\'api_arguments\\': [\\'prompt\\'], \\'python_environment_requirements\\': [\\'torch\\', \\'diffusers\\'], \\'example_code\\': \\'from diffusers import StableDiffusionPipeline\\\\nimport torch\\\\nmodel_id = nitrosocke/nitro-diffusion\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = archer arcane style magical princess with golden hair\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(./magical_princess.png)\\', \\'performance\\': {\\'dataset\\': \\'Stable Diffusion\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'Nitro Diffusion is a fine-tuned Stable Diffusion model trained on three artstyles simultaneously while keeping each style separate from the others. It allows for high control of mixing, weighting, and single style use.\\'}', metadata={})]", "category": "generic"}
{"question_id": 326, "text": " Develop a listening assistant device for audiobooks that is capable of detecting and reducing noise.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 327, "text": " The company needs to separate voice from background noise in a recorded podcast episode.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Voice Activity Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Voice Activity Detection, Overlapped Speech Detection, Resegmentation\\', \\'api_name\\': \\'pyannote/segmentation\\', \\'api_call\\': \"VoiceActivityDetection(segmentation=\\'pyannote/segmentation\\')\", \\'api_arguments\\': {\\'onset\\': 0.5, \\'offset\\': 0.5, \\'min_duration_on\\': 0.0, \\'min_duration_off\\': 0.0}, \\'python_environment_requirements\\': \\'pyannote.audio 2.0\\', \\'example_code\\': \\'from pyannote.audio.pipelines import VoiceActivityDetection\\\\npipeline = VoiceActivityDetection(segmentation=pyannote/segmentation)\\\\nHYPER_PARAMETERS = {\\\\n  onset: 0.5, offset: 0.5,\\\\n  min_duration_on: 0.0,\\\\n  min_duration_off: 0.0\\\\n}\\\\npipeline.instantiate(HYPER_PARAMETERS)\\\\nvad = pipeline(audio.wav)\\', \\'performance\\': {\\'dataset\\': {\\'ami\\': {\\'accuracy\\': {\\'onset\\': 0.684, \\'offset\\': 0.577, \\'min_duration_on\\': 0.181, \\'min_duration_off\\': 0.037}}, \\'dihard\\': {\\'accuracy\\': {\\'onset\\': 0.767, \\'offset\\': 0.377, \\'min_duration_on\\': 0.136, \\'min_duration_off\\': 0.067}}, \\'voxconverse\\': {\\'accuracy\\': {\\'onset\\': 0.767, \\'offset\\': 0.713, \\'min_duration_on\\': 0.182, \\'min_duration_off\\': 0.501}}}}, \\'description\\': \\'Model from End-to-end speaker segmentation for overlap-aware resegmentation, by Herv\u00e9 Bredin and Antoine Laurent. It provides voice activity detection, overlapped speech detection, and resegmentation functionalities.\\'}', metadata={})]", "category": "generic"}
{"question_id": 328, "text": " Develop a method to detect which languages are being spoken in international conference calls.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 329, "text": " Develop a system that can detect a keyword in a short audio clip. Make sure it recognizes a variety of keywords.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 330, "text": " I am the director of a movie theater chain, and we are thinking of implementing a chat product to get movie goers to classify movie reviews as positive or negative through AI.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Classification\\', \\'api_name\\': \\'lvwerra/distilbert-imdb\\', \\'api_call\\': \"pipeline(\\'sentiment-analysis\\', model=\\'lvwerra/distilbert-imdb\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\', \\'pytorch\\'], \\'example_code\\': \"classifier(\\'I love this movie!\\')\", \\'performance\\': {\\'dataset\\': \\'imdb\\', \\'accuracy\\': 0.928}, \\'description\\': \\'This model is a fine-tuned version of distilbert-base-uncased on the imdb dataset. It is used for sentiment analysis on movie reviews and achieves an accuracy of 0.928 on the evaluation set.\\'}', metadata={})]", "category": "generic"}
{"question_id": 331, "text": " You have been provided with the dataset of plants, and your task is to identify the species of plants among Iris Setosa, Iris Versicolor, and Iris Virginica.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Classification\\', \\'framework\\': \\'Joblib\\', \\'functionality\\': \\'Multi-class Classification\\', \\'api_name\\': \\'abhishek/autotrain-iris-xgboost\\', \\'api_call\\': \"joblib.load(\\'model.joblib\\')\", \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'scikit-learn/iris\\', \\'accuracy\\': 0.8666666666666667}, \\'description\\': \\'A tabular classification model trained on the Iris dataset using XGBoost and AutoTrain. The model is capable of multi-class classification and has an accuracy of 86.67%.\\'}', metadata={})]", "category": "generic"}
{"question_id": 332, "text": " Our energy company is trying to identify anomalies in the energy consumption data. Could you perform anomaly detection on the time series data?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Token Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Entity Extraction\\', \\'api_name\\': \\'903429548\\', \\'api_call\\': \"AutoModelForTokenClassification.from_pretrained(\\'ismail-lucifer011/autotrain-company_all-903429548\\', use_auth_token=True)\", \\'api_arguments\\': {\\'inputs\\': \\'I love AutoTrain\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'AutoModelForTokenClassification, AutoTokenizer\\'}, \\'example_code\\': \\'from transformers import AutoModelForTokenClassification, AutoTokenizer\\\\nmodel = AutoModelForTokenClassification.from_pretrained(ismail-lucifer011/autotrain-company_all-903429548, use_auth_token=True)\\\\ntokenizer = AutoTokenizer.from_pretrained(ismail-lucifer011/autotrain-company_all-903429548, use_auth_token=True)\\\\ninputs = tokenizer(I love AutoTrain, return_tensors=pt)\\\\noutputs = model(**inputs)\\', \\'performance\\': {\\'dataset\\': \\'ismail-lucifer011/autotrain-data-company_all\\', \\'accuracy\\': 0.9979930566588805}, \\'description\\': \\'A token classification model trained using AutoTrain for entity extraction. The model is based on the distilbert architecture and trained on the ismail-lucifer011/autotrain-data-company_all dataset. It can be used to identify and extract company names from text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 333, "text": " Our company seeks a solution to reduce the carbon emissions due to our operations. We have collected historical data about our emissions. We are looking for a machine learning model to predict the carbon emissions based on this data.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 334, "text": " Create a program that predicts carbon emissions for new data using the given model.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Regression\\', \\'framework\\': \\'Joblib\\', \\'functionality\\': \\'Carbon Emissions\\', \\'api_name\\': \\'pcoloc/autotrain-600-dragino-1839063122\\', \\'api_call\\': \"joblib.load(\\'model.joblib\\')\", \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'pcoloc/autotrain-data-600-dragino\\', \\'accuracy\\': {\\'Loss\\': 93.595, \\'R2\\': 0.502, \\'MSE\\': 8760.052, \\'MAE\\': 77.527, \\'RMSLE\\': 0.445}}, \\'description\\': \\'This model is trained to perform single column regression on carbon emissions data using the AutoTrain framework. It predicts CO2 emissions in grams given the input data.\\'}', metadata={})]", "category": "generic"}
{"question_id": 335, "text": " A company has collected data about fish measurements and weights. We need to estimate a fish's weight based on its measurements.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 336, "text": " Our company manages a restaurant. We need to analyze customers' tips and predict how much tip a new customer would give based on their total bill, sex, smoker, day, time, and party size. We should use a pre-trained model.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 337, "text": " This time, we want to build a player against the user with reinforcement learning. Can you help?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text-generation\\', \\'api_name\\': \\'pygmalion-1.3b\\', \\'api_call\\': \"pipeline(\\'text-generation\\', \\'PygmalionAI/pygmalion-1.3b\\')\", \\'api_arguments\\': \\'input_prompt\\', \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"[CHARACTER]\\'s Persona: [A few sentences about the character you want the model to play]\\\\n[DIALOGUE HISTORY]\\\\nYou: [Your input message here]\\\\n[CHARACTER]:\", \\'performance\\': {\\'dataset\\': \\'56MB of dialogue data\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \"Pygmalion 1.3B is a proof-of-concept dialogue model based on EleutherAI\\'s pythia-1.3b-deduped. It is designed for generating conversational responses and can be used with a specific input format that includes character persona, dialogue history, and user input message.\"}', metadata={})]", "category": "generic"}
{"question_id": 338, "text": " We are building a robot for indoor navigation. We need to configure it to recognize its surroundings and interact with objects in the environment.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 340, "text": " We are building a medical knowledge-based management system. We need to process and extract features from biomedical entity names.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 341, "text": " A product is built that analyzes book reviews in order to determine how similar two examples from multiple books are to each other.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'results-yelp\\', \\'api_call\\': \"AutoTokenizer.from_pretrained(\\'bert-base-uncased\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"AutoTokenizer.from_pretrained(\\'bert-base-uncased\\')\", \\'config\\': \"AutoConfig.from_pretrained(\\'potatobunny/results-yelp\\')\"}, \\'python_environment_requirements\\': {\\'Transformers\\': \\'4.18.0\\', \\'Pytorch\\': \\'1.10.0+cu111\\', \\'Datasets\\': \\'2.0.0\\', \\'Tokenizers\\': \\'0.12.1\\'}, \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'Yelp\\', \\'accuracy\\': 0.9302}, \\'description\\': \\'This model is a fine-tuned version of textattack/bert-base-uncased-yelp-polarity on a filtered and manually reviewed Yelp dataset containing restaurant reviews only. It is intended to perform text classification, specifically sentiment analysis, on text data obtained from restaurant reviews to determine if the particular review is positive or negative.\\'}', metadata={})]", "category": "generic"}
{"question_id": 342, "text": " Write a welcome email to a new employee joining the company.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 343, "text": " We need to extract useful features from Russian text for downstream tasks.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'PyTorch Transformers\\', \\'functionality\\': \\'Table Question Answering\\', \\'api_name\\': \\'table-question-answering-tapas\\', \\'api_call\\': \"pipeline(\\'table-question-answering\\', model=\\'Meena/table-question-answering-tapas\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'This model can be loaded on the Inference API on-demand.\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'SQA (Sequential Question Answering by Microsoft)\\', \\'accuracy\\': None}, {\\'name\\': \\'WTQ (Wiki Table Questions by Stanford University)\\', \\'accuracy\\': None}, {\\'name\\': \\'WikiSQL (by Salesforce)\\', \\'accuracy\\': None}]}, \\'description\\': \\'TAPAS, the model learns an inner representation of the English language used in tables and associated texts, which can then be used to extract features useful for downstream tasks such as answering questions about a table, or determining whether a sentence is entailed or refuted by the contents of a table. It is a BERT-based model specifically designed (and pre-trained) for answering questions about tabular data. TAPAS uses relative position embeddings and has 7 token types that encode tabular structure. It is pre-trained on the masked language modeling (MLM) objective on a large dataset comprising millions of tables from English Wikipedia and corresponding texts.\\'}', metadata={})]", "category": "generic"}
{"question_id": 344, "text": " We are working on a project to detect patterns and correlations in a dataset that contains a mix of code segments and comments. Find a suitable model to complete this task.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 345, "text": " The marketing department wants to use AI-generated images for their next social media campaign. They want a high-resolution image of a vintage sports car racing through a desert landscape during sunset. \\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 346, "text": " My company is working on a children's storybook. I need to generate images based on the text descriptions of scenes in the story.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'wavymulder/Analog-Diffusion\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'wavymulder/Analog-Diffusion\\')\", \\'api_arguments\\': [\\'prompt\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"text_to_image(\\'analog style landscape\\')\", \\'performance\\': {\\'dataset\\': \\'analog photographs\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \"Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token \\'analog style\\' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library.\"}', metadata={})]", "category": "generic"}
{"question_id": 347, "text": " Design a text-based interface where you can input text and get an image description from the given text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'distilbart-cnn-12-6-samsum\\', \\'api_call\\': \"pipeline(\\'summarization\\', model=\\'philschmid/distilbart-cnn-12-6-samsum\\')\", \\'api_arguments\\': {\\'model\\': \\'philschmid/distilbart-cnn-12-6-samsum\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 41.09, \\'ROUGE-2\\': 20.746, \\'ROUGE-L\\': 31.595, \\'ROUGE-LSUM\\': 38.339}}, \\'description\\': \\'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 348, "text": " I am designing an e-commerce website, and I want a program to generate images based on written descriptions to be used as mock product images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 349, "text": " In our company, we have to generate descriptive captions for photographs related to the products.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'sberbank-ai/sbert_large_mt_nlu_ru\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'sberbank-ai/sbert_large_mt_nlu_ru\\')\", \\'api_arguments\\': [\\'sentences\\', \\'padding\\', \\'truncation\\', \\'max_length\\', \\'return_tensors\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': \"from transformers import AutoTokenizer, AutoModel\\\\nimport torch\\\\n\\\\n\\\\n# Mean Pooling - Take attention mask into account for correct averaging\\\\ndef mean_pooling(model_output, attention_mask):\\\\n    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\\\\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\\\\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\\\\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\\\\n    return sum_embeddings / sum_mask\\\\n\\\\n\\\\n# Sentences we want sentence embeddings for sentences = [\\'?\\']\\\\n\\\\n# Load AutoModel from huggingface model repository\\\\ntokenizer = AutoTokenizer.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\\\nmodel = AutoModel.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\\\n\\\\n# Tokenize sentences\\\\nencoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=24, return_tensors=\\'pt\\')\\\\n\\\\n# Compute token embeddings\\\\nwith torch.no_grad():\\\\n    model_output = model(**encoded_input)\\\\n\\\\n# Perform pooling. In this case, mean pooling\\\\nsentence_embeddings = mean_pooling(model_output, encoded_input[\\'attention_mask\\'])\", \\'performance\\': {\\'dataset\\': \\'Russian SuperGLUE\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'BERT large model multitask (cased) for Sentence Embeddings in Russian language.\\'}', metadata={})]", "category": "generic"}
{"question_id": 350, "text": " We are building a visual tour guide application for smartphones. The app should be able to identify landmarks and provide information about them. I want to use the BLIP-2 model for this task.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text-generation\\', \\'api_name\\': \\'pygmalion-2.7b\\', \\'api_call\\': \"pipeline(\\'text-generation\\', model=\\'PygmalionAI/pygmalion-2.7b\\')\", \\'api_arguments\\': {\\'input_prompt\\': \\'string\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'latest\\'}, \\'example_code\\': \"model([CHARACTER]\\'s Persona: [A few sentences about the character you want the model to play]<START>[DIALOGUE HISTORY]You: [Your input message here][CHARACTER]:)\", \\'performance\\': {\\'dataset\\': \\'56MB of dialogue data\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \"Pygmalion 2.7B is a proof-of-concept dialogue model based on EleutherAI\\'s gpt-neo-2.7B. It is fine-tuned on 56MB of dialogue data gathered from multiple sources, including real and partially machine-generated conversations. The model is intended for use in generating conversational responses and can be used with a specific input format that includes character persona, dialogue history, and user input message.\"}', metadata={})]", "category": "generic"}
{"question_id": 351, "text": " I need a tool to analyze a chart image and generate a summary of the information contained within the chart.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'google/deplot\\', \\'api_call\\': \"Pix2StructForConditionalGeneration.from_pretrained(\\'google/deplot\\')\", \\'api_arguments\\': {\\'images\\': \\'image\\', \\'text\\': \\'question\\', \\'return_tensors\\': \\'pt\\', \\'max_new_tokens\\': 512}, \\'python_environment_requirements\\': {\\'transformers\\': \\'Pix2StructForConditionalGeneration, Pix2StructProcessor\\', \\'requests\\': \\'requests\\', \\'PIL\\': \\'Image\\'}, \\'example_code\\': \"from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\\\\nimport requests\\\\nfrom PIL import Image\\\\nmodel = Pix2StructForConditionalGeneration.from_pretrained(\\'google/deplot\\')\\\\nprocessor = Pix2StructProcessor.from_pretrained(\\'google/deplot\\')\\\\nurl = https://raw.githubusercontent.com/vis-nlp/ChartQA/main/ChartQA%20Dataset/val/png/5090.png\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = processor(images=image, text=Generate underlying data table of the figure below:, return_tensors=pt)\\\\npredictions = model.generate(**inputs, max_new_tokens=512)\\\\nprint(processor.decode(predictions[0], skip_special_tokens=True))\", \\'performance\\': {\\'dataset\\': \\'ChartQA\\', \\'accuracy\\': \\'24.0% improvement over finetuned SOTA\\'}, \\'description\\': \\'DePlot is a model that translates the image of a plot or chart to a linearized table. It decomposes the challenge of visual language reasoning into two steps: (1) plot-to-text translation, and (2) reasoning over the translated text. The output of DePlot can then be directly used to prompt a pretrained large language model (LLM), exploiting the few-shot reasoning capabilities of LLMs.\\'}', metadata={})]", "category": "generic"}
{"question_id": 352, "text": " Generate a video for a cooking show intro with the text \\\"Chef John's Culinary Adventures.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Video\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Video\\', \\'api_name\\': \\'camenduru/text2-video-zero\\', \\'api_call\\': \"pipeline(\\'text-to-video\\', model=\\'camenduru/text2-video-zero\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'This model is used for generating videos from text inputs. It is based on the Hugging Face framework and can be used with the transformers library. The model is trained on a variety of text and video datasets, and can be used for tasks such as video summarization, video generation from text prompts, and more.\\'}', metadata={})]", "category": "generic"}
{"question_id": 353, "text": " We are doing a creative commercial. We require a video that represents a person walking along a beach.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 354, "text": " Help me to build a self-learning assistant capable of answering questions about an image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'PyTorch Transformers\\', \\'functionality\\': \\'Table Question Answering\\', \\'api_name\\': \\'table-question-answering-tapas\\', \\'api_call\\': \"pipeline(\\'table-question-answering\\', model=\\'Meena/table-question-answering-tapas\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'This model can be loaded on the Inference API on-demand.\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'SQA (Sequential Question Answering by Microsoft)\\', \\'accuracy\\': None}, {\\'name\\': \\'WTQ (Wiki Table Questions by Stanford University)\\', \\'accuracy\\': None}, {\\'name\\': \\'WikiSQL (by Salesforce)\\', \\'accuracy\\': None}]}, \\'description\\': \\'TAPAS, the model learns an inner representation of the English language used in tables and associated texts, which can then be used to extract features useful for downstream tasks such as answering questions about a table, or determining whether a sentence is entailed or refuted by the contents of a table. It is a BERT-based model specifically designed (and pre-trained) for answering questions about tabular data. TAPAS uses relative position embeddings and has 7 token types that encode tabular structure. It is pre-trained on the masked language modeling (MLM) objective on a large dataset comprising millions of tables from English Wikipedia and corresponding texts.\\'}', metadata={})]", "category": "generic"}
{"question_id": 355, "text": " We have an online journal application that requires users to upload documents in which we need to automatically answer the questions related to the uploaded document.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 356, "text": " An educational company is building a document question answering system for their students. They want the system to answer questions about the contents of textbooks and study guides quickly and accurately.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'PyTorch Transformers\\', \\'functionality\\': \\'Table Question Answering\\', \\'api_name\\': \\'table-question-answering-tapas\\', \\'api_call\\': \"pipeline(\\'table-question-answering\\', model=\\'Meena/table-question-answering-tapas\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'This model can be loaded on the Inference API on-demand.\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'SQA (Sequential Question Answering by Microsoft)\\', \\'accuracy\\': None}, {\\'name\\': \\'WTQ (Wiki Table Questions by Stanford University)\\', \\'accuracy\\': None}, {\\'name\\': \\'WikiSQL (by Salesforce)\\', \\'accuracy\\': None}]}, \\'description\\': \\'TAPAS, the model learns an inner representation of the English language used in tables and associated texts, which can then be used to extract features useful for downstream tasks such as answering questions about a table, or determining whether a sentence is entailed or refuted by the contents of a table. It is a BERT-based model specifically designed (and pre-trained) for answering questions about tabular data. TAPAS uses relative position embeddings and has 7 token types that encode tabular structure. It is pre-trained on the masked language modeling (MLM) objective on a large dataset comprising millions of tables from English Wikipedia and corresponding texts.\\'}', metadata={})]", "category": "generic"}
{"question_id": 357, "text": " Our company focuses on predicting molecular properties. We need the best model to tackle this problem for our project. We heard about Graphormer and want to start with a pretrained model.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 358, "text": " We have a company that processes loan applications. Give us the answer for a question, based on a document.\\n###Input: Our company policy restricts the loan applicant's eligibility to the citizens of United States. The applicant needs to be 18 years old or above and their monthly salary should at least be $4,000. FetchTypeOfYear: 2019. Can anyone with a monthly salary of $3,000 apply?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'csebuetnlp/mT5_multilingual_XLSum\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'csebuetnlp/mT5_multilingual_XLSum\\')\", \\'api_arguments\\': [\\'model_name\\'], \\'python_environment_requirements\\': [\\'transformers==4.11.0.dev0\\'], \\'example_code\\': \"import re\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nWHITESPACE_HANDLER = lambda k: re.sub(\\'\\\\\\\\s+\\', \\' \\', re.sub(\\'\\\\\\\\n+\\', \\' \\', k.strip()))\\\\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people\\'s scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We\\'re expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\\\\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\\\ninput_ids = tokenizer(\\\\n [WHITESPACE_HANDLER(article_text)],\\\\n return_tensors=pt,\\\\n padding=max_length,\\\\n truncation=True,\\\\n max_length=512\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n max_length=84,\\\\n no_repeat_ngram_size=2,\\\\n num_beams=4\\\\n)[0]\\\\nsummary = tokenizer.decode(\\\\n output_ids,\\\\n skip_special_tokens=True,\\\\n clean_up_tokenization_spaces=False\\\\n)\\\\nprint(summary)\", \\'performance\\': {\\'dataset\\': \\'xsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.5, \\'ROUGE-2\\': 13.934, \\'ROUGE-L\\': 28.988, \\'ROUGE-LSUM\\': 28.996, \\'loss\\': 2.067, \\'gen_len\\': 26.973}}, \\'description\\': \\'This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 359, "text": " Our company is setting up an online shop for selling computer parts. We need a system that can identify the components in images uploaded by users.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 360, "text": " I want to classify the images of houseplants to find out their type, like whether it's a cactus, fern, or succulent.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'distilbart-cnn-12-6-samsum\\', \\'api_call\\': \"pipeline(\\'summarization\\', model=\\'philschmid/distilbart-cnn-12-6-samsum\\')\", \\'api_arguments\\': {\\'model\\': \\'philschmid/distilbart-cnn-12-6-samsum\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 41.09, \\'ROUGE-2\\': 20.746, \\'ROUGE-L\\': 31.595, \\'ROUGE-LSUM\\': 38.339}}, \\'description\\': \\'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 361, "text": " I have few picture from my album, and I would like to find out whether it is a hotdog or not.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 362, "text": " Our client is building an application that converts scanned tables into structured data. We need to detect the rows and columns in a given table image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 363, "text": " I am hosting a vegan food event and I want to know if any of the dishes contains meat from the picture I have taken.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 364, "text": " In a kitchen, as a robot chef, we need to identify different objects such as fruits and dishes.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Reinforcement Learning Robotics\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'EmbodiedAI tasks, such as object manipulation and indoor navigation\\', \\'api_name\\': \\'facebook/vc1-large\\', \\'api_call\\': \"model_utils.load_model(\\'model_utils.VC1_BASE_NAME\\')\", \\'api_arguments\\': \\'img\\', \\'python_environment_requirements\\': \\'from vc_models.models.vit import model_utils\\', \\'example_code\\': \\'model,embd_size,model_transforms,model_info = model_utils.load_model(model_utils.VC1_BASE_NAME)\\\\nimg = your_function_here ...\\\\ntransformed_img = model_transforms(img)\\\\nembedding = model(transformed_img)\\', \\'performance\\': {\\'dataset\\': \\'CortexBench\\', \\'accuracy\\': \\'68.7 (Mean Success)\\'}, \\'description\\': \\'The VC-1 model is a vision transformer (ViT) pre-trained on over 4,000 hours of egocentric videos from 7 different sources, together with ImageNet. The model is trained using Masked Auto-Encoding (MAE) and is available in two sizes: ViT-B and ViT-L. The model is intended for use for EmbodiedAI tasks, such as object manipulation and indoor navigation.\\'}', metadata={})]", "category": "generic"}
{"question_id": 365, "text": " We need to segment clothes in photos for our online fashion store.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 366, "text": " Automate the process of creating a normal map from a given object image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Automatic Speech Recognition\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'facebook/wav2vec2-xlsr-53-espeak-cv-ft\\', \\'api_call\\': \"Wav2Vec2ForCTC.from_pretrained(\\'facebook/wav2vec2-xlsr-53-espeak-cv-ft\\')\", \\'api_arguments\\': {\\'model_name\\': \\'facebook/wav2vec2-xlsr-53-espeak-cv-ft\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'4.13.0\\', \\'torch\\': \\'1.10.0\\', \\'datasets\\': \\'1.14.0\\'}, \\'example_code\\': \\'processor = Wav2Vec2Processor.from_pretrained(facebook/wav2vec2-xlsr-53-espeak-cv-ft)\\\\nmodel = Wav2Vec2ForCTC.from_pretrained(facebook/wav2vec2-xlsr-53-espeak-cv-ft)\\\\nds = load_dataset(patrickvonplaten/librispeech_asr_dummy, clean, split=validation)\\\\ninput_values = processor(ds[0][audio][array], return_tensors=pt).input_values\\\\nwith torch.no_grad():\\\\n    logits = model(input_values).logits\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.batch_decode(predicted_ids)\\', \\'performance\\': {\\'dataset\\': \\'common_voice\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Wav2Vec2-Large-XLSR-53 finetuned on multi-lingual Common Voice for phonetic label recognition in multiple languages. The model outputs a string of phonetic labels, and a dictionary mapping phonetic labels to words has to be used to map the phonetic output labels to output words.\\'}', metadata={})]", "category": "generic"}
{"question_id": 367, "text": " We have a picture of a landscape, and we'd like to add a building and a river in this picture.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 368, "text": " Our client, an architecture firm, needs a rendering of a \\\"luxury living room with a fireplace\\\" as a visualization for a prestigious project.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 369, "text": " We would like to restore an old and blurry image of our grandparents for an anniversary present.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 370, "text": " A creative artist wants to generate a painting from a given input text titled \\\"A head full of roses,\\\" utilizing the image provided in the API description.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Table Question Answering\\', \\'api_name\\': \\'google/tapas-base-finetuned-wikisql-supervised\\', \\'api_call\\': \"TapasForQuestionAnswering.from_pretrained(\\'google/tapas-base-finetuned-wikisql-supervised\\')\", \\'api_arguments\\': [\\'question\\', \\'table\\'], \\'python_environment_requirements\\': [\\'PyTorch\\', \\'TensorFlow\\'], \\'example_code\\': \\'This model can be loaded on the Inference API on-demand.\\', \\'performance\\': {\\'dataset\\': \\'wikisql\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'TAPAS is a BERT-like transformers model pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion. It was pretrained with two objectives: Masked language modeling (MLM) and Intermediate pre-training. Fine-tuning is done by adding a cell selection head and aggregation head on top of the pre-trained model, and then jointly train these randomly initialized classification heads with the base model on SQA and WikiSQL.\\'}', metadata={})]", "category": "generic"}
{"question_id": 371, "text": " I would like to generate a high-quality image of a church using unconditional image generation.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'dreamlike-art/dreamlike-photoreal-2.0\\', \\'api_call\\': \"StableDiffusionPipeline.from_pretrained(\\'dreamlike-art/dreamlike-photoreal-2.0\\', torch_dtype=torch.float16)(prompt).images[0]\", \\'api_arguments\\': {\\'prompt\\': \\'photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens\\'}, \\'python_environment_requirements\\': {\\'torch\\': \\'torch.float16\\', \\'diffusers\\': \\'StableDiffusionPipeline\\'}, \\'example_code\\': \\'from diffusers import StableDiffusionPipeline\\\\nimport torch\\\\nmodel_id = dreamlike-art/dreamlike-photoreal-2.0\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(./result.jpg)\\', \\'performance\\': {\\'dataset\\': \\'Stable Diffusion 1.5\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Dreamlike Photoreal 2.0 is a photorealistic model based on Stable Diffusion 1.5, made by dreamlike.art. It can be used to generate photorealistic images from text prompts.\\'}', metadata={})]", "category": "generic"}
{"question_id": 372, "text": " Recently, my company is asked to produce new portraits for the entire team. Therefore, I am looking for an AI which can generate human faces from scratch.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text-to-Text Generation\\', \\'api_name\\': \\'philschmid/bart-large-cnn-samsum\\', \\'api_call\\': \"pipeline(\\'summarization\\', model=\\'philschmid/bart-large-cnn-samsum\\')\", \\'api_arguments\\': {\\'model\\': \\'philschmid/bart-large-cnn-samsum\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'latest\\'}, \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/bart-large-cnn-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'eval_rouge1\\': 42.621, \\'eval_rouge2\\': 21.9825, \\'eval_rougeL\\': 33.034, \\'eval_rougeLsum\\': 39.6783, \\'test_rouge1\\': 41.3174, \\'test_rouge2\\': 20.8716, \\'test_rougeL\\': 32.1337, \\'test_rougeLsum\\': 38.4149}}, \\'description\\': \\'philschmid/bart-large-cnn-samsum is a BART-based model trained for text summarization on the SAMSum dataset. It can be used to generate abstractive summaries of conversations.\\'}', metadata={})]", "category": "generic"}
{"question_id": 373, "text": " Our team wants to make a cat-themed video game. We would like to design various cartoon cat characters through AI-generated images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 374, "text": " Can you teach me to build a program that will analyze video and describe what's happening in natural language?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'promptcap-coco-vqa\\', \\'api_call\\': \"PromptCap(\\'vqascore/promptcap-coco-vqa\\')\", \\'api_arguments\\': {\\'prompt\\': \\'string\\', \\'image\\': \\'string\\'}, \\'python_environment_requirements\\': \\'pip install promptcap\\', \\'example_code\\': [\\'import torch\\', \\'from promptcap import PromptCap\\', \\'model = PromptCap(vqascore/promptcap-coco-vqa)\\', \\'if torch.cuda.is_available():\\', \\'  model.cuda()\\', \\'prompt = please describe this image according to the given question: what piece of clothing is this boy putting on?\\', \\'image = glove_boy.jpeg\\', \\'print(model.caption(prompt, image))\\'], \\'performance\\': {\\'dataset\\': {\\'coco\\': {\\'accuracy\\': \\'150 CIDEr\\'}, \\'OK-VQA\\': {\\'accuracy\\': \\'60.4%\\'}, \\'A-OKVQA\\': {\\'accuracy\\': \\'59.6%\\'}}}, \\'description\\': \\'PromptCap is a captioning model that can be controlled by natural language instruction. The instruction may contain a question that the user is interested in. It achieves SOTA performance on COCO captioning (150 CIDEr) and knowledge-based VQA tasks when paired with GPT-3 (60.4% on OK-VQA and 59.6% on A-OKVQA).\\'}', metadata={})]", "category": "generic"}
{"question_id": 375, "text": " In a security context, the company is starting a project to detect any signs of violence in video streams from its CCTV cameras. It's essential to analyze the video content for any violent behaviors.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'emotion\\', \\'api_name\\': \\'bhadresh-savani/distilbert-base-uncased-emotion\\', \\'api_call\\': \"pipeline(\\'text-classification\\', model=\\'bhadresh-savani/distilbert-base-uncased-emotion\\', return_all_scores=True)\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"prediction = classifier(\\'I love using transformers. The best part is wide range of support and its easy to use\\')\", \\'performance\\': {\\'dataset\\': \\'Twitter-Sentiment-Analysis\\', \\'accuracy\\': 0.938}, \\'description\\': \"Distilbert is created with knowledge distillation during the pre-training phase which reduces the size of a BERT model by 40%, while retaining 97% of its language understanding. It\\'s smaller, faster than Bert and any other Bert-based model. Distilbert-base-uncased finetuned on the emotion dataset using HuggingFace Trainer.\"}', metadata={})]", "category": "generic"}
{"question_id": 376, "text": " Create a sports highlight generator that takes video clips as input and identifies the category of the sports activity happening in the video.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Visual Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ivelin/donut-refexp-combined-v1\\', \\'api_call\\': \"pipeline(\\'visual-question-answering\\', model=\\'ivelin/donut-refexp-combined-v1\\')\", \\'api_arguments\\': \\'image, question\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"vqa(image=\\'path/to/image.jpg\\', question=\\'What is the color of the object?\\')\", \\'performance\\': {\\'dataset\\': \\'ivelin/donut-refexp-combined-v1\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'A visual question answering model that takes an image and a question as input and provides an answer based on the visual content of the image and the context of the question.\\'}', metadata={})]", "category": "generic"}
{"question_id": 377, "text": " I am a movie director and I need to detect the genre of a movie based on its actions.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 378, "text": " Bob is designing an app for his city. He needs to quickly identify if the provided image has a bike or a car. Design a model that recognizes a bike or car and provides classification output for the given image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Zero-Shot Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Cross-Encoder for Natural Language Inference\\', \\'api_name\\': \\'cross-encoder/nli-deberta-v3-base\\', \\'api_call\\': \"CrossEncoder(\\'cross-encoder/nli-deberta-v3-base\\')\", \\'api_arguments\\': [\\'sentence_pairs\\'], \\'python_environment_requirements\\': [\\'sentence_transformers\\', \\'transformers\\'], \\'example_code\\': \"from sentence_transformers import CrossEncoder\\\\nmodel = CrossEncoder(\\'cross-encoder/nli-deberta-v3-base\\')\\\\nscores = model.predict([(\\'A man is eating pizza\\', \\'A man eats something\\'), (\\'A black race car starts up in front of a crowd of people.\\', \\'A man is driving down a lonely road.\\')])\", \\'performance\\': {\\'dataset\\': {\\'SNLI-test\\': \\'92.38\\', \\'MNLI mismatched set\\': \\'90.04\\'}}, \\'description\\': \\'This model is based on microsoft/deberta-v3-base and was trained on the SNLI and MultiNLI datasets. For a given sentence pair, it will output three scores corresponding to the labels: contradiction, entailment, neutral.\\'}', metadata={})]", "category": "generic"}
{"question_id": 379, "text": " I need to analyze the sentiment of tweets to find out which products are getting positive customer feedback.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'distilbart-cnn-12-6-samsum\\', \\'api_call\\': \"pipeline(\\'summarization\\', model=\\'philschmid/distilbart-cnn-12-6-samsum\\')\", \\'api_arguments\\': {\\'model\\': \\'philschmid/distilbart-cnn-12-6-samsum\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 41.09, \\'ROUGE-2\\': 20.746, \\'ROUGE-L\\': 31.595, \\'ROUGE-LSUM\\': 38.339}}, \\'description\\': \\'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 380, "text": " We are building a content moderation system. Our clients upload the content, it can be generated by human or AI. We want to have a filtering API to advise on the original text if it is generated by GPT-2.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Detect GPT-2 generated text\\', \\'api_name\\': \\'roberta-base-openai-detector\\', \\'api_call\\': \"pipeline(\\'text-classification\\', model=\\'roberta-base-openai-detector\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import pipeline\\\\npipe = pipeline(text-classification, model=roberta-base-openai-detector)\\\\nprint(pipe(Hello world! Is this content AI-generated?))\\', \\'performance\\': {\\'dataset\\': \\'WebText\\', \\'accuracy\\': \\'95%\\'}, \\'description\\': \\'RoBERTa base OpenAI Detector is the GPT-2 output detector model, obtained by fine-tuning a RoBERTa base model with the outputs of the 1.5B-parameter GPT-2 model. The model can be used to predict if text was generated by a GPT-2 model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 381, "text": " I have a list of comments related to stocks. Check the overall sentiment towards each stock.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \"PegasusForConditionalGeneration.from_pretrained(\\'tuner007/pegasus_summarizer\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 382, "text": " I'd like to give a potted plant to my friend. Write me a short and simple plant care instruction.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Generative Commonsense Reasoning\\', \\'api_name\\': \\'mrm8488/t5-base-finetuned-common_gen\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'mrm8488/t5-base-finetuned-common_gen\\')\", \\'api_arguments\\': [\\'words\\', \\'max_length\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import AutoModelWithLMHead, AutoTokenizer\\\\ntokenizer = AutoTokenizer.from_pretrained(mrm8488/t5-base-finetuned-common_gen)\\\\nmodel = AutoModelWithLMHead.from_pretrained(mrm8488/t5-base-finetuned-common_gen)\\\\ndef gen_sentence(words, max_length=32):\\\\n input_text = words\\\\n features = tokenizer([input_text], return_tensors=\\'pt\\')\\\\noutput = model.generate(input_ids=features[\\'input_ids\\'], attention_mask=features[\\'attention_mask\\'], max_length=max_length)\\\\nreturn tokenizer.decode(output[0], skip_special_tokens=True)\\\\nwords = tree plant ground hole dig\\\\ngen_sentence(words)\", \\'performance\\': {\\'dataset\\': \\'common_gen\\', \\'accuracy\\': {\\'ROUGE-2\\': 17.1, \\'ROUGE-L\\': 39.47}}, \\'description\\': \"Google\\'s T5 fine-tuned on CommonGen for Generative Commonsense Reasoning. CommonGen is a constrained text generation task, associated with a benchmark dataset, to explicitly test machines for the ability of generative commonsense reasoning. Given a set of common concepts; the task is to generate a coherent sentence describing an everyday scenario using these concepts.\"}', metadata={})]", "category": "generic"}
{"question_id": 383, "text": " I need to extract biomedical entities from a given set of case reports to make it easier for researchers to analyze the data.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Token Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Named Entity Recognition\\', \\'api_name\\': \\'d4data/biomedical-ner-all\\', \\'api_call\\': \"AutoModelForTokenClassification.from_pretrained(\\'d4data/biomedical-ner-all\\')\", \\'api_arguments\\': {\\'model\\': \\'AutoModelForTokenClassification.from_pretrained(d4data/biomedical-ner-all)\\', \\'tokenizer\\': \\'AutoTokenizer.from_pretrained(d4data/biomedical-ner-all)\\', \\'aggregation_strategy\\': \\'simple\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\\'}, \\'example_code\\': \\'pipe(The patient reported no recurrence of palpitations at follow-up 6 months after the ablation.)\\', \\'performance\\': {\\'dataset\\': \\'Maccrobat\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'An English Named Entity Recognition model, trained on Maccrobat to recognize the bio-medical entities (107 entities) from a given text corpus (case reports etc.). This model was built on top of distilbert-base-uncased.\\'}', metadata={})]", "category": "generic"}
{"question_id": 384, "text": " I need an AI feature capable of extracting names of people, organizations, and locations from various news articles in different languages.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Feature Extraction\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'DeepPavlov/rubert-base-cased\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'DeepPavlov/rubert-base-cased\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'Russian part of Wikipedia and news data\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'RuBERT (Russian, cased, 12\u2011layer, 768\u2011hidden, 12\u2011heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT\u2011base as an initialization for RuBERT[1].\\'}', metadata={})]", "category": "generic"}
{"question_id": 385, "text": " We are a consulting firm, and we want to easily identify company names from texts.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Token Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Entity Extraction\\', \\'api_name\\': \\'903429548\\', \\'api_call\\': \"AutoModelForTokenClassification.from_pretrained(\\'ismail-lucifer011/autotrain-company_all-903429548\\', use_auth_token=True)\", \\'api_arguments\\': {\\'inputs\\': \\'I love AutoTrain\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'AutoModelForTokenClassification, AutoTokenizer\\'}, \\'example_code\\': \\'from transformers import AutoModelForTokenClassification, AutoTokenizer\\\\nmodel = AutoModelForTokenClassification.from_pretrained(ismail-lucifer011/autotrain-company_all-903429548, use_auth_token=True)\\\\ntokenizer = AutoTokenizer.from_pretrained(ismail-lucifer011/autotrain-company_all-903429548, use_auth_token=True)\\\\ninputs = tokenizer(I love AutoTrain, return_tensors=pt)\\\\noutputs = model(**inputs)\\', \\'performance\\': {\\'dataset\\': \\'ismail-lucifer011/autotrain-data-company_all\\', \\'accuracy\\': 0.9979930566588805}, \\'description\\': \\'A token classification model trained using AutoTrain for entity extraction. The model is based on the distilbert architecture and trained on the ismail-lucifer011/autotrain-data-company_all dataset. It can be used to identify and extract company names from text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 386, "text": " Our company is making a chatbot that needs to extract information from a paragraph. Get the named entities in the paragraph.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'facebook/blenderbot-1B-distill\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'facebook/blenderbot-1B-distill\\')\", \\'api_arguments\\': {\\'input_message\\': \\'string\\'}, \\'python_environment_requirements\\': {\\'library\\': \\'transformers\\', \\'version\\': \\'latest\\'}, \\'example_code\\': \"inputs = tokenizer(\\'Hello, how are you?\\', return_tensors=\\'pt\\'); outputs = model.generate(inputs[\\'input_ids\\']); decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\", \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not mentioned\\'}, \\'description\\': \\'BlenderBot-1B is a large-scale open-domain chatbot model that can engage in conversations, ask and answer questions, and display knowledge, empathy, and personality. This distilled version is smaller and faster than the original 9.4B parameter model, making it more accessible for use.\\'}', metadata={})]", "category": "generic"}
{"question_id": 387, "text": " A journalist is looking for historical Olympic host cities and wants to find the year when Beijing hosted the games.\\n###Input: {\\\"table\\\": {\\n    \\\"year\\\": [1896, 1900, 1904, 2004, 2008, 2012],\\n    \\\"city\\\": [\\\"Athens\\\", \\\"Paris\\\", \\\"St. Louis\\\", \\\"Athens\\\", \\\"Beijing\\\", \\\"London\\\"]},\\n    \\\"query\\\": \\\"Select the year when Beijing hosted the Olympic games\\\"}\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 388, "text": " I am a Korean exchange student in the United States. I am building a Korean Table Question Answering assistant to help me with my studies.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 389, "text": " Create an API that processes large data sets of financial transactions and can deliver information on the number of transactions and their monetary value, based on a date range.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Token Classification\\', \\'framework\\': \\'Flair\\', \\'functionality\\': \\'Named Entity Recognition\\', \\'api_name\\': \\'flair/ner-english-ontonotes-large\\', \\'api_call\\': \"SequenceTagger.load(\\'flair/ner-english-ontonotes-large\\')\", \\'api_arguments\\': [\\'sentence\\'], \\'python_environment_requirements\\': [\\'flair\\'], \\'example_code\\': \"from flair.data import Sentence\\\\nfrom flair.models import SequenceTagger\\\\n\\\\n# load tagger\\\\ntagger = SequenceTagger.load(\\'flair/ner-english-ontonotes-large\\')\\\\n\\\\n# make example sentence\\\\nsentence = Sentence(\\'On September 1st George won 1 dollar while watching Game of Thrones.\\')\\\\n\\\\n# predict NER tags\\\\ntagger.predict(sentence)\\\\n\\\\n# print sentence\\\\nprint(sentence)\\\\n\\\\n# print predicted NER spans\\\\nprint(\\'The following NER tags are found:\\')\\\\n\\\\n# iterate over entities and print\\\\nfor entity in sentence.get_spans(\\'ner\\'):\\\\n    print(entity)\", \\'performance\\': {\\'dataset\\': \\'Ontonotes\\', \\'accuracy\\': 90.93}, \\'description\\': \\'English NER in Flair (Ontonotes large model). This is the large 18-class NER model for English that ships with Flair. It predicts 18 tags such as cardinal value, date value, event name, building name, geo-political entity, language name, law name, location name, money name, affiliation, ordinal value, organization name, percent value, person name, product name, quantity value, time value, and name of work of art. The model is based on document-level XLM-R embeddings and FLERT.\\'}', metadata={})]", "category": "generic"}
{"question_id": 390, "text": " A product manager needs an explanation related to model conversion. They are confused why it is important. Can you please help them by providing an answer?\\n###Input: \\n{\\n 'question': 'Why is model conversion important?',\\n 'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\\n}\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'deepset/tinyroberta-squad2\\', \\'api_call\\': \"AutoModelForQuestionAnswering.from_pretrained(\\'deepset/tinyroberta-squad2\\')\", \\'api_arguments\\': {\\'model_name_or_path\\': \\'deepset/tinyroberta-squad2\\', \\'question\\': \\'Why is model conversion important?\\', \\'context\\': \\'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\\\\nmodel_name = deepset/tinyroberta-squad2\\\\nnlp = pipeline(\\'question-answering\\', model=model_name, tokenizer=model_name)\\\\nQA_input = {\\\\n \\'question\\': \\'Why is model conversion important?\\',\\\\n \\'context\\': \\'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.\\'\\\\n}\\\\nres = nlp(QA_input)\", \\'performance\\': {\\'dataset\\': \\'squad_v2\\', \\'accuracy\\': {\\'exact\\': 78.69114798281817, \\'f1\\': 81.9198998536977}}, \\'description\\': \\'This is the distilled version of the deepset/roberta-base-squad2 model. This model has a comparable prediction quality and runs at twice the speed of the base model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 391, "text": " We are building a memory game where a description is displayed for a few seconds and later on, questions on what is shown comes up for the user to remember.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Automatic Speech Recognition\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli\\', \\'api_call\\': \"Wav2Vec2ForCTC.from_pretrained(\\'jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'https://github.com/neonbjb/ocotillo\\', \\'performance\\': {\\'dataset\\': \\'librispeech validation set\\', \\'accuracy\\': \\'4.45%\\'}, \\'description\\': \\'This checkpoint is a wav2vec2-large model that is useful for generating transcriptions with punctuation. It is intended for use in building transcriptions for TTS models, where punctuation is very important for prosody. This model was created by fine-tuning the facebook/wav2vec2-large-robust-ft-libri-960h checkpoint on the libritts and voxpopuli datasets with a new vocabulary that includes punctuation.\\'}', metadata={})]", "category": "generic"}
{"question_id": 392, "text": " We are a medical research team working on a COVID-19 project. We need assistance in question answering related to the pandemic and related research papers.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 393, "text": " The customer is writing a book about the solar system and just needs the answer to a question: How long does it take for Mars to orbit the sun?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text2Text Generation\\', \\'api_name\\': \\'DialogLED-base-16384\\', \\'api_call\\': \"LEDForConditionalGeneration.from_pretrained(\\'MingZhong/DialogLED-base-16384\\')\", \\'api_arguments\\': \\'input_text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'arxiv\\', \\'accuracy\\': \\'2109.02492\\'}, \\'description\\': \\'DialogLED is a pre-trained model for long dialogue understanding and summarization. It builds on the Longformer-Encoder-Decoder (LED) architecture and uses window-based denoising as the pre-training task on a large amount of long dialogue data for further training. Here is a base version of DialogLED, the input length is limited to 16,384 in the pre-training phase.\\'}', metadata={})]", "category": "generic"}
{"question_id": 394, "text": " I am designing a quiz app that caters to blind users. The app should interpret the images and answer questions based on the image contents.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Visual Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ivelin/donut-refexp-combined-v1\\', \\'api_call\\': \"pipeline(\\'visual-question-answering\\', model=\\'ivelin/donut-refexp-combined-v1\\')\", \\'api_arguments\\': \\'image, question\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"vqa(image=\\'path/to/image.jpg\\', question=\\'What is the color of the object?\\')\", \\'performance\\': {\\'dataset\\': \\'ivelin/donut-refexp-combined-v1\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'A visual question answering model that takes an image and a question as input and provides an answer based on the visual content of the image and the context of the question.\\'}', metadata={})]", "category": "generic"}
{"question_id": 395, "text": " The company develops a digital assistant that can answer questions about software products. Implement a feature to provide answers to user questions.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Visual Question Answering\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Visual Question Answering\\', \\'api_name\\': \\'JosephusCheung/GuanacoVQAOnConsumerHardware\\', \\'api_call\\': \"pipeline(\\'visual-question-answering\\', model=\\'JosephusCheung/GuanacoVQAOnConsumerHardware\\')\", \\'api_arguments\\': {\\'model\\': \\'JosephusCheung/GuanacoVQAOnConsumerHardware\\', \\'tokenizer\\': \\'JosephusCheung/GuanacoVQAOnConsumerHardware\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'latest\\', \\'torch\\': \\'latest\\'}, \\'example_code\\': \\'vqa(image_path, question)\\', \\'performance\\': {\\'dataset\\': \\'JosephusCheung/GuanacoVQADataset\\', \\'accuracy\\': \\'unknown\\'}, \\'description\\': \\'A Visual Question Answering model trained on the GuanacoVQADataset, designed to work on consumer hardware like Colab Free T4 GPU. The model can be used to answer questions about images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 396, "text": " I'm talking to a new person online. Can this API help keep me safe by filtering out any inappropriate messages they send me?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentiment Analysis\\', \\'api_name\\': \\'michellejieli/NSFW_text_classifier\\', \\'api_call\\': \"pipeline(\\'sentiment-analysis\\', model=\\'michellejieli/NSFW_text_classification\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'classifier(I see you\u2019ve set aside this special time to humiliate yourself in public.)\\', \\'performance\\': {\\'dataset\\': \\'Reddit posts\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'DistilBERT is a transformer model that performs sentiment analysis. I fine-tuned the model on Reddit posts with the purpose of classifying not safe for work (NSFW) content, specifically text that is considered inappropriate and unprofessional. The model predicts 2 classes, which are NSFW or safe for work (SFW). The model is a fine-tuned version of DistilBERT. It was fine-tuned on 14317 Reddit posts pulled from the Reddit API.\\'}', metadata={})]", "category": "generic"}
{"question_id": 397, "text": " I want to build a personal assistant to check the logical relationship between two sentences, especially when I have a lot of texts in English.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 398, "text": " I am reading a book and, whenever I engage with an interesting topic, I write a short sentence summarizing that topic. I would like to have an assistant that, given a sentence, automatically classifies that topic among 'technology', 'literature', and 'science'.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 399, "text": " Determine if one Russian sentence logically contradicts the information provided by another Russian sentence.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'sberbank-ai/sbert_large_mt_nlu_ru\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'sberbank-ai/sbert_large_mt_nlu_ru\\')\", \\'api_arguments\\': [\\'sentences\\', \\'padding\\', \\'truncation\\', \\'max_length\\', \\'return_tensors\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': \"from transformers import AutoTokenizer, AutoModel\\\\nimport torch\\\\n\\\\n\\\\n# Mean Pooling - Take attention mask into account for correct averaging\\\\ndef mean_pooling(model_output, attention_mask):\\\\n    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\\\\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\\\\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\\\\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\\\\n    return sum_embeddings / sum_mask\\\\n\\\\n\\\\n# Sentences we want sentence embeddings for sentences = [\\'?\\']\\\\n\\\\n# Load AutoModel from huggingface model repository\\\\ntokenizer = AutoTokenizer.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\\\nmodel = AutoModel.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\\\n\\\\n# Tokenize sentences\\\\nencoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=24, return_tensors=\\'pt\\')\\\\n\\\\n# Compute token embeddings\\\\nwith torch.no_grad():\\\\n    model_output = model(**encoded_input)\\\\n\\\\n# Perform pooling. In this case, mean pooling\\\\nsentence_embeddings = mean_pooling(model_output, encoded_input[\\'attention_mask\\'])\", \\'performance\\': {\\'dataset\\': \\'Russian SuperGLUE\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'BERT large model multitask (cased) for Sentence Embeddings in Russian language.\\'}', metadata={})]", "category": "generic"}
{"question_id": 400, "text": " The assignment is to develop a software for translating sentences from Spanish to English at a near-human performance.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Audio-to-Audio\\', \\'framework\\': \\'Fairseq\\', \\'functionality\\': \\'speech-to-speech-translation\\', \\'api_name\\': \\'facebook/textless_sm_en_fr\\', \\'api_call\\': \"load_model_ensemble_and_task_from_hf_hub(\\'facebook/textless_sm_en_fr\\')\", \\'api_arguments\\': [\\'input_file\\'], \\'python_environment_requirements\\': [\\'huggingface_hub\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'This model is a speech-to-speech translation model trained by Facebook. It is designed for translating English speech to French speech.\\'}', metadata={})]", "category": "generic"}
{"question_id": 401, "text": " I am an author. I have a new book that needs a summary. I want the summary to resemble the style of SparkNotes.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 402, "text": " An editor wants to summarize his article in French.\\n###Input: \\\"Selon un rapport r\\u00e9cent, les constructeurs automobiles pr\\u00e9voient d'acc\\u00e9l\\u00e9rer la production de voitures \\u00e9lectriques et de r\\u00e9duire la production de voitures \\u00e0 moteur \\u00e0 combustion interne. Les constructeurs pr\\u00e9voient de consacrer davantage de ressources \\u00e0 la recherche et au d\\u00e9veloppement de technologies pour am\\u00e9liorer l'efficacit\\u00e9 des batteries, r\\u00e9duire les co\\u00fbts de production et \\u00e9tendre le r\\u00e9seau de recharge. Les gouvernements du monde entier continuent de soutenir l'adoption de la mobilit\\u00e9 \\u00e9lectrique \\u00e0 travers des incitations financi\\u00e8res et des objectifs ambitieux de r\\u00e9duction des \\u00e9missions de CO2.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Token Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Named Entity Recognition\\', \\'api_name\\': \\'Jean-Baptiste/camembert-ner\\', \\'api_call\\': \"AutoModelForTokenClassification.from_pretrained(\\'Jean-Baptiste/camembert-ner\\')\", \\'api_arguments\\': {\\'model\\': \\'model\\', \\'tokenizer\\': \\'tokenizer\\', \\'aggregation_strategy\\': \\'simple\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'AutoTokenizer, AutoModelForTokenClassification, pipeline\\'}, \\'example_code\\': \"from transformers import AutoTokenizer, AutoModelForTokenClassification\\\\ntokenizer = AutoTokenizer.from_pretrained(Jean-Baptiste/camembert-ner)\\\\nmodel = AutoModelForTokenClassification.from_pretrained(Jean-Baptiste/camembert-ner)\\\\n\\\\nfrom transformers import pipeline\\\\nnlp = pipeline(\\'ner\\', model=model, tokenizer=tokenizer, aggregation_strategy=simple)\\\\nnlp(Apple est cre le 1er avril 1976 dans le garage de la maison d\\'enfance de Steve Jobs  Los Altos en Californie par Steve Jobs, Steve Wozniak et Ronald Wayne14, puis constitue sous forme de socit le 3 janvier 1977  l\\'origine sous le nom d\\'Apple Computer, mais pour ses 30 ans et pour reflter la diversification de ses produits, le mot \u00ab computer  est retir le 9 janvier 2015.)\", \\'performance\\': {\\'dataset\\': \\'wikiner-fr\\', \\'accuracy\\': {\\'overall_f1\\': 0.8914, \\'PER_f1\\': 0.9483, \\'ORG_f1\\': 0.8181, \\'LOC_f1\\': 0.8955, \\'MISC_f1\\': 0.8146}}, \\'description\\': \\'camembert-ner is a Named Entity Recognition (NER) model fine-tuned from camemBERT on the wikiner-fr dataset. It can recognize entities such as persons, organizations, locations, and miscellaneous entities.\\'}', metadata={})]", "category": "generic"}
{"question_id": 403, "text": " Design a chatbot for a school website, which can answer queries related to admissions, classes, teachers, and extracurriculars.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'facebook/blenderbot-1B-distill\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'facebook/blenderbot-1B-distill\\')\", \\'api_arguments\\': {\\'input_message\\': \\'string\\'}, \\'python_environment_requirements\\': {\\'library\\': \\'transformers\\', \\'version\\': \\'latest\\'}, \\'example_code\\': \"inputs = tokenizer(\\'Hello, how are you?\\', return_tensors=\\'pt\\'); outputs = model.generate(inputs[\\'input_ids\\']); decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\", \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not mentioned\\'}, \\'description\\': \\'BlenderBot-1B is a large-scale open-domain chatbot model that can engage in conversations, ask and answer questions, and display knowledge, empathy, and personality. This distilled version is smaller and faster than the original 9.4B parameter model, making it more accessible for use.\\'}', metadata={})]", "category": "generic"}
{"question_id": 404, "text": " We have a long article, and we want to generate a summary to understand the main points of the article.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 405, "text": " We need to prepare some sample conversations featuring frequently asked questions for helping customers with our products.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 406, "text": " Can you give me advice on how to choose the best video game for me?\\n###Input: {\\\"instruction\\\": \\\"what is the best way to choose a video game?\\\", \\\"knowledge\\\": \\\"Some factors to consider when choosing a video game are personal preferences, genre, graphics, gameplay, storyline, platform, and reviews from other players or gaming websites.\\\", \\\"dialog\\\": [\\\"What type of video games do you prefer playing?\\\", \\\"I enjoy action-adventure games and a decent storyline.\\\"]}\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Video\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Video\\', \\'api_name\\': \\'camenduru/text2-video-zero\\', \\'api_call\\': \"pipeline(\\'text-to-video\\', model=\\'camenduru/text2-video-zero\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'This model is used for generating videos from text inputs. It is based on the Hugging Face framework and can be used with the transformers library. The model is trained on a variety of text and video datasets, and can be used for tasks such as video summarization, video generation from text prompts, and more.\\'}', metadata={})]", "category": "generic"}
{"question_id": 407, "text": " Write a script to translate the following French sentence into English: \\\"Je t\\u2019aime.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'PyTorch Transformers\\', \\'functionality\\': \\'text-generation\\', \\'api_name\\': \\'satvikag/chatbot\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'output-small\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"AutoTokenizer.from_pretrained(\\'microsoft/DialoGPT-small\\')\", \\'model\\': \"AutoModelWithLMHead.from_pretrained(\\'output-small\\')\"}, \\'python_environment_requirements\\': [\\'torch\\', \\'transformers\\'], \\'example_code\\': \"for step in range(100):\\\\n  new_user_input_ids = tokenizer.encode(input(\\'&gt;&gt; User:\\') + tokenizer.eos_token, return_tensors=\\'pt\\')\\\\n  bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step &gt; 0 else new_user_input_ids\\\\n  chat_history_ids = model.generate(bot_input_ids, max_length=500, pad_token_id=tokenizer.eos_token_id, no_repeat_ngram_size=3, do_sample=True, top_k=100, top_p=0.7, temperature = 0.8)\\\\n  print(\\'AI: {}\\'.format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\", \\'performance\\': {\\'dataset\\': \\'Kaggle game script dataset\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'DialoGPT Trained on the Speech of a Game Character, Joshua from The World Ends With You.\\'}', metadata={})]", "category": "generic"}
{"question_id": 408, "text": " Your company is developing a chatbot and requires accurate summaries of lengthy dialogues without losing context.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'sshleifer/distilbart-cnn-12-6\\', \\'api_call\\': \"BartForConditionalGeneration.from_pretrained(\\'sshleifer/distilbart-cnn-12-6\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'huggingface/transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'Rouge 2\\': \\'22.12\\', \\'Rouge-L\\': \\'36.99\\'}}]}, \\'description\\': \"DistilBART is a distilled version of BART, a model for text summarization. This specific checkpoint, \\'sshleifer/distilbart-cnn-12-6\\', is trained on the cnn_dailymail dataset and provides a fast and effective way to generate summaries of text. The model can be loaded using the Hugging Face Transformers library.\"}', metadata={})]", "category": "generic"}
{"question_id": 409, "text": " I want a system that can translate an article from English to French automatically.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 410, "text": " We want our System to generate possible user queries for a document provided as a text input.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 411, "text": " Our client wants to create marketing slogans. Help them by completing this slogan \\\"Customer satisfaction is our top <mask>.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 412, "text": " I need a French language model to help me fill in the blanks in given sentences.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 413, "text": " We would like to build a semantic text search system that can find similar documents in a repository based on a given description.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 415, "text": " I want to create a playlist of similar songs based on lyrics. How can I compare the sentences and find similar ones?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'distilbart-cnn-12-6-samsum\\', \\'api_call\\': \"pipeline(\\'summarization\\', model=\\'philschmid/distilbart-cnn-12-6-samsum\\')\", \\'api_arguments\\': {\\'model\\': \\'philschmid/distilbart-cnn-12-6-samsum\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 41.09, \\'ROUGE-2\\': 20.746, \\'ROUGE-L\\': 31.595, \\'ROUGE-LSUM\\': 38.339}}, \\'description\\': \\'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 416, "text": " I\\u2019m putting together a dating site where users can submit questions they'd like the matching algorithm to ask. I want to suggest questions like the ones they have already submitted. Can you provide me with a model to do that?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'distilbart-cnn-12-6-samsum\\', \\'api_call\\': \"pipeline(\\'summarization\\', model=\\'philschmid/distilbart-cnn-12-6-samsum\\')\", \\'api_arguments\\': {\\'model\\': \\'philschmid/distilbart-cnn-12-6-samsum\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 41.09, \\'ROUGE-2\\': 20.746, \\'ROUGE-L\\': 31.595, \\'ROUGE-LSUM\\': 38.339}}, \\'description\\': \\'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 417, "text": " I'm working on a phonebot, and I need the bot to be able to read a sensitive warning message to the users.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Fill-Mask\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Masked Language Modeling\\', \\'api_name\\': \\'bert-base-multilingual-cased\\', \\'api_call\\': \"pipeline(\\'fill-mask\\', model=\\'bert-base-multilingual-cased\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nunmasker = pipeline(\\'fill-mask\\', model=\\'bert-base-multilingual-cased\\')\\\\nunmasker(Hello I\\'m a [MASK] model.)\", \\'performance\\': {\\'dataset\\': \\'wikipedia\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'BERT multilingual base model (cased) is pretrained on the top 104 languages with the largest Wikipedia using a masked language modeling (MLM) objective. The model is case sensitive and can be used for masked language modeling or next sentence prediction. It is intended to be fine-tuned on a downstream task, such as sequence classification, token classification, or question answering.\\'}', metadata={})]", "category": "generic"}
{"question_id": 418, "text": " Our customer wants to have this Chinese text \\\"<Chinese_text>\\\" read aloud as an audio file in female voice.\\n###Input: \\\"<Chinese_text>\\\": \\\"\\u4f60\\u597d\\uff0c\\u6b22\\u8fce\\u6765\\u5230\\u6570\\u5b57\\u4e16\\u754c\\u3002\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Automatic Speech Recognition\\', \\'framework\\': \\'pyannote.audio\\', \\'functionality\\': \\'overlapped-speech-detection\\', \\'api_name\\': \\'pyannote/overlapped-speech-detection\\', \\'api_call\\': \"pipeline.from_pretrained(\\'pyannote/overlapped-speech-detection\\', use_auth_token=\\'ACCESS_TOKEN_GOES_HERE\\')\", \\'api_arguments\\': [\\'audio.wav\\'], \\'python_environment_requirements\\': [\\'pyannote.audio 2.1\\'], \\'example_code\\': \\'from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained(pyannote/overlapped-speech-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\\\noutput = pipeline(audio.wav)\\\\nfor speech in output.get_timeline().support():\\\\n  # two or more speakers are active between speech.start and speech.end\\\\n  ...\\', \\'performance\\': {\\'dataset\\': \\'ami\\', \\'accuracy\\': None}, \\'description\\': \\'Automatic overlapped speech detection using pyannote.audio framework. The model detects when two or more speakers are active in an audio file.\\'}', metadata={})]", "category": "generic"}
{"question_id": 419, "text": " Develop a text-to-speech model for our mobile app to read news articles for our users.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 420, "text": " You are an Indian in Telugu pandit teaching kids how to pronounce conventional holy prayers.. Teach me mantras in Telugu synthesized by human like voice pronunciation.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'ESPnet\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'SYSPIN/Telugu_Male_TTS\\', \\'api_call\\': \"pipeline(\\'text-to-speech\\', model=\\'SYSPIN/Telugu_Male_TTS\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A Telugu Male Text-to-Speech model using the ESPnet framework, provided by Hugging Face.\\'}', metadata={})]", "category": "generic"}
{"question_id": 421, "text": " We are a French company creating an interactive voice response system for our call centers, and we need to convert text to speech for our telephony prompts.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 422, "text": " Create Japanese audio from the following text: \\\"\\u3053\\u3093\\u306b\\u3061\\u306f\\u3001\\u79c1\\u305f\\u3061\\u306f\\u3042\\u306a\\u305f\\u306e\\u52a9\\u3051\\u304c\\u5fc5\\u8981\\u3067\\u3059\\u3002\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Feature Extraction\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'facebook/dragon-plus-context-encoder\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'facebook/dragon-plus-context-encoder\\')\", \\'api_arguments\\': [\\'pretrained\\'], \\'python_environment_requirements\\': [\\'torch\\', \\'transformers\\'], \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModel\\\\ntokenizer = AutoTokenizer.from_pretrained(\\'facebook/dragon-plus-query-encoder\\')\\\\nquery_encoder = AutoModel.from_pretrained(\\'facebook/dragon-plus-query-encoder\\')\\\\ncontext_encoder = AutoModel.from_pretrained(\\'facebook/dragon-plus-context-encoder\\')\\\\nquery = \\'Where was Marie Curie born?\\'\\\\ncontexts = [\\\\n  \\'Maria Sklodowska, later known as Marie Curie, was born on November 7, 1867.\\',\\\\n  \\'Born in Paris on 15 May 1859, Pierre Curie was the son of Eug\u00e8ne Curie, a doctor of French Catholic origin from Alsace.\\'\\\\n]\\\\nquery_input = tokenizer(query, return_tensors=\\'pt\\')\\\\nctx_input = tokenizer(contexts, padding=True, truncation=True, return_tensors=\\'pt\\')\\\\nquery_emb = query_encoder(query_input).last_hidden_state[:, 0, :]\\\\nctx_emb = context_encoder(ctx_input).last_hidden_state[:, 0, :]\\\\nscore1 = query_emb @ ctx_emb[0]\\\\nscore2 = query_emb @ ctx_emb[1]\", \\'performance\\': {\\'dataset\\': \\'MS MARCO\\', \\'accuracy\\': 39.0}, \\'description\\': \\'DRAGON+ is a BERT-base sized dense retriever initialized from RetroMAE and further trained on the data augmented from MS MARCO corpus, following the approach described in How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval. The associated GitHub repository is available here https://github.com/facebookresearch/dpr-scale/tree/main/dragon. We use asymmetric dual encoder, with two distinctly parameterized encoders.\\'}', metadata={})]", "category": "generic"}
{"question_id": 423, "text": " We need to create automated transcripts from recorded podcasts that include punctuation for better readability.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'michellejieli/emotion_text_classifier\\', \\'api_call\\': \"pipeline(\\'sentiment-analysis\\', model=\\'michellejieli/emotion_text_classifier\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'classifier(I love this!)\\', \\'performance\\': {\\'dataset\\': [\\'Crowdflower (2016)\\', \\'Emotion Dataset, Elvis et al. (2018)\\', \\'GoEmotions, Demszky et al. (2020)\\', \\'ISEAR, Vikash (2018)\\', \\'MELD, Poria et al. (2019)\\', \\'SemEval-2018, EI-reg, Mohammad et al. (2018)\\', \\'Emotion Lines (Friends)\\'], \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'DistilRoBERTa-base is a transformer model that performs sentiment analysis. I fine-tuned the model on transcripts from the Friends show with the goal of classifying emotions from text data, specifically dialogue from Netflix shows or movies. The model predicts 6 Ekman emotions and a neutral class. These emotions include anger, disgust, fear, joy, neutrality, sadness, and surprise.\\'}', metadata={})]", "category": "generic"}
{"question_id": 424, "text": " Convert the audio file of a phone interview to text for further analysis.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'sentiment_analysis_generic_dataset\\', \\'api_call\\': \"pipeline(\\'text-classification\\', model=\\'Seethal/sentiment_analysis_generic_dataset\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"sentiment_analysis(\\'I love this product!\\')\", \\'performance\\': {\\'dataset\\': \\'generic_dataset\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'This is a fine-tuned downstream version of the bert-base-uncased model for sentiment analysis, this model is not intended for further downstream fine-tuning for any other tasks. This model is trained on a classified dataset for text classification.\\'}', metadata={})]", "category": "generic"}
{"question_id": 426, "text": " I want to use speech enhancement for an audio call of a coworker.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Automatic Speech Recognition\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'microsoft/wavlm-large\\', \\'api_call\\': \"Wav2Vec2Model.from_pretrained(\\'microsoft/wavlm-large\\')\", \\'api_arguments\\': \\'speech input\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'To fine-tune the model for speech recognition, see the official speech recognition example. To fine-tune the model for speech classification, see the official audio classification example.\\', \\'performance\\': {\\'dataset\\': \\'SUPERB benchmark\\', \\'accuracy\\': \\'state-of-the-art performance\\'}, \\'description\\': \\'WavLM-Large is a large model pretrained on 16kHz sampled speech audio. It is built based on the HuBERT framework, with an emphasis on both spoken content modeling and speaker identity preservation. WavLM is pretrained on 60,000 hours of Libri-Light, 10,000 hours of GigaSpeech, and 24,000 hours of VoxPopuli. It achieves state-of-the-art performance on the SUPERB benchmark and brings significant improvements for various speech processing tasks on their representative benchmarks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 427, "text": " How do I separate the speakers from an audio file using the pre-trained ConvTasNet_Libri2Mix_sepclean_8k model from Hugging Face?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Automatic Speech Recognition\\', \\'framework\\': \\'pyannote.audio\\', \\'functionality\\': \\'overlapped-speech-detection\\', \\'api_name\\': \\'pyannote/overlapped-speech-detection\\', \\'api_call\\': \"pipeline.from_pretrained(\\'pyannote/overlapped-speech-detection\\', use_auth_token=\\'ACCESS_TOKEN_GOES_HERE\\')\", \\'api_arguments\\': [\\'audio.wav\\'], \\'python_environment_requirements\\': [\\'pyannote.audio 2.1\\'], \\'example_code\\': \\'from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained(pyannote/overlapped-speech-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\\\noutput = pipeline(audio.wav)\\\\nfor speech in output.get_timeline().support():\\\\n  # two or more speakers are active between speech.start and speech.end\\\\n  ...\\', \\'performance\\': {\\'dataset\\': \\'ami\\', \\'accuracy\\': None}, \\'description\\': \\'Automatic overlapped speech detection using pyannote.audio framework. The model detects when two or more speakers are active in an audio file.\\'}', metadata={})]", "category": "generic"}
{"question_id": 428, "text": " A travel app we work on can translate language of a guide in real-time. We are targeting Spanish-speaking tourists.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 429, "text": " Design an audio description system for a user who needs help translating spoken English audio to spoken Hokkien audio.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Automatic Speech Recognition\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'microsoft/wavlm-large\\', \\'api_call\\': \"Wav2Vec2Model.from_pretrained(\\'microsoft/wavlm-large\\')\", \\'api_arguments\\': \\'speech input\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'To fine-tune the model for speech recognition, see the official speech recognition example. To fine-tune the model for speech classification, see the official audio classification example.\\', \\'performance\\': {\\'dataset\\': \\'SUPERB benchmark\\', \\'accuracy\\': \\'state-of-the-art performance\\'}, \\'description\\': \\'WavLM-Large is a large model pretrained on 16kHz sampled speech audio. It is built based on the HuBERT framework, with an emphasis on both spoken content modeling and speaker identity preservation. WavLM is pretrained on 60,000 hours of Libri-Light, 10,000 hours of GigaSpeech, and 24,000 hours of VoxPopuli. It achieves state-of-the-art performance on the SUPERB benchmark and brings significant improvements for various speech processing tasks on their representative benchmarks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 430, "text": " Implement a method to perform speech-to-speech translation between Hokkien and English using the xm_transformer_s2ut_hk-en model.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Audio-to-Audio\\', \\'framework\\': \\'Fairseq\\', \\'functionality\\': \\'audio\\', \\'api_name\\': \\'textless_sm_cs_en\\', \\'api_call\\': \"Wav2Vec2Model.from_pretrained(cached_download(\\'https://huggingface.co/facebook/textless_sm_cs_en/resolve/main/model.pt\\'))\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'fairseq\\', \\'huggingface_hub\\'], \\'example_code\\': \"from fairseq.models.wav2vec.wav2vec2_asr import Wav2Vec2Model\\\\nfrom huggingface_hub import cached_download\\\\n\\\\nmodel = Wav2Vec2Model.from_pretrained(cached_download(\\'https://huggingface.co/facebook/textless_sm_cs_en/resolve/main/model.pt\\'))\", \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A speech-to-speech translation model for converting between languages without using text as an intermediate representation. This model is designed for the task of audio-to-audio translation.\\'}', metadata={})]", "category": "generic"}
{"question_id": 431, "text": " A company wants to analyze the sentiment of the customer feedback in their Spanish-speaking call center. Generate a script for this task.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'financial-sentiment-analysis\\', \\'api_name\\': \\'ProsusAI/finbert\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'ProsusAI/finbert\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline; classifier = pipeline(\\'sentiment-analysis\\', model=\\'ProsusAI/finbert\\'); classifier(\\'your_text_here\\')\", \\'performance\\': {\\'dataset\\': \\'Financial PhraseBank\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'FinBERT is a pre-trained NLP model to analyze sentiment of financial text. It is built by further training the BERT language model in the finance domain, using a large financial corpus and thereby fine-tuning it for financial sentiment classification. Financial PhraseBank by Malo et al. (2014) is used for fine-tuning.\\'}', metadata={})]", "category": "generic"}
{"question_id": 432, "text": " Our company is working on an AI-powered language learning app for German. We need to build a feature that classifies emotions in German speech.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 433, "text": " We are a voice assistant service, and we need to verify the speaker identity for enhanced security when users access the system.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 434, "text": " I run a call center and I need a system that can identify the person on the other end of the line by analyzing their voice.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 435, "text": " A transcription service wants to identify which speaker said which words in an audio file. Help them with this task.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Automatic Speech Recognition\\', \\'framework\\': \\'pyannote.audio\\', \\'functionality\\': \\'Speaker Diarization\\', \\'api_name\\': \\'pyannote/speaker-diarization\\', \\'api_call\\': \"Pipeline.from_pretrained(\\'pyannote/speaker-diarization@2.1\\', use_auth_token=\\'ACCESS_TOKEN_GOES_HERE\\')\", \\'api_arguments\\': {\\'num_speakers\\': \\'int (optional)\\', \\'min_speakers\\': \\'int (optional)\\', \\'max_speakers\\': \\'int (optional)\\'}, \\'python_environment_requirements\\': \\'pyannote.audio 2.1.1\\', \\'example_code\\': [\\'from pyannote.audio import Pipeline\\', \\'pipeline = Pipeline.from_pretrained(pyannote/speaker-diarization@2.1, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\', \\'diarization = pipeline(audio.wav)\\', \\'with open(audio.rttm, w) as rttm:\\', \\'  diarization.write_rttm(rttm)\\'], \\'performance\\': {\\'dataset\\': \\'ami\\', \\'accuracy\\': {\\'DER%\\': \\'18.91\\', \\'FA%\\': \\'4.48\\', \\'Miss%\\': \\'9.51\\', \\'Conf%\\': \\'4.91\\'}}, \\'description\\': \\'This API provides an automatic speaker diarization pipeline using the pyannote.audio framework. It can process audio files and output speaker diarization results in RTTM format. The pipeline can also handle cases where the number of speakers is known in advance or when providing lower and/or upper bounds on the number of speakers.\\'}', metadata={})]", "category": "generic"}
{"question_id": 436, "text": " I am running a chain of wine stores and I want to categorize wines and recommend them based on their quality.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 437, "text": " I own an e-commerce platform and I need to predict which customers will make a purchase based on their browsing behavior.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Text-to-Text Generation\\', \\'api_name\\': \\'optimum/t5-small\\', \\'api_call\\': \"ORTModelForSeq2SeqLM.from_pretrained(\\'optimum/t5-small\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'optimum.onnxruntime\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, pipeline\\\\nfrom optimum.onnxruntime import ORTModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained(optimum/t5-small)\\\\nmodel = ORTModelForSeq2SeqLM.from_pretrained(optimum/t5-small)\\\\ntranslator = pipeline(translation_en_to_fr, model=model, tokenizer=tokenizer)\\\\nresults = translator(My name is Eustache and I have a pet raccoon)\\\\nprint(results)\\', \\'performance\\': {\\'dataset\\': \\'c4\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'T5 is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks and for which each task is converted into a text-to-text format. It can be used for translation, text-to-text generation, and summarization.\\'}', metadata={})]", "category": "generic"}
{"question_id": 438, "text": " Predict the income category of a person, based on their demographic information. The model should use Tensorflow decision trees to make binary classifications.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Classification\\', \\'framework\\': \\'Keras\\', \\'functionality\\': \\'Binary Classification\\', \\'api_name\\': \\'TF_Decision_Trees\\', \\'api_call\\': \\'TF_Decision_Trees(input_features, target)\\', \\'api_arguments\\': [\\'input_features\\', \\'target\\'], \\'python_environment_requirements\\': [\\'tensorflow >= 7.0\\'], \\'example_code\\': \\'https://github.com/tdubon/TF-GB-Forest/blob/c0cf4c7e3e29d819b996cfe4eecc1f2728115e52/TFDecisionTrees_Final.ipynb\\', \\'performance\\': {\\'dataset\\': \\'Census-Income Data Set\\', \\'accuracy\\': 96.57}, \\'description\\': \"Use TensorFlow\\'s Gradient Boosted Trees model in binary classification of structured data. Build a decision forests model by specifying the input feature usage. Implement a custom Binary Target encoder as a Keras Preprocessing layer to encode the categorical features with respect to their target value co-occurrences, and then use the encoded features to build a decision forests model. The model is trained on the US Census Income Dataset containing approximately 300k instances with 41 numerical and categorical variables. The task is to determine whether a person makes over 50k a year.\"}', metadata={})]", "category": "generic"}
{"question_id": 439, "text": " A travel agency needs a system to predict whether a client's vacation will be successful based on their chosen destination, accommodation, and travel style. We want to provide suggestions for clients who may need additional support.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 440, "text": " A new city planning company wants to estimate carbon emissions for different types of buildings to provide sustainable living solutions. We need a model to classify the carbon emissions.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Regression\\', \\'framework\\': \\'Joblib\\', \\'functionality\\': \\'Carbon Emissions\\', \\'api_name\\': \\'pcoloc/autotrain-600-dragino-1839063122\\', \\'api_call\\': \"joblib.load(\\'model.joblib\\')\", \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'pcoloc/autotrain-data-600-dragino\\', \\'accuracy\\': {\\'Loss\\': 93.595, \\'R2\\': 0.502, \\'MSE\\': 8760.052, \\'MAE\\': 77.527, \\'RMSLE\\': 0.445}}, \\'description\\': \\'This model is trained to perform single column regression on carbon emissions data using the AutoTrain framework. It predicts CO2 emissions in grams given the input data.\\'}', metadata={})]", "category": "generic"}
{"question_id": 441, "text": " We want to evaluate the carbon footprint of a construction project based on the tabular data of material consumption.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 442, "text": " Our company is an environmental consultancy firm. Determine the carbon emissions of different facilities based on the provided data.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Regression\\', \\'framework\\': \\'Joblib\\', \\'functionality\\': \\'Carbon Emissions\\', \\'api_name\\': \\'pcoloc/autotrain-600-dragino-1839063122\\', \\'api_call\\': \"joblib.load(\\'model.joblib\\')\", \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'pcoloc/autotrain-data-600-dragino\\', \\'accuracy\\': {\\'Loss\\': 93.595, \\'R2\\': 0.502, \\'MSE\\': 8760.052, \\'MAE\\': 77.527, \\'RMSLE\\': 0.445}}, \\'description\\': \\'This model is trained to perform single column regression on carbon emissions data using the AutoTrain framework. It predicts CO2 emissions in grams given the input data.\\'}', metadata={})]", "category": "generic"}
{"question_id": 443, "text": " I need assistance in predicting carbon emissions of a city based on historical data. Use the dataset provided to predict future carbon emissions.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Classification\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Carbon Emissions\\', \\'api_name\\': \\'Xinhhd/autotrain-zhongxin-contest-49402119333\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'Xinhhd/autotrain-zhongxin-contest-49402119333\\')\", \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'Xinhhd/autotrain-data-zhongxin-contest\\', \\'accuracy\\': 0.889}, \\'description\\': \\'A multi-class classification model trained with AutoTrain to predict carbon emissions based on input features.\\'}', metadata={})]", "category": "generic"}
{"question_id": 444, "text": " We have some clients asking for estimating CO2 emissions based on their historic data, which is in a CSV file.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 445, "text": " Determine the electricity consumption of a residential area based on historical data.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Information Retrieval\\', \\'api_name\\': \\'cross-encoder/ms-marco-TinyBERT-L-2-v2\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'cross-encoder/ms-marco-TinyBERT-L-2-v2\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\"}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': {\\'import\\': \\'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\', \\'model\\': \"model = AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\", \\'scores\\': \\'with torch.no_grad():\\\\n    scores = model(**features).logits\\\\n    print(scores)\\'}, \\'performance\\': {\\'dataset\\': \\'TREC Deep Learning 2019\\', \\'accuracy\\': \\'69.84 (NDCG@10)\\'}, \\'description\\': \\'This model was trained on the MS Marco Passage Ranking task. It can be used for Information Retrieval: Given a query, encode the query with all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. The training code is available here: SBERT.net Training MS Marco.\\'}', metadata={})]", "category": "generic"}
{"question_id": 446, "text": " A soccer simulation company wants to use a reinforcement learning agent that can play SoccerTwos effectively.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Reinforcement Learning\\', \\'framework\\': \\'ML-Agents\\', \\'functionality\\': \\'SoccerTwos\\', \\'api_name\\': \\'Raiden-1001/poca-Soccerv7\\', \\'api_call\\': \"mlagents-load-from-hf --repo-id=\\'Raiden-1001/poca-Soccerv7.1\\' --local-dir=\\'./downloads\\'\", \\'api_arguments\\': [\\'your_configuration_file_path.yaml\\', \\'run_id\\'], \\'python_environment_requirements\\': [\\'unity-ml-agents\\', \\'deep-reinforcement-learning\\', \\'ML-Agents-SoccerTwos\\'], \\'example_code\\': \\'Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\\\\nStep 2: Select your .nn /.onnx file\\\\nClick on Watch the agent play \ud83d\udc40\\', \\'performance\\': {\\'dataset\\': \\'SoccerTwos\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library.\\'}', metadata={})]", "category": "generic"}
{"question_id": 447, "text": " Our company is developing a game, and they want to include an AI agent to play the game. We need to evaluate its performance on the CartPole-v1 environment for consistency.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Reinforcement Learning\\', \\'framework\\': \\'Stable-Baselines3\\', \\'functionality\\': \\'CartPole-v1\\', \\'api_name\\': \\'dqn-CartPole-v1\\', \\'api_call\\': \"load_from_hub(repo_id=\\'sb3/dqn-CartPole-v1\\',filename=\\'{MODEL FILENAME}.zip\\',)\", \\'api_arguments\\': [\\'algo\\', \\'env\\', \\'logs\\'], \\'python_environment_requirements\\': [\\'rl_zoo3\\', \\'stable-baselines3\\', \\'stable-baselines3-contrib\\'], \\'example_code\\': \\'python train.py --algo dqn --env CartPole-v1 -f logs/\\', \\'performance\\': {\\'dataset\\': \\'CartPole-v1\\', \\'accuracy\\': \\'500.00 +/- 0.00\\'}, \\'description\\': \\'This is a trained model of a DQN agent playing CartPole-v1 using the stable-baselines3 library and the RL Zoo. The RL Zoo is a training framework for Stable Baselines3 reinforcement learning agents, with hyperparameter optimization and pre-trained agents included.\\'}', metadata={})]", "category": "generic"}
{"question_id": 448, "text": " We need help from a AI to continuously develop a virtual environment for our production robots.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 449, "text": " We are a news agency that wants to extract useful features from Korean news articles for a content recommendation service.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Feature Extraction\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'DeepPavlov/rubert-base-cased\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'DeepPavlov/rubert-base-cased\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'Russian part of Wikipedia and news data\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'RuBERT (Russian, cased, 12\u2011layer, 768\u2011hidden, 12\u2011heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT\u2011base as an initialization for RuBERT[1].\\'}', metadata={})]", "category": "generic"}
{"question_id": 450, "text": " We want to automatically generate hashtags for the provided image URL to improve social media post performance.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'csebuetnlp/mT5_multilingual_XLSum\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'csebuetnlp/mT5_multilingual_XLSum\\')\", \\'api_arguments\\': [\\'model_name\\'], \\'python_environment_requirements\\': [\\'transformers==4.11.0.dev0\\'], \\'example_code\\': \"import re\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nWHITESPACE_HANDLER = lambda k: re.sub(\\'\\\\\\\\s+\\', \\' \\', re.sub(\\'\\\\\\\\n+\\', \\' \\', k.strip()))\\\\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people\\'s scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We\\'re expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\\\\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\\\ninput_ids = tokenizer(\\\\n [WHITESPACE_HANDLER(article_text)],\\\\n return_tensors=pt,\\\\n padding=max_length,\\\\n truncation=True,\\\\n max_length=512\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n max_length=84,\\\\n no_repeat_ngram_size=2,\\\\n num_beams=4\\\\n)[0]\\\\nsummary = tokenizer.decode(\\\\n output_ids,\\\\n skip_special_tokens=True,\\\\n clean_up_tokenization_spaces=False\\\\n)\\\\nprint(summary)\", \\'performance\\': {\\'dataset\\': \\'xsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.5, \\'ROUGE-2\\': 13.934, \\'ROUGE-L\\': 28.988, \\'ROUGE-LSUM\\': 28.996, \\'loss\\': 2.067, \\'gen_len\\': 26.973}}, \\'description\\': \\'This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 451, "text": " I am building a recommendation engine for TV shows. Can you evaluate the TV shows using a BERT-based model trained on sentence embedding to find the most similar TV shows based on description?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'distilbart-cnn-12-6-samsum\\', \\'api_call\\': \"pipeline(\\'summarization\\', model=\\'philschmid/distilbart-cnn-12-6-samsum\\')\", \\'api_arguments\\': {\\'model\\': \\'philschmid/distilbart-cnn-12-6-samsum\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 41.09, \\'ROUGE-2\\': 20.746, \\'ROUGE-L\\': 31.595, \\'ROUGE-LSUM\\': 38.339}}, \\'description\\': \\'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 452, "text": " I want to create a solution that can answer questions related to an image of my pet dogs.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Visual Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ivelin/donut-refexp-combined-v1\\', \\'api_call\\': \"pipeline(\\'visual-question-answering\\', model=\\'ivelin/donut-refexp-combined-v1\\')\", \\'api_arguments\\': \\'image, question\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"vqa(image=\\'path/to/image.jpg\\', question=\\'What is the color of the object?\\')\", \\'performance\\': {\\'dataset\\': \\'ivelin/donut-refexp-combined-v1\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'A visual question answering model that takes an image and a question as input and provides an answer based on the visual content of the image and the context of the question.\\'}', metadata={})]", "category": "generic"}
{"question_id": 453, "text": " Design an explanation on how to use Pix2Struct to analyze and generate text based on visuals such as graphs and charts.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image Generation\\', \\'api_name\\': \\'stabilityai/stable-diffusion-2\\', \\'api_call\\': \"StableDiffusionPipeline.from_pretrained(\\'stabilityai/stable-diffusion-2\\', scheduler=EulerDiscreteScheduler.from_pretrained(\\'stabilityai/stable-diffusion-2\\', subfolder=scheduler), torch_dtype=torch.float16)\", \\'api_arguments\\': {\\'prompt\\': \\'a photo of an astronaut riding a horse on mars\\'}, \\'python_environment_requirements\\': [\\'diffusers\\', \\'transformers\\', \\'accelerate\\', \\'scipy\\', \\'safetensors\\'], \\'example_code\\': \\'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\\\nmodel_id = stabilityai/stable-diffusion-2\\\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = a photo of an astronaut riding a horse on mars\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(astronaut_rides_horse.png)\\', \\'performance\\': {\\'dataset\\': \\'COCO2017 validation set\\', \\'accuracy\\': \\'Not optimized for FID scores\\'}, \\'description\\': \\'Stable Diffusion v2 is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is primarily intended for research purposes, such as safe deployment of models with potential to generate harmful content, understanding limitations and biases of generative models, and generation of artworks for design and artistic processes.\\'}', metadata={})]", "category": "generic"}
{"question_id": 454, "text": " Design a product that allows users to read street signs in a foreign language.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Zero-Shot Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Zero-Shot Image Classification\\', \\'api_name\\': \\'patrickjohncyh/fashion-clip\\', \\'api_call\\': \"CLIPModel.from_pretrained(\\'patrickjohncyh/fashion-clip\\')\", \\'api_arguments\\': {\\'image\\': \\'File\\', \\'class_names\\': \\'String (comma-separated)\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import CLIPProcessor, CLIPModel; model = CLIPModel.from_pretrained(\\'patrickjohncyh/fashion-clip\\'); processor = CLIPProcessor.from_pretrained(\\'patrickjohncyh/fashion-clip\\'); inputs = processor(text=\\'blue shoes\\', images=image, return_tensors=\\'pt\\', padding=True); logits_per_image = model(**inputs).logits_per_image; probs = logits_per_image.softmax(dim=-1).tolist()[0]\", \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'FMNIST\\', \\'accuracy\\': 0.83}, {\\'name\\': \\'KAGL\\', \\'accuracy\\': 0.73}, {\\'name\\': \\'DEEP\\', \\'accuracy\\': 0.62}]}, \\'description\\': \\'FashionCLIP is a CLIP-based model developed to produce general product representations for fashion concepts. Leveraging the pre-trained checkpoint (ViT-B/32) released by OpenAI, it is trained on a large, high-quality novel fashion dataset to study whether domain specific fine-tuning of CLIP-like models is sufficient to produce product representations that are zero-shot transferable to entirely new datasets and tasks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 455, "text": " I have a text question about an image, and I would like to receive an appropriate answer.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Text-to-Text Generation\\', \\'api_name\\': \\'optimum/t5-small\\', \\'api_call\\': \"ORTModelForSeq2SeqLM.from_pretrained(\\'optimum/t5-small\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'optimum.onnxruntime\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, pipeline\\\\nfrom optimum.onnxruntime import ORTModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained(optimum/t5-small)\\\\nmodel = ORTModelForSeq2SeqLM.from_pretrained(optimum/t5-small)\\\\ntranslator = pipeline(translation_en_to_fr, model=model, tokenizer=tokenizer)\\\\nresults = translator(My name is Eustache and I have a pet raccoon)\\\\nprint(results)\\', \\'performance\\': {\\'dataset\\': \\'c4\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'T5 is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks and for which each task is converted into a text-to-text format. It can be used for translation, text-to-text generation, and summarization.\\'}', metadata={})]", "category": "generic"}
{"question_id": 456, "text": " Create software that extracts answers from input documents when given a set of questions.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'promptcap-coco-vqa\\', \\'api_call\\': \"PromptCap(\\'vqascore/promptcap-coco-vqa\\')\", \\'api_arguments\\': {\\'prompt\\': \\'string\\', \\'image\\': \\'string\\'}, \\'python_environment_requirements\\': \\'pip install promptcap\\', \\'example_code\\': [\\'import torch\\', \\'from promptcap import PromptCap\\', \\'model = PromptCap(vqascore/promptcap-coco-vqa)\\', \\'if torch.cuda.is_available():\\', \\'  model.cuda()\\', \\'prompt = please describe this image according to the given question: what piece of clothing is this boy putting on?\\', \\'image = glove_boy.jpeg\\', \\'print(model.caption(prompt, image))\\'], \\'performance\\': {\\'dataset\\': {\\'coco\\': {\\'accuracy\\': \\'150 CIDEr\\'}, \\'OK-VQA\\': {\\'accuracy\\': \\'60.4%\\'}, \\'A-OKVQA\\': {\\'accuracy\\': \\'59.6%\\'}}}, \\'description\\': \\'PromptCap is a captioning model that can be controlled by natural language instruction. The instruction may contain a question that the user is interested in. It achieves SOTA performance on COCO captioning (150 CIDEr) and knowledge-based VQA tasks when paired with GPT-3 (60.4% on OK-VQA and 59.6% on A-OKVQA).\\'}', metadata={})]", "category": "generic"}
{"question_id": 457, "text": " We received a scanned document with a lot of information. We need to go through it and find answers to specific questions. Create a program to extract information from the document.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 458, "text": " In a pharmaceutical company, we are working on developing a new drug, and I want you to help classify the molecular structures of my dataset.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'distilbart-cnn-12-6-samsum\\', \\'api_call\\': \"pipeline(\\'summarization\\', model=\\'philschmid/distilbart-cnn-12-6-samsum\\')\", \\'api_arguments\\': {\\'model\\': \\'philschmid/distilbart-cnn-12-6-samsum\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 41.09, \\'ROUGE-2\\': 20.746, \\'ROUGE-L\\': 31.595, \\'ROUGE-LSUM\\': 38.339}}, \\'description\\': \\'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 459, "text": " Calculate the distance between the objects in an image for an autonomous vehicle driving in a parking lot.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Information Retrieval\\', \\'api_name\\': \\'cross-encoder/ms-marco-TinyBERT-L-2-v2\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'cross-encoder/ms-marco-TinyBERT-L-2-v2\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\"}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': {\\'import\\': \\'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\', \\'model\\': \"model = AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\", \\'scores\\': \\'with torch.no_grad():\\\\n    scores = model(**features).logits\\\\n    print(scores)\\'}, \\'performance\\': {\\'dataset\\': \\'TREC Deep Learning 2019\\', \\'accuracy\\': \\'69.84 (NDCG@10)\\'}, \\'description\\': \\'This model was trained on the MS Marco Passage Ranking task. It can be used for Information Retrieval: Given a query, encode the query with all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. The training code is available here: SBERT.net Training MS Marco.\\'}', metadata={})]", "category": "generic"}
{"question_id": 460, "text": " We want a technology to be developed in the field of autonomous vehicles. Provide an example to estimate the depth of the environment using monocular images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'dreamlike-art/dreamlike-photoreal-2.0\\', \\'api_call\\': \"StableDiffusionPipeline.from_pretrained(\\'dreamlike-art/dreamlike-photoreal-2.0\\', torch_dtype=torch.float16)(prompt).images[0]\", \\'api_arguments\\': {\\'prompt\\': \\'photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens\\'}, \\'python_environment_requirements\\': {\\'torch\\': \\'torch.float16\\', \\'diffusers\\': \\'StableDiffusionPipeline\\'}, \\'example_code\\': \\'from diffusers import StableDiffusionPipeline\\\\nimport torch\\\\nmodel_id = dreamlike-art/dreamlike-photoreal-2.0\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(./result.jpg)\\', \\'performance\\': {\\'dataset\\': \\'Stable Diffusion 1.5\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Dreamlike Photoreal 2.0 is a photorealistic model based on Stable Diffusion 1.5, made by dreamlike.art. It can be used to generate photorealistic images from text prompts.\\'}', metadata={})]", "category": "generic"}
{"question_id": 461, "text": " As a construction company, we want to have a system to estimate the depth of images taken from different construction sites.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \"PegasusForConditionalGeneration.from_pretrained(\\'tuner007/pegasus_summarizer\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 462, "text": " Our team has built a Japanese language learning app for students. We want to make sure that the user-submitted image is an anime art created by humans and not AI-generated.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 463, "text": " We need to classify the type of an image for an inventory.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 464, "text": " Imagine we run an e-commerce platform and we need to tag new products with relevant categories based on their product images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'financial-sentiment-analysis\\', \\'api_name\\': \\'yiyanghkust/finbert-tone\\', \\'api_call\\': \"BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\", \\'api_arguments\\': [\\'sentences\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import BertTokenizer, BertForSequenceClassification\\\\nfrom transformers import pipeline\\\\nfinbert = BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\\\\ntokenizer = BertTokenizer.from_pretrained(\\'yiyanghkust/finbert-tone\\')\\\\nnlp = pipeline(sentiment-analysis, model=finbert, tokenizer=tokenizer)\\\\nsentences = [there is a shortage of capital, and we need extra financing,\\\\n growth is strong and we have plenty of liquidity,\\\\n there are doubts about our finances,\\\\n profits are flat]\\\\nresults = nlp(sentences)\\\\nprint(results)\", \\'performance\\': {\\'dataset\\': \\'10,000 manually annotated sentences from analyst reports\\', \\'accuracy\\': \\'superior performance on financial tone analysis task\\'}, \\'description\\': \\'FinBERT is a BERT model pre-trained on financial communication text. It is trained on the following three financial communication corpus: Corporate Reports 10-K & 10-Q, Earnings Call Transcripts, and Analyst Reports. This released finbert-tone model is the FinBERT model fine-tuned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports. This model achieves superior performance on financial tone analysis task.\\'}', metadata={})]", "category": "generic"}
{"question_id": 465, "text": " We are building an AI-based camera system to ensure safety on a construction site. Detect workers wearing hard hats in a given image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Object Detection\\', \\'api_name\\': \\'keremberke/yolov8m-hard-hat-detection\\', \\'api_call\\': \"YOLO(\\'keremberke/yolov8m-hard-hat-detection\\')\", \\'api_arguments\\': {\\'image\\': \\'URL or local path to the image\\'}, \\'python_environment_requirements\\': [\\'ultralyticsplus==0.0.24\\', \\'ultralytics==8.0.23\\'], \\'example_code\\': \"from ultralyticsplus import YOLO, render_result\\\\n\\\\nmodel = YOLO(\\'keremberke/yolov8m-hard-hat-detection\\')\\\\n\\\\nmodel.overrides[\\'conf\\'] = 0.25\\\\nmodel.overrides[\\'iou\\'] = 0.45\\\\nmodel.overrides[\\'agnostic_nms\\'] = False\\\\nmodel.overrides[\\'max_det\\'] = 1000\\\\n\\\\nimage = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\\\\n\\\\nresults = model.predict(image)\\\\n\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\", \\'performance\\': {\\'dataset\\': \\'hard-hat-detection\\', \\'accuracy\\': 0.811}, \\'description\\': \"A YOLOv8 model for detecting hard hats in images. The model can distinguish between \\'Hardhat\\' and \\'NO-Hardhat\\' classes. It can be used to ensure safety compliance in construction sites or other industrial environments where hard hats are required.\"}', metadata={})]", "category": "generic"}
{"question_id": 466, "text": " My client needs help with security. They have a surveillance camera set up and we need to identify if any unknown person enters their property.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 467, "text": " We need a system to control the access into parking lot. Analyze an image and find out the license plate numbers to detect whether it's an authorized vehicle or not.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'vision-encoder-decoder\\', \\'api_name\\': \\'naver-clova-ix/donut-base-finetuned-docvqa\\', \\'api_call\\': \"pipeline(\\'document-question-answering\\', model=\\'donut-base-finetuned-docvqa\\')\", \\'api_arguments\\': {\\'image\\': \\'path_to_image\\', \\'question\\': \\'your_question\\'}, \\'python_environment_requirements\\': \\'Transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\n\\\\n# Initialize the pipeline\\\\ndoc_qa = pipeline(\\'document-question-answering\\', model=\\'naver-clova-ix/donut-base-finetuned-docvqa\\')\\\\n\\\\n# Load an image and ask a question\\\\nimage_path = \\'path_to_image\\'\\\\nquestion = \\'your_question\\'\\\\n\\\\n# Get the answer\\\\nanswer = doc_qa({\\'image\\': image_path, \\'question\\': question})\\\\nprint(answer)\", \\'performance\\': {\\'dataset\\': \\'DocVQA\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Donut model fine-tuned on DocVQA. It was introduced in the paper OCR-free Document Understanding Transformer by Geewok et al. and first released in this repository. Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.\\'}', metadata={})]", "category": "generic"}
{"question_id": 468, "text": " Analyze an image of an urban scene to identify and separate regions with different semantics, such as streets, pedestrians, buildings, and vehicles.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Unconditional Image Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Denoising Diffusion Probabilistic Models (DDPM)\\', \\'api_name\\': \\'google/ddpm-ema-cat-256\\', \\'api_call\\': \"DDPMPipeline.from_pretrained(\\'google/ddpm-ema-cat-256\\')\", \\'api_arguments\\': [\\'model_id\\'], \\'python_environment_requirements\\': [\\'!pip install diffusers\\'], \\'example_code\\': \\'from diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\\\nmodel_id = google/ddpm-ema-cat-256\\\\nddpm = DDPMPipeline.from_pretrained(model_id)\\\\nimage = ddpm().images[0]\\\\nimage.save(ddpm_generated_image.png)\\', \\'performance\\': {\\'dataset\\': \\'CIFAR10\\', \\'accuracy\\': {\\'Inception_score\\': 9.46, \\'FID_score\\': 3.17}}, \\'description\\': \\'Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. It can generate high-quality images, and supports different noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm. On the unconditional CIFAR10 dataset, it achieves an Inception score of 9.46 and a state-of-the-art FID score of 3.17.\\'}', metadata={})]", "category": "generic"}
{"question_id": 469, "text": " We are developing an application to help customers visualize themselves wearing clothes available on our e-commerce website. We need identification of clothing items in the image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 470, "text": " We need an image segmentation solution for our smart city planning project. The solution should be able to handle different types of segmentation tasks, from semantic to instance to panoptic.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Segmentation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'facebook/mask2former-swin-tiny-coco-instance\\', \\'api_call\\': \"Mask2FormerForUniversalSegmentation.from_pretrained(\\'facebook/mask2former-swin-tiny-coco-instance\\')\", \\'api_arguments\\': {\\'pretrained_model_name_or_path\\': \\'facebook/mask2former-swin-tiny-coco-instance\\'}, \\'python_environment_requirements\\': [\\'torch\\', \\'transformers\\', \\'PIL\\', \\'requests\\'], \\'example_code\\': \"processor = AutoImageProcessor.from_pretrained(\\'facebook/mask2former-swin-tiny-coco-instance\\')\\\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained(\\'facebook/mask2former-swin-tiny-coco-instance\\')\\\\nurl = \\'http://images.cocodataset.org/val2017/000000039769.jpg\\'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = processor(images=image, return_tensors=\\'pt\\')\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\nclass_queries_logits = outputs.class_queries_logits\\\\nmasks_queries_logits = outputs.masks_queries_logits\\\\nresult = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\\\npredicted_instance_map = result[\\'segmentation\\']\", \\'performance\\': {\\'dataset\\': \\'COCO\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Mask2Former model trained on COCO instance segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. You can use this particular checkpoint for instance segmentation.\\'}', metadata={})]", "category": "generic"}
{"question_id": 471, "text": " We have a road safety app that helps identify potholes. Can we use an image segmentation model to detect if there are potholes in the pictures of roads?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 472, "text": " We are making a mobile app related to fitness. We need to estimate the human pose from an image of a user performing an exercise.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 473, "text": " I wish to edit my images by detecting straight lines and controlling the diffusion models in the image's diffusion process.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Unconditional Image Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Denoising Diffusion Probabilistic Models (DDPM)\\', \\'api_name\\': \\'google/ddpm-bedroom-256\\', \\'api_call\\': \"DDPMPipeline.from_pretrained(\\'google/ddpm-bedroom-256\\')\", \\'api_arguments\\': \\'None\\', \\'python_environment_requirements\\': \\'diffusers\\', \\'example_code\\': \\'!pip install diffusers\\\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\\\nmodel_id = google/ddpm-bedroom-256\\\\nddpm = DDPMPipeline.from_pretrained(model_id)\\\\nimage = ddpm().images[0]\\\\nimage.save(ddpm_generated_image.png)\\', \\'performance\\': {\\'dataset\\': \\'CIFAR10\\', \\'accuracy\\': {\\'Inception score\\': 9.46, \\'FID score\\': 3.17}}, \\'description\\': \\'We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.\\'}', metadata={})]", "category": "generic"}
{"question_id": 474, "text": " A photographer wants to create artistic interpretations of some of her pictures. Let's help her to get variated styles of her image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'lidiya/bart-large-xsum-samsum\\', \\'api_call\\': \"pipeline(\\'summarization\\', model=\\'lidiya/bart-large-xsum-samsum\\')\", \\'api_arguments\\': \\'conversation\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=lidiya/bart-large-xsum-samsum)\\\\nconversation = \\'\\'\\'Hannah: Hey, do you have Betty\\'s number?\\\\nAmanda: Lemme check\\\\nAmanda: Sorry, can\\'t find it.\\\\nAmanda: Ask Larry\\\\nAmanda: He called her last time we were at the park together\\\\nHannah: I don\\'t know him well\\\\nAmanda: Don\\'t be shy, he\\'s very nice\\\\nHannah: If you say so..\\\\nHannah: I\\'d rather you texted him\\\\nAmanda: Just text him \ud83d\ude42\\\\nHannah: Urgh.. Alright\\\\nHannah: Bye\\\\nAmanda: Bye bye <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization\\', \\'accuracy\\': {\\'rouge1\\': 53.306, \\'rouge2\\': 28.355, \\'rougeL\\': 44.095}}, \\'description\\': \\'This model was obtained by fine-tuning facebook/bart-large-xsum on Samsum dataset.\\'}', metadata={})]", "category": "generic"}
{"question_id": 475, "text": " We have a client that manages a car website for selling used cars and they need new images of cars for the website.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 476, "text": " I am interested in building a wall of generated pictures for my gallery. My specifications include a size of 256x256 pixels.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 477, "text": " We are a product company selling personalized gadgets, and we want to build a recommender system that shows generated, high-resolution images of human faces on our website.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 478, "text": " I want to build an AI-based software that can identify the activities or actions in a video clip.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 479, "text": " The marketing team wants a tool to quickly classify new advertisement videos.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 480, "text": " A sports league wants to analyze their videos and extract information on game highlights.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'PyTorch Transformers\\', \\'functionality\\': \\'text-generation\\', \\'api_name\\': \\'satvikag/chatbot\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'output-small\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"AutoTokenizer.from_pretrained(\\'microsoft/DialoGPT-small\\')\", \\'model\\': \"AutoModelWithLMHead.from_pretrained(\\'output-small\\')\"}, \\'python_environment_requirements\\': [\\'torch\\', \\'transformers\\'], \\'example_code\\': \"for step in range(100):\\\\n  new_user_input_ids = tokenizer.encode(input(\\'&gt;&gt; User:\\') + tokenizer.eos_token, return_tensors=\\'pt\\')\\\\n  bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step &gt; 0 else new_user_input_ids\\\\n  chat_history_ids = model.generate(bot_input_ids, max_length=500, pad_token_id=tokenizer.eos_token_id, no_repeat_ngram_size=3, do_sample=True, top_k=100, top_p=0.7, temperature = 0.8)\\\\n  print(\\'AI: {}\\'.format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\", \\'performance\\': {\\'dataset\\': \\'Kaggle game script dataset\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'DialoGPT Trained on the Speech of a Game Character, Joshua from The World Ends With You.\\'}', metadata={})]", "category": "generic"}
{"question_id": 481, "text": " We are a sports broadcasting company, and we need to automatically identify the sports events taking place in the videos we receive.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 482, "text": " My organization wants to create a video categorization tool to classify and categorize various videos. Utilize the appropriate API to build this video classifier.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'facebook/timesformer-base-finetuned-k400\\', \\'api_call\\': \"TimesformerForVideoClassification.from_pretrained(\\'facebook/timesformer-base-finetuned-k400\\')\", \\'api_arguments\\': \\'video, return_tensors\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'from transformers import AutoImageProcessor, TimesformerForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(8, 3, 224, 224))\\\\nprocessor = AutoImageProcessor.from_pretrained(facebook/timesformer-base-finetuned-k400)\\\\nmodel = TimesformerForVideoClassification.from_pretrained(facebook/timesformer-base-finetuned-k400)\\\\ninputs = processor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Kinetics-400\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'TimeSformer is a video classification model pre-trained on Kinetics-400. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository. The model can be used for video classification into one of the 400 possible Kinetics-400 labels.\\'}', metadata={})]", "category": "generic"}
{"question_id": 483, "text": " We have a surveillance camera in our backyard. We would like to analyze the captured videos to recognize the activities taking place in the backyard.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 484, "text": " We need to classify an image's content and check if it contains a cat or a dog.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 485, "text": " Analyze the type of plants in the image provided and provide the name of the probable plant.\\n \n Use this API documentation for reference: [Document(page_content=\"{'domain': 'Natural Language Processing Token Classification', 'framework': 'Transformers', 'functionality': 'punctuation prediction', 'api_name': 'oliverguhr/fullstop-punctuation-multilang-large', 'api_call': 'PunctuationModel()', 'api_arguments': ['text'], 'python_environment_requirements': ['pip install deepmultilingualpunctuation'], 'example_code': 'from deepmultilingualpunctuation import PunctuationModel\\\\nmodel = PunctuationModel()\\\\ntext = My name is Clara and I live in Berkeley California Ist das eine Frage Frau M\u00fcller\\\\nresult = model.restore_punctuation(text)\\\\nprint(result)', 'performance': {'dataset': 'wmt/europarl', 'EN_accuracy': 0.775, 'DE_accuracy': 0.814, 'FR_accuracy': 0.782, 'IT_accuracy': 0.762}, 'description': 'This model predicts the punctuation of English, Italian, French and German texts. It was developed to restore the punctuation of transcribed spoken language and trained on the Europarl Dataset provided by the SEPP-NLG Shared Task. The model restores the following punctuation markers: ., ,, ?, -, :.'}\", metadata={})]", "category": "generic"}
{"question_id": 486, "text": " I want to organize my images based on the scene content. The categories I want are landscape, cityscape, beach, forest, and animals.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 487, "text": " We run an online store for selling electronic devices. We need to classify product images from our inventory and organize them.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 488, "text": " Our customer wants to analyze the sentiment of their customers' feedback. The feedback is in Spanish.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'financial-sentiment-analysis\\', \\'api_name\\': \\'ProsusAI/finbert\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'ProsusAI/finbert\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline; classifier = pipeline(\\'sentiment-analysis\\', model=\\'ProsusAI/finbert\\'); classifier(\\'your_text_here\\')\", \\'performance\\': {\\'dataset\\': \\'Financial PhraseBank\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'FinBERT is a pre-trained NLP model to analyze sentiment of financial text. It is built by further training the BERT language model in the finance domain, using a large financial corpus and thereby fine-tuning it for financial sentiment classification. Financial PhraseBank by Malo et al. (2014) is used for fine-tuning.\\'}', metadata={})]", "category": "generic"}
{"question_id": 489, "text": " A stock investor is looking to analyze the sentiment of a stock forum, such as StockTwits, to gain insights into the market sentiment for a specific stock.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'financial-sentiment-analysis\\', \\'api_name\\': \\'ProsusAI/finbert\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'ProsusAI/finbert\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline; classifier = pipeline(\\'sentiment-analysis\\', model=\\'ProsusAI/finbert\\'); classifier(\\'your_text_here\\')\", \\'performance\\': {\\'dataset\\': \\'Financial PhraseBank\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'FinBERT is a pre-trained NLP model to analyze sentiment of financial text. It is built by further training the BERT language model in the finance domain, using a large financial corpus and thereby fine-tuning it for financial sentiment classification. Financial PhraseBank by Malo et al. (2014) is used for fine-tuning.\\'}', metadata={})]", "category": "generic"}
{"question_id": 490, "text": " A forum moderator wants a tool to assess user-generated comments for toxic content. How does this model help?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'MCG-NJU/videomae-base-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-base-finetuned-ssv2\\')\", \\'api_arguments\\': \\'video\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nprocessor = VideoMAEImageProcessor.from_pretrained(MCG-NJU/videomae-base-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-base-finetuned-ssv2)\\\\ninputs = processor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n    logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something-v2\\', \\'accuracy\\': {\\'top-1\\': 70.6, \\'top-5\\': 92.6}}, \\'description\\': \\'VideoMAE model pre-trained for 2400 epochs in a self-supervised way and fine-tuned in a supervised way on Something-Something-v2. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository.\\'}', metadata={})]", "category": "generic"}
{"question_id": 491, "text": " I am developing a news analysis platform. I need to predict the named entities from the articles.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 492, "text": " We need to analyze the user's text for extracting entities and improve our virtual assistant interaction.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 493, "text": " Extract the named entities from a given text snippet.\\n###Input: \\\"On June 7th, Jane Smith visited the Empire State Building in New York with an entry fee of 35 dollars.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Information Retrieval\\', \\'api_name\\': \\'cross-encoder/ms-marco-TinyBERT-L-2-v2\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'cross-encoder/ms-marco-TinyBERT-L-2-v2\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\"}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': {\\'import\\': \\'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\', \\'model\\': \"model = AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\", \\'scores\\': \\'with torch.no_grad():\\\\n    scores = model(**features).logits\\\\n    print(scores)\\'}, \\'performance\\': {\\'dataset\\': \\'TREC Deep Learning 2019\\', \\'accuracy\\': \\'69.84 (NDCG@10)\\'}, \\'description\\': \\'This model was trained on the MS Marco Passage Ranking task. It can be used for Information Retrieval: Given a query, encode the query with all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. The training code is available here: SBERT.net Training MS Marco.\\'}', metadata={})]", "category": "generic"}
{"question_id": 494, "text": " As a surveillance analyst, my main task this week is to find out all the names and locations mentioned in the online chat rooms.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Segmentation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'shi-labs/oneformer_ade20k_swin_large\\', \\'api_call\\': \"OneFormerForUniversalSegmentation.from_pretrained(\\'shi-labs/oneformer_ade20k_swin_large\\')\", \\'api_arguments\\': [\\'images\\', \\'task_inputs\\', \\'return_tensors\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'PIL\\', \\'requests\\'], \\'example_code\\': \\'from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/ade20k.jpeg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nprocessor = OneFormerProcessor.from_pretrained(shi-labs/oneformer_ade20k_swin_large)\\\\nmodel = OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_ade20k_swin_large)\\\\nsemantic_inputs = processor(images=image, task_inputs=[semantic], return_tensors=pt)\\\\nsemantic_outputs = model(**semantic_inputs)\\\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\', \\'performance\\': {\\'dataset\\': \\'scene_parse_150\\', \\'accuracy\\': None}, \\'description\\': \\'OneFormer model trained on the ADE20k dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 495, "text": " I need a program to identify the entities like persons, locations, organizations, and other names in a given German text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Token Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Named Entity Recognition\\', \\'api_name\\': \\'Jean-Baptiste/camembert-ner\\', \\'api_call\\': \"AutoModelForTokenClassification.from_pretrained(\\'Jean-Baptiste/camembert-ner\\')\", \\'api_arguments\\': {\\'model\\': \\'model\\', \\'tokenizer\\': \\'tokenizer\\', \\'aggregation_strategy\\': \\'simple\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'AutoTokenizer, AutoModelForTokenClassification, pipeline\\'}, \\'example_code\\': \"from transformers import AutoTokenizer, AutoModelForTokenClassification\\\\ntokenizer = AutoTokenizer.from_pretrained(Jean-Baptiste/camembert-ner)\\\\nmodel = AutoModelForTokenClassification.from_pretrained(Jean-Baptiste/camembert-ner)\\\\n\\\\nfrom transformers import pipeline\\\\nnlp = pipeline(\\'ner\\', model=model, tokenizer=tokenizer, aggregation_strategy=simple)\\\\nnlp(Apple est cre le 1er avril 1976 dans le garage de la maison d\\'enfance de Steve Jobs  Los Altos en Californie par Steve Jobs, Steve Wozniak et Ronald Wayne14, puis constitue sous forme de socit le 3 janvier 1977  l\\'origine sous le nom d\\'Apple Computer, mais pour ses 30 ans et pour reflter la diversification de ses produits, le mot \u00ab computer  est retir le 9 janvier 2015.)\", \\'performance\\': {\\'dataset\\': \\'wikiner-fr\\', \\'accuracy\\': {\\'overall_f1\\': 0.8914, \\'PER_f1\\': 0.9483, \\'ORG_f1\\': 0.8181, \\'LOC_f1\\': 0.8955, \\'MISC_f1\\': 0.8146}}, \\'description\\': \\'camembert-ner is a Named Entity Recognition (NER) model fine-tuned from camemBERT on the wikiner-fr dataset. It can recognize entities such as persons, organizations, locations, and miscellaneous entities.\\'}', metadata={})]", "category": "generic"}
{"question_id": 496, "text": " We are a multi-national company that would like to better understand global events and extract relevant named entities across 9 languages (de, en, es, fr, it, nl, pl, pt, ru). We would like to utilize a pre-trained NER model.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Token Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Named Entity Recognition\\', \\'api_name\\': \\'Babelscape/wikineural-multilingual-ner\\', \\'api_call\\': \"AutoModelForTokenClassification.from_pretrained(\\'Babelscape/wikineural-multilingual-ner\\')\", \\'api_arguments\\': {\\'model\\': \\'AutoModelForTokenClassification.from_pretrained(Babelscape/wikineural-multilingual-ner)\\', \\'tokenizer\\': \\'AutoTokenizer.from_pretrained(Babelscape/wikineural-multilingual-ner)\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\\'}, \\'example_code\\': \\'tokenizer = AutoTokenizer.from_pretrained(Babelscape/wikineural-multilingual-ner)\\\\nmodel = AutoModelForTokenClassification.from_pretrained(Babelscape/wikineural-multilingual-ner)\\\\nnlp = pipeline(ner, model=model, tokenizer=tokenizer)\\\\nexample = My name is Wolfgang and I live in Berlin\\\\nner_results = nlp(example)\\\\nprint(ner_results)\\', \\'performance\\': {\\'dataset\\': \\'Babelscape/wikineural-multilingual-ner\\', \\'accuracy\\': \\'span-based F1-score up to 6 points over previous state-of-the-art systems for data creation\\'}, \\'description\\': \\'A multilingual Named Entity Recognition (NER) model fine-tuned on the WikiNEuRal dataset, supporting 9 languages (de, en, es, fr, it, nl, pl, pt, ru). It is based on the mBERT architecture and trained on all 9 languages jointly. The model can be used with the Hugging Face Transformers pipeline for NER tasks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 497, "text": " We have a multimedia app in the Korean language. To deal with customer queries automatically, we want to incorporate question answering capability.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 498, "text": " Guide me to create a quiz project where I will summarize an article into a paragraph and from the summary I will develop a question with some multiple options. I need to check the correct answer for that question.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 499, "text": " I have a website text about technology and I want to know if it represents a positive sentiment or a negative one.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'results-yelp\\', \\'api_call\\': \"AutoTokenizer.from_pretrained(\\'bert-base-uncased\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"AutoTokenizer.from_pretrained(\\'bert-base-uncased\\')\", \\'config\\': \"AutoConfig.from_pretrained(\\'potatobunny/results-yelp\\')\"}, \\'python_environment_requirements\\': {\\'Transformers\\': \\'4.18.0\\', \\'Pytorch\\': \\'1.10.0+cu111\\', \\'Datasets\\': \\'2.0.0\\', \\'Tokenizers\\': \\'0.12.1\\'}, \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'Yelp\\', \\'accuracy\\': 0.9302}, \\'description\\': \\'This model is a fine-tuned version of textattack/bert-base-uncased-yelp-polarity on a filtered and manually reviewed Yelp dataset containing restaurant reviews only. It is intended to perform text classification, specifically sentiment analysis, on text data obtained from restaurant reviews to determine if the particular review is positive or negative.\\'}', metadata={})]", "category": "generic"}
{"question_id": 500, "text": " Translate a French website description containing information about the company, services, and copyright notice into English.\\n###Input: \\\"Bienvenue sur notre site ! Nous sommes une entreprise sp\\u00e9cialis\\u00e9e dans la gestion des projets informatiques et la cr\\u00e9ation de logiciels sur mesure. Nos services incluent la conception, le d\\u00e9veloppement, la maintenance et le support 24/7. Tous droits r\\u00e9serv\\u00e9s.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Token Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Named Entity Recognition\\', \\'api_name\\': \\'Jean-Baptiste/camembert-ner\\', \\'api_call\\': \"AutoModelForTokenClassification.from_pretrained(\\'Jean-Baptiste/camembert-ner\\')\", \\'api_arguments\\': {\\'model\\': \\'model\\', \\'tokenizer\\': \\'tokenizer\\', \\'aggregation_strategy\\': \\'simple\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'AutoTokenizer, AutoModelForTokenClassification, pipeline\\'}, \\'example_code\\': \"from transformers import AutoTokenizer, AutoModelForTokenClassification\\\\ntokenizer = AutoTokenizer.from_pretrained(Jean-Baptiste/camembert-ner)\\\\nmodel = AutoModelForTokenClassification.from_pretrained(Jean-Baptiste/camembert-ner)\\\\n\\\\nfrom transformers import pipeline\\\\nnlp = pipeline(\\'ner\\', model=model, tokenizer=tokenizer, aggregation_strategy=simple)\\\\nnlp(Apple est cre le 1er avril 1976 dans le garage de la maison d\\'enfance de Steve Jobs  Los Altos en Californie par Steve Jobs, Steve Wozniak et Ronald Wayne14, puis constitue sous forme de socit le 3 janvier 1977  l\\'origine sous le nom d\\'Apple Computer, mais pour ses 30 ans et pour reflter la diversification de ses produits, le mot \u00ab computer  est retir le 9 janvier 2015.)\", \\'performance\\': {\\'dataset\\': \\'wikiner-fr\\', \\'accuracy\\': {\\'overall_f1\\': 0.8914, \\'PER_f1\\': 0.9483, \\'ORG_f1\\': 0.8181, \\'LOC_f1\\': 0.8955, \\'MISC_f1\\': 0.8146}}, \\'description\\': \\'camembert-ner is a Named Entity Recognition (NER) model fine-tuned from camemBERT on the wikiner-fr dataset. It can recognize entities such as persons, organizations, locations, and miscellaneous entities.\\'}', metadata={})]", "category": "generic"}
{"question_id": 501, "text": " We have a list of documents written in multiple Romance languages, including texts in French, Spanish, and Italian. We want to make these texts accessible to our English-speaking audience by translating them.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 502, "text": " We have an international audience for our website and require our website content to be translated into multiple languages for better understanding.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 503, "text": " I am working for the review section of a book company. I want to convert a book summary into a positive book review.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 504, "text": " We are a company that wants to expand globally. We need to translate our website content from English to Italian.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 505, "text": " Our company is working on a news website. We want to present summaries of news articles written in French to the users. \\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Feature Extraction\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'DeepPavlov/rubert-base-cased\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'DeepPavlov/rubert-base-cased\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'Russian part of Wikipedia and news data\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'RuBERT (Russian, cased, 12\u2011layer, 768\u2011hidden, 12\u2011heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT\u2011base as an initialization for RuBERT[1].\\'}', metadata={})]", "category": "generic"}
{"question_id": 506, "text": " We are an international business and need to translate our documents from French to Spanish.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 507, "text": " The company needs to summarize articles for its news application. Provide guidelines to use PEGASUS for this purpose.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'financial-summarization-pegasus\\', \\'api_call\\': \"PegasusForConditionalGeneration.from_pretrained(\\'human-centered-summarization/financial-summarization-pegasus\\')\", \\'api_arguments\\': [\\'model_name\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import PegasusTokenizer, PegasusForConditionalGeneration, TFPegasusForConditionalGeneration\\\\nmodel_name = human-centered-summarization/financial-summarization-pegasus\\\\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\\\\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name)\\\\ntext_to_summarize = National Commercial Bank (NCB), Saudi Arabia\u2019s largest lender by assets, agreed to buy rival Samba Financial Group for $15 billion in the biggest banking takeover this year.NCB will pay 28.45 riyals ($7.58) for each Samba share, according to a statement on Sunday, valuing it at about 55.7 billion riyals. NCB will offer 0.739 new shares for each Samba share, at the lower end of the 0.736-0.787 ratio the banks set when they signed an initial framework agreement in June.The offer is a 3.5% premium to Samba\u2019s Oct. 8 closing price of 27.50 riyals and about 24% higher than the level the shares traded at before the talks were made public. Bloomberg News first reported the merger discussions.The new bank will have total assets of more than $220 billion, creating the Gulf region\u2019s third-largest lender. The entity\u2019s $46 billion market capitalization nearly matches that of Qatar National Bank QPSC, which is still the Middle East\u2019s biggest lender with about $268 billion of assets.\\\\ninput_ids = tokenizer(text_to_summarize, return_tensors=pt).input_ids\\\\noutput = model.generate(input_ids, max_length=32, num_beams=5, early_stopping=True)\\\\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\\', \\'performance\\': {\\'dataset\\': \\'xsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 35.206, \\'ROUGE-2\\': 16.569, \\'ROUGE-L\\': 30.128, \\'ROUGE-LSUM\\': 30.171}}, \\'description\\': \\'This model was fine-tuned on a novel financial news dataset, which consists of 2K articles from Bloomberg, on topics such as stock, markets, currencies, rate and cryptocurrencies. It is based on the PEGASUS model and in particular PEGASUS fine-tuned on the Extreme Summarization (XSum) dataset: google/pegasus-xsum model. PEGASUS was originally proposed by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu in PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization.\\'}', metadata={})]", "category": "generic"}
{"question_id": 508, "text": " You are working as a news curator for a media company and need to make a short summary of a long news article.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Feature Extraction\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'DeepPavlov/rubert-base-cased\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'DeepPavlov/rubert-base-cased\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'Russian part of Wikipedia and news data\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'RuBERT (Russian, cased, 12\u2011layer, 768\u2011hidden, 12\u2011heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT\u2011base as an initialization for RuBERT[1].\\'}', metadata={})]", "category": "generic"}
{"question_id": 509, "text": " Assemble a conversational bot for my online business to answer questions regarding our products.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'deepset/roberta-large-squad2\\', \\'api_call\\': \"pipeline(\\'question-answering\\', model=\\'deepset/roberta-large-squad2\\')\", \\'api_arguments\\': [\\'question\\', \\'context\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline; nlp = pipeline(\\'question-answering\\', model=\\'deepset/roberta-large-squad2\\'); nlp({\\'question\\': \\'What is the capital of Germany?\\', \\'context\\': \\'Berlin is the capital of Germany.\\'})\", \\'performance\\': {\\'dataset\\': \\'squad_v2\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'A pre-trained RoBERTa model for question answering tasks, specifically trained on the SQuAD v2 dataset. It can be used to answer questions based on a given context.\\'}', metadata={})]", "category": "generic"}
{"question_id": 510, "text": " We are developing a game and need a text-based AI to control the main character's behavior. Help us use this chatbot based on the Joshua character.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 511, "text": " We are creating an AI assistant for banking clients. The customers should be able to talk to the bot to do various tasks. We need to have a meaningful dialogue with them.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'Kirili4ik/mbart_ruDialogSum\\', \\'api_call\\': \"MBartForConditionalGeneration.from_pretrained(\\'Kirili4ik/mbart_ruDialogSum\\')\", \\'api_arguments\\': {\\'model_name\\': \\'Kirili4ik/mbart_ruDialogSum\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import MBartTokenizer, MBartForConditionalGeneration\\\\nmodel_name = Kirili4ik/mbart_ruDialogSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = MBartForConditionalGeneration.from_pretrained(model_name)\\\\nmodel.eval()\\\\narticle_text = ...\\\\ninput_ids = tokenizer(\\\\n [article_text],\\\\n max_length=600,\\\\n padding=max_length,\\\\n truncation=True,\\\\n return_tensors=pt,\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n top_k=0,\\\\n num_beams=3,\\\\n no_repeat_ngram_size=3\\\\n)[0]\\\\nsummary = tokenizer.decode(output_ids, skip_special_tokens=True)\\\\nprint(summary)\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'SAMSum Corpus (translated to Russian)\\', \\'accuracy\\': {\\'Validation ROGUE-1\\': 34.5, \\'Validation ROGUE-L\\': 33, \\'Test ROGUE-1\\': 31, \\'Test ROGUE-L\\': 28}}]}, \\'description\\': \\'MBart for Russian summarization fine-tuned for dialogues summarization. This model was firstly fine-tuned by Ilya Gusev on Gazeta dataset. We have fine tuned that model on SamSum dataset translated to Russian using GoogleTranslateAPI. Moreover! We have implemented a ! telegram bot @summarization_bot ! with the inference of this model. Add it to the chat and get summaries instead of dozens spam messages!\\'}', metadata={})]", "category": "generic"}
{"question_id": 512, "text": " We want to facilitate a conversation with our Russian customers. Build a model that can respond to customer inquiries in Russian.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 513, "text": " We are launching a new blog and need a paragraph with tips on how to take care of houseplants.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 514, "text": " The development team is trying to create a function in Python to print \\\"Hello, World!\\\" but they're not sure how to proceed. Generate this function for them.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Token Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Named Entity Recognition\\', \\'api_name\\': \\'flair/ner-english\\', \\'api_call\\': \"SequenceTagger.load(\\'flair/ner-english\\')\", \\'api_arguments\\': [\\'sentence\\'], \\'python_environment_requirements\\': [\\'flair\\'], \\'example_code\\': \"from flair.data import Sentence\\\\nfrom flair.models import SequenceTagger\\\\n\\\\n# load tagger\\\\ntagger = SequenceTagger.load(\\'flair/ner-english\\')\\\\n\\\\n# make example sentence\\\\nsentence = Sentence(\\'George Washington went to Washington\\')\\\\n\\\\n# predict NER tags\\\\ntagger.predict(sentence)\\\\n\\\\n# print sentence\\\\nprint(sentence)\\\\n\\\\n# print predicted NER spans\\\\nprint(\\'The following NER tags are found:\\')\\\\n\\\\n# iterate over entities and print\\\\nfor entity in sentence.get_spans(\\'ner\\'):\\\\n    print(entity)\", \\'performance\\': {\\'dataset\\': \\'conll2003\\', \\'accuracy\\': \\'93.06\\'}, \\'description\\': \\'This is the standard 4-class NER model for English that ships with Flair. It predicts 4 tags: PER (person name), LOC (location name), ORG (organization name), and MISC (other name). The model is based on Flair embeddings and LSTM-CRF.\\'}', metadata={})]", "category": "generic"}
{"question_id": 515, "text": " We need to generate a motivational quote related to sports.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 516, "text": " Give me a tool that I can use to generate stories based on a starting phrase.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'distilbart-cnn-12-6-samsum\\', \\'api_call\\': \"pipeline(\\'summarization\\', model=\\'philschmid/distilbart-cnn-12-6-samsum\\')\", \\'api_arguments\\': {\\'model\\': \\'philschmid/distilbart-cnn-12-6-samsum\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 41.09, \\'ROUGE-2\\': 20.746, \\'ROUGE-L\\': 31.595, \\'ROUGE-LSUM\\': 38.339}}, \\'description\\': \\'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 517, "text": " Our PR team requires a machine learning approach to creative sentence generation for marketing content creation.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 518, "text": " We are developing an AI-powered code review system. Our model should provide a short summary of the provided code snippet.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 519, "text": " I want a language model that can fill in short blanks for example sentences, quizzes, or trivia questions.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'gsdf/Counterfeit-V2.5\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'gsdf/Counterfeit-V2.5\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'((masterpiece,best quality)),1girl, solo, animal ears, rabbit, barefoot, knees up, dress, sitting, rabbit ears, short sleeves, looking at viewer, grass, short hair, smile, white hair, puffy sleeves, outdoors, puffy short sleeves, bangs, on ground, full body, animal, white dress, sunlight, brown eyes, dappled sunlight, day, depth of field\\', \\'performance\\': {\\'dataset\\': \\'EasyNegative\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 520, "text": " As a language teacher, I sometimes need help completing Dutch sentences with the most suitable word. Can you fill in the gaps?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'distilbart-cnn-12-6-samsum\\', \\'api_call\\': \"pipeline(\\'summarization\\', model=\\'philschmid/distilbart-cnn-12-6-samsum\\')\", \\'api_arguments\\': {\\'model\\': \\'philschmid/distilbart-cnn-12-6-samsum\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 41.09, \\'ROUGE-2\\': 20.746, \\'ROUGE-L\\': 31.595, \\'ROUGE-LSUM\\': 38.339}}, \\'description\\': \\'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 521, "text": " Translate some lyrics of a Portuguese song into English\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'sberbank-ai/sbert_large_mt_nlu_ru\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'sberbank-ai/sbert_large_mt_nlu_ru\\')\", \\'api_arguments\\': [\\'sentences\\', \\'padding\\', \\'truncation\\', \\'max_length\\', \\'return_tensors\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': \"from transformers import AutoTokenizer, AutoModel\\\\nimport torch\\\\n\\\\n\\\\n# Mean Pooling - Take attention mask into account for correct averaging\\\\ndef mean_pooling(model_output, attention_mask):\\\\n    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\\\\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\\\\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\\\\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\\\\n    return sum_embeddings / sum_mask\\\\n\\\\n\\\\n# Sentences we want sentence embeddings for sentences = [\\'?\\']\\\\n\\\\n# Load AutoModel from huggingface model repository\\\\ntokenizer = AutoTokenizer.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\\\nmodel = AutoModel.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\\\n\\\\n# Tokenize sentences\\\\nencoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=24, return_tensors=\\'pt\\')\\\\n\\\\n# Compute token embeddings\\\\nwith torch.no_grad():\\\\n    model_output = model(**encoded_input)\\\\n\\\\n# Perform pooling. In this case, mean pooling\\\\nsentence_embeddings = mean_pooling(model_output, encoded_input[\\'attention_mask\\'])\", \\'performance\\': {\\'dataset\\': \\'Russian SuperGLUE\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'BERT large model multitask (cased) for Sentence Embeddings in Russian language.\\'}', metadata={})]", "category": "generic"}
{"question_id": 522, "text": " We are a news portal website and recently acquired a breaking news article. Determine the similarity between the main text of this article and other articles in our database.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Feature Extraction\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'DeepPavlov/rubert-base-cased\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'DeepPavlov/rubert-base-cased\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'Russian part of Wikipedia and news data\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'RuBERT (Russian, cased, 12\u2011layer, 768\u2011hidden, 12\u2011heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT\u2011base as an initialization for RuBERT[1].\\'}', metadata={})]", "category": "generic"}
{"question_id": 523, "text": " As a business assistant of an international company, find the most relevant sentence among a list of sentences that answers a specific question.\\n###Input: {\\\"question\\\": \\\"What is the main purpose of photosynthesis?\\\", \\\"sentences\\\": [\\\"Photosynthesis is the process used by plants to convert light energy into chemical energy to fuel their growth.\\\", \\\"The Eiffel Tower is a famous landmark in Paris.\\\", \\\"Photosynthesis also produces oxygen as a byproduct, which is necessary for life on Earth.\\\"]}\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 524, "text": " We need to cluster customer reviews based on their content similarity to understand the common issues customers are facing.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 525, "text": " Create a program to calculate sentence similarity scores between a list of sentences.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Sentence Similarity\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Sentence Transformers\\', \\'api_name\\': \\'flax-sentence-embeddings/all_datasets_v4_MiniLM-L6\\', \\'api_call\\': \"SentenceTransformer(\\'flax-sentence-embeddings/all_datasets_v4_MiniLM-L6\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'sentence-transformers library\\', \\'example_code\\': \"from sentence_transformers import SentenceTransformer\\\\nmodel = SentenceTransformer(\\'flax-sentence-embeddings/all_datasets_v4_MiniLM-L6\\')\\\\ntext = Replace me by any text you\\'d like.\\\\ntext_embbedding = model.encode(text)\", \\'performance\\': {\\'dataset\\': \\'1,097,953,922\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'The model is trained on very large sentence level datasets using a self-supervised contrastive learning objective. It is fine-tuned on a 1B sentence pairs dataset, and it aims to capture the semantic information of input sentences. The sentence vector can be used for information retrieval, clustering, or sentence similarity tasks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 526, "text": " We need to find a similar Chinese sentence to my source sentence for a suggestion.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Sentence Similarity\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'setu4993/LaBSE\\', \\'api_call\\': \"BertModel.from_pretrained(\\'setu4993/LaBSE\\')\", \\'api_arguments\\': [\\'english_sentences\\', \\'italian_sentences\\', \\'japanese_sentences\\'], \\'python_environment_requirements\\': [\\'torch\\', \\'transformers\\'], \\'example_code\\': \"import torch\\\\nfrom transformers import BertModel, BertTokenizerFast\\\\ntokenizer = BertTokenizerFast.from_pretrained(\\'setu4993/LaBSE\\')\\\\nmodel = BertModel.from_pretrained(\\'setu4993/LaBSE\\')\\\\nmodel = model.eval()\\\\nenglish_sentences = [\\\\n \\'dog\\',\\\\n \\'Puppies are nice.\\',\\\\n \\'I enjoy taking long walks along the beach with my dog.\\',\\\\n]\\\\nenglish_inputs = tokenizer(english_sentences, return_tensors=\\'pt\\', padding=True)\\\\nwith torch.no_grad():\\\\n english_outputs = model(**english_inputs)\\\\nenglish_embeddings = english_outputs.pooler_output\", \\'performance\\': {\\'dataset\\': \\'CommonCrawl and Wikipedia\\', \\'accuracy\\': \\'Not Specified\\'}, \\'description\\': \\'Language-agnostic BERT Sentence Encoder (LaBSE) is a BERT-based model trained for sentence embedding for 109 languages. The pre-training process combines masked language modeling with translation language modeling. The model is useful for getting multilingual sentence embeddings and for bi-text retrieval.\\'}', metadata={})]", "category": "generic"}
{"question_id": 527, "text": " I am building a voice assistant for my mobile app. Give me an example of how I can implement this Text-to-Speech model.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 528, "text": " Our team wants to transcribe audio files into text including punctuation marks for usability studies.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Audio Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'padmalcom/wav2vec2-large-emotion-detection-german\\', \\'api_call\\': \"pipeline(\\'audio-classification\\', model=\\'padmalcom/wav2vec2-large-emotion-detection-german\\')\", \\'api_arguments\\': \\'audio_file\\', \\'python_environment_requirements\\': \\'transformers, torch\\', \\'example_code\\': \"from transformers import pipeline\\\\naudio_classifier = pipeline(\\'audio-classification\\', model=\\'padmalcom/wav2vec2-large-emotion-detection-german\\')\\\\nresult = audio_classifier(audio_file)\", \\'performance\\': {\\'dataset\\': \\'emo-DB\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'This wav2vec2 based emotion detection model is trained on the emo-DB dataset. It can classify emotions in German audio files into seven classes: anger, boredom, disgust, fear, happiness, sadness, and neutral.\\'}', metadata={})]", "category": "generic"}
{"question_id": 529, "text": " Your client is a podcast aggregator website looking to transcribe episodes and display transcriptions on the site. Help them transcribe an audio file containing spoken English words.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Automatic Speech Recognition\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Transcription\\', \\'api_name\\': \\'facebook/wav2vec2-base-960h\\', \\'api_call\\': \"Wav2Vec2ForCTC.from_pretrained(\\'facebook/wav2vec2-base-960h\\')\", \\'api_arguments\\': [\\'input_values\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'datasets\\', \\'torch\\', \\'jiwer\\'], \\'example_code\\': \"from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\\\\nfrom datasets import load_dataset\\\\nimport torch\\\\n\\\\nprocessor = Wav2Vec2Processor.from_pretrained(\\'facebook/wav2vec2-base-960h\\')\\\\nmodel = Wav2Vec2ForCTC.from_pretrained(\\'facebook/wav2vec2-base-960h\\')\\\\nds = load_dataset(\\'patrickvonplaten/librispeech_asr_dummy\\', \\'clean\\', split=\\'validation\\')\\\\ninput_values = processor(ds[0][\\'audio\\'][\\'array\\'], return_tensors=\\'pt\\', padding=\\'longest\\').input_values\\\\nlogits = model(input_values).logits\\\\npredicted_ids = torch.argmax(logits, dim=-1)\\\\ntranscription = processor.batch_decode(predicted_ids)\", \\'performance\\': {\\'dataset\\': \\'LibriSpeech\\', \\'accuracy\\': {\\'clean\\': 3.4, \\'other\\': 8.6}}, \\'description\\': \"Facebook\\'s Wav2Vec2 base model pretrained and fine-tuned on 960 hours of Librispeech on 16kHz sampled speech audio. It is designed for automatic speech recognition and can transcribe audio files.\"}', metadata={})]", "category": "generic"}
{"question_id": 530, "text": " Our app offers assistance to people with hearing problems by enhancing the clarity of speech. We need a feature to clean and enhance the audio.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 531, "text": " Find a way to separate the background music and vocal from an audio file.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'MCG-NJU/videomae-base-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-base-finetuned-ssv2\\')\", \\'api_arguments\\': \\'video\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nprocessor = VideoMAEImageProcessor.from_pretrained(MCG-NJU/videomae-base-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-base-finetuned-ssv2)\\\\ninputs = processor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n    logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something-v2\\', \\'accuracy\\': {\\'top-1\\': 70.6, \\'top-5\\': 92.6}}, \\'description\\': \\'VideoMAE model pre-trained for 2400 epochs in a self-supervised way and fine-tuned in a supervised way on Something-Something-v2. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository.\\'}', metadata={})]", "category": "generic"}
{"question_id": 532, "text": " An educational software company needs an advanced method to separate voices from background noise. Develop a solution.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Classification\\', \\'api_name\\': \\'microsoft/swinv2-tiny-patch4-window8-256\\', \\'api_call\\': \"AutoModelForImageClassification.from_pretrained(\\'microsoft/swinv2-tiny-patch4-window8-256\\')\", \\'api_arguments\\': {\\'image\\': \\'http://images.cocodataset.org/val2017/000000039769.jpg\\'}, \\'python_environment_requirements\\': [\\'transformers\\', \\'PIL\\', \\'requests\\'], \\'example_code\\': \\'from transformers import AutoImageProcessor, AutoModelForImageClassification\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nprocessor = AutoImageProcessor.from_pretrained(microsoft/swinv2-tiny-patch4-window8-256)\\\\nmodel = AutoModelForImageClassification.from_pretrained(microsoft/swinv2-tiny-patch4-window8-256)\\\\ninputs = processor(images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'imagenet-1k\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Swin Transformer v2 model pre-trained on ImageNet-1k at resolution 256x256. It was introduced in the paper Swin Transformer V2: Scaling Up Capacity and Resolution by Liu et al. and first released in this repository. The Swin Transformer is a type of Vision Transformer. It builds hierarchical feature maps by merging image patches in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window. Swin Transformer v2 adds 3 main improvements: 1) a residual-post-norm method combined with cosine attention to improve training stability; 2) a log-spaced continuous position bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with high-resolution inputs; 3) a self-supervised pre-training method, SimMIM, to reduce the needs of vast labeled images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 533, "text": " Develop a voice command security system that distinguishes between specific command phrases like \\\"disarm security\\\" or \\\"activate alarm\\\".\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Voice Activity Detection\\', \\'framework\\': \\'pyannote.audio\\', \\'functionality\\': \\'Automatic Speech Recognition\\', \\'api_name\\': \\'pyannote/voice-activity-detection\\', \\'api_call\\': \"Pipeline.from_pretrained(\\'pyannote/voice-activity-detection\\')\", \\'api_arguments\\': [\\'audio.wav\\'], \\'python_environment_requirements\\': [\\'pyannote.audio 2.1\\'], \\'example_code\\': \\'from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained(pyannote/voice-activity-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\\\noutput = pipeline(audio.wav)\\\\nfor speech in output.get_timeline().support():\\\\n  # active speech between speech.start and speech.end\\', \\'performance\\': {\\'dataset\\': \\'ami\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'A pretrained voice activity detection pipeline that detects active speech in audio files.\\'}', metadata={})]", "category": "generic"}
{"question_id": 534, "text": " I have an audio recording that I want to analyze to understand the emotion of the speaker.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \"PegasusForConditionalGeneration.from_pretrained(\\'tuner007/pegasus_summarizer\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 535, "text": " We are working on an assistant that can help tourists translate spoken language from one language to another. Let's build a solution for Hokkien to English translation.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 536, "text": " Trying to implement an assistant that detects user's emotions on a given audio file to help them manage their emotions better.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'michellejieli/emotion_text_classifier\\', \\'api_call\\': \"pipeline(\\'sentiment-analysis\\', model=\\'michellejieli/emotion_text_classifier\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'classifier(I love this!)\\', \\'performance\\': {\\'dataset\\': [\\'Crowdflower (2016)\\', \\'Emotion Dataset, Elvis et al. (2018)\\', \\'GoEmotions, Demszky et al. (2020)\\', \\'ISEAR, Vikash (2018)\\', \\'MELD, Poria et al. (2019)\\', \\'SemEval-2018, EI-reg, Mohammad et al. (2018)\\', \\'Emotion Lines (Friends)\\'], \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'DistilRoBERTa-base is a transformer model that performs sentiment analysis. I fine-tuned the model on transcripts from the Friends show with the goal of classifying emotions from text data, specifically dialogue from Netflix shows or movies. The model predicts 6 Ekman emotions and a neutral class. These emotions include anger, disgust, fear, joy, neutrality, sadness, and surprise.\\'}', metadata={})]", "category": "generic"}
{"question_id": 537, "text": " We are building a customer service analytics software. The software should recognize the voice of the customers.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 538, "text": " We are developing a voice-controlled device. Help categorize spoken commands into specific keywords to trigger various actions.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 539, "text": " Develop an emotion analysis system to understand customer satisfaction over the phone for a telecommunication company in Russia.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Information Retrieval\\', \\'api_name\\': \\'cross-encoder/ms-marco-TinyBERT-L-2-v2\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'cross-encoder/ms-marco-TinyBERT-L-2-v2\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\"}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': {\\'import\\': \\'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\', \\'model\\': \"model = AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\", \\'scores\\': \\'with torch.no_grad():\\\\n    scores = model(**features).logits\\\\n    print(scores)\\'}, \\'performance\\': {\\'dataset\\': \\'TREC Deep Learning 2019\\', \\'accuracy\\': \\'69.84 (NDCG@10)\\'}, \\'description\\': \\'This model was trained on the MS Marco Passage Ranking task. It can be used for Information Retrieval: Given a query, encode the query with all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. The training code is available here: SBERT.net Training MS Marco.\\'}', metadata={})]", "category": "generic"}
{"question_id": 540, "text": " We are a company that provides transcription services. We require voice activity detection in our audio recordings.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 541, "text": " We need to predict the survival of passengers on the Titanic based on certain demographics like age, gender, etc.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Classification\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Binary Classification\\', \\'api_name\\': \\'harithapliyal/autotrain-tatanic-survival-51030121311\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'harithapliyal/autotrain-tatanic-survival-51030121311\\')\", \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'harithapliyal/autotrain-data-tatanic-survival\\', \\'accuracy\\': 0.872}, \\'description\\': \\'A tabular classification model trained on the Titanic survival dataset using Hugging Face AutoTrain. The model predicts whether a passenger survived or not based on features such as age, gender, and passenger class.\\'}', metadata={})]", "category": "generic"}
{"question_id": 542, "text": " Implement a system that predicts the carbon emissions in a given dataset.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Regression\\', \\'framework\\': \\'Joblib\\', \\'functionality\\': \\'Carbon Emissions\\', \\'api_name\\': \\'pcoloc/autotrain-600-dragino-1839063122\\', \\'api_call\\': \"joblib.load(\\'model.joblib\\')\", \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'pcoloc/autotrain-data-600-dragino\\', \\'accuracy\\': {\\'Loss\\': 93.595, \\'R2\\': 0.502, \\'MSE\\': 8760.052, \\'MAE\\': 77.527, \\'RMSLE\\': 0.445}}, \\'description\\': \\'This model is trained to perform single column regression on carbon emissions data using the AutoTrain framework. It predicts CO2 emissions in grams given the input data.\\'}', metadata={})]", "category": "generic"}
{"question_id": 543, "text": " Predict the carbon emissions of several power plants based on their characteristics.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Regression\\', \\'framework\\': \\'Joblib\\', \\'functionality\\': \\'Carbon Emissions\\', \\'api_name\\': \\'pcoloc/autotrain-600-dragino-1839063122\\', \\'api_call\\': \"joblib.load(\\'model.joblib\\')\", \\'api_arguments\\': [\\'data\\'], \\'python_environment_requirements\\': [\\'joblib\\', \\'pandas\\'], \\'example_code\\': \"import json\\\\nimport joblib\\\\nimport pandas as pd\\\\nmodel = joblib.load(\\'model.joblib\\')\\\\nconfig = json.load(open(\\'config.json\\'))\\\\nfeatures = config[\\'features\\']\\\\ndata = pd.read_csv(\\'data.csv\\')\\\\ndata = data[features]\\\\ndata.columns = [\\'feat_\\' + str(col) for col in data.columns]\\\\npredictions = model.predict(data)\", \\'performance\\': {\\'dataset\\': \\'pcoloc/autotrain-data-600-dragino\\', \\'accuracy\\': {\\'Loss\\': 93.595, \\'R2\\': 0.502, \\'MSE\\': 8760.052, \\'MAE\\': 77.527, \\'RMSLE\\': 0.445}}, \\'description\\': \\'This model is trained to perform single column regression on carbon emissions data using the AutoTrain framework. It predicts CO2 emissions in grams given the input data.\\'}', metadata={})]", "category": "generic"}
{"question_id": 544, "text": " Our robotics team needs a way to stabilize a two-wheeled self-balancing robot. Would you suggest and apply any RL algorithm for this purpose?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'MCG-NJU/videomae-base-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-base-finetuned-ssv2\\')\", \\'api_arguments\\': \\'video\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\\\\nimport numpy as np\\\\nimport torch\\\\nvideo = list(np.random.randn(16, 3, 224, 224))\\\\nprocessor = VideoMAEImageProcessor.from_pretrained(MCG-NJU/videomae-base-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-base-finetuned-ssv2)\\\\ninputs = processor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\n    logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something-v2\\', \\'accuracy\\': {\\'top-1\\': 70.6, \\'top-5\\': 92.6}}, \\'description\\': \\'VideoMAE model pre-trained for 2400 epochs in a self-supervised way and fine-tuned in a supervised way on Something-Something-v2. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository.\\'}', metadata={})]", "category": "generic"}
{"question_id": 545, "text": " We have a large text dataset and want to extract some important features from it for our data analysis.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 546, "text": " I'm working on a biomedical research project and need to extract features from a set of entity names to help me understand their relationships.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Zero-Shot Image Classification\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Zero-Shot Image Classification\\', \\'api_name\\': \\'microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224\\', \\'api_call\\': \"pipeline(\\'zero-shot-image-classification\\', model=\\'microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224\\')\", \\'api_arguments\\': \\'image, possible_class_names\\', \\'python_environment_requirements\\': \\'transformers, torch, torchvision\\', \\'example_code\\': \"from transformers import pipeline\\\\nclip = pipeline(\\'zero-shot-image-classification\\', model=\\'microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224\\')\\\\nimage = \\'path/to/image.png\\'\\\\npossible_class_names = [\\'class1\\', \\'class2\\', \\'class3\\']\\\\nresult = clip(image, possible_class_names)\", \\'performance\\': {\\'dataset\\': \\'PMC-15M\\', \\'accuracy\\': \\'State of the art\\'}, \\'description\\': \\'BiomedCLIP is a biomedical vision-language foundation model pretrained on PMC-15M, a dataset of 15 million figure-caption pairs extracted from biomedical research articles in PubMed Central, using contrastive learning. It uses PubMedBERT as the text encoder and Vision Transformer as the image encoder, with domain-specific adaptations. It can perform various vision-language processing (VLP) tasks such as cross-modal retrieval, image classification, and visual question answering.\\'}', metadata={})]", "category": "generic"}
{"question_id": 547, "text": " Our company is working on an app that allows music producers to detect beats in a sample. We want to use the Hubert-large-ll60k model for this.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text-generation\\', \\'api_name\\': \\'pygmalion-2.7b\\', \\'api_call\\': \"pipeline(\\'text-generation\\', model=\\'PygmalionAI/pygmalion-2.7b\\')\", \\'api_arguments\\': {\\'input_prompt\\': \\'string\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'latest\\'}, \\'example_code\\': \"model([CHARACTER]\\'s Persona: [A few sentences about the character you want the model to play]<START>[DIALOGUE HISTORY]You: [Your input message here][CHARACTER]:)\", \\'performance\\': {\\'dataset\\': \\'56MB of dialogue data\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \"Pygmalion 2.7B is a proof-of-concept dialogue model based on EleutherAI\\'s gpt-neo-2.7B. It is fine-tuned on 56MB of dialogue data gathered from multiple sources, including real and partially machine-generated conversations. The model is intended for use in generating conversational responses and can be used with a specific input format that includes character persona, dialogue history, and user input message.\"}', metadata={})]", "category": "generic"}
{"question_id": 548, "text": " As an art director, generating ideas from descriptions can be difficult. Produce an image of a serene lake at sunset.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'prompthero/openjourney\\', \\'api_call\\': \"StableDiffusionPipeline.from_pretrained(\\'prompthero/openjourney\\', torch_dtype=torch.float16)\", \\'api_arguments\\': {\\'prompt\\': \\'string\\'}, \\'python_environment_requirements\\': [\\'diffusers\\', \\'torch\\'], \\'example_code\\': \\'from diffusers import StableDiffusionPipeline\\\\nimport torch\\\\nmodel_id = prompthero/openjourney\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = retro serie of different cars with different colors and shapes, mdjrny-v4 style\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(./retro_cars.png)\\', \\'performance\\': {\\'dataset\\': \\'Midjourney images\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Openjourney is an open source Stable Diffusion fine-tuned model on Midjourney images, by PromptHero. It can be used for generating AI art based on text prompts.\\'}', metadata={})]", "category": "generic"}
{"question_id": 549, "text": " There is a new small restaurant opening and they need a store sign featuring a kangaroo eating pizza.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 550, "text": " Design a promotional poster for a new line of summer clothing featuring people wearing the clothes with a beach background.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Information Retrieval\\', \\'api_name\\': \\'cross-encoder/ms-marco-TinyBERT-L-2-v2\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'cross-encoder/ms-marco-TinyBERT-L-2-v2\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\"}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': {\\'import\\': \\'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\', \\'model\\': \"model = AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\", \\'scores\\': \\'with torch.no_grad():\\\\n    scores = model(**features).logits\\\\n    print(scores)\\'}, \\'performance\\': {\\'dataset\\': \\'TREC Deep Learning 2019\\', \\'accuracy\\': \\'69.84 (NDCG@10)\\'}, \\'description\\': \\'This model was trained on the MS Marco Passage Ranking task. It can be used for Information Retrieval: Given a query, encode the query with all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. The training code is available here: SBERT.net Training MS Marco.\\'}', metadata={})]", "category": "generic"}
{"question_id": 551, "text": " We are developing an application for Japanese language learners. We need to extract text from Japanese manga pages for our users.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 552, "text": " I am a museum curator, I need to learn more about an artwork in my archive so I can create better exhibition materials.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 553, "text": " We want an application that can answer questions about an image. For example, how many people are in this photo?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Information Retrieval\\', \\'api_name\\': \\'cross-encoder/ms-marco-TinyBERT-L-2-v2\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'cross-encoder/ms-marco-TinyBERT-L-2-v2\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\"}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': {\\'import\\': \\'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\', \\'model\\': \"model = AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\", \\'scores\\': \\'with torch.no_grad():\\\\n    scores = model(**features).logits\\\\n    print(scores)\\'}, \\'performance\\': {\\'dataset\\': \\'TREC Deep Learning 2019\\', \\'accuracy\\': \\'69.84 (NDCG@10)\\'}, \\'description\\': \\'This model was trained on the MS Marco Passage Ranking task. It can be used for Information Retrieval: Given a query, encode the query with all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. The training code is available here: SBERT.net Training MS Marco.\\'}', metadata={})]", "category": "generic"}
{"question_id": 554, "text": " I am developing a home security software which can detect intruders entering the house. In case any door or object is tampered, the application will ask, \\\"Who entered the room?\\\", to assist quick analysis of the CCTV recordings.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Object Detection\\', \\'api_name\\': \\'hustvl/yolos-tiny\\', \\'api_call\\': \"YolosForObjectDetection.from_pretrained(\\'hustvl/yolos-tiny\\')\", \\'api_arguments\\': {\\'images\\': \\'image\\', \\'return_tensors\\': \\'pt\\'}, \\'python_environment_requirements\\': [\\'transformers\\', \\'PIL\\', \\'requests\\'], \\'example_code\\': \"from transformers import YolosFeatureExtractor, YolosForObjectDetection\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = \\'http://images.cocodataset.org/val2017/000000039769.jpg\\'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nfeature_extractor = YolosFeatureExtractor.from_pretrained(\\'hustvl/yolos-tiny\\')\\\\nmodel = YolosForObjectDetection.from_pretrained(\\'hustvl/yolos-tiny\\')\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\\nbboxes = outputs.pred_boxes\", \\'performance\\': {\\'dataset\\': \\'COCO 2017 validation\\', \\'accuracy\\': \\'28.7 AP\\'}, \\'description\\': \\'YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN). The model is trained using a bipartite matching loss: one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a no object as class and no bounding box as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 555, "text": " Our business is growing rapidly, and we've received an increasing number of questions related to product images. We need the model to provide answers based on images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 556, "text": " Our company receives invoices in different formats. We need to extract specific information from these documents to process payments and keep records.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 557, "text": " Design an AI algorithm to answer questions from scanned documents.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Visual Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'vilt-finetuned-vqasi\\', \\'api_call\\': \"ViltModel.from_pretrained(\\'tufa15nik/vilt-finetuned-vqasi\\')\", \\'api_arguments\\': {\\'model\\': \\'tufa15nik/vilt-finetuned-vqasi\\', \\'tokenizer\\': \\'tufa15nik/vilt-finetuned-vqasi\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'>=4.11.3\\'}, \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A Visual Question Answering model fine-tuned on the VQASI dataset by tufa15nik using the ViLT architecture. The model is designed to answer questions based on the content of an input image.\\'}', metadata={})]", "category": "generic"}
{"question_id": 558, "text": " I want to analyze the text and images in a document and extract answers to questions based on the content.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023\\', \\'api_call\\': \"AutoModelForDocumentQuestionAnswering.from_pretrained(\\'L-oenai/LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023\\')\", \\'api_arguments\\': {}, \\'python_environment_requirements\\': {\\'transformers\\': \\'>=4.11.0\\'}, \\'example_code\\': {}, \\'performance\\': {\\'dataset\\': {}, \\'accuracy\\': {}}, \\'description\\': \\'A document question answering model based on LayoutLMv2, which can be used to extract answers from images with text and layout information.\\'}', metadata={})]", "category": "generic"}
{"question_id": 559, "text": " Locate specific information from an invoice image, such as total amount due, invoice number, and due date.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Document Question Answering\\', \\'api_name\\': \\'CQI_Visual_Question_Awnser_PT_v0\\', \\'api_call\\': \"pipeline(\\'question-answering\\', model=LayoutLMForQuestionAnswering.from_pretrained(\\'microsoft/layoutlm-base-uncased\\'))\", \\'api_arguments\\': [\\'url\\', \\'question\\'], \\'python_environment_requirements\\': [\\'PIL\\', \\'pytesseract\\', \\'PyTorch\\', \\'transformers\\'], \\'example_code\\': [\"nlp(\\'https://templates.invoicehome.com/invoice-template-us-neat-750px.png\\', \\'What is the invoice number?\\')\", \"nlp(\\'https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg\\', \\'What is the purchase amount?\\')\", \"nlp(\\'https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png\\', \\'What are the 2020 net sales?\\')\"], \\'performance\\': {\\'dataset\\': [{\\'accuracy\\': 0.9943977}, {\\'accuracy\\': 0.9912159}, {\\'accuracy\\': 0.59147286}]}, \\'description\\': \\'A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions.\\'}', metadata={})]", "category": "generic"}
{"question_id": 560, "text": " The company director needs a summary of a recent financial report. You should provide the answer of following question: What were the total revenues for the last quarter?\\n###Input: In the last quarter, the company's total revenues were reported at $3.2 million with a gross profit of $1.5 million. The operating expenses during the same quarter were $1 million.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'csebuetnlp/mT5_multilingual_XLSum\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'csebuetnlp/mT5_multilingual_XLSum\\')\", \\'api_arguments\\': [\\'model_name\\'], \\'python_environment_requirements\\': [\\'transformers==4.11.0.dev0\\'], \\'example_code\\': \"import re\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nWHITESPACE_HANDLER = lambda k: re.sub(\\'\\\\\\\\s+\\', \\' \\', re.sub(\\'\\\\\\\\n+\\', \\' \\', k.strip()))\\\\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people\\'s scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We\\'re expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\\\\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\\\ninput_ids = tokenizer(\\\\n [WHITESPACE_HANDLER(article_text)],\\\\n return_tensors=pt,\\\\n padding=max_length,\\\\n truncation=True,\\\\n max_length=512\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n max_length=84,\\\\n no_repeat_ngram_size=2,\\\\n num_beams=4\\\\n)[0]\\\\nsummary = tokenizer.decode(\\\\n output_ids,\\\\n skip_special_tokens=True,\\\\n clean_up_tokenization_spaces=False\\\\n)\\\\nprint(summary)\", \\'performance\\': {\\'dataset\\': \\'xsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.5, \\'ROUGE-2\\': 13.934, \\'ROUGE-L\\': 28.988, \\'ROUGE-LSUM\\': 28.996, \\'loss\\': 2.067, \\'gen_len\\': 26.973}}, \\'description\\': \\'This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 561, "text": " You must create a tool to assess safety risks automatically in construction sites by estimating the distance of tools and workers.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Object Detection\\', \\'api_name\\': \\'keremberke/yolov8m-hard-hat-detection\\', \\'api_call\\': \"YOLO(\\'keremberke/yolov8m-hard-hat-detection\\')\", \\'api_arguments\\': {\\'image\\': \\'URL or local path to the image\\'}, \\'python_environment_requirements\\': [\\'ultralyticsplus==0.0.24\\', \\'ultralytics==8.0.23\\'], \\'example_code\\': \"from ultralyticsplus import YOLO, render_result\\\\n\\\\nmodel = YOLO(\\'keremberke/yolov8m-hard-hat-detection\\')\\\\n\\\\nmodel.overrides[\\'conf\\'] = 0.25\\\\nmodel.overrides[\\'iou\\'] = 0.45\\\\nmodel.overrides[\\'agnostic_nms\\'] = False\\\\nmodel.overrides[\\'max_det\\'] = 1000\\\\n\\\\nimage = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\\\\n\\\\nresults = model.predict(image)\\\\n\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\", \\'performance\\': {\\'dataset\\': \\'hard-hat-detection\\', \\'accuracy\\': 0.811}, \\'description\\': \"A YOLOv8 model for detecting hard hats in images. The model can distinguish between \\'Hardhat\\' and \\'NO-Hardhat\\' classes. It can be used to ensure safety compliance in construction sites or other industrial environments where hard hats are required.\"}', metadata={})]", "category": "generic"}
{"question_id": 562, "text": " Develop a solution to estimate the depth of objects in an image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Information Retrieval\\', \\'api_name\\': \\'cross-encoder/ms-marco-TinyBERT-L-2-v2\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'cross-encoder/ms-marco-TinyBERT-L-2-v2\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\"}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': {\\'import\\': \\'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\', \\'model\\': \"model = AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\", \\'scores\\': \\'with torch.no_grad():\\\\n    scores = model(**features).logits\\\\n    print(scores)\\'}, \\'performance\\': {\\'dataset\\': \\'TREC Deep Learning 2019\\', \\'accuracy\\': \\'69.84 (NDCG@10)\\'}, \\'description\\': \\'This model was trained on the MS Marco Passage Ranking task. It can be used for Information Retrieval: Given a query, encode the query with all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. The training code is available here: SBERT.net Training MS Marco.\\'}', metadata={})]", "category": "generic"}
{"question_id": 563, "text": " We need to analyze some pictures from nature and classify them to protect some species of animals.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Paraphrasing\\', \\'api_name\\': \\'prithivida/parrot_paraphraser_on_T5\\', \\'api_call\\': \"Parrot(model_tag=\\'prithivida/parrot_paraphraser_on_T5\\', use_gpu=False)\", \\'api_arguments\\': [\\'input_phrase\\', \\'diversity_ranker\\', \\'do_diverse\\', \\'max_return_phrases\\', \\'max_length\\', \\'adequacy_threshold\\', \\'fluency_threshold\\'], \\'python_environment_requirements\\': [\\'torch\\', \\'transformers\\'], \\'example_code\\': \\'from parrot import Parrot\\\\nimport torch\\\\nimport warnings\\\\nwarnings.filterwarnings(ignore)\\\\n\\\\nparrot = Parrot(model_tag=prithivida/parrot_paraphraser_on_T5, use_gpu=False)\\\\nphrases = [Can you recommed some upscale restaurants in Newyork?,\\\\n What are the famous places we should not miss in Russia?\\\\n]\\\\nfor phrase in phrases:\\\\n print(-*100)\\\\n print(Input_phrase: , phrase)\\\\n print(-*100)\\\\n para_phrases = parrot.augment(input_phrase=phrase)\\\\n for para_phrase in para_phrases:\\\\n  print(para_phrase)\\', \\'performance\\': {\\'dataset\\': \\'Not mentioned\\', \\'accuracy\\': \\'Not mentioned\\'}, \\'description\\': \\'Parrot is a paraphrase based utterance augmentation framework purpose built to accelerate training NLU models. It offers knobs to control Adequacy, Fluency, and Diversity as per your needs. It mainly focuses on augmenting texts typed-into or spoken-to conversational interfaces for building robust NLU models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 564, "text": " We need to detect objects in an image provided by a user and describe their locations.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Object Detection\\', \\'api_name\\': \\'fcakyon/yolov5s-v7.0\\', \\'api_call\\': \"yolov5.load(\\'fcakyon/yolov5s-v7.0\\')\", \\'api_arguments\\': {\\'conf\\': 0.25, \\'iou\\': 0.45, \\'agnostic\\': False, \\'multi_label\\': False, \\'max_det\\': 1000, \\'img\\': \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\', \\'size\\': 640, \\'augment\\': True}, \\'python_environment_requirements\\': \\'pip install -U yolov5\\', \\'example_code\\': \"import yolov5\\\\nmodel = yolov5.load(\\'fcakyon/yolov5s-v7.0\\')\\\\nmodel.conf = 0.25\\\\nmodel.iou = 0.45\\\\nmodel.agnostic = False\\\\nmodel.multi_label = False\\\\nmodel.max_det = 1000\\\\nimg = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\\\\nresults = model(img)\\\\nresults = model(img, size=640)\\\\nresults = model(img, augment=True)\\\\npredictions = results.pred[0]\\\\nboxes = predictions[:, :4]\\\\nscores = predictions[:, 4]\\\\ncategories = predictions[:, 5]\\\\nresults.show()\\\\nresults.save(save_dir=\\'results/\\')\", \\'performance\\': {\\'dataset\\': \\'detection-datasets/coco\\', \\'accuracy\\': None}, \\'description\\': \\'Yolov5s-v7.0 is an object detection model trained on the COCO dataset. It can detect objects in images and return their bounding boxes, scores, and categories.\\'}', metadata={})]", "category": "generic"}
{"question_id": 565, "text": " We are creating an analysis platform for Counter-Strike: Global Offensive. Detect and locate players in the given image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 566, "text": " Build an AI model that can recognize the objects in a given image and draw a boundary around them.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'promptcap-coco-vqa\\', \\'api_call\\': \"PromptCap(\\'vqascore/promptcap-coco-vqa\\')\", \\'api_arguments\\': {\\'prompt\\': \\'string\\', \\'image\\': \\'string\\'}, \\'python_environment_requirements\\': \\'pip install promptcap\\', \\'example_code\\': [\\'import torch\\', \\'from promptcap import PromptCap\\', \\'model = PromptCap(vqascore/promptcap-coco-vqa)\\', \\'if torch.cuda.is_available():\\', \\'  model.cuda()\\', \\'prompt = please describe this image according to the given question: what piece of clothing is this boy putting on?\\', \\'image = glove_boy.jpeg\\', \\'print(model.caption(prompt, image))\\'], \\'performance\\': {\\'dataset\\': {\\'coco\\': {\\'accuracy\\': \\'150 CIDEr\\'}, \\'OK-VQA\\': {\\'accuracy\\': \\'60.4%\\'}, \\'A-OKVQA\\': {\\'accuracy\\': \\'59.6%\\'}}}, \\'description\\': \\'PromptCap is a captioning model that can be controlled by natural language instruction. The instruction may contain a question that the user is interested in. It achieves SOTA performance on COCO captioning (150 CIDEr) and knowledge-based VQA tasks when paired with GPT-3 (60.4% on OK-VQA and 59.6% on A-OKVQA).\\'}', metadata={})]", "category": "generic"}
{"question_id": 567, "text": " Design a pipeline to create artistic variations of an input image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Image Variations\\', \\'api_name\\': \\'lambdalabs/sd-image-variations-diffusers\\', \\'api_call\\': \"StableDiffusionImageVariationPipeline.from_pretrained(\\'lambdalabs/sd-image-variations-diffusers\\', revision=\\'v2.0\\')\", \\'api_arguments\\': {\\'revision\\': \\'v2.0\\'}, \\'python_environment_requirements\\': \\'Diffusers >=0.8.0\\', \\'example_code\\': \\'from diffusers import StableDiffusionImageVariationPipeline\\\\nfrom PIL import Image\\\\ndevice = cuda:0\\\\nsd_pipe = StableDiffusionImageVariationPipeline.from_pretrained(\\\\n lambdalabs/sd-image-variations-diffusers,\\\\n revision=v2.0,\\\\n)\\\\nsd_pipe = sd_pipe.to(device)\\\\nim = Image.open(path/to/image.jpg)\\\\ntform = transforms.Compose([\\\\n transforms.ToTensor(),\\\\n transforms.Resize(\\\\n  (224, 224),\\\\n  interpolation=transforms.InterpolationMode.BICUBIC,\\\\n  antialias=False,\\\\n ),\\\\n transforms.Normalize(\\\\n  [0.48145466, 0.4578275, 0.40821073],\\\\n  [0.26862954, 0.26130258, 0.27577711]),\\\\n])\\\\ninp = tform(im).to(device).unsqueeze(0)\\\\nout = sd_pipe(inp, guidance_scale=3)\\\\nout[images][0].save(result.jpg)\\', \\'performance\\': {\\'dataset\\': \\'ChristophSchuhmann/improved_aesthetics_6plus\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\\'}', metadata={})]", "category": "generic"}
{"question_id": 568, "text": " A user wants to improve the quality of a small sized picture from her vacation. We need to upscale the picture to 2x its size without losing quality.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Feature Extraction\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'facebook/dragon-plus-context-encoder\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'facebook/dragon-plus-context-encoder\\')\", \\'api_arguments\\': [\\'pretrained\\'], \\'python_environment_requirements\\': [\\'torch\\', \\'transformers\\'], \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModel\\\\ntokenizer = AutoTokenizer.from_pretrained(\\'facebook/dragon-plus-query-encoder\\')\\\\nquery_encoder = AutoModel.from_pretrained(\\'facebook/dragon-plus-query-encoder\\')\\\\ncontext_encoder = AutoModel.from_pretrained(\\'facebook/dragon-plus-context-encoder\\')\\\\nquery = \\'Where was Marie Curie born?\\'\\\\ncontexts = [\\\\n  \\'Maria Sklodowska, later known as Marie Curie, was born on November 7, 1867.\\',\\\\n  \\'Born in Paris on 15 May 1859, Pierre Curie was the son of Eug\u00e8ne Curie, a doctor of French Catholic origin from Alsace.\\'\\\\n]\\\\nquery_input = tokenizer(query, return_tensors=\\'pt\\')\\\\nctx_input = tokenizer(contexts, padding=True, truncation=True, return_tensors=\\'pt\\')\\\\nquery_emb = query_encoder(query_input).last_hidden_state[:, 0, :]\\\\nctx_emb = context_encoder(ctx_input).last_hidden_state[:, 0, :]\\\\nscore1 = query_emb @ ctx_emb[0]\\\\nscore2 = query_emb @ ctx_emb[1]\", \\'performance\\': {\\'dataset\\': \\'MS MARCO\\', \\'accuracy\\': 39.0}, \\'description\\': \\'DRAGON+ is a BERT-base sized dense retriever initialized from RetroMAE and further trained on the data augmented from MS MARCO corpus, following the approach described in How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval. The associated GitHub repository is available here https://github.com/facebookresearch/dpr-scale/tree/main/dragon. We use asymmetric dual encoder, with two distinctly parameterized encoders.\\'}', metadata={})]", "category": "generic"}
{"question_id": 569, "text": " I want to create a visual representation based on a short description: \\\"A magical forest with unicorns and a rainbow.\\\".\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'gsdf/Counterfeit-V2.5\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'gsdf/Counterfeit-V2.5\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'((masterpiece,best quality)),1girl, solo, animal ears, rabbit, barefoot, knees up, dress, sitting, rabbit ears, short sleeves, looking at viewer, grass, short hair, smile, white hair, puffy sleeves, outdoors, puffy short sleeves, bangs, on ground, full body, animal, white dress, sunlight, brown eyes, dappled sunlight, day, depth of field\\', \\'performance\\': {\\'dataset\\': \\'EasyNegative\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 570, "text": " Explain how to use GPT-3 to create a slogan for an e-commerce website that sells eco-friendly products.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 571, "text": " I'm working on a project that requires generating high-quality images of faces for a set of characters in a video game.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image Generation\\', \\'api_name\\': \\'stabilityai/stable-diffusion-2-1\\', \\'api_call\\': \"StableDiffusionPipeline.from_pretrained(\\'stabilityai/stable-diffusion-2-1\\', torch_dtype=torch.float16)\", \\'api_arguments\\': {\\'prompt\\': \\'a photo of an astronaut riding a horse on mars\\'}, \\'python_environment_requirements\\': [\\'diffusers\\', \\'transformers\\', \\'accelerate\\', \\'scipy\\', \\'safetensors\\'], \\'example_code\\': \\'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\\\nmodel_id = stabilityai/stable-diffusion-2-1\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe = pipe.to(cuda)\\\\nprompt = a photo of an astronaut riding a horse on mars\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(astronaut_rides_horse.png)\\', \\'performance\\': {\\'dataset\\': \\'COCO2017\\', \\'accuracy\\': \\'Not optimized for FID scores\\'}, \\'description\\': \\'Stable Diffusion v2-1 is a diffusion-based text-to-image generation model developed by Robin Rombach and Patrick Esser. It is capable of generating and modifying images based on text prompts in English. The model is trained on a subset of the LAION-5B dataset and is primarily intended for research purposes.\\'}', metadata={})]", "category": "generic"}
{"question_id": 572, "text": " Our client wants to analyze videos for their marketing website. They need a quick solution to categorize video content without worrying about accuracy.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'csebuetnlp/mT5_multilingual_XLSum\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'csebuetnlp/mT5_multilingual_XLSum\\')\", \\'api_arguments\\': [\\'model_name\\'], \\'python_environment_requirements\\': [\\'transformers==4.11.0.dev0\\'], \\'example_code\\': \"import re\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nWHITESPACE_HANDLER = lambda k: re.sub(\\'\\\\\\\\s+\\', \\' \\', re.sub(\\'\\\\\\\\n+\\', \\' \\', k.strip()))\\\\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people\\'s scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We\\'re expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\\\\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\\\ninput_ids = tokenizer(\\\\n [WHITESPACE_HANDLER(article_text)],\\\\n return_tensors=pt,\\\\n padding=max_length,\\\\n truncation=True,\\\\n max_length=512\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n max_length=84,\\\\n no_repeat_ngram_size=2,\\\\n num_beams=4\\\\n)[0]\\\\nsummary = tokenizer.decode(\\\\n output_ids,\\\\n skip_special_tokens=True,\\\\n clean_up_tokenization_spaces=False\\\\n)\\\\nprint(summary)\", \\'performance\\': {\\'dataset\\': \\'xsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.5, \\'ROUGE-2\\': 13.934, \\'ROUGE-L\\': 28.988, \\'ROUGE-LSUM\\': 28.996, \\'loss\\': 2.067, \\'gen_len\\': 26.973}}, \\'description\\': \\'This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 573, "text": " I want to classify images of vehicles including cars, motorcycles, trucks, and bicycles, based on their appearance.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text-generation\\', \\'api_name\\': \\'pygmalion-2.7b\\', \\'api_call\\': \"pipeline(\\'text-generation\\', model=\\'PygmalionAI/pygmalion-2.7b\\')\", \\'api_arguments\\': {\\'input_prompt\\': \\'string\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'latest\\'}, \\'example_code\\': \"model([CHARACTER]\\'s Persona: [A few sentences about the character you want the model to play]<START>[DIALOGUE HISTORY]You: [Your input message here][CHARACTER]:)\", \\'performance\\': {\\'dataset\\': \\'56MB of dialogue data\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \"Pygmalion 2.7B is a proof-of-concept dialogue model based on EleutherAI\\'s gpt-neo-2.7B. It is fine-tuned on 56MB of dialogue data gathered from multiple sources, including real and partially machine-generated conversations. The model is intended for use in generating conversational responses and can be used with a specific input format that includes character persona, dialogue history, and user input message.\"}', metadata={})]", "category": "generic"}
{"question_id": 574, "text": " I need to classify images of animals into their specific categories.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Zero-Shot Image Classification\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Zero-Shot Image Classification\\', \\'api_name\\': \\'laion/CLIP-ViT-g-14-laion2B-s34B-b88K\\', \\'api_call\\': \"pipeline(\\'zero-shot-image-classification\\', model=\\'laion/CLIP-ViT-g-14-laion2B-s34B-b88K\\')\", \\'api_arguments\\': {\\'image\\': \\'path/to/image/file\\', \\'class_names\\': \\'list_of_class_names\\'}, \\'python_environment_requirements\\': {\\'huggingface_hub\\': \\'0.0.17\\', \\'transformers\\': \\'4.11.3\\', \\'torch\\': \\'1.9.0\\', \\'torchvision\\': \\'0.10.0\\'}, \\'example_code\\': None, \\'performance\\': {\\'dataset\\': None, \\'accuracy\\': None}, \\'description\\': \\'A zero-shot image classification model based on OpenCLIP, which can classify images into various categories without requiring any training data for those categories.\\'}', metadata={})]", "category": "generic"}
{"question_id": 575, "text": " We are building a location recommendation system that identifies possible locations for new stores based on images from potential locations. Use the StreetCLIP model to generate probabilities for various cities.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Automatic Speech Recognition\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli\\', \\'api_call\\': \"Wav2Vec2ForCTC.from_pretrained(\\'jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'https://github.com/neonbjb/ocotillo\\', \\'performance\\': {\\'dataset\\': \\'librispeech validation set\\', \\'accuracy\\': \\'4.45%\\'}, \\'description\\': \\'This checkpoint is a wav2vec2-large model that is useful for generating transcriptions with punctuation. It is intended for use in building transcriptions for TTS models, where punctuation is very important for prosody. This model was created by fine-tuning the facebook/wav2vec2-large-robust-ft-libri-960h checkpoint on the libritts and voxpopuli datasets with a new vocabulary that includes punctuation.\\'}', metadata={})]", "category": "generic"}
{"question_id": 576, "text": " We are receiving mixed reviews about our new product and we need to understand customers' reactions.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 577, "text": " Detect if there are any harmful messages in a chat room.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 578, "text": " I am developing software that needs to retrieve relevant information from a collection of documents based on a user's query.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 579, "text": " Our client is a media organization that wants to extract named entities from a large collection of news articles in order to identify key people and places\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Token Classification\\', \\'framework\\': \\'Flair\\', \\'functionality\\': \\'Named Entity Recognition\\', \\'api_name\\': \\'flair/ner-english-ontonotes-large\\', \\'api_call\\': \"SequenceTagger.load(\\'flair/ner-english-ontonotes-large\\')\", \\'api_arguments\\': [\\'sentence\\'], \\'python_environment_requirements\\': [\\'flair\\'], \\'example_code\\': \"from flair.data import Sentence\\\\nfrom flair.models import SequenceTagger\\\\n\\\\n# load tagger\\\\ntagger = SequenceTagger.load(\\'flair/ner-english-ontonotes-large\\')\\\\n\\\\n# make example sentence\\\\nsentence = Sentence(\\'On September 1st George won 1 dollar while watching Game of Thrones.\\')\\\\n\\\\n# predict NER tags\\\\ntagger.predict(sentence)\\\\n\\\\n# print sentence\\\\nprint(sentence)\\\\n\\\\n# print predicted NER spans\\\\nprint(\\'The following NER tags are found:\\')\\\\n\\\\n# iterate over entities and print\\\\nfor entity in sentence.get_spans(\\'ner\\'):\\\\n    print(entity)\", \\'performance\\': {\\'dataset\\': \\'Ontonotes\\', \\'accuracy\\': 90.93}, \\'description\\': \\'English NER in Flair (Ontonotes large model). This is the large 18-class NER model for English that ships with Flair. It predicts 18 tags such as cardinal value, date value, event name, building name, geo-political entity, language name, law name, location name, money name, affiliation, ordinal value, organization name, percent value, person name, product name, quantity value, time value, and name of work of art. The model is based on document-level XLM-R embeddings and FLERT.\\'}', metadata={})]", "category": "generic"}
{"question_id": 580, "text": " Extract entities from a provided sentence mentioning various companies and their CEOs.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Token Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Named Entity Recognition\\', \\'api_name\\': \\'Jean-Baptiste/camembert-ner\\', \\'api_call\\': \"AutoModelForTokenClassification.from_pretrained(\\'Jean-Baptiste/camembert-ner\\')\", \\'api_arguments\\': {\\'model\\': \\'model\\', \\'tokenizer\\': \\'tokenizer\\', \\'aggregation_strategy\\': \\'simple\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'AutoTokenizer, AutoModelForTokenClassification, pipeline\\'}, \\'example_code\\': \"from transformers import AutoTokenizer, AutoModelForTokenClassification\\\\ntokenizer = AutoTokenizer.from_pretrained(Jean-Baptiste/camembert-ner)\\\\nmodel = AutoModelForTokenClassification.from_pretrained(Jean-Baptiste/camembert-ner)\\\\n\\\\nfrom transformers import pipeline\\\\nnlp = pipeline(\\'ner\\', model=model, tokenizer=tokenizer, aggregation_strategy=simple)\\\\nnlp(Apple est cre le 1er avril 1976 dans le garage de la maison d\\'enfance de Steve Jobs  Los Altos en Californie par Steve Jobs, Steve Wozniak et Ronald Wayne14, puis constitue sous forme de socit le 3 janvier 1977  l\\'origine sous le nom d\\'Apple Computer, mais pour ses 30 ans et pour reflter la diversification de ses produits, le mot \u00ab computer  est retir le 9 janvier 2015.)\", \\'performance\\': {\\'dataset\\': \\'wikiner-fr\\', \\'accuracy\\': {\\'overall_f1\\': 0.8914, \\'PER_f1\\': 0.9483, \\'ORG_f1\\': 0.8181, \\'LOC_f1\\': 0.8955, \\'MISC_f1\\': 0.8146}}, \\'description\\': \\'camembert-ner is a Named Entity Recognition (NER) model fine-tuned from camemBERT on the wikiner-fr dataset. It can recognize entities such as persons, organizations, locations, and miscellaneous entities.\\'}', metadata={})]", "category": "generic"}
{"question_id": 581, "text": " I am developing a food application where food keywords need to be extracted from user's input text. The model should be able to recognize food-related named entities.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Token Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Named Entity Recognition\\', \\'api_name\\': \\'Dizex/InstaFoodRoBERTa-NER\\', \\'api_call\\': \"AutoModelForTokenClassification.from_pretrained(\\'Dizex/InstaFoodRoBERTa-NER\\')\", \\'api_arguments\\': {\\'model\\': \"AutoModelForTokenClassification.from_pretrained(\\'Dizex/InstaFoodRoBERTa-NER\\')\", \\'tokenizer\\': \"AutoTokenizer.from_pretrained(\\'Dizex/InstaFoodRoBERTa-NER\\')\"}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\\'}, \\'example_code\\': \"example = Today\\'s meal: Fresh olive pok bowl topped with chia seeds. Very delicious!\\\\nner_entity_results = pipe(example, aggregation_strategy=\\'simple\\')\\\\nprint(ner_entity_results)\", \\'performance\\': {\\'dataset\\': \\'Dizex/InstaFoodSet\\', \\'accuracy\\': {\\'f1\\': 0.91, \\'precision\\': 0.89, \\'recall\\': 0.93}}, \\'description\\': \\'InstaFoodRoBERTa-NER is a fine-tuned BERT model that is ready to use for Named Entity Recognition of Food entities on informal text (social media like). It has been trained to recognize a single entity: food (FOOD). Specifically, this model is a roberta-base model that was fine-tuned on a dataset consisting of 400 English Instagram posts related to food.\\'}', metadata={})]", "category": "generic"}
{"question_id": 582, "text": " Create a script for an AI bot that automatically adds punctuation to users' messages in a chat app.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 583, "text": " I am writing an article on the history of technology companies, and I want to extract the names of companies and people mentioned in the text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \"PegasusForConditionalGeneration.from_pretrained(\\'tuner007/pegasus_summarizer\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 584, "text": " I have a diary entry and want to identify the names of people and locations mentioned in it.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \"PegasusForConditionalGeneration.from_pretrained(\\'tuner007/pegasus_summarizer\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 585, "text": " Our company has a deal with international clients, and it's important to detect the proper locations of meetings from multilingual texts provided to us.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'csebuetnlp/mT5_multilingual_XLSum\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'csebuetnlp/mT5_multilingual_XLSum\\')\", \\'api_arguments\\': [\\'model_name\\'], \\'python_environment_requirements\\': [\\'transformers==4.11.0.dev0\\'], \\'example_code\\': \"import re\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nWHITESPACE_HANDLER = lambda k: re.sub(\\'\\\\\\\\s+\\', \\' \\', re.sub(\\'\\\\\\\\n+\\', \\' \\', k.strip()))\\\\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people\\'s scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We\\'re expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\\\\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\\\ninput_ids = tokenizer(\\\\n [WHITESPACE_HANDLER(article_text)],\\\\n return_tensors=pt,\\\\n padding=max_length,\\\\n truncation=True,\\\\n max_length=512\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n max_length=84,\\\\n no_repeat_ngram_size=2,\\\\n num_beams=4\\\\n)[0]\\\\nsummary = tokenizer.decode(\\\\n output_ids,\\\\n skip_special_tokens=True,\\\\n clean_up_tokenization_spaces=False\\\\n)\\\\nprint(summary)\", \\'performance\\': {\\'dataset\\': \\'xsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.5, \\'ROUGE-2\\': 13.934, \\'ROUGE-L\\': 28.988, \\'ROUGE-LSUM\\': 28.996, \\'loss\\': 2.067, \\'gen_len\\': 26.973}}, \\'description\\': \\'This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 586, "text": " We need to extract information from a table to answer a user's question regarding a company's revenue.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'distilbert-base-uncased-distilled-squad\\', \\'api_call\\': \"pipeline(\\'question-answering\\', model=\\'distilbert-base-uncased-distilled-squad\\')\", \\'api_arguments\\': [\\'question\\', \\'context\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nquestion_answerer = pipeline(question-answering, model=\\'distilbert-base-uncased-distilled-squad\\')\\\\ncontext = r\\\\n... Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\\\\n... question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\\\\n... a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\\\\n... \\\\nresult = question_answerer(question=What is a good example of a question answering dataset?, context=context)\\\\nprint(\\\\n... fAnswer: \\'{result[\\'answer\\']}\\', score: {round(result[\\'score\\'], 4)}, start: {result[\\'start\\']}, end: {result[\\'end\\']}\\\\n...)\", \\'performance\\': {\\'dataset\\': \\'SQuAD v1.1\\', \\'accuracy\\': \\'86.9 F1 score\\'}, \\'description\\': \"DistilBERT base uncased distilled SQuAD is a fine-tuned version of DistilBERT-base-uncased for the task of question answering. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT\\'s performances as measured on the GLUE language understanding benchmark.\"}', metadata={})]", "category": "generic"}
{"question_id": 587, "text": " Develop a table-based question answering system that can answer queries based on the input data.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Visual Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ivelin/donut-refexp-combined-v1\\', \\'api_call\\': \"pipeline(\\'visual-question-answering\\', model=\\'ivelin/donut-refexp-combined-v1\\')\", \\'api_arguments\\': \\'image, question\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"vqa(image=\\'path/to/image.jpg\\', question=\\'What is the color of the object?\\')\", \\'performance\\': {\\'dataset\\': \\'ivelin/donut-refexp-combined-v1\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'A visual question answering model that takes an image and a question as input and provides an answer based on the visual content of the image and the context of the question.\\'}', metadata={})]", "category": "generic"}
{"question_id": 588, "text": " Create a tool that receives a table and a question in natural language, and returns an answer to the question based on the inputted table.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Visual Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ivelin/donut-refexp-combined-v1\\', \\'api_call\\': \"pipeline(\\'visual-question-answering\\', model=\\'ivelin/donut-refexp-combined-v1\\')\", \\'api_arguments\\': \\'image, question\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"vqa(image=\\'path/to/image.jpg\\', question=\\'What is the color of the object?\\')\", \\'performance\\': {\\'dataset\\': \\'ivelin/donut-refexp-combined-v1\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'A visual question answering model that takes an image and a question as input and provides an answer based on the visual content of the image and the context of the question.\\'}', metadata={})]", "category": "generic"}
{"question_id": 589, "text": " We are a medical company providing health FAQs. We need to answer customers' questions accurately.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 590, "text": " A group of students are doing a project on European capitals. They need to know the capital city of Germany.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 591, "text": " Assist us in creating a question answering system to provide quick answers for customer inquiries.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023\\', \\'api_call\\': \"AutoModelForDocumentQuestionAnswering.from_pretrained(\\'L-oenai/LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023\\')\", \\'api_arguments\\': {}, \\'python_environment_requirements\\': {\\'transformers\\': \\'>=4.11.0\\'}, \\'example_code\\': {}, \\'performance\\': {\\'dataset\\': {}, \\'accuracy\\': {}}, \\'description\\': \\'A document question answering model based on LayoutLMv2, which can be used to extract answers from images with text and layout information.\\'}', metadata={})]", "category": "generic"}
{"question_id": 592, "text": " We are worried about price inflation in our country. Can you answer our questions on price inflation using the BERT large cased whole word masking finetuned model on SQuAD?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'bert-large-cased-whole-word-masking-finetuned-squad\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'bert-large-cased-whole-word-masking-finetuned-squad\\')\", \\'api_arguments\\': {\\'model_name_or_path\\': \\'bert-large-cased-whole-word-masking\\', \\'dataset_name\\': \\'squad\\', \\'do_train\\': True, \\'do_eval\\': True, \\'learning_rate\\': 3e-05, \\'num_train_epochs\\': 2, \\'max_seq_length\\': 384, \\'doc_stride\\': 128, \\'output_dir\\': \\'./examples/models/wwm_cased_finetuned_squad/\\', \\'per_device_eval_batch_size\\': 3, \\'per_device_train_batch_size\\': 3}, \\'python_environment_requirements\\': [\\'torch\\', \\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nqa_pipeline = pipeline(\\'question-answering\\', model=\\'bert-large-cased-whole-word-masking-finetuned-squad\\')\\\\nresult = qa_pipeline({\\'context\\': \\'This is a context example.\\', \\'question\\': \\'What is this example for?\\'})\\\\nprint(result)\", \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'BookCorpus\\', \\'accuracy\\': \\'N/A\\'}, {\\'name\\': \\'English Wikipedia\\', \\'accuracy\\': \\'N/A\\'}]}, \\'description\\': \\'BERT large model (cased) whole word masking finetuned on SQuAD. This model is cased and trained with a new technique: Whole Word Masking. After pre-training, this model was fine-tuned on the SQuAD dataset.\\'}', metadata={})]", "category": "generic"}
{"question_id": 593, "text": " I am interested in developing a system that can categorize my text messages into different subjects like finances, health, and entertainment.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 594, "text": " An online magazine editor is trying to clujster articles into topics. We want to classify articles into categories like sport, politics, health, and technology written in French.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Zero-Shot Image Classification\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Zero-Shot Image Classification\\', \\'api_name\\': \\'laion/CLIP-ViT-g-14-laion2B-s34B-b88K\\', \\'api_call\\': \"pipeline(\\'zero-shot-image-classification\\', model=\\'laion/CLIP-ViT-g-14-laion2B-s34B-b88K\\')\", \\'api_arguments\\': {\\'image\\': \\'path/to/image/file\\', \\'class_names\\': \\'list_of_class_names\\'}, \\'python_environment_requirements\\': {\\'huggingface_hub\\': \\'0.0.17\\', \\'transformers\\': \\'4.11.3\\', \\'torch\\': \\'1.9.0\\', \\'torchvision\\': \\'0.10.0\\'}, \\'example_code\\': None, \\'performance\\': {\\'dataset\\': None, \\'accuracy\\': None}, \\'description\\': \\'A zero-shot image classification model based on OpenCLIP, which can classify images into various categories without requiring any training data for those categories.\\'}', metadata={})]", "category": "generic"}
{"question_id": 595, "text": " We have a user manual in English that needs to be translated into Spanish to be displayed on our Spanish website.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'Kirili4ik/mbart_ruDialogSum\\', \\'api_call\\': \"MBartForConditionalGeneration.from_pretrained(\\'Kirili4ik/mbart_ruDialogSum\\')\", \\'api_arguments\\': {\\'model_name\\': \\'Kirili4ik/mbart_ruDialogSum\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import MBartTokenizer, MBartForConditionalGeneration\\\\nmodel_name = Kirili4ik/mbart_ruDialogSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = MBartForConditionalGeneration.from_pretrained(model_name)\\\\nmodel.eval()\\\\narticle_text = ...\\\\ninput_ids = tokenizer(\\\\n [article_text],\\\\n max_length=600,\\\\n padding=max_length,\\\\n truncation=True,\\\\n return_tensors=pt,\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n top_k=0,\\\\n num_beams=3,\\\\n no_repeat_ngram_size=3\\\\n)[0]\\\\nsummary = tokenizer.decode(output_ids, skip_special_tokens=True)\\\\nprint(summary)\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'SAMSum Corpus (translated to Russian)\\', \\'accuracy\\': {\\'Validation ROGUE-1\\': 34.5, \\'Validation ROGUE-L\\': 33, \\'Test ROGUE-1\\': 31, \\'Test ROGUE-L\\': 28}}]}, \\'description\\': \\'MBart for Russian summarization fine-tuned for dialogues summarization. This model was firstly fine-tuned by Ilya Gusev on Gazeta dataset. We have fine tuned that model on SamSum dataset translated to Russian using GoogleTranslateAPI. Moreover! We have implemented a ! telegram bot @summarization_bot ! with the inference of this model. Add it to the chat and get summaries instead of dozens spam messages!\\'}', metadata={})]", "category": "generic"}
{"question_id": 596, "text": " We need a classifier that can sort movie synopses in German into three categories: crime, tragedy, and theft.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'speechbrain\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'tts-hifigan-german\\', \\'api_call\\': \"HIFIGAN.from_hparams(source=\\'padmalcom/tts-hifigan-german\\', savedir=tmpdir_vocoder)\", \\'api_arguments\\': [\\'mel_output\\'], \\'python_environment_requirements\\': [\\'speechbrain\\'], \\'example_code\\': \"import torchaudio\\\\nfrom speechbrain.pretrained import Tacotron2\\\\nfrom speechbrain.pretrained import HIFIGAN\\\\ntacotron2 = Tacotron2.from_hparams(source=padmalcom/tts-tacotron2-german, savedir=tmpdir_tts)\\\\nhifi_gan = HIFIGAN.from_hparams(source=padmalcom/tts-hifigan-german, savedir=tmpdir_vocoder)\\\\nmel_output, mel_length, alignment = tacotron2.encode_text(Mary had a little lamb)\\\\nwaveforms = hifi_gan.decode_batch(mel_output)\\\\ntorchaudio.save(\\'example_TTS.wav\\',waveforms.squeeze(1), 22050)\", \\'performance\\': {\\'dataset\\': \\'custom German dataset\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'A HiFIGAN vocoder trained on a generated German dataset using mp3_to_training_data. The pre-trained model takes in input a spectrogram and produces a waveform in output. Typically, a vocoder is used after a TTS model that converts an input text into a spectrogram.\\'}', metadata={})]", "category": "generic"}
{"question_id": 597, "text": " Write an abstract about the impacts of social media on mental health, summarizing key findings from previous studies.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'csebuetnlp/mT5_multilingual_XLSum\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'csebuetnlp/mT5_multilingual_XLSum\\')\", \\'api_arguments\\': [\\'model_name\\'], \\'python_environment_requirements\\': [\\'transformers==4.11.0.dev0\\'], \\'example_code\\': \"import re\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nWHITESPACE_HANDLER = lambda k: re.sub(\\'\\\\\\\\s+\\', \\' \\', re.sub(\\'\\\\\\\\n+\\', \\' \\', k.strip()))\\\\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people\\'s scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We\\'re expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\\\\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\\\ninput_ids = tokenizer(\\\\n [WHITESPACE_HANDLER(article_text)],\\\\n return_tensors=pt,\\\\n padding=max_length,\\\\n truncation=True,\\\\n max_length=512\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n max_length=84,\\\\n no_repeat_ngram_size=2,\\\\n num_beams=4\\\\n)[0]\\\\nsummary = tokenizer.decode(\\\\n output_ids,\\\\n skip_special_tokens=True,\\\\n clean_up_tokenization_spaces=False\\\\n)\\\\nprint(summary)\", \\'performance\\': {\\'dataset\\': \\'xsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.5, \\'ROUGE-2\\': 13.934, \\'ROUGE-L\\': 28.988, \\'ROUGE-LSUM\\': 28.996, \\'loss\\': 2.067, \\'gen_len\\': 26.973}}, \\'description\\': \\'This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 598, "text": " Help me to translate a Spanish text to English. The Spanish text is: \\\"Lo siento, pero no puedo ir a la reuni\\u00f3n debido a una emergencia personal. Avisar\\u00e9 al equipo y nos pondremos en contacto para reprogramar la reuni\\u00f3n.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Token Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Named Entity Recognition\\', \\'api_name\\': \\'Jean-Baptiste/camembert-ner\\', \\'api_call\\': \"AutoModelForTokenClassification.from_pretrained(\\'Jean-Baptiste/camembert-ner\\')\", \\'api_arguments\\': {\\'model\\': \\'model\\', \\'tokenizer\\': \\'tokenizer\\', \\'aggregation_strategy\\': \\'simple\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'AutoTokenizer, AutoModelForTokenClassification, pipeline\\'}, \\'example_code\\': \"from transformers import AutoTokenizer, AutoModelForTokenClassification\\\\ntokenizer = AutoTokenizer.from_pretrained(Jean-Baptiste/camembert-ner)\\\\nmodel = AutoModelForTokenClassification.from_pretrained(Jean-Baptiste/camembert-ner)\\\\n\\\\nfrom transformers import pipeline\\\\nnlp = pipeline(\\'ner\\', model=model, tokenizer=tokenizer, aggregation_strategy=simple)\\\\nnlp(Apple est cre le 1er avril 1976 dans le garage de la maison d\\'enfance de Steve Jobs  Los Altos en Californie par Steve Jobs, Steve Wozniak et Ronald Wayne14, puis constitue sous forme de socit le 3 janvier 1977  l\\'origine sous le nom d\\'Apple Computer, mais pour ses 30 ans et pour reflter la diversification de ses produits, le mot \u00ab computer  est retir le 9 janvier 2015.)\", \\'performance\\': {\\'dataset\\': \\'wikiner-fr\\', \\'accuracy\\': {\\'overall_f1\\': 0.8914, \\'PER_f1\\': 0.9483, \\'ORG_f1\\': 0.8181, \\'LOC_f1\\': 0.8955, \\'MISC_f1\\': 0.8146}}, \\'description\\': \\'camembert-ner is a Named Entity Recognition (NER) model fine-tuned from camemBERT on the wikiner-fr dataset. It can recognize entities such as persons, organizations, locations, and miscellaneous entities.\\'}', metadata={})]", "category": "generic"}
{"question_id": 599, "text": " I'm building a chatbot and I need it to respond to user questions based on the persona of an elderly person.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Conversational\\', \\'api_name\\': \\'facebook/blenderbot-400M-distill\\', \\'api_call\\': \"BlenderbotForConditionalGeneration.from_pretrained(\\'facebook/blenderbot-400M-distill\\')\", \\'api_arguments\\': \"[\\'message\\']\", \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': \\'Input a message to start chatting with facebook/blenderbot-400M-distill.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'BlenderBot-400M-distill is a distilled version of the BlenderBot model, trained on the Blended Skill Talk dataset. It is designed for open-domain chatbot tasks and can generate text-to-text responses in a conversational manner. The model is based on the Transformers library and can be used with PyTorch, TensorFlow, and JAX.\\'}', metadata={})]", "category": "generic"}
{"question_id": 600, "text": " Design a chatbot that can have conversations about a variety of topics with the users.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 601, "text": " We have a new platform that offers various services related to digital art, and we want it to be able to produce creative story ideas based on a short description. Can you help me providing suggestions?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 602, "text": " We are creating a chatbot to answer customer queries on our website. Please provide a way to generate human-like text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 603, "text": " To improve customer satisfaction, we want to automate the creation of written explanations that sound more natural and less robotic. Can you suggest a way to generate text that sounds conscious and alive?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Conversational\\', \\'api_name\\': \\'microsoft/GODEL-v1_1-large-seq2seq\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'microsoft/GODEL-v1_1-large-seq2seq\\')\", \\'api_arguments\\': {\\'instruction\\': \\'Instruction: given a dialog context, you need to response empathically.\\', \\'knowledge\\': \\'\\', \\'dialog\\': [\\'Does money buy happiness?\\', \\'It is a question. Money buys you a lot of things, but not enough to buy happiness.\\', \\'What is the best way to buy happiness ?\\']}, \\'python_environment_requirements\\': {\\'transformers\\': \\'AutoTokenizer, AutoModelForSeq2SeqLM\\'}, \\'example_code\\': \"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained(microsoft/GODEL-v1_1-large-seq2seq)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(microsoft/GODEL-v1_1-large-seq2seq)\\\\ndef generate(instruction, knowledge, dialog):\\\\n if knowledge != \\'\\':\\\\n knowledge = \\'[KNOWLEDGE] \\' + knowledge\\\\n dialog = \\' EOS \\'.join(dialog)\\\\n query = f{instruction} [CONTEXT] {dialog} {knowledge}\\\\n input_ids = tokenizer(f{query}, return_tensors=pt).input_ids\\\\n outputs = model.generate(input_ids, max_length=128, min_length=8, top_p=0.9, do_sample=True)\\\\n output = tokenizer.decode(outputs[0], skip_special_tokens=True)\\\\n return output\\\\n\\\\ninstruction = f\\'Instruction: given a dialog context, you need to response empathically.\\'\\\\nknowledge = \\'\\'\\\\ndialog = [\\\\n \\'Does money buy happiness?\\',\\\\n \\'It is a question. Money buys you a lot of things, but not enough to buy happiness.\\',\\\\n \\'What is the best way to buy happiness ?\\'\\\\n]\\\\nresponse = generate(instruction, knowledge, dialog)\\\\nprint(response)\", \\'performance\\': {\\'dataset\\': \\'Reddit discussion thread, instruction and knowledge grounded dialogs\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'GODEL is a large-scale pre-trained model for goal-directed dialogs. It is parameterized with a Transformer-based encoder-decoder model and trained for response generation grounded in external text, which allows more effective fine-tuning on dialog tasks that require conditioning the response on information that is external to the current conversation (e.g., a retrieved document). The pre-trained model can be efficiently fine-tuned and adapted to accomplish a new dialog task with a handful of task-specific dialogs. The v1.1 model is trained on 551M multi-turn dialogs from Reddit discussion thread, and 5M instruction and knowledge grounded dialogs.\\'}', metadata={})]", "category": "generic"}
{"question_id": 604, "text": " Our company is growing and expanding into multiple countries. We need a solution for translating English sentences to German to ensure effective communication.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'csebuetnlp/mT5_multilingual_XLSum\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'csebuetnlp/mT5_multilingual_XLSum\\')\", \\'api_arguments\\': [\\'model_name\\'], \\'python_environment_requirements\\': [\\'transformers==4.11.0.dev0\\'], \\'example_code\\': \"import re\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nWHITESPACE_HANDLER = lambda k: re.sub(\\'\\\\\\\\s+\\', \\' \\', re.sub(\\'\\\\\\\\n+\\', \\' \\', k.strip()))\\\\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people\\'s scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We\\'re expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\\\\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\\\ninput_ids = tokenizer(\\\\n [WHITESPACE_HANDLER(article_text)],\\\\n return_tensors=pt,\\\\n padding=max_length,\\\\n truncation=True,\\\\n max_length=512\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n max_length=84,\\\\n no_repeat_ngram_size=2,\\\\n num_beams=4\\\\n)[0]\\\\nsummary = tokenizer.decode(\\\\n output_ids,\\\\n skip_special_tokens=True,\\\\n clean_up_tokenization_spaces=False\\\\n)\\\\nprint(summary)\", \\'performance\\': {\\'dataset\\': \\'xsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.5, \\'ROUGE-2\\': 13.934, \\'ROUGE-L\\': 28.988, \\'ROUGE-LSUM\\': 28.996, \\'loss\\': 2.067, \\'gen_len\\': 26.973}}, \\'description\\': \\'This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 605, "text": " We need a solution for creating Polish subtitles for YouTube videos in Spanish. The AI should provide the translation.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 606, "text": " I am an English teacher looking to brainstorm synonyms for the word \\\"happy.\\\" Can a model help me generate similar words?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text-to-Text Generation\\', \\'api_name\\': \\'philschmid/bart-large-cnn-samsum\\', \\'api_call\\': \"pipeline(\\'summarization\\', model=\\'philschmid/bart-large-cnn-samsum\\')\", \\'api_arguments\\': {\\'model\\': \\'philschmid/bart-large-cnn-samsum\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'latest\\'}, \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/bart-large-cnn-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'eval_rouge1\\': 42.621, \\'eval_rouge2\\': 21.9825, \\'eval_rougeL\\': 33.034, \\'eval_rougeLsum\\': 39.6783, \\'test_rouge1\\': 41.3174, \\'test_rouge2\\': 20.8716, \\'test_rougeL\\': 32.1337, \\'test_rougeLsum\\': 38.4149}}, \\'description\\': \\'philschmid/bart-large-cnn-samsum is a BART-based model trained for text summarization on the SAMSum dataset. It can be used to generate abstractive summaries of conversations.\\'}', metadata={})]", "category": "generic"}
{"question_id": 607, "text": " Generate a sentence which will complete the following: \\\"Hello, I'm a ...\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Fill-Mask\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Fill-Mask\\', \\'api_name\\': \\'distilbert-base-uncased\\', \\'api_call\\': \"pipeline(\\'fill-mask\\', model=\\'distilbert-base-uncased\\')\", \\'api_arguments\\': {\\'model\\': \\'distilbert-base-uncased\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nunmasker = pipeline(\\'fill-mask\\', model=\\'distilbert-base-uncased\\')\\\\nunmasker(Hello I\\'m a [MASK] model.)\", \\'performance\\': {\\'dataset\\': \\'GLUE\\', \\'accuracy\\': {\\'MNLI\\': 82.2, \\'QQP\\': 88.5, \\'QNLI\\': 89.2, \\'SST-2\\': 91.3, \\'CoLA\\': 51.3, \\'STS-B\\': 85.8, \\'MRPC\\': 87.5, \\'RTE\\': 59.9}}, \\'description\\': \"DistilBERT is a transformers model, smaller and faster than BERT, which was pretrained on the same corpus in a self-supervised fashion, using the BERT base model as a teacher. It was pretrained with three objectives: Distillation loss, Masked language modeling (MLM), and Cosine embedding loss. This model is uncased and can be used for masked language modeling or next sentence prediction, but it\\'s mostly intended to be fine-tuned on a downstream task.\"}', metadata={})]", "category": "generic"}
{"question_id": 608, "text": " Can you help me find the best model to fill in the gap in my legal document? I want a smaller model with higher efficiency but maintains a high level of accuracy.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'distilbart-cnn-12-6-samsum\\', \\'api_call\\': \"pipeline(\\'summarization\\', model=\\'philschmid/distilbart-cnn-12-6-samsum\\')\", \\'api_arguments\\': {\\'model\\': \\'philschmid/distilbart-cnn-12-6-samsum\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 41.09, \\'ROUGE-2\\': 20.746, \\'ROUGE-L\\': 31.595, \\'ROUGE-LSUM\\': 38.339}}, \\'description\\': \\'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 609, "text": " I am learning English literature. I plan to build a system that has a huge database of English sentences and keeps the important ones.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 610, "text": " Our company specializes in providing information on similar topics. We want to find similar sentences in a text document.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'pszemraj/long-t5-tglobal-base-16384-book-summary\\', \\'api_call\\': \"T5ForConditionalGeneration.from_pretrained(\\'pszemraj/long-t5-tglobal-base-16384-book-summary\\')\", \\'api_arguments\\': [\\'long_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nimport torch\\\\nsummarizer = pipeline(\\\\n summarization,\\\\n pszemraj/long-t5-tglobal-base-16384-book-summary,\\\\n device=0 if torch.cuda.is_available() else -1,\\\\n)\\\\nlong_text = Here is a lot of text I don\\'t want to read. Replace me\\\\nresult = summarizer(long_text)\\\\nprint(result[0][summary_text])\", \\'performance\\': {\\'dataset\\': \\'kmfoda/booksum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.408, \\'ROUGE-2\\': 6.065, \\'ROUGE-L\\': 16.721, \\'ROUGE-LSUM\\': 33.34}}, \\'description\\': \\'A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset, which can be used to summarize long text and generate SparkNotes-esque summaries of arbitrary topics. The model generalizes reasonably well to academic and narrative text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 611, "text": " The company is building an AI chatbot, and we need a way to process users' questions and tell if two questions are similar.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Automatic Speech Recognition\\', \\'framework\\': \\'pyannote.audio\\', \\'functionality\\': \\'overlapped-speech-detection\\', \\'api_name\\': \\'pyannote/overlapped-speech-detection\\', \\'api_call\\': \"pipeline.from_pretrained(\\'pyannote/overlapped-speech-detection\\', use_auth_token=\\'ACCESS_TOKEN_GOES_HERE\\')\", \\'api_arguments\\': [\\'audio.wav\\'], \\'python_environment_requirements\\': [\\'pyannote.audio 2.1\\'], \\'example_code\\': \\'from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained(pyannote/overlapped-speech-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\\\noutput = pipeline(audio.wav)\\\\nfor speech in output.get_timeline().support():\\\\n  # two or more speakers are active between speech.start and speech.end\\\\n  ...\\', \\'performance\\': {\\'dataset\\': \\'ami\\', \\'accuracy\\': None}, \\'description\\': \\'Automatic overlapped speech detection using pyannote.audio framework. The model detects when two or more speakers are active in an audio file.\\'}', metadata={})]", "category": "generic"}
{"question_id": 612, "text": " I am a CEO, and I want to create an audio announcement that will be played to my customers. The text announcement is: \\\"Dear valued customers, we are glad to announce a 30% discount on our special offer. Hurry up!\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 613, "text": " Our user would like to develop an audiobook using a Text-to-Speech API. Convert the text of a book into an audio file.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'distilbert-base-uncased-distilled-squad\\', \\'api_call\\': \"pipeline(\\'question-answering\\', model=\\'distilbert-base-uncased-distilled-squad\\')\", \\'api_arguments\\': [\\'question\\', \\'context\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nquestion_answerer = pipeline(question-answering, model=\\'distilbert-base-uncased-distilled-squad\\')\\\\ncontext = r\\\\n... Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\\\\n... question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\\\\n... a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\\\\n... \\\\nresult = question_answerer(question=What is a good example of a question answering dataset?, context=context)\\\\nprint(\\\\n... fAnswer: \\'{result[\\'answer\\']}\\', score: {round(result[\\'score\\'], 4)}, start: {result[\\'start\\']}, end: {result[\\'end\\']}\\\\n...)\", \\'performance\\': {\\'dataset\\': \\'SQuAD v1.1\\', \\'accuracy\\': \\'86.9 F1 score\\'}, \\'description\\': \"DistilBERT base uncased distilled SQuAD is a fine-tuned version of DistilBERT-base-uncased for the task of question answering. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT\\'s performances as measured on the GLUE language understanding benchmark.\"}', metadata={})]", "category": "generic"}
{"question_id": 614, "text": " In our language app, we want to include text to speech functionality for Hokkien, a dialect of Chinese, using the TAT-TTS dataset.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'sberbank-ai/sbert_large_mt_nlu_ru\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'sberbank-ai/sbert_large_mt_nlu_ru\\')\", \\'api_arguments\\': [\\'sentences\\', \\'padding\\', \\'truncation\\', \\'max_length\\', \\'return_tensors\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': \"from transformers import AutoTokenizer, AutoModel\\\\nimport torch\\\\n\\\\n\\\\n# Mean Pooling - Take attention mask into account for correct averaging\\\\ndef mean_pooling(model_output, attention_mask):\\\\n    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\\\\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\\\\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\\\\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\\\\n    return sum_embeddings / sum_mask\\\\n\\\\n\\\\n# Sentences we want sentence embeddings for sentences = [\\'?\\']\\\\n\\\\n# Load AutoModel from huggingface model repository\\\\ntokenizer = AutoTokenizer.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\\\nmodel = AutoModel.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\\\n\\\\n# Tokenize sentences\\\\nencoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=24, return_tensors=\\'pt\\')\\\\n\\\\n# Compute token embeddings\\\\nwith torch.no_grad():\\\\n    model_output = model(**encoded_input)\\\\n\\\\n# Perform pooling. In this case, mean pooling\\\\nsentence_embeddings = mean_pooling(model_output, encoded_input[\\'attention_mask\\'])\", \\'performance\\': {\\'dataset\\': \\'Russian SuperGLUE\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'BERT large model multitask (cased) for Sentence Embeddings in Russian language.\\'}', metadata={})]", "category": "generic"}
{"question_id": 615, "text": " Develop an application to transcribe audio files with punctuation marks for a podcast platform.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Automatic Speech Recognition\\', \\'framework\\': \\'pyannote.audio\\', \\'functionality\\': \\'Speaker Diarization\\', \\'api_name\\': \\'pyannote/speaker-diarization\\', \\'api_call\\': \"Pipeline.from_pretrained(\\'pyannote/speaker-diarization@2.1\\', use_auth_token=\\'ACCESS_TOKEN_GOES_HERE\\')\", \\'api_arguments\\': {\\'num_speakers\\': \\'int (optional)\\', \\'min_speakers\\': \\'int (optional)\\', \\'max_speakers\\': \\'int (optional)\\'}, \\'python_environment_requirements\\': \\'pyannote.audio 2.1.1\\', \\'example_code\\': [\\'from pyannote.audio import Pipeline\\', \\'pipeline = Pipeline.from_pretrained(pyannote/speaker-diarization@2.1, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\', \\'diarization = pipeline(audio.wav)\\', \\'with open(audio.rttm, w) as rttm:\\', \\'  diarization.write_rttm(rttm)\\'], \\'performance\\': {\\'dataset\\': \\'ami\\', \\'accuracy\\': {\\'DER%\\': \\'18.91\\', \\'FA%\\': \\'4.48\\', \\'Miss%\\': \\'9.51\\', \\'Conf%\\': \\'4.91\\'}}, \\'description\\': \\'This API provides an automatic speaker diarization pipeline using the pyannote.audio framework. It can process audio files and output speaker diarization results in RTTM format. The pipeline can also handle cases where the number of speakers is known in advance or when providing lower and/or upper bounds on the number of speakers.\\'}', metadata={})]", "category": "generic"}
{"question_id": 616, "text": " Our company has an audio archive of Chinese podcasts, and we want to create transcripts for them.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'sberbank-ai/sbert_large_mt_nlu_ru\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'sberbank-ai/sbert_large_mt_nlu_ru\\')\", \\'api_arguments\\': [\\'sentences\\', \\'padding\\', \\'truncation\\', \\'max_length\\', \\'return_tensors\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': \"from transformers import AutoTokenizer, AutoModel\\\\nimport torch\\\\n\\\\n\\\\n# Mean Pooling - Take attention mask into account for correct averaging\\\\ndef mean_pooling(model_output, attention_mask):\\\\n    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\\\\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\\\\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\\\\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\\\\n    return sum_embeddings / sum_mask\\\\n\\\\n\\\\n# Sentences we want sentence embeddings for sentences = [\\'?\\']\\\\n\\\\n# Load AutoModel from huggingface model repository\\\\ntokenizer = AutoTokenizer.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\\\nmodel = AutoModel.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\\\n\\\\n# Tokenize sentences\\\\nencoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=24, return_tensors=\\'pt\\')\\\\n\\\\n# Compute token embeddings\\\\nwith torch.no_grad():\\\\n    model_output = model(**encoded_input)\\\\n\\\\n# Perform pooling. In this case, mean pooling\\\\nsentence_embeddings = mean_pooling(model_output, encoded_input[\\'attention_mask\\'])\", \\'performance\\': {\\'dataset\\': \\'Russian SuperGLUE\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'BERT large model multitask (cased) for Sentence Embeddings in Russian language.\\'}', metadata={})]", "category": "generic"}
{"question_id": 617, "text": " We are a podcast platform. We have a library of multiple podcasts that are mixed with background noise. We need to enhance the podcast audio quality to improve the user experience on our platform.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 618, "text": " A podcast producer is looking to improve the quality of their audio files by removing background noise. What can they do?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Automatic Speech Recognition\\', \\'framework\\': \\'pyannote.audio\\', \\'functionality\\': \\'Speaker Diarization\\', \\'api_name\\': \\'pyannote/speaker-diarization\\', \\'api_call\\': \"Pipeline.from_pretrained(\\'pyannote/speaker-diarization@2.1\\', use_auth_token=\\'ACCESS_TOKEN_GOES_HERE\\')\", \\'api_arguments\\': {\\'num_speakers\\': \\'int (optional)\\', \\'min_speakers\\': \\'int (optional)\\', \\'max_speakers\\': \\'int (optional)\\'}, \\'python_environment_requirements\\': \\'pyannote.audio 2.1.1\\', \\'example_code\\': [\\'from pyannote.audio import Pipeline\\', \\'pipeline = Pipeline.from_pretrained(pyannote/speaker-diarization@2.1, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\', \\'diarization = pipeline(audio.wav)\\', \\'with open(audio.rttm, w) as rttm:\\', \\'  diarization.write_rttm(rttm)\\'], \\'performance\\': {\\'dataset\\': \\'ami\\', \\'accuracy\\': {\\'DER%\\': \\'18.91\\', \\'FA%\\': \\'4.48\\', \\'Miss%\\': \\'9.51\\', \\'Conf%\\': \\'4.91\\'}}, \\'description\\': \\'This API provides an automatic speaker diarization pipeline using the pyannote.audio framework. It can process audio files and output speaker diarization results in RTTM format. The pipeline can also handle cases where the number of speakers is known in advance or when providing lower and/or upper bounds on the number of speakers.\\'}', metadata={})]", "category": "generic"}
{"question_id": 619, "text": " I work at a call center, and I need to convert a recorded customer call into an audio file with a different voice without changing the content.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Table Question Answering\\', \\'api_name\\': \\'google/tapas-large-finetuned-wtq\\', \\'api_call\\': \"pipeline(\\'table-question-answering\\', model=\\'google/tapas-large-finetuned-wtq\\')\", \\'api_arguments\\': {\\'model\\': \\'google/tapas-large-finetuned-wtq\\', \\'task\\': \\'table-question-answering\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\n\\\\nqa_pipeline = pipeline(\\'table-question-answering\\', model=\\'google/tapas-large-finetuned-wtq\\')\\\\n\\\\nresult = qa_pipeline(table=table, query=query)\", \\'performance\\': {\\'dataset\\': \\'wikitablequestions\\', \\'accuracy\\': 0.5097}, \\'description\\': \\'TAPAS large model fine-tuned on WikiTable Questions (WTQ). This model was pre-trained on MLM and an additional step which the authors call intermediate pre-training, and then fine-tuned in a chain on SQA, WikiSQL and finally WTQ. It uses relative position embeddings (i.e. resetting the position index at every cell of the table).\\'}', metadata={})]", "category": "generic"}
{"question_id": 620, "text": " In a customer support system, we have received a recorded audio file from a customer complaining about an issue in Czech language. Translate it to English language preserving the audio format.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'PyTorch Transformers\\', \\'functionality\\': \\'Table Question Answering\\', \\'api_name\\': \\'table-question-answering-tapas\\', \\'api_call\\': \"pipeline(\\'table-question-answering\\', model=\\'Meena/table-question-answering-tapas\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'This model can be loaded on the Inference API on-demand.\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'SQA (Sequential Question Answering by Microsoft)\\', \\'accuracy\\': None}, {\\'name\\': \\'WTQ (Wiki Table Questions by Stanford University)\\', \\'accuracy\\': None}, {\\'name\\': \\'WikiSQL (by Salesforce)\\', \\'accuracy\\': None}]}, \\'description\\': \\'TAPAS, the model learns an inner representation of the English language used in tables and associated texts, which can then be used to extract features useful for downstream tasks such as answering questions about a table, or determining whether a sentence is entailed or refuted by the contents of a table. It is a BERT-based model specifically designed (and pre-trained) for answering questions about tabular data. TAPAS uses relative position embeddings and has 7 token types that encode tabular structure. It is pre-trained on the masked language modeling (MLM) objective on a large dataset comprising millions of tables from English Wikipedia and corresponding texts.\\'}', metadata={})]", "category": "generic"}
{"question_id": 621, "text": " We are a podcast editing company aiming at separating speakers from a recorded audio.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Automatic Speech Recognition\\', \\'framework\\': \\'pyannote.audio\\', \\'functionality\\': \\'overlapped-speech-detection\\', \\'api_name\\': \\'pyannote/overlapped-speech-detection\\', \\'api_call\\': \"pipeline.from_pretrained(\\'pyannote/overlapped-speech-detection\\', use_auth_token=\\'ACCESS_TOKEN_GOES_HERE\\')\", \\'api_arguments\\': [\\'audio.wav\\'], \\'python_environment_requirements\\': [\\'pyannote.audio 2.1\\'], \\'example_code\\': \\'from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained(pyannote/overlapped-speech-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\\\noutput = pipeline(audio.wav)\\\\nfor speech in output.get_timeline().support():\\\\n  # two or more speakers are active between speech.start and speech.end\\\\n  ...\\', \\'performance\\': {\\'dataset\\': \\'ami\\', \\'accuracy\\': None}, \\'description\\': \\'Automatic overlapped speech detection using pyannote.audio framework. The model detects when two or more speakers are active in an audio file.\\'}', metadata={})]", "category": "generic"}
{"question_id": 622, "text": " We are building a virtual assistant which can translate English speech input to Hokkien in real-time.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 623, "text": " A language service wants to incorporate a speech-to-speech translation feature that assists users in translating Hokkien to English on an audio file.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Audio-to-Audio\\', \\'framework\\': \\'Fairseq\\', \\'functionality\\': \\'speech-to-speech-translation\\', \\'api_name\\': \\'facebook/textless_sm_en_fr\\', \\'api_call\\': \"load_model_ensemble_and_task_from_hf_hub(\\'facebook/textless_sm_en_fr\\')\", \\'api_arguments\\': [\\'input_file\\'], \\'python_environment_requirements\\': [\\'huggingface_hub\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'This model is a speech-to-speech translation model trained by Facebook. It is designed for translating English speech to French speech.\\'}', metadata={})]", "category": "generic"}
{"question_id": 624, "text": " Our company is building an interactive platform for children to learn random trivia quickly. We need a model to understand what is being spoken and what category it belongs to.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'promptcap-coco-vqa\\', \\'api_call\\': \"PromptCap(\\'vqascore/promptcap-coco-vqa\\')\", \\'api_arguments\\': {\\'prompt\\': \\'string\\', \\'image\\': \\'string\\'}, \\'python_environment_requirements\\': \\'pip install promptcap\\', \\'example_code\\': [\\'import torch\\', \\'from promptcap import PromptCap\\', \\'model = PromptCap(vqascore/promptcap-coco-vqa)\\', \\'if torch.cuda.is_available():\\', \\'  model.cuda()\\', \\'prompt = please describe this image according to the given question: what piece of clothing is this boy putting on?\\', \\'image = glove_boy.jpeg\\', \\'print(model.caption(prompt, image))\\'], \\'performance\\': {\\'dataset\\': {\\'coco\\': {\\'accuracy\\': \\'150 CIDEr\\'}, \\'OK-VQA\\': {\\'accuracy\\': \\'60.4%\\'}, \\'A-OKVQA\\': {\\'accuracy\\': \\'59.6%\\'}}}, \\'description\\': \\'PromptCap is a captioning model that can be controlled by natural language instruction. The instruction may contain a question that the user is interested in. It achieves SOTA performance on COCO captioning (150 CIDEr) and knowledge-based VQA tasks when paired with GPT-3 (60.4% on OK-VQA and 59.6% on A-OKVQA).\\'}', metadata={})]", "category": "generic"}
{"question_id": 625, "text": " We are a company that develops voice assistants, and we need to verify a user's voice to authenticate them.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 626, "text": " We recently launched a customer service hotline, and we want to estimate the demographics of callers.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 627, "text": " Create a smart speaker that can recognize voice commands such as \\\"Turn on the lights,\\\" \\\"Play music,\\\" or \\\"Set a timer.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'microsoft/speecht5_tts\\', \\'api_call\\': \"SpeechT5ForTextToSpeech.from_pretrained(\\'microsoft/speecht5_tts\\')\", \\'api_arguments\\': [\\'text\\', \\'return_tensors\\', \\'input_ids\\', \\'speaker_embeddings\\', \\'vocoder\\'], \\'python_environment_requirements\\': \\'!pip install git+https://github.com/huggingface/transformers sentencepiece datasets\\', \\'example_code\\': \"from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\\\\nfrom datasets import load_dataset\\\\nimport torch\\\\nimport soundfile as sf\\\\nprocessor = SpeechT5Processor.from_pretrained(\\'microsoft/speecht5_tts\\')\\\\nmodel = SpeechT5ForTextToSpeech.from_pretrained(\\'microsoft/speecht5_tts\\')\\\\nvocoder = SpeechT5HifiGan.from_pretrained(\\'microsoft/speecht5_hifigan\\')\\\\ninputs = processor(text=\\'Hello, my dog is cute\\', return_tensors=\\'pt\\')\\\\nembeddings_dataset = load_dataset(\\'Matthijs/cmu-arctic-xvectors\\', split=\\'validation\\')\\\\nspeaker_embeddings = torch.tensor(embeddings_dataset[7306][\\'xvector\\']).unsqueeze(0)\\\\nspeech = model.generate_speech(inputs[\\'input_ids\\'], speaker_embeddings, vocoder=vocoder)\\\\nsf.write(\\'speech.wav\\', speech.numpy(), samplerate=16000)\", \\'performance\\': {\\'dataset\\': \\'LibriTTS\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'SpeechT5 model fine-tuned for speech synthesis (text-to-speech) on LibriTTS. It is a unified-modal SpeechT5 framework that explores the encoder-decoder pre-training for self-supervised speech/text representation learning. It can be used for a wide variety of spoken language processing tasks, including automatic speech recognition, speech synthesis, speech translation, voice conversion, speech enhancement, and speaker identification.\\'}', metadata={})]", "category": "generic"}
{"question_id": 628, "text": " Our company's voice assistant needs to be able to detect voice activity in a conversation.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Voice Activity Detection\\', \\'framework\\': \\'pyannote.audio\\', \\'functionality\\': \\'Automatic Speech Recognition\\', \\'api_name\\': \\'pyannote/voice-activity-detection\\', \\'api_call\\': \"Pipeline.from_pretrained(\\'pyannote/voice-activity-detection\\')\", \\'api_arguments\\': [\\'audio.wav\\'], \\'python_environment_requirements\\': [\\'pyannote.audio 2.1\\'], \\'example_code\\': \\'from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained(pyannote/voice-activity-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\\\noutput = pipeline(audio.wav)\\\\nfor speech in output.get_timeline().support():\\\\n  # active speech between speech.start and speech.end\\', \\'performance\\': {\\'dataset\\': \\'ami\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'A pretrained voice activity detection pipeline that detects active speech in audio files.\\'}', metadata={})]", "category": "generic"}
{"question_id": 629, "text": " Implement a conference call control program which can recognize if there are any interruptions among the speakers during the meeting.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 630, "text": " We are trying to create a solution for an HR department to predict whether a candidate would be a potential employee based on a list of background information.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 631, "text": " I have just downloaded a dataset that measures CO2 Emissions. I want to classify this dataset using a pre-trained model.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Conversational\\', \\'api_name\\': \\'Pi3141/DialoGPT-medium-elon-3\\', \\'api_call\\': \"pipeline(\\'text-generation\\', model=\\'Pi3141/DialoGPT-medium-elon-3\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'Input a message to start chatting with Pi3141/DialoGPT-medium-elon-3.\\', \\'performance\\': {\\'dataset\\': \\'Twitter tweets by Elon Musk\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'DialoGPT model that talks like Elon Musk, trained on Twitter tweets by Elon Musk. This model will spew meaningless shit about 40% of the time. Trained on 8 epochs. But with a larger dataset this time. The AI can now use more emojis, I think.\\'}', metadata={})]", "category": "generic"}
{"question_id": 632, "text": " We are building an app that simulates Pokemon battles. Can you help us predict the HP of a Pokemon given its input attributes?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Regression\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Predicting Pokemon HP\\', \\'api_name\\': \\'julien-c/pokemon-predict-hp\\', \\'api_call\\': \"pipeline(\\'regression\\', model=\\'julien-c/pokemon-predict-hp\\')\", \\'api_arguments\\': \\'input_data\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'julien-c/kaggle-rounakbanik-pokemon\\', \\'accuracy\\': {\\'mean_absolute_error\\': 15.909, \\'model_loss\\': 647.605}}, \\'description\\': \\'A tabular regression model trained on the julien-c/kaggle-rounakbanik-pokemon dataset to predict the HP of Pokemon.\\'}', metadata={})]", "category": "generic"}
{"question_id": 633, "text": " We are a content marketing agency and we are focusing on promoting our clients' products. We want to use a tool to generate interesting marketing messages.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 634, "text": " Analyze a set of sentences to find the most similar pairs.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 635, "text": " I need a tool to extract code syntax and named entities from a text taken from StackOverflow.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Token Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Named Entity Recognition\\', \\'api_name\\': \\'dslim/bert-base-NER-uncased\\', \\'api_call\\': \"pipeline(\\'ner\\', model=\\'dslim/bert-base-NER-uncased\\')\", \\'api_arguments\\': {}, \\'python_environment_requirements\\': {\\'transformers\\': \\'>=4.0.0\\'}, \\'example_code\\': \"nlp(\\'My name is John and I live in New York.\\')\", \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A pretrained BERT model for Named Entity Recognition (NER) on uncased text. It can be used to extract entities such as person names, locations, and organizations from text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 636, "text": " Our video streaming platform wants to categorize thousands of movies into genres. Please help us classify them without any genre labels.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Zero-Shot Image Classification\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Zero-Shot Image Classification\\', \\'api_name\\': \\'laion/CLIP-ViT-g-14-laion2B-s34B-b88K\\', \\'api_call\\': \"pipeline(\\'zero-shot-image-classification\\', model=\\'laion/CLIP-ViT-g-14-laion2B-s34B-b88K\\')\", \\'api_arguments\\': {\\'image\\': \\'path/to/image/file\\', \\'class_names\\': \\'list_of_class_names\\'}, \\'python_environment_requirements\\': {\\'huggingface_hub\\': \\'0.0.17\\', \\'transformers\\': \\'4.11.3\\', \\'torch\\': \\'1.9.0\\', \\'torchvision\\': \\'0.10.0\\'}, \\'example_code\\': None, \\'performance\\': {\\'dataset\\': None, \\'accuracy\\': None}, \\'description\\': \\'A zero-shot image classification model based on OpenCLIP, which can classify images into various categories without requiring any training data for those categories.\\'}', metadata={})]", "category": "generic"}
{"question_id": 637, "text": " Can you generate an image of a lighthouse on a foggy island, based on my description?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'distilbart-cnn-12-6-samsum\\', \\'api_call\\': \"pipeline(\\'summarization\\', model=\\'philschmid/distilbart-cnn-12-6-samsum\\')\", \\'api_arguments\\': {\\'model\\': \\'philschmid/distilbart-cnn-12-6-samsum\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 41.09, \\'ROUGE-2\\': 20.746, \\'ROUGE-L\\': 31.595, \\'ROUGE-LSUM\\': 38.339}}, \\'description\\': \\'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 638, "text": " I am an illustrator, I want to create an appealing image based on a text description for commercial purposes.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 639, "text": " We want to build a Twitter Bot that creates an image based on users' textual requests. Generate an image with an astronaut playing guitar in space using a model.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 640, "text": " Generate an image of a beautiful fantasy landscape based on the description provided: a peaceful scene in a lush green forest with a crystal-clear river flowing through it, under a blue sky with fluffy white clouds.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 641, "text": " Tell me a text summary and answer a question from an image.\\n###Input: img_url=\\\"https://example.com/image.jpg\\\", question=\\\"What is the main color of the object?\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Visual Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ivelin/donut-refexp-combined-v1\\', \\'api_call\\': \"pipeline(\\'visual-question-answering\\', model=\\'ivelin/donut-refexp-combined-v1\\')\", \\'api_arguments\\': \\'image, question\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"vqa(image=\\'path/to/image.jpg\\', question=\\'What is the color of the object?\\')\", \\'performance\\': {\\'dataset\\': \\'ivelin/donut-refexp-combined-v1\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'A visual question answering model that takes an image and a question as input and provides an answer based on the visual content of the image and the context of the question.\\'}', metadata={})]", "category": "generic"}
{"question_id": 642, "text": " My boss wants me to extract captions from images of people in different settings.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Information Retrieval\\', \\'api_name\\': \\'cross-encoder/ms-marco-TinyBERT-L-2-v2\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'cross-encoder/ms-marco-TinyBERT-L-2-v2\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\"}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': {\\'import\\': \\'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\', \\'model\\': \"model = AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\", \\'scores\\': \\'with torch.no_grad():\\\\n    scores = model(**features).logits\\\\n    print(scores)\\'}, \\'performance\\': {\\'dataset\\': \\'TREC Deep Learning 2019\\', \\'accuracy\\': \\'69.84 (NDCG@10)\\'}, \\'description\\': \\'This model was trained on the MS Marco Passage Ranking task. It can be used for Information Retrieval: Given a query, encode the query with all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. The training code is available here: SBERT.net Training MS Marco.\\'}', metadata={})]", "category": "generic"}
{"question_id": 643, "text": " Create an AI system capable of producing short videos based on text input in Persian and English.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Unconditional Image Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Denoising Diffusion Probabilistic Models (DDPM)\\', \\'api_name\\': \\'google/ddpm-ema-bedroom-256\\', \\'api_call\\': \"DDPMPipeline.from_pretrained(\\'google/ddpm-ema-bedroom-256\\')\", \\'api_arguments\\': [\\'model_id\\'], \\'python_environment_requirements\\': [\\'diffusers\\'], \\'example_code\\': \\'!pip install diffusers\\\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\\\nmodel_id = google/ddpm-ema-bedroom-256\\\\nddpm = DDPMPipeline.from_pretrained(model_id)\\\\nimage = ddpm().images[0]\\\\nimage.save(ddpm_generated_image.png)\\', \\'performance\\': {\\'dataset\\': \\'CIFAR10\\', \\'accuracy\\': {\\'Inception_score\\': 9.46, \\'FID_score\\': 3.17}}, \\'description\\': \\'Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by nonequilibrium thermodynamics, capable of producing high-quality image synthesis results. The model can use discrete noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm for inference. It obtains an Inception score of 9.46 and a state-of-the-art FID score of 3.17 on the unconditional CIFAR10 dataset.\\'}', metadata={})]", "category": "generic"}
{"question_id": 644, "text": " I am organizing a virtual party and want to create a short, autogenerated video based on a text description (e.g., \\\"cats playing with laser pointer\\\"). Can you accomplish this with a text-to-video generation API?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Video\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Video Synthesis\\', \\'api_name\\': \\'modelscope-damo-text-to-video-synthesis\\', \\'api_call\\': \"pipeline(\\'text-to-video-synthesis\\', model_dir.as_posix())\", \\'api_arguments\\': {\\'text\\': \\'A short text description in English\\'}, \\'python_environment_requirements\\': [\\'modelscope==1.4.2\\', \\'open_clip_torch\\', \\'pytorch-lightning\\'], \\'example_code\\': \"from huggingface_hub import snapshot_download\\\\nfrom modelscope.pipelines import pipeline\\\\nfrom modelscope.outputs import OutputKeys\\\\nimport pathlib\\\\n\\\\nmodel_dir = pathlib.Path(\\'weights\\')\\\\nsnapshot_download(\\'damo-vilab/modelscope-damo-text-to-video-synthesis\\',\\\\n repo_type=\\'model\\', local_dir=model_dir)\\\\n\\\\npipe = pipeline(\\'text-to-video-synthesis\\', model_dir.as_posix())\\\\n\\\\ntest_text = {\\\\n \\'text\\': \\'A panda eating bamboo on a rock.\\',\\\\n}\\\\n\\\\noutput_video_path = pipe(test_text,)[OutputKeys.OUTPUT_VIDEO]\\\\nprint(\\'output_video_path:\\', output_video_path)\", \\'performance\\': {\\'dataset\\': \\'Webvid, ImageNet, LAION5B\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported.\\'}', metadata={})]", "category": "generic"}
{"question_id": 645, "text": " I need to create a system that can answer questions related to a document provided. The system should use a pre-trained model.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Audio Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Speaker Verification\\', \\'api_name\\': \\'speechbrain/spkrec-xvect-voxceleb\\', \\'api_call\\': \"EncoderClassifier.from_hparams(source=\\'speechbrain/spkrec-xvect-voxceleb\\', savedir=\\'pretrained_models/spkrec-xvect-voxceleb\\')\", \\'api_arguments\\': [\\'source\\', \\'savedir\\'], \\'python_environment_requirements\\': [\\'pip install speechbrain\\'], \\'example_code\\': \"import torchaudio\\\\nfrom speechbrain.pretrained import EncoderClassifier\\\\nclassifier = EncoderClassifier.from_hparams(source=speechbrain/spkrec-xvect-voxceleb, savedir=pretrained_models/spkrec-xvect-voxceleb)\\\\nsignal, fs =torchaudio.load(\\'tests/samples/ASR/spk1_snt1.wav\\')\\\\nembeddings = classifier.encode_batch(signal)\", \\'performance\\': {\\'dataset\\': \\'Voxceleb1-test set (Cleaned)\\', \\'accuracy\\': \\'EER(%) 3.2\\'}, \\'description\\': \\'This repository provides all the necessary tools to extract speaker embeddings with a pretrained TDNN model using SpeechBrain. The system is trained on Voxceleb 1+ Voxceleb2 training data.\\'}', metadata={})]", "category": "generic"}
{"question_id": 646, "text": " We are developing a document management system. We want to extract the relevant information from them through OCR text scanning and answering questions.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 647, "text": " The team is creating a home security system. We are currently looking at understanding the depth of objects in the video stream.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 648, "text": " I want to know the depth information of an image for a robot navigation project.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \"PegasusForConditionalGeneration.from_pretrained(\\'tuner007/pegasus_summarizer\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 649, "text": " A start-up is looking to develop a robot for navigation; they require a solution to estimate depth from a single image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Segmentation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'shi-labs/oneformer_ade20k_swin_tiny\\', \\'api_call\\': \"OneFormerForUniversalSegmentation.from_pretrained(\\'shi-labs/oneformer_ade20k_swin_tiny\\')\", \\'api_arguments\\': {\\'images\\': \\'image\\', \\'task_inputs\\': [\\'semantic\\', \\'instance\\', \\'panoptic\\'], \\'return_tensors\\': \\'pt\\'}, \\'python_environment_requirements\\': [\\'transformers\\', \\'PIL\\', \\'requests\\'], \\'example_code\\': \\'from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/ade20k.jpeg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\n\\\\nprocessor = OneFormerProcessor.from_pretrained(shi-labs/oneformer_ade20k_swin_tiny)\\\\nmodel = OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_ade20k_swin_tiny)\\\\n\\\\nsemantic_inputs = processor(images=image, task_inputs=[semantic], return_tensors=pt)\\\\nsemantic_outputs = model(**semantic_inputs)\\\\n\\\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\\\n\\\\ninstance_inputs = processor(images=image, task_inputs=[instance], return_tensors=pt)\\\\ninstance_outputs = model(**instance_inputs)\\\\n\\\\npredicted_instance_map = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0][segmentation]\\\\n\\\\npanoptic_inputs = processor(images=image, task_inputs=[panoptic], return_tensors=pt)\\\\npanoptic_outputs = model(**panoptic_inputs)\\\\n\\\\npredicted_semantic_map = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0][segmentation]\\', \\'performance\\': {\\'dataset\\': \\'ADE20k\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 650, "text": " We are building a smart security system to determine if a person is an adult\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 651, "text": " Design a program that can help farmers detect diseases in bean crops by analyzing images of the crop leaves.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 652, "text": " To build an intelligent system to recognize and identify objects in submitted pictures within the household, which includes furniture, electronics and ornaments.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Feature Extraction\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'DeepPavlov/rubert-base-cased\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'DeepPavlov/rubert-base-cased\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'Russian part of Wikipedia and news data\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'RuBERT (Russian, cased, 12\u2011layer, 768\u2011hidden, 12\u2011heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT\u2011base as an initialization for RuBERT[1].\\'}', metadata={})]", "category": "generic"}
{"question_id": 653, "text": " Give me a solution to find out the license plate in the given car images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Zero-Shot Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Cross-Encoder for Natural Language Inference\\', \\'api_name\\': \\'cross-encoder/nli-deberta-v3-base\\', \\'api_call\\': \"CrossEncoder(\\'cross-encoder/nli-deberta-v3-base\\')\", \\'api_arguments\\': [\\'sentence_pairs\\'], \\'python_environment_requirements\\': [\\'sentence_transformers\\', \\'transformers\\'], \\'example_code\\': \"from sentence_transformers import CrossEncoder\\\\nmodel = CrossEncoder(\\'cross-encoder/nli-deberta-v3-base\\')\\\\nscores = model.predict([(\\'A man is eating pizza\\', \\'A man eats something\\'), (\\'A black race car starts up in front of a crowd of people.\\', \\'A man is driving down a lonely road.\\')])\", \\'performance\\': {\\'dataset\\': {\\'SNLI-test\\': \\'92.38\\', \\'MNLI mismatched set\\': \\'90.04\\'}}, \\'description\\': \\'This model is based on microsoft/deberta-v3-base and was trained on the SNLI and MultiNLI datasets. For a given sentence pair, it will output three scores corresponding to the labels: contradiction, entailment, neutral.\\'}', metadata={})]", "category": "generic"}
{"question_id": 654, "text": " I want to build a social media app for outdoor enthusiasts, and I need to identify objects related to outdoor activities in images. Come up with a proper API call to identify these objects.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 655, "text": " I want an AI that can separate objects in an image based on semantic segmentation.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Conversational\\', \\'api_name\\': \\'Pi3141/DialoGPT-medium-elon-3\\', \\'api_call\\': \"pipeline(\\'text-generation\\', model=\\'Pi3141/DialoGPT-medium-elon-3\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'Input a message to start chatting with Pi3141/DialoGPT-medium-elon-3.\\', \\'performance\\': {\\'dataset\\': \\'Twitter tweets by Elon Musk\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'DialoGPT model that talks like Elon Musk, trained on Twitter tweets by Elon Musk. This model will spew meaningless shit about 40% of the time. Trained on 8 epochs. But with a larger dataset this time. The AI can now use more emojis, I think.\\'}', metadata={})]", "category": "generic"}
{"question_id": 656, "text": " Our image-processing service has to extract contextual information from images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Image Variations\\', \\'api_name\\': \\'lambdalabs/sd-image-variations-diffusers\\', \\'api_call\\': \"StableDiffusionImageVariationPipeline.from_pretrained(\\'lambdalabs/sd-image-variations-diffusers\\', revision=\\'v2.0\\')\", \\'api_arguments\\': {\\'revision\\': \\'v2.0\\'}, \\'python_environment_requirements\\': \\'Diffusers >=0.8.0\\', \\'example_code\\': \\'from diffusers import StableDiffusionImageVariationPipeline\\\\nfrom PIL import Image\\\\ndevice = cuda:0\\\\nsd_pipe = StableDiffusionImageVariationPipeline.from_pretrained(\\\\n lambdalabs/sd-image-variations-diffusers,\\\\n revision=v2.0,\\\\n)\\\\nsd_pipe = sd_pipe.to(device)\\\\nim = Image.open(path/to/image.jpg)\\\\ntform = transforms.Compose([\\\\n transforms.ToTensor(),\\\\n transforms.Resize(\\\\n  (224, 224),\\\\n  interpolation=transforms.InterpolationMode.BICUBIC,\\\\n  antialias=False,\\\\n ),\\\\n transforms.Normalize(\\\\n  [0.48145466, 0.4578275, 0.40821073],\\\\n  [0.26862954, 0.26130258, 0.27577711]),\\\\n])\\\\ninp = tform(im).to(device).unsqueeze(0)\\\\nout = sd_pipe(inp, guidance_scale=3)\\\\nout[images][0].save(result.jpg)\\', \\'performance\\': {\\'dataset\\': \\'ChristophSchuhmann/improved_aesthetics_6plus\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\\'}', metadata={})]", "category": "generic"}
{"question_id": 657, "text": " Our company develops a city planning application. We need to segment streets, buildings, and trees in aerial photographs.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 658, "text": " We are building a product which can identify birds in the images. Design the model which can help us segment the birds in an image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Segmentation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Segmentation\\', \\'api_name\\': \\'keremberke/yolov8m-building-segmentation\\', \\'api_call\\': \"YOLO(\\'keremberke/yolov8m-building-segmentation\\')\", \\'api_arguments\\': {\\'image\\': \\'URL or local path to the image\\'}, \\'python_environment_requirements\\': \\'pip install ultralyticsplus==0.0.21\\', \\'example_code\\': \"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO(\\'keremberke/yolov8m-building-segmentation\\')\\\\nmodel.overrides[\\'conf\\'] = 0.25\\\\nmodel.overrides[\\'iou\\'] = 0.45\\\\nmodel.overrides[\\'agnostic_nms\\'] = False\\\\nmodel.overrides[\\'max_det\\'] = 1000\\\\nimage = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nprint(results[0].masks)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\", \\'performance\\': {\\'dataset\\': \\'satellite-building-segmentation\\', \\'accuracy\\': {\\'mAP@0.5(box)\\': 0.623, \\'mAP@0.5(mask)\\': 0.613}}, \\'description\\': \\'A YOLOv8 model for building segmentation in satellite images. It can detect and segment buildings in the input images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 659, "text": " In this Star Wars movie scene, I want to create a depth estimation for the stormtroopers.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'sberbank-ai/sbert_large_mt_nlu_ru\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'sberbank-ai/sbert_large_mt_nlu_ru\\')\", \\'api_arguments\\': [\\'sentences\\', \\'padding\\', \\'truncation\\', \\'max_length\\', \\'return_tensors\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': \"from transformers import AutoTokenizer, AutoModel\\\\nimport torch\\\\n\\\\n\\\\n# Mean Pooling - Take attention mask into account for correct averaging\\\\ndef mean_pooling(model_output, attention_mask):\\\\n    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\\\\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\\\\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\\\\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\\\\n    return sum_embeddings / sum_mask\\\\n\\\\n\\\\n# Sentences we want sentence embeddings for sentences = [\\'?\\']\\\\n\\\\n# Load AutoModel from huggingface model repository\\\\ntokenizer = AutoTokenizer.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\\\nmodel = AutoModel.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\\\n\\\\n# Tokenize sentences\\\\nencoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=24, return_tensors=\\'pt\\')\\\\n\\\\n# Compute token embeddings\\\\nwith torch.no_grad():\\\\n    model_output = model(**encoded_input)\\\\n\\\\n# Perform pooling. In this case, mean pooling\\\\nsentence_embeddings = mean_pooling(model_output, encoded_input[\\'attention_mask\\'])\", \\'performance\\': {\\'dataset\\': \\'Russian SuperGLUE\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'BERT large model multitask (cased) for Sentence Embeddings in Russian language.\\'}', metadata={})]", "category": "generic"}
{"question_id": 660, "text": " I am a drone maker that is building a navigation module for my drones. I need to sharpen the images captured from the drone in real-time.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 661, "text": " Develop a method to generate images of realistic-looking churches.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Classification\\', \\'api_name\\': \\'microsoft/swinv2-tiny-patch4-window8-256\\', \\'api_call\\': \"AutoModelForImageClassification.from_pretrained(\\'microsoft/swinv2-tiny-patch4-window8-256\\')\", \\'api_arguments\\': {\\'image\\': \\'http://images.cocodataset.org/val2017/000000039769.jpg\\'}, \\'python_environment_requirements\\': [\\'transformers\\', \\'PIL\\', \\'requests\\'], \\'example_code\\': \\'from transformers import AutoImageProcessor, AutoModelForImageClassification\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nprocessor = AutoImageProcessor.from_pretrained(microsoft/swinv2-tiny-patch4-window8-256)\\\\nmodel = AutoModelForImageClassification.from_pretrained(microsoft/swinv2-tiny-patch4-window8-256)\\\\ninputs = processor(images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\nlogits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'imagenet-1k\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Swin Transformer v2 model pre-trained on ImageNet-1k at resolution 256x256. It was introduced in the paper Swin Transformer V2: Scaling Up Capacity and Resolution by Liu et al. and first released in this repository. The Swin Transformer is a type of Vision Transformer. It builds hierarchical feature maps by merging image patches in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window. Swin Transformer v2 adds 3 main improvements: 1) a residual-post-norm method combined with cosine attention to improve training stability; 2) a log-spaced continuous position bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with high-resolution inputs; 3) a self-supervised pre-training method, SimMIM, to reduce the needs of vast labeled images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 662, "text": " Generate a classical image by using Diffusion Model\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Image Variations\\', \\'api_name\\': \\'lambdalabs/sd-image-variations-diffusers\\', \\'api_call\\': \"StableDiffusionImageVariationPipeline.from_pretrained(\\'lambdalabs/sd-image-variations-diffusers\\', revision=\\'v2.0\\')\", \\'api_arguments\\': {\\'revision\\': \\'v2.0\\'}, \\'python_environment_requirements\\': \\'Diffusers >=0.8.0\\', \\'example_code\\': \\'from diffusers import StableDiffusionImageVariationPipeline\\\\nfrom PIL import Image\\\\ndevice = cuda:0\\\\nsd_pipe = StableDiffusionImageVariationPipeline.from_pretrained(\\\\n lambdalabs/sd-image-variations-diffusers,\\\\n revision=v2.0,\\\\n)\\\\nsd_pipe = sd_pipe.to(device)\\\\nim = Image.open(path/to/image.jpg)\\\\ntform = transforms.Compose([\\\\n transforms.ToTensor(),\\\\n transforms.Resize(\\\\n  (224, 224),\\\\n  interpolation=transforms.InterpolationMode.BICUBIC,\\\\n  antialias=False,\\\\n ),\\\\n transforms.Normalize(\\\\n  [0.48145466, 0.4578275, 0.40821073],\\\\n  [0.26862954, 0.26130258, 0.27577711]),\\\\n])\\\\ninp = tform(im).to(device).unsqueeze(0)\\\\nout = sd_pipe(inp, guidance_scale=3)\\\\nout[images][0].save(result.jpg)\\', \\'performance\\': {\\'dataset\\': \\'ChristophSchuhmann/improved_aesthetics_6plus\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\\'}', metadata={})]", "category": "generic"}
{"question_id": 663, "text": " We need a picture that has a nostalgic look in high quality for the cover of our upcoming magazine.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 664, "text": " We are an e-learning provider who wants to classify the content of a video lecture automatically.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 665, "text": " We need to classify actions of athletes in sports videos. Can you help us to analyze and classify these videos?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 666, "text": " We want to classify images supplied by our users into categories such as cats, dogs, birds, and more.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Zero-Shot Image Classification\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Zero-Shot Image Classification\\', \\'api_name\\': \\'laion/CLIP-ViT-g-14-laion2B-s34B-b88K\\', \\'api_call\\': \"pipeline(\\'zero-shot-image-classification\\', model=\\'laion/CLIP-ViT-g-14-laion2B-s34B-b88K\\')\", \\'api_arguments\\': {\\'image\\': \\'path/to/image/file\\', \\'class_names\\': \\'list_of_class_names\\'}, \\'python_environment_requirements\\': {\\'huggingface_hub\\': \\'0.0.17\\', \\'transformers\\': \\'4.11.3\\', \\'torch\\': \\'1.9.0\\', \\'torchvision\\': \\'0.10.0\\'}, \\'example_code\\': None, \\'performance\\': {\\'dataset\\': None, \\'accuracy\\': None}, \\'description\\': \\'A zero-shot image classification model based on OpenCLIP, which can classify images into various categories without requiring any training data for those categories.\\'}', metadata={})]", "category": "generic"}
{"question_id": 667, "text": " Identify which smartphone brand is featured in an image and predict the intensity of luxury level.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'impira/layoutlm-invoices\\', \\'api_call\\': \"pipeline(\\'question-answering\\', model=\\'impira/layoutlm-invoices\\')\", \\'api_arguments\\': \\'question, context\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"qa_pipeline(question=\\'your question\\', context=\\'your document context\\')\", \\'performance\\': {\\'dataset\\': \\'proprietary dataset of invoices, SQuAD2.0, and DocVQA\\', \\'accuracy\\': \\'not provided\\'}, \\'description\\': \\'This is a fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens (because they predict the start and end of a sequence), this model can predict longer-range, non-consecutive sequences with an additional classifier head.\\'}', metadata={})]", "category": "generic"}
{"question_id": 668, "text": " A Chinese social media company has approached us for help in content moderation. We recorded an API to classify images which don't belong to suitable content.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'csebuetnlp/mT5_multilingual_XLSum\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'csebuetnlp/mT5_multilingual_XLSum\\')\", \\'api_arguments\\': [\\'model_name\\'], \\'python_environment_requirements\\': [\\'transformers==4.11.0.dev0\\'], \\'example_code\\': \"import re\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nWHITESPACE_HANDLER = lambda k: re.sub(\\'\\\\\\\\s+\\', \\' \\', re.sub(\\'\\\\\\\\n+\\', \\' \\', k.strip()))\\\\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people\\'s scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We\\'re expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\\\\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\\\ninput_ids = tokenizer(\\\\n [WHITESPACE_HANDLER(article_text)],\\\\n return_tensors=pt,\\\\n padding=max_length,\\\\n truncation=True,\\\\n max_length=512\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n max_length=84,\\\\n no_repeat_ngram_size=2,\\\\n num_beams=4\\\\n)[0]\\\\nsummary = tokenizer.decode(\\\\n output_ids,\\\\n skip_special_tokens=True,\\\\n clean_up_tokenization_spaces=False\\\\n)\\\\nprint(summary)\", \\'performance\\': {\\'dataset\\': \\'xsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.5, \\'ROUGE-2\\': 13.934, \\'ROUGE-L\\': 28.988, \\'ROUGE-LSUM\\': 28.996, \\'loss\\': 2.067, \\'gen_len\\': 26.973}}, \\'description\\': \\'This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 669, "text": " We are working on environmental studies and there is a specific image we need to geolocalize. Let's get the probabilities of different cities for the given image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 670, "text": " Our e-commerce platform needs automatic classification of product images without retraining. Develop a classifier for the e-commerce platform.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 671, "text": " We are developing a customer support platform for our telecommunication company in Spain. We want to know if they are happy or unhappy with our services, based on the content of their message.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 672, "text": " I need a function to find the most relevent passage given a question and several candidate passages.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'distilbart-cnn-12-6-samsum\\', \\'api_call\\': \"pipeline(\\'summarization\\', model=\\'philschmid/distilbart-cnn-12-6-samsum\\')\", \\'api_arguments\\': {\\'model\\': \\'philschmid/distilbart-cnn-12-6-samsum\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 41.09, \\'ROUGE-2\\': 20.746, \\'ROUGE-L\\': 31.595, \\'ROUGE-LSUM\\': 38.339}}, \\'description\\': \\'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 673, "text": " As a community manager, I would like to monitor my forum's comment section for toxic or harmful content. I want to find a solution that can flag these types of comments automatically, so I can address them promptly.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text-to-Text Generation\\', \\'api_name\\': \\'philschmid/bart-large-cnn-samsum\\', \\'api_call\\': \"pipeline(\\'summarization\\', model=\\'philschmid/bart-large-cnn-samsum\\')\", \\'api_arguments\\': {\\'model\\': \\'philschmid/bart-large-cnn-samsum\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'latest\\'}, \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/bart-large-cnn-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'eval_rouge1\\': 42.621, \\'eval_rouge2\\': 21.9825, \\'eval_rougeL\\': 33.034, \\'eval_rougeLsum\\': 39.6783, \\'test_rouge1\\': 41.3174, \\'test_rouge2\\': 20.8716, \\'test_rougeL\\': 32.1337, \\'test_rougeLsum\\': 38.4149}}, \\'description\\': \\'philschmid/bart-large-cnn-samsum is a BART-based model trained for text summarization on the SAMSum dataset. It can be used to generate abstractive summaries of conversations.\\'}', metadata={})]", "category": "generic"}
{"question_id": 674, "text": " I am making a keyword search engine that ranks text passages based on their importance regarding a given keyword.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 675, "text": " Identify the type of emotion in a movie review.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 676, "text": " We are developing a medical records analysis software that automatically recognizes biomedical entities from physicians' case reports.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 677, "text": " We have customer reviews of various software products. We want to extract company names in those reviews.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'Kirili4ik/mbart_ruDialogSum\\', \\'api_call\\': \"MBartForConditionalGeneration.from_pretrained(\\'Kirili4ik/mbart_ruDialogSum\\')\", \\'api_arguments\\': {\\'model_name\\': \\'Kirili4ik/mbart_ruDialogSum\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import MBartTokenizer, MBartForConditionalGeneration\\\\nmodel_name = Kirili4ik/mbart_ruDialogSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = MBartForConditionalGeneration.from_pretrained(model_name)\\\\nmodel.eval()\\\\narticle_text = ...\\\\ninput_ids = tokenizer(\\\\n [article_text],\\\\n max_length=600,\\\\n padding=max_length,\\\\n truncation=True,\\\\n return_tensors=pt,\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n top_k=0,\\\\n num_beams=3,\\\\n no_repeat_ngram_size=3\\\\n)[0]\\\\nsummary = tokenizer.decode(output_ids, skip_special_tokens=True)\\\\nprint(summary)\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'SAMSum Corpus (translated to Russian)\\', \\'accuracy\\': {\\'Validation ROGUE-1\\': 34.5, \\'Validation ROGUE-L\\': 33, \\'Test ROGUE-1\\': 31, \\'Test ROGUE-L\\': 28}}]}, \\'description\\': \\'MBart for Russian summarization fine-tuned for dialogues summarization. This model was firstly fine-tuned by Ilya Gusev on Gazeta dataset. We have fine tuned that model on SamSum dataset translated to Russian using GoogleTranslateAPI. Moreover! We have implemented a ! telegram bot @summarization_bot ! with the inference of this model. Add it to the chat and get summaries instead of dozens spam messages!\\'}', metadata={})]", "category": "generic"}
{"question_id": 678, "text": " I am working on a Chinese language project and I need to tokenize the sentences for better processing.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 679, "text": " I need a system that extracts all the well-known named entities such as person names, locations, and organizations from news articles.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Token Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Named Entity Recognition\\', \\'api_name\\': \\'dslim/bert-base-NER-uncased\\', \\'api_call\\': \"pipeline(\\'ner\\', model=\\'dslim/bert-base-NER-uncased\\')\", \\'api_arguments\\': {}, \\'python_environment_requirements\\': {\\'transformers\\': \\'>=4.0.0\\'}, \\'example_code\\': \"nlp(\\'My name is John and I live in New York.\\')\", \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A pretrained BERT model for Named Entity Recognition (NER) on uncased text. It can be used to extract entities such as person names, locations, and organizations from text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 680, "text": " I have a list of Olympic Game host cities and their corresponding years. I want to know which year the games were held in Beijing.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \"PegasusForConditionalGeneration.from_pretrained(\\'tuner007/pegasus_summarizer\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 681, "text": " I have a table with data about different types of bards and their magical abilities. Let me see if I can find the best bard given the data in the table.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \"PegasusForConditionalGeneration.from_pretrained(\\'tuner007/pegasus_summarizer\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 682, "text": " I want to automate the process of answering questions about historical facts. When given a question and a surrounding context, it should provide an accurate response.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'PyTorch Transformers\\', \\'functionality\\': \\'Table Question Answering\\', \\'api_name\\': \\'table-question-answering-tapas\\', \\'api_call\\': \"pipeline(\\'table-question-answering\\', model=\\'Meena/table-question-answering-tapas\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'This model can be loaded on the Inference API on-demand.\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'SQA (Sequential Question Answering by Microsoft)\\', \\'accuracy\\': None}, {\\'name\\': \\'WTQ (Wiki Table Questions by Stanford University)\\', \\'accuracy\\': None}, {\\'name\\': \\'WikiSQL (by Salesforce)\\', \\'accuracy\\': None}]}, \\'description\\': \\'TAPAS, the model learns an inner representation of the English language used in tables and associated texts, which can then be used to extract features useful for downstream tasks such as answering questions about a table, or determining whether a sentence is entailed or refuted by the contents of a table. It is a BERT-based model specifically designed (and pre-trained) for answering questions about tabular data. TAPAS uses relative position embeddings and has 7 token types that encode tabular structure. It is pre-trained on the masked language modeling (MLM) objective on a large dataset comprising millions of tables from English Wikipedia and corresponding texts.\\'}', metadata={})]", "category": "generic"}
{"question_id": 683, "text": " I am a lawyer, I have a lot of text. I need a system which reads context and answers the questions based on the context.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 684, "text": " I want to create a question answering script that can help me answer questions about a given passage of text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'deepset/roberta-large-squad2\\', \\'api_call\\': \"pipeline(\\'question-answering\\', model=\\'deepset/roberta-large-squad2\\')\", \\'api_arguments\\': [\\'question\\', \\'context\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline; nlp = pipeline(\\'question-answering\\', model=\\'deepset/roberta-large-squad2\\'); nlp({\\'question\\': \\'What is the capital of Germany?\\', \\'context\\': \\'Berlin is the capital of Germany.\\'})\", \\'performance\\': {\\'dataset\\': \\'squad_v2\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'A pre-trained RoBERTa model for question answering tasks, specifically trained on the SQuAD v2 dataset. It can be used to answer questions based on a given context.\\'}', metadata={})]", "category": "generic"}
{"question_id": 685, "text": " To enhance our FAQ bot, we need to extract answers from a given knowledge base text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Conversational\\', \\'api_name\\': \\'microsoft/GODEL-v1_1-large-seq2seq\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'microsoft/GODEL-v1_1-large-seq2seq\\')\", \\'api_arguments\\': {\\'instruction\\': \\'Instruction: given a dialog context, you need to response empathically.\\', \\'knowledge\\': \\'\\', \\'dialog\\': [\\'Does money buy happiness?\\', \\'It is a question. Money buys you a lot of things, but not enough to buy happiness.\\', \\'What is the best way to buy happiness ?\\']}, \\'python_environment_requirements\\': {\\'transformers\\': \\'AutoTokenizer, AutoModelForSeq2SeqLM\\'}, \\'example_code\\': \"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained(microsoft/GODEL-v1_1-large-seq2seq)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(microsoft/GODEL-v1_1-large-seq2seq)\\\\ndef generate(instruction, knowledge, dialog):\\\\n if knowledge != \\'\\':\\\\n knowledge = \\'[KNOWLEDGE] \\' + knowledge\\\\n dialog = \\' EOS \\'.join(dialog)\\\\n query = f{instruction} [CONTEXT] {dialog} {knowledge}\\\\n input_ids = tokenizer(f{query}, return_tensors=pt).input_ids\\\\n outputs = model.generate(input_ids, max_length=128, min_length=8, top_p=0.9, do_sample=True)\\\\n output = tokenizer.decode(outputs[0], skip_special_tokens=True)\\\\n return output\\\\n\\\\ninstruction = f\\'Instruction: given a dialog context, you need to response empathically.\\'\\\\nknowledge = \\'\\'\\\\ndialog = [\\\\n \\'Does money buy happiness?\\',\\\\n \\'It is a question. Money buys you a lot of things, but not enough to buy happiness.\\',\\\\n \\'What is the best way to buy happiness ?\\'\\\\n]\\\\nresponse = generate(instruction, knowledge, dialog)\\\\nprint(response)\", \\'performance\\': {\\'dataset\\': \\'Reddit discussion thread, instruction and knowledge grounded dialogs\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'GODEL is a large-scale pre-trained model for goal-directed dialogs. It is parameterized with a Transformer-based encoder-decoder model and trained for response generation grounded in external text, which allows more effective fine-tuning on dialog tasks that require conditioning the response on information that is external to the current conversation (e.g., a retrieved document). The pre-trained model can be efficiently fine-tuned and adapted to accomplish a new dialog task with a handful of task-specific dialogs. The v1.1 model is trained on 551M multi-turn dialogs from Reddit discussion thread, and 5M instruction and knowledge grounded dialogs.\\'}', metadata={})]", "category": "generic"}
{"question_id": 686, "text": " Our customer is a Spanish travel agency. They need to classify customer reviews into categories such as 'travel', 'cooking', and 'dancing'.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Zero-Shot Image Classification\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Zero-Shot Image Classification\\', \\'api_name\\': \\'laion/CLIP-ViT-g-14-laion2B-s34B-b88K\\', \\'api_call\\': \"pipeline(\\'zero-shot-image-classification\\', model=\\'laion/CLIP-ViT-g-14-laion2B-s34B-b88K\\')\", \\'api_arguments\\': {\\'image\\': \\'path/to/image/file\\', \\'class_names\\': \\'list_of_class_names\\'}, \\'python_environment_requirements\\': {\\'huggingface_hub\\': \\'0.0.17\\', \\'transformers\\': \\'4.11.3\\', \\'torch\\': \\'1.9.0\\', \\'torchvision\\': \\'0.10.0\\'}, \\'example_code\\': None, \\'performance\\': {\\'dataset\\': None, \\'accuracy\\': None}, \\'description\\': \\'A zero-shot image classification model based on OpenCLIP, which can classify images into various categories without requiring any training data for those categories.\\'}', metadata={})]", "category": "generic"}
{"question_id": 687, "text": " Create a method to determine, as an entertainment recommendation system, which category a text message about a daily activity belongs to.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Generation\\', \\'api_name\\': \\'pygmalion-6b\\', \\'api_call\\': \"AutoModelForCausalLM.from_pretrained(\\'waifu-workshop/pygmalion-6b\\')\", \\'api_arguments\\': [\\'input_ids\\', \\'max_length\\', \\'num_return_sequences\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': \"from transformers import AutoTokenizer, AutoModelForCausalLM\\\\n\\\\ntokenizer = AutoTokenizer.from_pretrained(\\'waifu-workshop/pygmalion-6b\\')\\\\nmodel = AutoModelForCausalLM.from_pretrained(\\'waifu-workshop/pygmalion-6b\\')\\\\n\\\\ninput_text = [CHARACTER]\\'s Persona: [A few sentences about the character you want the model to play]\\\\\\\\n<START>\\\\\\\\n[DIALOGUE HISTORY]\\\\\\\\nYou: [Your input message here]\\\\\\\\n[CHARACTER]:\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=\\'pt\\')\\\\n\\\\noutput = model.generate(input_ids, max_length=100, num_return_sequences=1)\\\\n\\\\noutput_text = tokenizer.decode(output[0], skip_special_tokens=True)\", \\'performance\\': {\\'dataset\\': \\'56MB of dialogue data gathered from multiple sources\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \"Pygmalion 6B is a proof-of-concept dialogue model based on EleutherAI\\'s GPT-J-6B. It is fine-tuned on 56MB of dialogue data gathered from multiple sources, which includes both real and partially machine-generated conversations. The model is intended for conversational text generation and can be used to play a character in a dialogue.\"}', metadata={})]", "category": "generic"}
{"question_id": 688, "text": " We have written a summary of a new book's plot. Now, we want to ensure if the summary contains conflicting information.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 689, "text": " We have a user comment about a product, and we want to infer if the sentiment of the comment is positive or negative.\\n###Input: \\\"I recently purchased this product and it completely exceeded my expectations! The build quality is top-notch, and I've already recommended it to several friends.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 690, "text": " Extract a conclusion from the following text: \\\"Studies have been shown that owning a dog is good for you. Having a dog can help decrease stress levels, improve your mood, and increase physical activity.\\\"\\n###Input: Studies have been shown that owning a dog is good for you. Having a dog can help decrease stress levels, improve your mood, and increase physical activity.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Translation, Summarization, Question Answering, Text Classification\\', \\'api_name\\': \\'t5-base\\', \\'api_call\\': \"T5Model.from_pretrained(\\'t5-base\\')\", \\'api_arguments\\': [\\'input_ids\\', \\'decoder_input_ids\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import T5Tokenizer, T5Model\\\\ntokenizer = T5Tokenizer.from_pretrained(\\'t5-base\\')\\\\nmodel = T5Model.from_pretrained(\\'t5-base\\')\\\\ninput_ids = tokenizer(\\'Studies have been shown that owning a dog is good for you\\', return_tensors=\\'pt\\').input_ids\\\\ndecoder_input_ids = tokenizer(\\'Studies show that\\', return_tensors=\\'pt\\').input_ids\\\\noutputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\\\\nlast_hidden_states = outputs.last_hidden_state\", \\'performance\\': {\\'dataset\\': \\'c4\\', \\'accuracy\\': \\'See research paper, Table 14\\'}, \\'description\\': \\'T5-Base is a Text-To-Text Transfer Transformer (T5) model with 220 million parameters. It is designed to perform various NLP tasks, including machine translation, document summarization, question answering, and text classification. The model is pre-trained on the Colossal Clean Crawled Corpus (C4) and can be used with the Transformers library.\\'}', metadata={})]", "category": "generic"}
{"question_id": 691, "text": " Our company needs a versatile NLP model to build a social media manager to generate summaries of lengthy articles for sharing on social media.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'csebuetnlp/mT5_multilingual_XLSum\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'csebuetnlp/mT5_multilingual_XLSum\\')\", \\'api_arguments\\': [\\'model_name\\'], \\'python_environment_requirements\\': [\\'transformers==4.11.0.dev0\\'], \\'example_code\\': \"import re\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nWHITESPACE_HANDLER = lambda k: re.sub(\\'\\\\\\\\s+\\', \\' \\', re.sub(\\'\\\\\\\\n+\\', \\' \\', k.strip()))\\\\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people\\'s scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We\\'re expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\\\\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\\\ninput_ids = tokenizer(\\\\n [WHITESPACE_HANDLER(article_text)],\\\\n return_tensors=pt,\\\\n padding=max_length,\\\\n truncation=True,\\\\n max_length=512\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n max_length=84,\\\\n no_repeat_ngram_size=2,\\\\n num_beams=4\\\\n)[0]\\\\nsummary = tokenizer.decode(\\\\n output_ids,\\\\n skip_special_tokens=True,\\\\n clean_up_tokenization_spaces=False\\\\n)\\\\nprint(summary)\", \\'performance\\': {\\'dataset\\': \\'xsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.5, \\'ROUGE-2\\': 13.934, \\'ROUGE-L\\': 28.988, \\'ROUGE-LSUM\\': 28.996, \\'loss\\': 2.067, \\'gen_len\\': 26.973}}, \\'description\\': \\'This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 692, "text": " We have a real estate website and would like to translate property descriptions in English to French.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'Kirili4ik/mbart_ruDialogSum\\', \\'api_call\\': \"MBartForConditionalGeneration.from_pretrained(\\'Kirili4ik/mbart_ruDialogSum\\')\", \\'api_arguments\\': {\\'model_name\\': \\'Kirili4ik/mbart_ruDialogSum\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import MBartTokenizer, MBartForConditionalGeneration\\\\nmodel_name = Kirili4ik/mbart_ruDialogSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = MBartForConditionalGeneration.from_pretrained(model_name)\\\\nmodel.eval()\\\\narticle_text = ...\\\\ninput_ids = tokenizer(\\\\n [article_text],\\\\n max_length=600,\\\\n padding=max_length,\\\\n truncation=True,\\\\n return_tensors=pt,\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n top_k=0,\\\\n num_beams=3,\\\\n no_repeat_ngram_size=3\\\\n)[0]\\\\nsummary = tokenizer.decode(output_ids, skip_special_tokens=True)\\\\nprint(summary)\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'SAMSum Corpus (translated to Russian)\\', \\'accuracy\\': {\\'Validation ROGUE-1\\': 34.5, \\'Validation ROGUE-L\\': 33, \\'Test ROGUE-1\\': 31, \\'Test ROGUE-L\\': 28}}]}, \\'description\\': \\'MBart for Russian summarization fine-tuned for dialogues summarization. This model was firstly fine-tuned by Ilya Gusev on Gazeta dataset. We have fine tuned that model on SamSum dataset translated to Russian using GoogleTranslateAPI. Moreover! We have implemented a ! telegram bot @summarization_bot ! with the inference of this model. Add it to the chat and get summaries instead of dozens spam messages!\\'}', metadata={})]", "category": "generic"}
{"question_id": 693, "text": " My friend wrote me an English text about the upcoming holiday. I need to translate it to Arabic so I can share it with my family.\\n###Input: \\\"My friend is planning a holiday trip for our families. He found a beautiful place with a beach, swimming pool, and a wide range of outdoor activities for kids. There's also a famous seafood restaurant nearby! I think our families will have a great time together.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 694, "text": " We received a customer feedback document which is very lengthy. We need a summarization of it.\\n###Input: The customer support service was excellent. All our concerns were attended to promptly by the friendly and knowledgeable staff. The user interface, however, could use some improvement. Navigating through the platform can be challenging, and it took us quite some time to find the relevant information we needed.\\nAdditionally, some of our team members faced technical issues while using the platform, particularly with the integration of third-party tools. We had to reach out to the support team multiple times to resolve these issues. Overall, while we genuinely appreciate your team's assistance, we expect better performance from the platform itself.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 695, "text": " I would like to give a brief overview of our team meeting to my supervisor, so I need a summary of the conversation.\\n###Input: \\\"Anna: In today's meeting, we discussed increasing marketing budget. Tom: I suggested allocating more funds to social media campaigns. Sarah: I proposed focusing on improving SEO. Anna: We agreed on investing in content creation, too. Tom: The team will revise the strategy and present it next week. Sarah: Let's determine new KPIs for evaluating our progress.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'csebuetnlp/mT5_multilingual_XLSum\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'csebuetnlp/mT5_multilingual_XLSum\\')\", \\'api_arguments\\': [\\'model_name\\'], \\'python_environment_requirements\\': [\\'transformers==4.11.0.dev0\\'], \\'example_code\\': \"import re\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nWHITESPACE_HANDLER = lambda k: re.sub(\\'\\\\\\\\s+\\', \\' \\', re.sub(\\'\\\\\\\\n+\\', \\' \\', k.strip()))\\\\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people\\'s scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We\\'re expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\\\\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\\\ninput_ids = tokenizer(\\\\n [WHITESPACE_HANDLER(article_text)],\\\\n return_tensors=pt,\\\\n padding=max_length,\\\\n truncation=True,\\\\n max_length=512\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n max_length=84,\\\\n no_repeat_ngram_size=2,\\\\n num_beams=4\\\\n)[0]\\\\nsummary = tokenizer.decode(\\\\n output_ids,\\\\n skip_special_tokens=True,\\\\n clean_up_tokenization_spaces=False\\\\n)\\\\nprint(summary)\", \\'performance\\': {\\'dataset\\': \\'xsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.5, \\'ROUGE-2\\': 13.934, \\'ROUGE-L\\': 28.988, \\'ROUGE-LSUM\\': 28.996, \\'loss\\': 2.067, \\'gen_len\\': 26.973}}, \\'description\\': \\'This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 696, "text": " We need to develop a conversational chatbot that can answer users' queries, ask questions, and have a friendly conversation.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'facebook/blenderbot-1B-distill\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'facebook/blenderbot-1B-distill\\')\", \\'api_arguments\\': {\\'input_message\\': \\'string\\'}, \\'python_environment_requirements\\': {\\'library\\': \\'transformers\\', \\'version\\': \\'latest\\'}, \\'example_code\\': \"inputs = tokenizer(\\'Hello, how are you?\\', return_tensors=\\'pt\\'); outputs = model.generate(inputs[\\'input_ids\\']); decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\", \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not mentioned\\'}, \\'description\\': \\'BlenderBot-1B is a large-scale open-domain chatbot model that can engage in conversations, ask and answer questions, and display knowledge, empathy, and personality. This distilled version is smaller and faster than the original 9.4B parameter model, making it more accessible for use.\\'}', metadata={})]", "category": "generic"}
{"question_id": 697, "text": " My daughter left her laptop logged in, and I found her English essay unfinished. There's a sentence that goes like \\\"In the story, the antagonist represents the <mask> nature of humanity.\\\" Can you help me complete her sentence with an appropriate word?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'lidiya/bart-large-xsum-samsum\\', \\'api_call\\': \"pipeline(\\'summarization\\', model=\\'lidiya/bart-large-xsum-samsum\\')\", \\'api_arguments\\': \\'conversation\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=lidiya/bart-large-xsum-samsum)\\\\nconversation = \\'\\'\\'Hannah: Hey, do you have Betty\\'s number?\\\\nAmanda: Lemme check\\\\nAmanda: Sorry, can\\'t find it.\\\\nAmanda: Ask Larry\\\\nAmanda: He called her last time we were at the park together\\\\nHannah: I don\\'t know him well\\\\nAmanda: Don\\'t be shy, he\\'s very nice\\\\nHannah: If you say so..\\\\nHannah: I\\'d rather you texted him\\\\nAmanda: Just text him \ud83d\ude42\\\\nHannah: Urgh.. Alright\\\\nHannah: Bye\\\\nAmanda: Bye bye <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization\\', \\'accuracy\\': {\\'rouge1\\': 53.306, \\'rouge2\\': 28.355, \\'rougeL\\': 44.095}}, \\'description\\': \\'This model was obtained by fine-tuning facebook/bart-large-xsum on Samsum dataset.\\'}', metadata={})]", "category": "generic"}
{"question_id": 698, "text": " We are developing a program to teach French to English speakers. The program should complete a sentence with a missing word in French.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Program Synthesis\\', \\'api_name\\': \\'Salesforce/codegen-350M-multi\\', \\'api_call\\': \"AutoTokenizer.from_pretrained(\\'Salesforce/codegen-350M-multi\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelForCausalLM\\\\ntokenizer = AutoTokenizer.from_pretrained(Salesforce/codegen-350M-multi)\\\\nmodel = AutoModelForCausalLM.from_pretrained(Salesforce/codegen-350M-multi)\\\\ntext = def hello_world():\\\\ninput_ids = tokenizer(text, return_tensors=pt).input_ids\\\\ngenerated_ids = model.generate(input_ids, max_length=128)\\\\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\\', \\'performance\\': {\\'dataset\\': \\'HumanEval and MTPB\\', \\'accuracy\\': \\'Refer to the paper for accuracy details\\'}, \\'description\\': \\'CodeGen is a family of autoregressive language models for program synthesis. The checkpoint included in this repository is denoted as CodeGen-Multi 350M, where Multi means the model is initialized with CodeGen-NL 350M and further pre-trained on a dataset of multiple programming languages, and 350M refers to the number of trainable parameters. The model is capable of extracting features from given natural language and programming language texts, and calculating the likelihood of them. It is best at program synthesis, generating executable code given English prompts, and can complete partially-generated code as well.\\'}', metadata={})]", "category": "generic"}
{"question_id": 699, "text": " I have a multilingual document, and there is a missing word in the document. Please help me find out what this missing word might be.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'Realistic_Vision_V1.4\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=SG161222/Realistic_Vision_V1.4)\", \\'api_arguments\\': {\\'prompt\\': \\'string\\', \\'negative_prompt\\': \\'string\\'}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': \"from transformers import pipeline\\\\n\\\\nmodel = pipeline(\\'text-to-image\\', model=\\'SG161222/Realistic_Vision_V1.4\\')\\\\n\\\\nprompt = \\'a close up portrait photo of 26 y.o woman in wastelander clothes, long haircut, pale skin, slim body, background is city ruins, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3\\'\\\\nnegative_prompt = \\'(deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime:1.4), text, close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck\\'\\\\n\\\\nresult = model(prompt, negative_prompt=negative_prompt)\", \\'performance\\': {\\'dataset\\': \\'N/A\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'Realistic_Vision_V1.4 is a text-to-image model that generates high-quality and detailed images based on textual prompts. It can be used for various applications such as generating realistic portraits, landscapes, and other types of images.\\'}', metadata={})]", "category": "generic"}
{"question_id": 700, "text": " As a translation company, we are translating messages between co-workers in a multinational company. Translate the message from Hindi to French.\\n###Input: \\\"\\u0906\\u092a\\u0915\\u0940 \\u092a\\u094d\\u0930\\u0947\\u091c\\u093c\\u091f\\u0947\\u0936\\u0928 \\u0915\\u093e \\u0906\\u0927\\u093e\\u0930 \\u0905\\u091a\\u094d\\u091b\\u093e \\u0925\\u093e, \\u0932\\u0947\\u0915\\u093f\\u0928 \\u0921\\u0947\\u091f\\u093e \\u0935\\u093f\\u0936\\u094d\\u0932\\u0947\\u0937\\u0923 \\u092a\\u0930 \\u0927\\u094d\\u092f\\u093e\\u0928 \\u0926\\u0947\\u0928\\u093e \\u091a\\u093e\\u0939\\u093f\\u090f\\u0964\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Unconditional Image Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Denoising Diffusion Probabilistic Models (DDPM)\\', \\'api_name\\': \\'google/ddpm-bedroom-256\\', \\'api_call\\': \"DDPMPipeline.from_pretrained(\\'google/ddpm-bedroom-256\\')\", \\'api_arguments\\': \\'None\\', \\'python_environment_requirements\\': \\'diffusers\\', \\'example_code\\': \\'!pip install diffusers\\\\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\\\\nmodel_id = google/ddpm-bedroom-256\\\\nddpm = DDPMPipeline.from_pretrained(model_id)\\\\nimage = ddpm().images[0]\\\\nimage.save(ddpm_generated_image.png)\\', \\'performance\\': {\\'dataset\\': \\'CIFAR10\\', \\'accuracy\\': {\\'Inception score\\': 9.46, \\'FID score\\': 3.17}}, \\'description\\': \\'We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.\\'}', metadata={})]", "category": "generic"}
{"question_id": 701, "text": " Our task is to complete a given sentence with a missing word. The sentence is from an electronic health record.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Sentence Similarity\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Sentence Transformers\\', \\'api_name\\': \\'sentence-transformers/gtr-t5-base\\', \\'api_call\\': \"SentenceTransformer(\\'sentence-transformers/gtr-t5-base\\')\", \\'api_arguments\\': [\\'sentences\\'], \\'python_environment_requirements\\': [\\'sentence-transformers>=2.2.0\\'], \\'example_code\\': \"from sentence_transformers import SentenceTransformer\\\\nsentences = [This is an example sentence, Each sentence is converted]\\\\nmodel = SentenceTransformer(\\'sentence-transformers/gtr-t5-base\\')\\\\nembeddings = model.encode(sentences)\\\\nprint(embeddings)\", \\'performance\\': {\\'dataset\\': \\'https://seb.sbert.net\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'This is a sentence-transformers model that maps sentences & paragraphs to a 768 dimensional dense vector space. The model was specifically trained for the task of semantic search.\\'}', metadata={})]", "category": "generic"}
{"question_id": 702, "text": " A Dutch friend asked for help in completing a sentence with a missing word. Can you fill in the blank?\\n###Input: \\\"Het is vandaag erg koud, dus vergeet niet je ___ mee te nemen.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 703, "text": " Our school needs to create fill-in-the-blank quizzes for students. Is it possible to generate a fill-in-the-blank question from the following sentence: \\\"The cat chased the mouse and then climbed the tree.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'git-large-textcaps\\', \\'api_call\\': \"AutoModelForCausalLM.from_pretrained(\\'microsoft/git-large-textcaps\\')\", \\'api_arguments\\': \\'image, text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'N/A\\', \\'performance\\': {\\'dataset\\': \\'TextCaps\\', \\'accuracy\\': \\'Refer to the paper\\'}, \\'description\\': \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using \\'teacher forcing\\' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\"}', metadata={})]", "category": "generic"}
{"question_id": 704, "text": " We are building a customer support chatbot that needs to find the most related FAQ for a given customer query.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 705, "text": " We want to make a system that reads our email messages out loud in different voices.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 706, "text": " We are working on a language learning app for Chinese. We need to give audio examples for each lesson.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 707, "text": " We are building an AI chatbot that reads out user messages using synthesized human-like speech. For this purpose, we need to convert text messages to audio. \\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Abstractive Russian Summarization\\', \\'api_name\\': \\'cointegrated/rut5-base-absum\\', \\'api_call\\': \"T5ForConditionalGeneration.from_pretrained(\\'cointegrated/rut5-base-absum\\')\", \\'api_arguments\\': {\\'n_words\\': \\'int\\', \\'compression\\': \\'float\\', \\'max_length\\': \\'int\\', \\'num_beams\\': \\'int\\', \\'do_sample\\': \\'bool\\', \\'repetition_penalty\\': \\'float\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'latest\\', \\'torch\\': \\'latest\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\\\\nMODEL_NAME = \\'cointegrated/rut5-base-absum\\'\\\\nmodel = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\\\\ntokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\\\\nmodel.cuda();\\\\nmodel.eval();\\\\ndef summarize(\\\\n text, n_words=None, compression=None,\\\\n max_length=1000, num_beams=3, do_sample=False, repetition_penalty=10.0, \\\\n <strong>kwargs\\\\n):\\\\n \\\\n Summarize the text\\\\n The following parameters are mutually exclusive:\\\\n - n_words (int) is an approximate number of words to generate.\\\\n - compression (float) is an approximate length ratio of summary and original text.\\\\n \\\\n if n_words:\\\\n text = \\'[{}] \\'.format(n_words) + text\\\\n elif compression:\\\\n text = \\'[{0:.1g}] \\'.format(compression) + text\\\\n x = tokenizer(text, return_tensors=\\'pt\\', padding=True).to(model.device)\\\\n with torch.inference_mode():\\\\n out = model.generate(\\\\n </strong>x, \\\\n max_length=max_length, num_beams=num_beams, \\\\n do_sample=do_sample, repetition_penalty=repetition_penalty, \\\\n **kwargs\\\\n )\\\\n return tokenizer.decode(out[0], skip_special_tokens=True)\", \\'performance\\': {\\'dataset\\': [\\'csebuetnlp/xlsum\\', \\'IlyaGusev/gazeta\\', \\'mlsum\\'], \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'This is a model for abstractive Russian summarization, based on cointegrated/rut5-base-multitask and fine-tuned on 4 datasets.\\'}', metadata={})]", "category": "generic"}
{"question_id": 708, "text": " I want to identify when people are speaking in an audio file.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Automatic Speech Recognition\\', \\'framework\\': \\'pyannote.audio\\', \\'functionality\\': \\'overlapped-speech-detection\\', \\'api_name\\': \\'pyannote/overlapped-speech-detection\\', \\'api_call\\': \"pipeline.from_pretrained(\\'pyannote/overlapped-speech-detection\\', use_auth_token=\\'ACCESS_TOKEN_GOES_HERE\\')\", \\'api_arguments\\': [\\'audio.wav\\'], \\'python_environment_requirements\\': [\\'pyannote.audio 2.1\\'], \\'example_code\\': \\'from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained(pyannote/overlapped-speech-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\\\noutput = pipeline(audio.wav)\\\\nfor speech in output.get_timeline().support():\\\\n  # two or more speakers are active between speech.start and speech.end\\\\n  ...\\', \\'performance\\': {\\'dataset\\': \\'ami\\', \\'accuracy\\': None}, \\'description\\': \\'Automatic overlapped speech detection using pyannote.audio framework. The model detects when two or more speakers are active in an audio file.\\'}', metadata={})]", "category": "generic"}
{"question_id": 709, "text": " I have a collection of audio recordings from different sources, and I want to convert them into text to make an archive.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Text-to-Text Generation\\', \\'api_name\\': \\'optimum/t5-small\\', \\'api_call\\': \"ORTModelForSeq2SeqLM.from_pretrained(\\'optimum/t5-small\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'optimum.onnxruntime\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, pipeline\\\\nfrom optimum.onnxruntime import ORTModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained(optimum/t5-small)\\\\nmodel = ORTModelForSeq2SeqLM.from_pretrained(optimum/t5-small)\\\\ntranslator = pipeline(translation_en_to_fr, model=model, tokenizer=tokenizer)\\\\nresults = translator(My name is Eustache and I have a pet raccoon)\\\\nprint(results)\\', \\'performance\\': {\\'dataset\\': \\'c4\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'T5 is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks and for which each task is converted into a text-to-text format. It can be used for translation, text-to-text generation, and summarization.\\'}', metadata={})]", "category": "generic"}
{"question_id": 710, "text": " A company developing an application for transcribing customer service calls requires a model that can understand spoken language.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Audio Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Spoken Language Identification\\', \\'api_name\\': \\'TalTechNLP/voxlingua107-epaca-tdnn\\', \\'api_call\\': \"EncoderClassifier.from_hparams(source=\\'TalTechNLP/voxlingua107-epaca-tdnn\\')\", \\'api_arguments\\': [\\'signal\\'], \\'python_environment_requirements\\': [\\'speechbrain\\', \\'torchaudio\\'], \\'example_code\\': \\'import torchaudio\\\\nfrom speechbrain.pretrained import EncoderClassifier\\\\nlanguage_id = EncoderClassifier.from_hparams(source=TalTechNLP/voxlingua107-epaca-tdnn, savedir=tmp)\\\\nsignal = language_id.load_audio(https://omniglot.com/soundfiles/udhr/udhr_th.mp3)\\\\nprediction = language_id.classify_batch(signal)\\\\nprint(prediction)\\', \\'performance\\': {\\'dataset\\': \\'VoxLingua107\\', \\'accuracy\\': \\'93%\\'}, \\'description\\': \\'This is a spoken language recognition model trained on the VoxLingua107 dataset using SpeechBrain. The model uses the ECAPA-TDNN architecture that has previously been used for speaker recognition. The model can classify a speech utterance according to the language spoken. It covers 107 different languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 711, "text": " We are building an audio assistant. Apply noise suppression to our new voice commands.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 712, "text": " We have an audio recording with overlapping speakers talking. We need to remove the overlaps and make it a clear single speaker recording.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'Kirili4ik/mbart_ruDialogSum\\', \\'api_call\\': \"MBartForConditionalGeneration.from_pretrained(\\'Kirili4ik/mbart_ruDialogSum\\')\", \\'api_arguments\\': {\\'model_name\\': \\'Kirili4ik/mbart_ruDialogSum\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import MBartTokenizer, MBartForConditionalGeneration\\\\nmodel_name = Kirili4ik/mbart_ruDialogSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = MBartForConditionalGeneration.from_pretrained(model_name)\\\\nmodel.eval()\\\\narticle_text = ...\\\\ninput_ids = tokenizer(\\\\n [article_text],\\\\n max_length=600,\\\\n padding=max_length,\\\\n truncation=True,\\\\n return_tensors=pt,\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n top_k=0,\\\\n num_beams=3,\\\\n no_repeat_ngram_size=3\\\\n)[0]\\\\nsummary = tokenizer.decode(output_ids, skip_special_tokens=True)\\\\nprint(summary)\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'SAMSum Corpus (translated to Russian)\\', \\'accuracy\\': {\\'Validation ROGUE-1\\': 34.5, \\'Validation ROGUE-L\\': 33, \\'Test ROGUE-1\\': 31, \\'Test ROGUE-L\\': 28}}]}, \\'description\\': \\'MBart for Russian summarization fine-tuned for dialogues summarization. This model was firstly fine-tuned by Ilya Gusev on Gazeta dataset. We have fine tuned that model on SamSum dataset translated to Russian using GoogleTranslateAPI. Moreover! We have implemented a ! telegram bot @summarization_bot ! with the inference of this model. Add it to the chat and get summaries instead of dozens spam messages!\\'}', metadata={})]", "category": "generic"}
{"question_id": 713, "text": " We have just built an app for hosting podcasts, and we need a speech enhancement model to clean noise from the audio recordings submitted by our users.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 714, "text": " My coworker sent me a voice message in Spanish. Translate this audio message to English, so that I can understand it.\\n###Input: spanish_voice_message.wav\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Conversational\\', \\'api_name\\': \\'Pi3141/DialoGPT-medium-elon-3\\', \\'api_call\\': \"pipeline(\\'text-generation\\', model=\\'Pi3141/DialoGPT-medium-elon-3\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'Input a message to start chatting with Pi3141/DialoGPT-medium-elon-3.\\', \\'performance\\': {\\'dataset\\': \\'Twitter tweets by Elon Musk\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'DialoGPT model that talks like Elon Musk, trained on Twitter tweets by Elon Musk. This model will spew meaningless shit about 40% of the time. Trained on 8 epochs. But with a larger dataset this time. The AI can now use more emojis, I think.\\'}', metadata={})]", "category": "generic"}
{"question_id": 715, "text": " We are developing a virtual assistant and need to integrate a speech enhancement feature in it.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 716, "text": " A kid's educational toy company is including a voice assistant that recognizes simple voice commands from children. They need an example on how to recognize spoken digits.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'microsoft/speecht5_tts\\', \\'api_call\\': \"SpeechT5ForTextToSpeech.from_pretrained(\\'microsoft/speecht5_tts\\')\", \\'api_arguments\\': [\\'text\\', \\'return_tensors\\', \\'input_ids\\', \\'speaker_embeddings\\', \\'vocoder\\'], \\'python_environment_requirements\\': \\'!pip install git+https://github.com/huggingface/transformers sentencepiece datasets\\', \\'example_code\\': \"from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\\\\nfrom datasets import load_dataset\\\\nimport torch\\\\nimport soundfile as sf\\\\nprocessor = SpeechT5Processor.from_pretrained(\\'microsoft/speecht5_tts\\')\\\\nmodel = SpeechT5ForTextToSpeech.from_pretrained(\\'microsoft/speecht5_tts\\')\\\\nvocoder = SpeechT5HifiGan.from_pretrained(\\'microsoft/speecht5_hifigan\\')\\\\ninputs = processor(text=\\'Hello, my dog is cute\\', return_tensors=\\'pt\\')\\\\nembeddings_dataset = load_dataset(\\'Matthijs/cmu-arctic-xvectors\\', split=\\'validation\\')\\\\nspeaker_embeddings = torch.tensor(embeddings_dataset[7306][\\'xvector\\']).unsqueeze(0)\\\\nspeech = model.generate_speech(inputs[\\'input_ids\\'], speaker_embeddings, vocoder=vocoder)\\\\nsf.write(\\'speech.wav\\', speech.numpy(), samplerate=16000)\", \\'performance\\': {\\'dataset\\': \\'LibriTTS\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'SpeechT5 model fine-tuned for speech synthesis (text-to-speech) on LibriTTS. It is a unified-modal SpeechT5 framework that explores the encoder-decoder pre-training for self-supervised speech/text representation learning. It can be used for a wide variety of spoken language processing tasks, including automatic speech recognition, speech synthesis, speech translation, voice conversion, speech enhancement, and speaker identification.\\'}', metadata={})]", "category": "generic"}
{"question_id": 717, "text": " Our company needs to analyze customer phone call recordings and identify specific numbers mentioned by customers during the call.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Token Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Entity Extraction\\', \\'api_name\\': \\'903429548\\', \\'api_call\\': \"AutoModelForTokenClassification.from_pretrained(\\'ismail-lucifer011/autotrain-company_all-903429548\\', use_auth_token=True)\", \\'api_arguments\\': {\\'inputs\\': \\'I love AutoTrain\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'AutoModelForTokenClassification, AutoTokenizer\\'}, \\'example_code\\': \\'from transformers import AutoModelForTokenClassification, AutoTokenizer\\\\nmodel = AutoModelForTokenClassification.from_pretrained(ismail-lucifer011/autotrain-company_all-903429548, use_auth_token=True)\\\\ntokenizer = AutoTokenizer.from_pretrained(ismail-lucifer011/autotrain-company_all-903429548, use_auth_token=True)\\\\ninputs = tokenizer(I love AutoTrain, return_tensors=pt)\\\\noutputs = model(**inputs)\\', \\'performance\\': {\\'dataset\\': \\'ismail-lucifer011/autotrain-data-company_all\\', \\'accuracy\\': 0.9979930566588805}, \\'description\\': \\'A token classification model trained using AutoTrain for entity extraction. The model is based on the distilbert architecture and trained on the ismail-lucifer011/autotrain-data-company_all dataset. It can be used to identify and extract company names from text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 718, "text": " You are building a virtual global tour guide that can identify languages from the audio of people speaking. Use a model to identify which language is being spoken.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Information Retrieval\\', \\'api_name\\': \\'cross-encoder/ms-marco-TinyBERT-L-2-v2\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'cross-encoder/ms-marco-TinyBERT-L-2-v2\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\"}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': {\\'import\\': \\'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\', \\'model\\': \"model = AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\", \\'scores\\': \\'with torch.no_grad():\\\\n    scores = model(**features).logits\\\\n    print(scores)\\'}, \\'performance\\': {\\'dataset\\': \\'TREC Deep Learning 2019\\', \\'accuracy\\': \\'69.84 (NDCG@10)\\'}, \\'description\\': \\'This model was trained on the MS Marco Passage Ranking task. It can be used for Information Retrieval: Given a query, encode the query with all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. The training code is available here: SBERT.net Training MS Marco.\\'}', metadata={})]", "category": "generic"}
{"question_id": 719, "text": " I am working as the head of customer service for a Spanish speaking market. I want to know the sentiment of my customers on their last call with our support agents.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \"PegasusForConditionalGeneration.from_pretrained(\\'tuner007/pegasus_summarizer\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 720, "text": " I have a recording of a meeting from which I need to extract the parts where people are talking and eliminate the silent parts.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 721, "text": " We are a company producing wine. Based on the chemical properties of our wine data, we need to analyze the quality of our products and determine whether they are good or bad.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 722, "text": " Our team is working on a podcast app, and we want to automatically generate transcripts with timestamps for each speaker.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Visual Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'git-large-textvqa\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'microsoft/git-large-textvqa\\')\", \\'api_arguments\\': \\'image, question\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'For code examples, we refer to the documentation.\\', \\'performance\\': {\\'dataset\\': \\'TextVQA\\', \\'accuracy\\': \\'See table 11 in the paper for more details.\\'}, \\'description\\': \"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextVQA. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using \\'teacher forcing\\' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like: image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).\"}', metadata={})]", "category": "generic"}
{"question_id": 723, "text": " I have a large dataset with both numerical and categorical features related to customer behavior. I want to classify them into different segments for targeted marketing. How do I use a TabTransformer for this task?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Classification\\', \\'framework\\': \\'Keras\\', \\'functionality\\': \\'Structured data learning with TabTransformer\\', \\'api_name\\': \\'keras-io/tab_transformer\\', \\'api_call\\': \\'TabTransformer.from_config()\\', \\'api_arguments\\': \\'N/A\\', \\'python_environment_requirements\\': \\'Hugging Face\\', \\'example_code\\': \\'N/A\\', \\'performance\\': {\\'dataset\\': \\'United States Census Income Dataset\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \"This model uses self-attention based Transformers structure followed by multiple feed forward layers to serve supervised and semi-supervised learning tasks on tabular data. The model\\'s inputs can contain both numerical and categorical features. Categorical features are encoded into embedding vectors before being fed into a stack of Transformer blocks. The contextual embeddings of the categorical features after the final Transformer layer are concatenated with the input numerical features and fed into a final MLP block. A SoftMax function is applied at the end of the model.\"}', metadata={})]", "category": "generic"}
{"question_id": 724, "text": " Determine if an employee's annual salary meets or exceeds $50000.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 725, "text": " Identify whether a newly-setup chemical plant is exceeding carbon emission limits based on a CSV file containing data collected.\\n###Input: data.csv\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Fill-Mask\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Masked Language Modeling\\', \\'api_name\\': \\'xlm-roberta-base\\', \\'api_call\\': \"pipeline(\\'fill-mask\\', model=\\'xlm-roberta-base\\')\", \\'api_arguments\\': {\\'model\\': \\'xlm-roberta-base\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nunmasker = pipeline(\\'fill-mask\\', model=\\'xlm-roberta-base\\')\\\\nunmasker(Hello I\\'m a <mask> model.)\", \\'performance\\': {\\'dataset\\': \\'CommonCrawl\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'XLM-RoBERTa is a multilingual version of RoBERTa pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It can be used for masked language modeling and is intended to be fine-tuned on a downstream task.\\'}', metadata={})]", "category": "generic"}
{"question_id": 726, "text": " I am working in a bank, I want to estimate the mortgage for a given housing using the housing's features.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 727, "text": " We are building a robot for hopping in a controlled environment. Train it to perform a hop using Decision Transformers.\\n**Input:<noinput>**\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 728, "text": " We are building a robot for elderly care. The robot should be able to understand what's happening and take appropriate actions based on the elderly's current activities.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 729, "text": " To build a source code recommendation engine, we need to extract features from text and code using Transformer models.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Code Understanding and Generation\\', \\'api_name\\': \\'Salesforce/codet5-base\\', \\'api_call\\': \"T5ForConditionalGeneration.from_pretrained(\\'Salesforce/codet5-base\\')\", \\'api_arguments\\': [\\'text\\', \\'return_tensors\\', \\'input_ids\\', \\'max_length\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import RobertaTokenizer, T5ForConditionalGeneration\\\\ntokenizer = RobertaTokenizer.from_pretrained(\\'Salesforce/codet5-base\\')\\\\nmodel = T5ForConditionalGeneration.from_pretrained(\\'Salesforce/codet5-base\\')\\\\ntext = def greet(user): print(f\\'hello <extra_id_0>!\\')\\\\ninput_ids = tokenizer(text, return_tensors=pt).input_ids\\\\ngenerated_ids = model.generate(input_ids, max_length=8)\\\\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\", \\'performance\\': {\\'dataset\\': \\'code_search_net\\', \\'accuracy\\': \\'Refer to the paper for evaluation results on several downstream benchmarks\\'}, \\'description\\': \\'CodeT5 is a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. It supports both code understanding and generation tasks and allows for multi-task learning. The model can be used for tasks such as code summarization, code generation, code translation, code refinement, code defect detection, and code clone detection.\\'}', metadata={})]", "category": "generic"}
{"question_id": 730, "text": " Our team needs to prepare a feature matrix based on the given source code's programming constructs and comments for data analysis.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Program Synthesis\\', \\'api_name\\': \\'Salesforce/codegen-350M-multi\\', \\'api_call\\': \"AutoTokenizer.from_pretrained(\\'Salesforce/codegen-350M-multi\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelForCausalLM\\\\ntokenizer = AutoTokenizer.from_pretrained(Salesforce/codegen-350M-multi)\\\\nmodel = AutoModelForCausalLM.from_pretrained(Salesforce/codegen-350M-multi)\\\\ntext = def hello_world():\\\\ninput_ids = tokenizer(text, return_tensors=pt).input_ids\\\\ngenerated_ids = model.generate(input_ids, max_length=128)\\\\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\\', \\'performance\\': {\\'dataset\\': \\'HumanEval and MTPB\\', \\'accuracy\\': \\'Refer to the paper for accuracy details\\'}, \\'description\\': \\'CodeGen is a family of autoregressive language models for program synthesis. The checkpoint included in this repository is denoted as CodeGen-Multi 350M, where Multi means the model is initialized with CodeGen-NL 350M and further pre-trained on a dataset of multiple programming languages, and 350M refers to the number of trainable parameters. The model is capable of extracting features from given natural language and programming language texts, and calculating the likelihood of them. It is best at program synthesis, generating executable code given English prompts, and can complete partially-generated code as well.\\'}', metadata={})]", "category": "generic"}
{"question_id": 731, "text": " We are developing an interactive story app that would display a picture of each character as mentioned in the story given to our GPT-3 model.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 732, "text": " As a landscape architect, I want to generate a description of an image of a park I've designed to use for promotional purposes.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 733, "text": " I need to build a solution that generates textual descriptions for images in my database.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 734, "text": " We are a company managing hotel bookings. We need to answer our customer's questions regarding rental rates from our pricing document.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 735, "text": " Provide me a pipeline for extracting relevant information from an insurance policy document.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Sentence Similarity\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Sentence Transformers\\', \\'api_name\\': \\'flax-sentence-embeddings/all_datasets_v4_MiniLM-L6\\', \\'api_call\\': \"SentenceTransformer(\\'flax-sentence-embeddings/all_datasets_v4_MiniLM-L6\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'sentence-transformers library\\', \\'example_code\\': \"from sentence_transformers import SentenceTransformer\\\\nmodel = SentenceTransformer(\\'flax-sentence-embeddings/all_datasets_v4_MiniLM-L6\\')\\\\ntext = Replace me by any text you\\'d like.\\\\ntext_embbedding = model.encode(text)\", \\'performance\\': {\\'dataset\\': \\'1,097,953,922\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'The model is trained on very large sentence level datasets using a self-supervised contrastive learning objective. It is fine-tuned on a 1B sentence pairs dataset, and it aims to capture the semantic information of input sentences. The sentence vector can be used for information retrieval, clustering, or sentence similarity tasks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 736, "text": " Create an accurate system to estimate the distance between the camera and the objects in a photo captured by a drone flying over a terrain.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image Generation\\', \\'api_name\\': \\'stabilityai/stable-diffusion-2-1\\', \\'api_call\\': \"StableDiffusionPipeline.from_pretrained(\\'stabilityai/stable-diffusion-2-1\\', torch_dtype=torch.float16)\", \\'api_arguments\\': {\\'prompt\\': \\'a photo of an astronaut riding a horse on mars\\'}, \\'python_environment_requirements\\': [\\'diffusers\\', \\'transformers\\', \\'accelerate\\', \\'scipy\\', \\'safetensors\\'], \\'example_code\\': \\'from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\\\\nmodel_id = stabilityai/stable-diffusion-2-1\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\\\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\\\\npipe = pipe.to(cuda)\\\\nprompt = a photo of an astronaut riding a horse on mars\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(astronaut_rides_horse.png)\\', \\'performance\\': {\\'dataset\\': \\'COCO2017\\', \\'accuracy\\': \\'Not optimized for FID scores\\'}, \\'description\\': \\'Stable Diffusion v2-1 is a diffusion-based text-to-image generation model developed by Robin Rombach and Patrick Esser. It is capable of generating and modifying images based on text prompts in English. The model is trained on a subset of the LAION-5B dataset and is primarily intended for research purposes.\\'}', metadata={})]", "category": "generic"}
{"question_id": 737, "text": " We are a team of architects and civil engineers looking to estimate the depth of elements in architectural designs from 2D images of the structures. We want to implement a depth estimation model that will transform these images into depictions of depth.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 738, "text": " I have an image on my computer named \\\"eye.jpg\\\". I want to use a model to predict whether this image indicates diabetic retinopathy.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 739, "text": " We need to build a model that recognizes objects in images. Build a model using the Vision Transformer (ViT) for this purpose.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'facebook/dino-vitb16\\', \\'api_call\\': \"ViTModel.from_pretrained(\\'facebook/dino-vitb16\\')\", \\'api_arguments\\': {\\'pretrained_model_name_or_path\\': \\'facebook/dino-vitb16\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'latest\\', \\'PIL\\': \\'latest\\', \\'requests\\': \\'latest\\'}, \\'example_code\\': \"from transformers import ViTFeatureExtractor, ViTModel\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = \\'http://images.cocodataset.org/val2017/000000039769.jpg\\'\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nfeature_extractor = ViTFeatureExtractor.from_pretrained(\\'facebook/dino-vitb16\\')\\\\nmodel = ViTModel.from_pretrained(\\'facebook/dino-vitb16\\')\\\\ninputs = feature_extractor(images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\nlast_hidden_states = outputs.last_hidden_state\", \\'performance\\': {\\'dataset\\': \\'imagenet-1k\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Vision Transformer (ViT) model trained using the DINO method. The model is pretrained on a large collection of images in a self-supervised fashion, namely ImageNet-1k, at a resolution of 224x224 pixels. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Note that this model does not include any fine-tuned heads.\\'}', metadata={})]", "category": "generic"}
{"question_id": 740, "text": " My pet store website needs a tool to recognize different dog breeds from user uploaded images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Token Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Named Entity Recognition\\', \\'api_name\\': \\'dslim/bert-base-NER\\', \\'api_call\\': \"AutoModelForTokenClassification.from_pretrained(\\'dslim/bert-base-NER\\')\", \\'api_arguments\\': {\\'model\\': \\'AutoModelForTokenClassification.from_pretrained(dslim/bert-base-NER)\\', \\'tokenizer\\': \\'AutoTokenizer.from_pretrained(dslim/bert-base-NER)\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelForTokenClassification\\\\nfrom transformers import pipeline\\\\ntokenizer = AutoTokenizer.from_pretrained(dslim/bert-base-NER)\\\\nmodel = AutoModelForTokenClassification.from_pretrained(dslim/bert-base-NER)\\\\nnlp = pipeline(ner, model=model, tokenizer=tokenizer)\\\\nexample = My name is Wolfgang and I live in Berlin\\\\nner_results = nlp(example)\\\\nprint(ner_results)\\', \\'performance\\': {\\'dataset\\': \\'conll2003\\', \\'accuracy\\': {\\'f1\\': 91.3, \\'precision\\': 90.7, \\'recall\\': 91.9}}, \\'description\\': \\'bert-base-NER is a fine-tuned BERT model that is ready to use for Named Entity Recognition and achieves state-of-the-art performance for the NER task. It has been trained to recognize four types of entities: location (LOC), organizations (ORG), person (PER) and Miscellaneous (MISC). Specifically, this model is a bert-base-cased model that was fine-tuned on the English version of the standard CoNLL-2003 Named Entity Recognition dataset.\\'}', metadata={})]", "category": "generic"}
{"question_id": 741, "text": " As a programmer, I want to classify different animal species based on their images. Help me determine the type of species. \\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 742, "text": " We are a robotics company specializing in object detection for logistics. We need to detect objects in warehouses using an advanced object detection model.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Object Detection\\', \\'api_name\\': \\'fcakyon/yolov5s-v7.0\\', \\'api_call\\': \"yolov5.load(\\'fcakyon/yolov5s-v7.0\\')\", \\'api_arguments\\': {\\'conf\\': 0.25, \\'iou\\': 0.45, \\'agnostic\\': False, \\'multi_label\\': False, \\'max_det\\': 1000, \\'img\\': \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\', \\'size\\': 640, \\'augment\\': True}, \\'python_environment_requirements\\': \\'pip install -U yolov5\\', \\'example_code\\': \"import yolov5\\\\nmodel = yolov5.load(\\'fcakyon/yolov5s-v7.0\\')\\\\nmodel.conf = 0.25\\\\nmodel.iou = 0.45\\\\nmodel.agnostic = False\\\\nmodel.multi_label = False\\\\nmodel.max_det = 1000\\\\nimg = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\\\\nresults = model(img)\\\\nresults = model(img, size=640)\\\\nresults = model(img, augment=True)\\\\npredictions = results.pred[0]\\\\nboxes = predictions[:, :4]\\\\nscores = predictions[:, 4]\\\\ncategories = predictions[:, 5]\\\\nresults.show()\\\\nresults.save(save_dir=\\'results/\\')\", \\'performance\\': {\\'dataset\\': \\'detection-datasets/coco\\', \\'accuracy\\': None}, \\'description\\': \\'Yolov5s-v7.0 is an object detection model trained on the COCO dataset. It can detect objects in images and return their bounding boxes, scores, and categories.\\'}', metadata={})]", "category": "generic"}
{"question_id": 743, "text": " An IoT device collects images from different locations. Create a model to detect objects in these images to analyze the surroundings.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Object Detection\\', \\'api_name\\': \\'fcakyon/yolov5s-v7.0\\', \\'api_call\\': \"yolov5.load(\\'fcakyon/yolov5s-v7.0\\')\", \\'api_arguments\\': {\\'conf\\': 0.25, \\'iou\\': 0.45, \\'agnostic\\': False, \\'multi_label\\': False, \\'max_det\\': 1000, \\'img\\': \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\', \\'size\\': 640, \\'augment\\': True}, \\'python_environment_requirements\\': \\'pip install -U yolov5\\', \\'example_code\\': \"import yolov5\\\\nmodel = yolov5.load(\\'fcakyon/yolov5s-v7.0\\')\\\\nmodel.conf = 0.25\\\\nmodel.iou = 0.45\\\\nmodel.agnostic = False\\\\nmodel.multi_label = False\\\\nmodel.max_det = 1000\\\\nimg = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\\\\nresults = model(img)\\\\nresults = model(img, size=640)\\\\nresults = model(img, augment=True)\\\\npredictions = results.pred[0]\\\\nboxes = predictions[:, :4]\\\\nscores = predictions[:, 4]\\\\ncategories = predictions[:, 5]\\\\nresults.show()\\\\nresults.save(save_dir=\\'results/\\')\", \\'performance\\': {\\'dataset\\': \\'detection-datasets/coco\\', \\'accuracy\\': None}, \\'description\\': \\'Yolov5s-v7.0 is an object detection model trained on the COCO dataset. It can detect objects in images and return their bounding boxes, scores, and categories.\\'}', metadata={})]", "category": "generic"}
{"question_id": 744, "text": " I'm building a drone surveillance system for detecting airplanes in the sky. What can I use to achieve the required object detection?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Automatic Speech Recognition\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli\\', \\'api_call\\': \"Wav2Vec2ForCTC.from_pretrained(\\'jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'https://github.com/neonbjb/ocotillo\\', \\'performance\\': {\\'dataset\\': \\'librispeech validation set\\', \\'accuracy\\': \\'4.45%\\'}, \\'description\\': \\'This checkpoint is a wav2vec2-large model that is useful for generating transcriptions with punctuation. It is intended for use in building transcriptions for TTS models, where punctuation is very important for prosody. This model was created by fine-tuning the facebook/wav2vec2-large-robust-ft-libri-960h checkpoint on the libritts and voxpopuli datasets with a new vocabulary that includes punctuation.\\'}', metadata={})]", "category": "generic"}
{"question_id": 745, "text": " We are building an IoT device to monitor apartment corridors for security. Please detect objects in the image and notify if anything abnormal is detected.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 746, "text": " Design an application for a smart drone that can detect and segment objects within the area it's flying.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Information Retrieval\\', \\'api_name\\': \\'cross-encoder/ms-marco-TinyBERT-L-2-v2\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'cross-encoder/ms-marco-TinyBERT-L-2-v2\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\"}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': {\\'import\\': \\'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\', \\'model\\': \"model = AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\", \\'scores\\': \\'with torch.no_grad():\\\\n    scores = model(**features).logits\\\\n    print(scores)\\'}, \\'performance\\': {\\'dataset\\': \\'TREC Deep Learning 2019\\', \\'accuracy\\': \\'69.84 (NDCG@10)\\'}, \\'description\\': \\'This model was trained on the MS Marco Passage Ranking task. It can be used for Information Retrieval: Given a query, encode the query with all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. The training code is available here: SBERT.net Training MS Marco.\\'}', metadata={})]", "category": "generic"}
{"question_id": 747, "text": " I want to build a tool to recognize urban landscapes and identify different objects in the image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Token Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Named Entity Recognition\\', \\'api_name\\': \\'dslim/bert-large-NER\\', \\'api_call\\': \"AutoModelForTokenClassification.from_pretrained(\\'dslim/bert-large-NER\\')\", \\'api_arguments\\': {\\'model\\': \"AutoModelForTokenClassification.from_pretrained(\\'dslim/bert-large-NER\\')\", \\'tokenizer\\': \"AutoTokenizer.from_pretrained(\\'dslim/bert-large-NER\\')\"}, \\'python_environment_requirements\\': {\\'transformers\\': \\'4.0.1\\'}, \\'example_code\\': {\\'example\\': \\'My name is Wolfgang and I live in Berlin\\', \\'ner_results\\': \\'nlp(example)\\'}, \\'performance\\': {\\'dataset\\': \\'conll2003\\', \\'accuracy\\': {\\'f1\\': 0.92, \\'precision\\': 0.92, \\'recall\\': 0.919}}, \\'description\\': \\'bert-large-NER is a fine-tuned BERT model that is ready to use for Named Entity Recognition and achieves state-of-the-art performance for the NER task. It has been trained to recognize four types of entities: location (LOC), organizations (ORG), person (PER) and Miscellaneous (MISC).\\'}', metadata={})]", "category": "generic"}
{"question_id": 748, "text": " Show me how to detect defects of PCB boards from an image in real-time.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Segmentation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Image Segmentation\\', \\'api_name\\': \\'keremberke/yolov8s-pcb-defect-segmentation\\', \\'api_call\\': \"YOLO(\\'keremberke/yolov8s-pcb-defect-segmentation\\')\", \\'api_arguments\\': {\\'image\\': \\'URL or local path to image\\'}, \\'python_environment_requirements\\': [\\'ultralyticsplus==0.0.23\\', \\'ultralytics==8.0.21\\'], \\'example_code\\': \"from ultralyticsplus import YOLO, render_result\\\\n\\\\nmodel = YOLO(\\'keremberke/yolov8s-pcb-defect-segmentation\\')\\\\nmodel.overrides[\\'conf\\'] = 0.25\\\\nmodel.overrides[\\'iou\\'] = 0.45\\\\nmodel.overrides[\\'agnostic_nms\\'] = False\\\\nmodel.overrides[\\'max_det\\'] = 1000\\\\n\\\\nimage = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nprint(results[0].masks)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\", \\'performance\\': {\\'dataset\\': \\'pcb-defect-segmentation\\', \\'accuracy\\': {\\'mAP@0.5(box)\\': 0.515, \\'mAP@0.5(mask)\\': 0.491}}, \\'description\\': \\'YOLOv8s model for PCB defect segmentation. The model is trained to detect and segment PCB defects such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.\\'}', metadata={})]", "category": "generic"}
{"question_id": 749, "text": " I want to assess the condition of roads in a city by analyzing drone footage. Create an image segmentation model to identify potholes in images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Segmentation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'shi-labs/oneformer_ade20k_swin_large\\', \\'api_call\\': \"OneFormerForUniversalSegmentation.from_pretrained(\\'shi-labs/oneformer_ade20k_swin_large\\')\", \\'api_arguments\\': [\\'images\\', \\'task_inputs\\', \\'return_tensors\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'PIL\\', \\'requests\\'], \\'example_code\\': \\'from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\\\\nfrom PIL import Image\\\\nimport requests\\\\nurl = https://huggingface.co/datasets/shi-labs/oneformer_demo/blob/main/ade20k.jpeg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\nprocessor = OneFormerProcessor.from_pretrained(shi-labs/oneformer_ade20k_swin_large)\\\\nmodel = OneFormerForUniversalSegmentation.from_pretrained(shi-labs/oneformer_ade20k_swin_large)\\\\nsemantic_inputs = processor(images=image, task_inputs=[semantic], return_tensors=pt)\\\\nsemantic_outputs = model(**semantic_inputs)\\\\npredicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\', \\'performance\\': {\\'dataset\\': \\'scene_parse_150\\', \\'accuracy\\': None}, \\'description\\': \\'OneFormer model trained on the ADE20k dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 750, "text": " We are building a road maintenance reporting application. We need to use images to identify and segment road potholes.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 751, "text": " The marketing team needs different variations of a product image to use in advertising and promotional materials.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Image Variations\\', \\'api_name\\': \\'lambdalabs/sd-image-variations-diffusers\\', \\'api_call\\': \"StableDiffusionImageVariationPipeline.from_pretrained(\\'lambdalabs/sd-image-variations-diffusers\\', revision=\\'v2.0\\')\", \\'api_arguments\\': {\\'revision\\': \\'v2.0\\'}, \\'python_environment_requirements\\': \\'Diffusers >=0.8.0\\', \\'example_code\\': \\'from diffusers import StableDiffusionImageVariationPipeline\\\\nfrom PIL import Image\\\\ndevice = cuda:0\\\\nsd_pipe = StableDiffusionImageVariationPipeline.from_pretrained(\\\\n lambdalabs/sd-image-variations-diffusers,\\\\n revision=v2.0,\\\\n)\\\\nsd_pipe = sd_pipe.to(device)\\\\nim = Image.open(path/to/image.jpg)\\\\ntform = transforms.Compose([\\\\n transforms.ToTensor(),\\\\n transforms.Resize(\\\\n  (224, 224),\\\\n  interpolation=transforms.InterpolationMode.BICUBIC,\\\\n  antialias=False,\\\\n ),\\\\n transforms.Normalize(\\\\n  [0.48145466, 0.4578275, 0.40821073],\\\\n  [0.26862954, 0.26130258, 0.27577711]),\\\\n])\\\\ninp = tform(im).to(device).unsqueeze(0)\\\\nout = sd_pipe(inp, guidance_scale=3)\\\\nout[images][0].save(result.jpg)\\', \\'performance\\': {\\'dataset\\': \\'ChristophSchuhmann/improved_aesthetics_6plus\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\\'}', metadata={})]", "category": "generic"}
{"question_id": 752, "text": " A movie studio needs to estimate the human pose of an actor from an image for an upcoming film project.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Classification\\', \\'api_name\\': \\'lvwerra/distilbert-imdb\\', \\'api_call\\': \"pipeline(\\'sentiment-analysis\\', model=\\'lvwerra/distilbert-imdb\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\', \\'pytorch\\'], \\'example_code\\': \"classifier(\\'I love this movie!\\')\", \\'performance\\': {\\'dataset\\': \\'imdb\\', \\'accuracy\\': 0.928}, \\'description\\': \\'This model is a fine-tuned version of distilbert-base-uncased on the imdb dataset. It is used for sentiment analysis on movie reviews and achieves an accuracy of 0.928 on the evaluation set.\\'}', metadata={})]", "category": "generic"}
{"question_id": 753, "text": " Create a program to determine the depth map from an input image of a street filled with people.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Classification\\', \\'framework\\': \\'Keras\\', \\'functionality\\': \\'Binary Classification\\', \\'api_name\\': \\'TF_Decision_Trees\\', \\'api_call\\': \\'TF_Decision_Trees(input_features, target)\\', \\'api_arguments\\': [\\'input_features\\', \\'target\\'], \\'python_environment_requirements\\': [\\'tensorflow >= 7.0\\'], \\'example_code\\': \\'https://github.com/tdubon/TF-GB-Forest/blob/c0cf4c7e3e29d819b996cfe4eecc1f2728115e52/TFDecisionTrees_Final.ipynb\\', \\'performance\\': {\\'dataset\\': \\'Census-Income Data Set\\', \\'accuracy\\': 96.57}, \\'description\\': \"Use TensorFlow\\'s Gradient Boosted Trees model in binary classification of structured data. Build a decision forests model by specifying the input feature usage. Implement a custom Binary Target encoder as a Keras Preprocessing layer to encode the categorical features with respect to their target value co-occurrences, and then use the encoded features to build a decision forests model. The model is trained on the US Census Income Dataset containing approximately 300k instances with 41 numerical and categorical variables. The task is to determine whether a person makes over 50k a year.\"}', metadata={})]", "category": "generic"}
{"question_id": 754, "text": " I need to create a new piece of art to add to my digital gallery that resembles WikiArt images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 755, "text": " Our team is designing a butterfly-themed stationery set for children. We need to generate images of cute butterflies to use in the design.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Unconditional Image Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Unconditional Image Generation\\', \\'api_name\\': \\'ocariz/butterfly_200\\', \\'api_call\\': \"DDPMPipeline.from_pretrained(\\'ocariz/butterfly_200\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'diffusers\\', \\'example_code\\': \"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained(\\'ocariz/butterfly_200\\')\\\\nimage = pipeline().images[0]\\\\nimage\", \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'This model is a diffusion model for unconditional image generation of cute butterflies trained for 200 epochs.\\'}', metadata={})]", "category": "generic"}
{"question_id": 756, "text": " Design a mobile application that suggests pictures of butterflies whenever the user attempts to change their theme.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 757, "text": " We need to generate some vintage images for a promotion poster.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Image-to-Image\\', \\'api_name\\': \\'GreeneryScenery/SheepsControlV3\\', \\'api_call\\': \"pipeline(\\'image-to-image\\', model=\\'GreeneryScenery/SheepsControlV3\\')\", \\'api_arguments\\': {\\'image\\': \\'Path to image file\\', \\'text_guidance\\': \\'Optional text guidance for the model\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'latest\\', \\'torch\\': \\'latest\\'}, \\'example_code\\': [\\'from transformers import pipeline\\', \"model = pipeline(\\'image-to-image\\', model=\\'GreeneryScenery/SheepsControlV3\\')\", \"result = model({\\'image\\': \\'path/to/image.jpg\\', \\'text_guidance\\': \\'Optional text guidance\\'})\"], \\'performance\\': {\\'dataset\\': \\'GreeneryScenery/SheepsControlV3\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\\'}', metadata={})]", "category": "generic"}
{"question_id": 758, "text": " Generate images of cute butterflies using myunus1/diffmodels_galaxies_scratchbook model.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Unconditional Image Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Unconditional Image Generation\\', \\'api_name\\': \\'ocariz/butterfly_200\\', \\'api_call\\': \"DDPMPipeline.from_pretrained(\\'ocariz/butterfly_200\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'diffusers\\', \\'example_code\\': \"from diffusers import DDPMPipeline\\\\npipeline = DDPMPipeline.from_pretrained(\\'ocariz/butterfly_200\\')\\\\nimage = pipeline().images[0]\\\\nimage\", \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'This model is a diffusion model for unconditional image generation of cute butterflies trained for 200 epochs.\\'}', metadata={})]", "category": "generic"}
{"question_id": 759, "text": " As part of the online platform's development, we need an AI that, given input text about a video, estimates the content of the video and judges its category.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Visual Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ivelin/donut-refexp-combined-v1\\', \\'api_call\\': \"pipeline(\\'visual-question-answering\\', model=\\'ivelin/donut-refexp-combined-v1\\')\", \\'api_arguments\\': \\'image, question\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"vqa(image=\\'path/to/image.jpg\\', question=\\'What is the color of the object?\\')\", \\'performance\\': {\\'dataset\\': \\'ivelin/donut-refexp-combined-v1\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'A visual question answering model that takes an image and a question as input and provides an answer based on the visual content of the image and the context of the question.\\'}', metadata={})]", "category": "generic"}
{"question_id": 760, "text": " We need to develop a model to classify sports clips by identifying the type of sports being played in the video.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 761, "text": " We are designing an application for professional athletes. They need a tool to categorize their exercises based on videos.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 762, "text": " We are building a video analysis tool that can automatically detect the main action happening in a given video clip.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 763, "text": " I would like to create an application that identifies animals in Chinese language image captions. Specifically, we want to know if a picture includes a cat or a dog.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'distilbert-base-uncased-distilled-squad\\', \\'api_call\\': \"pipeline(\\'question-answering\\', model=\\'distilbert-base-uncased-distilled-squad\\')\", \\'api_arguments\\': [\\'question\\', \\'context\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\nquestion_answerer = pipeline(question-answering, model=\\'distilbert-base-uncased-distilled-squad\\')\\\\ncontext = r\\\\n... Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\\\\n... question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\\\\n... a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\\\\n... \\\\nresult = question_answerer(question=What is a good example of a question answering dataset?, context=context)\\\\nprint(\\\\n... fAnswer: \\'{result[\\'answer\\']}\\', score: {round(result[\\'score\\'], 4)}, start: {result[\\'start\\']}, end: {result[\\'end\\']}\\\\n...)\", \\'performance\\': {\\'dataset\\': \\'SQuAD v1.1\\', \\'accuracy\\': \\'86.9 F1 score\\'}, \\'description\\': \"DistilBERT base uncased distilled SQuAD is a fine-tuned version of DistilBERT-base-uncased for the task of question answering. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT\\'s performances as measured on the GLUE language understanding benchmark.\"}', metadata={})]", "category": "generic"}
{"question_id": 764, "text": " I need a portable system to perform quick sentiment analysis of customer reviews.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Classification\\', \\'api_name\\': \\'lvwerra/distilbert-imdb\\', \\'api_call\\': \"pipeline(\\'sentiment-analysis\\', model=\\'lvwerra/distilbert-imdb\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\', \\'pytorch\\'], \\'example_code\\': \"classifier(\\'I love this movie!\\')\", \\'performance\\': {\\'dataset\\': \\'imdb\\', \\'accuracy\\': 0.928}, \\'description\\': \\'This model is a fine-tuned version of distilbert-base-uncased on the imdb dataset. It is used for sentiment analysis on movie reviews and achieves an accuracy of 0.928 on the evaluation set.\\'}', metadata={})]", "category": "generic"}
{"question_id": 765, "text": " A language model has identified a recent book review and provided a summary of its content. Perform sentiment analysis on the summary to evaluate the reviewer's feelings.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Sentiment Analysis\\', \\'api_name\\': \\'bert-base-multilingual-uncased-sentiment\\', \\'api_call\\': \"pipeline(\\'sentiment-analysis\\', model=\\'nlptown/bert-base-multilingual-uncased-sentiment\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"result = sentiment_pipeline(\\'I love this product!\\')\", \\'performance\\': {\\'dataset\\': [{\\'language\\': \\'English\\', \\'accuracy\\': {\\'exact\\': \\'67%\\', \\'off-by-1\\': \\'95%\\'}}, {\\'language\\': \\'Dutch\\', \\'accuracy\\': {\\'exact\\': \\'57%\\', \\'off-by-1\\': \\'93%\\'}}, {\\'language\\': \\'German\\', \\'accuracy\\': {\\'exact\\': \\'61%\\', \\'off-by-1\\': \\'94%\\'}}, {\\'language\\': \\'French\\', \\'accuracy\\': {\\'exact\\': \\'59%\\', \\'off-by-1\\': \\'94%\\'}}, {\\'language\\': \\'Italian\\', \\'accuracy\\': {\\'exact\\': \\'59%\\', \\'off-by-1\\': \\'95%\\'}}, {\\'language\\': \\'Spanish\\', \\'accuracy\\': {\\'exact\\': \\'58%\\', \\'off-by-1\\': \\'95%\\'}}]}, \\'description\\': \\'This a bert-base-multilingual-uncased model finetuned for sentiment analysis on product reviews in six languages: English, Dutch, German, French, Spanish and Italian. It predicts the sentiment of the review as a number of stars (between 1 and 5).\\'}', metadata={})]", "category": "generic"}
{"question_id": 766, "text": " We have movie reviews on our website, and we need to display whether the review is positive or negative.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'results-yelp\\', \\'api_call\\': \"AutoTokenizer.from_pretrained(\\'bert-base-uncased\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"AutoTokenizer.from_pretrained(\\'bert-base-uncased\\')\", \\'config\\': \"AutoConfig.from_pretrained(\\'potatobunny/results-yelp\\')\"}, \\'python_environment_requirements\\': {\\'Transformers\\': \\'4.18.0\\', \\'Pytorch\\': \\'1.10.0+cu111\\', \\'Datasets\\': \\'2.0.0\\', \\'Tokenizers\\': \\'0.12.1\\'}, \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'Yelp\\', \\'accuracy\\': 0.9302}, \\'description\\': \\'This model is a fine-tuned version of textattack/bert-base-uncased-yelp-polarity on a filtered and manually reviewed Yelp dataset containing restaurant reviews only. It is intended to perform text classification, specifically sentiment analysis, on text data obtained from restaurant reviews to determine if the particular review is positive or negative.\\'}', metadata={})]", "category": "generic"}
{"question_id": 767, "text": " I wish to automatically classify a given text's emotion. What Transformers model should I use?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'distilbart-cnn-12-6-samsum\\', \\'api_call\\': \"pipeline(\\'summarization\\', model=\\'philschmid/distilbart-cnn-12-6-samsum\\')\", \\'api_arguments\\': {\\'model\\': \\'philschmid/distilbart-cnn-12-6-samsum\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 41.09, \\'ROUGE-2\\': 20.746, \\'ROUGE-L\\': 31.595, \\'ROUGE-LSUM\\': 38.339}}, \\'description\\': \\'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 768, "text": " Develop a tool to analyze restaurant reviews from Yelp for positive or negative sentiments.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'results-yelp\\', \\'api_call\\': \"AutoTokenizer.from_pretrained(\\'bert-base-uncased\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"AutoTokenizer.from_pretrained(\\'bert-base-uncased\\')\", \\'config\\': \"AutoConfig.from_pretrained(\\'potatobunny/results-yelp\\')\"}, \\'python_environment_requirements\\': {\\'Transformers\\': \\'4.18.0\\', \\'Pytorch\\': \\'1.10.0+cu111\\', \\'Datasets\\': \\'2.0.0\\', \\'Tokenizers\\': \\'0.12.1\\'}, \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'Yelp\\', \\'accuracy\\': 0.9302}, \\'description\\': \\'This model is a fine-tuned version of textattack/bert-base-uncased-yelp-polarity on a filtered and manually reviewed Yelp dataset containing restaurant reviews only. It is intended to perform text classification, specifically sentiment analysis, on text data obtained from restaurant reviews to determine if the particular review is positive or negative.\\'}', metadata={})]", "category": "generic"}
{"question_id": 769, "text": " We're creating a chatbot that will detect the user's emotion. I want to start by implementing the basic functionality of emotion detection from the user's responses.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Conversational\\', \\'api_name\\': \\'Pi3141/DialoGPT-medium-elon-3\\', \\'api_call\\': \"pipeline(\\'text-generation\\', model=\\'Pi3141/DialoGPT-medium-elon-3\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'Input a message to start chatting with Pi3141/DialoGPT-medium-elon-3.\\', \\'performance\\': {\\'dataset\\': \\'Twitter tweets by Elon Musk\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'DialoGPT model that talks like Elon Musk, trained on Twitter tweets by Elon Musk. This model will spew meaningless shit about 40% of the time. Trained on 8 epochs. But with a larger dataset this time. The AI can now use more emojis, I think.\\'}', metadata={})]", "category": "generic"}
{"question_id": 770, "text": " I am a journalist. I am writing an article about European start-ups. I need to see all the dates and company names in the text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 771, "text": " Create a tool to extract entities from news articles to help journalists research important figures and organizations.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Token Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Named Entity Recognition\\', \\'api_name\\': \\'dslim/bert-base-NER-uncased\\', \\'api_call\\': \"pipeline(\\'ner\\', model=\\'dslim/bert-base-NER-uncased\\')\", \\'api_arguments\\': {}, \\'python_environment_requirements\\': {\\'transformers\\': \\'>=4.0.0\\'}, \\'example_code\\': \"nlp(\\'My name is John and I live in New York.\\')\", \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A pretrained BERT model for Named Entity Recognition (NER) on uncased text. It can be used to extract entities such as person names, locations, and organizations from text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 772, "text": " A famous writer is working on a novel. He needs your help to predict the punctuation marks needed in his written draft.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Information Retrieval\\', \\'api_name\\': \\'cross-encoder/ms-marco-TinyBERT-L-2-v2\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'cross-encoder/ms-marco-TinyBERT-L-2-v2\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\"}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': {\\'import\\': \\'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\', \\'model\\': \"model = AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\", \\'scores\\': \\'with torch.no_grad():\\\\n    scores = model(**features).logits\\\\n    print(scores)\\'}, \\'performance\\': {\\'dataset\\': \\'TREC Deep Learning 2019\\', \\'accuracy\\': \\'69.84 (NDCG@10)\\'}, \\'description\\': \\'This model was trained on the MS Marco Passage Ranking task. It can be used for Information Retrieval: Given a query, encode the query with all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. The training code is available here: SBERT.net Training MS Marco.\\'}', metadata={})]", "category": "generic"}
{"question_id": 773, "text": " There's a collection of texts that we want to be able to analyze for their part-of-speech tags to better understand the structure of the sentences within the texts.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Zero-Shot Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Zero-Shot Image Classification\\', \\'api_name\\': \\'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k\\', \\'api_call\\': \"pipeline(\\'image-classification\\', model=\\'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k\\')\", \\'api_arguments\\': [\\'image\\', \\'possible_class_names\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline; classifier = pipeline(\\'image-classification\\', model=\\'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k\\'); classifier(image, possible_class_names=[\\'cat\\', \\'dog\\'])\", \\'performance\\': {\\'dataset\\': \\'ImageNet-1k\\', \\'accuracy\\': \\'80.1\\'}, \\'description\\': \\'A CLIP ViT-bigG/14 model trained with the LAION-2B English subset of LAION-5B using OpenCLIP. The model is intended for research purposes and enables researchers to better understand and explore zero-shot, arbitrary image classification. It can be used for interdisciplinary studies of the potential impact of such models. The model achieves a 80.1 zero-shot top-1 accuracy on ImageNet-1k.\\'}', metadata={})]", "category": "generic"}
{"question_id": 774, "text": " I am working on a project where I want to make predictions from my data that is stored in structured tables. Find a pre-trained model for table question answering.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 775, "text": " I have a table in CSV format and a query related to it. Could you obtain an answer for my query?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Text-to-Text Generation\\', \\'api_name\\': \\'optimum/t5-small\\', \\'api_call\\': \"ORTModelForSeq2SeqLM.from_pretrained(\\'optimum/t5-small\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'optimum.onnxruntime\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, pipeline\\\\nfrom optimum.onnxruntime import ORTModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained(optimum/t5-small)\\\\nmodel = ORTModelForSeq2SeqLM.from_pretrained(optimum/t5-small)\\\\ntranslator = pipeline(translation_en_to_fr, model=model, tokenizer=tokenizer)\\\\nresults = translator(My name is Eustache and I have a pet raccoon)\\\\nprint(results)\\', \\'performance\\': {\\'dataset\\': \\'c4\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'T5 is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks and for which each task is converted into a text-to-text format. It can be used for translation, text-to-text generation, and summarization.\\'}', metadata={})]", "category": "generic"}
{"question_id": 776, "text": " Develop a tool that helps me get answers to questions related to a specific text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Table Question Answering\\', \\'api_name\\': \\'google/tapas-small-finetuned-wikisql-supervised\\', \\'api_call\\': \"TapasForQuestionAnswering.from_pretrained(\\'google/tapas-small-finetuned-wikisql-supervised\\')\", \\'api_arguments\\': \"model = TapasForQuestionAnswering.from_pretrained(\\'google/tapas-small-finetuned-wikisql-supervised\\')\", \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import TapasTokenizer, TapasForQuestionAnswering\\\\ntokenizer = TapasTokenizer.from_pretrained(\\'google/tapas-small-finetuned-wikisql-supervised\\')\\\\nmodel = TapasForQuestionAnswering.from_pretrained(\\'google/tapas-small-finetuned-wikisql-supervised\\')\", \\'performance\\': {\\'dataset\\': \\'wikisql\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'TAPAS is a BERT-like transformers model pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion. This model is fine-tuned on WikiSQL and can be used for answering questions related to a table.\\'}', metadata={})]", "category": "generic"}
{"question_id": 777, "text": " A healthcare professional wants to get quick answers to COVID-19 related questions from the latest research articles.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Table Question Answering\\', \\'api_name\\': \\'google/tapas-base-finetuned-sqa\\', \\'api_call\\': \"TapasTokenizer.from_pretrained(\\'google/tapas-base-finetuned-sqa\\')\", \\'api_arguments\\': [\\'question\\', \\'table\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\n\\\\n# Initialize the pipeline\\\\ntapas_pipeline = pipeline(\\'table-question-answering\\', model=\\'google/tapas-base-finetuned-sqa\\')\\\\n\\\\n# Define the table and question\\\\nquestion = \\'How many goals did player A score?\\'\\\\ntable = [[\\'Player\\', \\'Goals\\'], [\\'Player A\\', 5], [\\'Player B\\', 3]]\\\\n\\\\n# Get the answer\\\\nresult = tapas_pipeline(question=question, table=table)\\\\nprint(result)\", \\'performance\\': {\\'dataset\\': \\'msr_sqa\\', \\'accuracy\\': 0.6874}, \\'description\\': \\'TAPAS base model fine-tuned on Sequential Question Answering (SQA). It is a BERT-like transformers model pretrained on a large corpus of English data from Wikipedia and fine-tuned on SQA. It can be used for answering questions related to a table in a conversational set-up.\\'}', metadata={})]", "category": "generic"}
{"question_id": 778, "text": " I have this app for sharing cooking recipes. Users upload photos and ask questions about the showcased recipe. I need to automatically answer their questions based on the recipe image provided.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \"PegasusForConditionalGeneration.from_pretrained(\\'tuner007/pegasus_summarizer\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 779, "text": " The company's legal team is working on a case. They need a highly accurate tool to extract answers from a large set of legal documents. Develop a tool for this purpose.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Fill-Mask\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Fill-Mask\\', \\'api_name\\': \\'nlpaueb/legal-bert-small-uncased\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'nlpaueb/legal-bert-small-uncased\\')\", \\'api_arguments\\': {\\'pretrained_model_name_or_path\\': \\'nlpaueb/legal-bert-small-uncased\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'AutoTokenizer, AutoModel\\'}, \\'example_code\\': \"from transformers import AutoTokenizer, AutoModel\\\\ntokenizer = AutoTokenizer.from_pretrained(\\'nlpaueb/legal-bert-small-uncased\\')\\\\nmodel = AutoModel.from_pretrained(\\'nlpaueb/legal-bert-small-uncased\\')\", \\'performance\\': {\\'dataset\\': \\'Legal Corpora\\', \\'accuracy\\': \\'Comparable to larger models\\'}, \\'description\\': \\'LEGAL-BERT is a family of BERT models for the legal domain, intended to assist legal NLP research, computational law, and legal technology applications. This is the light-weight version of BERT-BASE (33% the size of BERT-BASE) pre-trained from scratch on legal data, which achieves comparable performance to larger models, while being much more efficient (approximately 4 times faster) with a smaller environmental footprint.\\'}', metadata={})]", "category": "generic"}
{"question_id": 780, "text": " We have received a customer inquiry. Help us categorize the inquiry into one of the following categories: \\\"sales\\\", \\\"technical support\\\", or \\\"billing\\\".\\n###Input: \\\"I am experiencing difficulty with the installation process of your software.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 781, "text": " Imagine you have a news aggregator platform, and you want it to separate news articles into categories like Politics, Sports, Technology, Business, and Entertainment. Implement the classification using this API.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Feature Extraction\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'DeepPavlov/rubert-base-cased\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'DeepPavlov/rubert-base-cased\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'Russian part of Wikipedia and news data\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'RuBERT (Russian, cased, 12\u2011layer, 768\u2011hidden, 12\u2011heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT\u2011base as an initialization for RuBERT[1].\\'}', metadata={})]", "category": "generic"}
{"question_id": 782, "text": " I need to classify German news articles into categories like crime, tragedy, and theft. The classifier should be able to understand German as well.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Audio Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'padmalcom/wav2vec2-large-emotion-detection-german\\', \\'api_call\\': \"pipeline(\\'audio-classification\\', model=\\'padmalcom/wav2vec2-large-emotion-detection-german\\')\", \\'api_arguments\\': \\'audio_file\\', \\'python_environment_requirements\\': \\'transformers, torch\\', \\'example_code\\': \"from transformers import pipeline\\\\naudio_classifier = pipeline(\\'audio-classification\\', model=\\'padmalcom/wav2vec2-large-emotion-detection-german\\')\\\\nresult = audio_classifier(audio_file)\", \\'performance\\': {\\'dataset\\': \\'emo-DB\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'This wav2vec2 based emotion detection model is trained on the emo-DB dataset. It can classify emotions in German audio files into seven classes: anger, boredom, disgust, fear, happiness, sadness, and neutral.\\'}', metadata={})]", "category": "generic"}
{"question_id": 783, "text": " The company has just received a document written in French, and they need it translated into English.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Document Question Answer\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'impira/layoutlm-invoices\\', \\'api_call\\': \"pipeline(\\'question-answering\\', model=\\'impira/layoutlm-invoices\\')\", \\'api_arguments\\': \\'question, context\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"qa_pipeline(question=\\'your question\\', context=\\'your document context\\')\", \\'performance\\': {\\'dataset\\': \\'proprietary dataset of invoices, SQuAD2.0, and DocVQA\\', \\'accuracy\\': \\'not provided\\'}, \\'description\\': \\'This is a fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens (because they predict the start and end of a sequence), this model can predict longer-range, non-consecutive sequences with an additional classifier head.\\'}', metadata={})]", "category": "generic"}
{"question_id": 784, "text": " A business collaboration project requires staff to translate Russian documents into English.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'sberbank-ai/sbert_large_mt_nlu_ru\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'sberbank-ai/sbert_large_mt_nlu_ru\\')\", \\'api_arguments\\': [\\'sentences\\', \\'padding\\', \\'truncation\\', \\'max_length\\', \\'return_tensors\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': \"from transformers import AutoTokenizer, AutoModel\\\\nimport torch\\\\n\\\\n\\\\n# Mean Pooling - Take attention mask into account for correct averaging\\\\ndef mean_pooling(model_output, attention_mask):\\\\n    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\\\\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\\\\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\\\\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\\\\n    return sum_embeddings / sum_mask\\\\n\\\\n\\\\n# Sentences we want sentence embeddings for sentences = [\\'?\\']\\\\n\\\\n# Load AutoModel from huggingface model repository\\\\ntokenizer = AutoTokenizer.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\\\nmodel = AutoModel.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\\\\n\\\\n# Tokenize sentences\\\\nencoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=24, return_tensors=\\'pt\\')\\\\n\\\\n# Compute token embeddings\\\\nwith torch.no_grad():\\\\n    model_output = model(**encoded_input)\\\\n\\\\n# Perform pooling. In this case, mean pooling\\\\nsentence_embeddings = mean_pooling(model_output, encoded_input[\\'attention_mask\\'])\", \\'performance\\': {\\'dataset\\': \\'Russian SuperGLUE\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'BERT large model multitask (cased) for Sentence Embeddings in Russian language.\\'}', metadata={})]", "category": "generic"}
{"question_id": 785, "text": " In this fast world, a user spends very little time on reading news articles and requires summary of the news articles.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Feature Extraction\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'DeepPavlov/rubert-base-cased\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'DeepPavlov/rubert-base-cased\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'Russian part of Wikipedia and news data\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'RuBERT (Russian, cased, 12\u2011layer, 768\u2011hidden, 12\u2011heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT\u2011base as an initialization for RuBERT[1].\\'}', metadata={})]", "category": "generic"}
{"question_id": 786, "text": " We need to summarize a scientific article. The input must include all the important points discussed in the article, and the result should be a concise abstraction of the content.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 787, "text": " Now I need to create a summary of my chat with my friend last night.\\n###Input: conversation = '''Hannah: Hey, do you have Betty's number?\\nAmanda: Lemme check\\nAmanda: Sorry, can't find it.\\nAmanda: Ask Larry\\nAmanda: He called her last time we were at the park together\\nHannah: I don't know him well\\nAmanda: Don't be shy, he's very nice\\nHannah: If you say so..\\nHannah: I'd rather you texted him\\nAmanda: Just text him \\ud83d\\ude42\\nHannah: Urgh.. Alright\\nHannah: Bye\\nAmanda: Bye bye\\n'''\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Summarization\\', \\'api_name\\': \\'lidiya/bart-large-xsum-samsum\\', \\'api_call\\': \"pipeline(\\'summarization\\', model=\\'lidiya/bart-large-xsum-samsum\\')\", \\'api_arguments\\': \\'conversation\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=lidiya/bart-large-xsum-samsum)\\\\nconversation = \\'\\'\\'Hannah: Hey, do you have Betty\\'s number?\\\\nAmanda: Lemme check\\\\nAmanda: Sorry, can\\'t find it.\\\\nAmanda: Ask Larry\\\\nAmanda: He called her last time we were at the park together\\\\nHannah: I don\\'t know him well\\\\nAmanda: Don\\'t be shy, he\\'s very nice\\\\nHannah: If you say so..\\\\nHannah: I\\'d rather you texted him\\\\nAmanda: Just text him \ud83d\ude42\\\\nHannah: Urgh.. Alright\\\\nHannah: Bye\\\\nAmanda: Bye bye <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization\\', \\'accuracy\\': {\\'rouge1\\': 53.306, \\'rouge2\\': 28.355, \\'rougeL\\': 44.095}}, \\'description\\': \\'This model was obtained by fine-tuning facebook/bart-large-xsum on Samsum dataset.\\'}', metadata={})]", "category": "generic"}
{"question_id": 788, "text": " As a news agency, we need a summarized version of a recent article about YouTube's new policy on vaccine misinformation.\\n###Input: Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people's scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We're expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'csebuetnlp/mT5_multilingual_XLSum\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'csebuetnlp/mT5_multilingual_XLSum\\')\", \\'api_arguments\\': [\\'model_name\\'], \\'python_environment_requirements\\': [\\'transformers==4.11.0.dev0\\'], \\'example_code\\': \"import re\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nWHITESPACE_HANDLER = lambda k: re.sub(\\'\\\\\\\\s+\\', \\' \\', re.sub(\\'\\\\\\\\n+\\', \\' \\', k.strip()))\\\\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people\\'s scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We\\'re expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\\\\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\\\ninput_ids = tokenizer(\\\\n [WHITESPACE_HANDLER(article_text)],\\\\n return_tensors=pt,\\\\n padding=max_length,\\\\n truncation=True,\\\\n max_length=512\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n max_length=84,\\\\n no_repeat_ngram_size=2,\\\\n num_beams=4\\\\n)[0]\\\\nsummary = tokenizer.decode(\\\\n output_ids,\\\\n skip_special_tokens=True,\\\\n clean_up_tokenization_spaces=False\\\\n)\\\\nprint(summary)\", \\'performance\\': {\\'dataset\\': \\'xsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.5, \\'ROUGE-2\\': 13.934, \\'ROUGE-L\\': 28.988, \\'ROUGE-LSUM\\': 28.996, \\'loss\\': 2.067, \\'gen_len\\': 26.973}}, \\'description\\': \\'This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 789, "text": " Let's build a Chatbot that can solve daily life problems of a research Assistant.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image Generation\\', \\'api_name\\': \\'stabilityai/stable-diffusion-2-1-base\\', \\'api_call\\': \"StableDiffusionPipeline.from_pretrained(\\'stabilityai/stable-diffusion-2-1-base\\', scheduler=EulerDiscreteScheduler.from_pretrained(stabilityai/stable-diffusion-2-1-base, subfolder=scheduler), torch_dtype=torch.float16)\", \\'api_arguments\\': {\\'prompt\\': \\'a photo of an astronaut riding a horse on mars\\'}, \\'python_environment_requirements\\': [\\'diffusers\\', \\'transformers\\', \\'accelerate\\', \\'scipy\\', \\'safetensors\\'], \\'example_code\\': {\\'install_dependencies\\': \\'pip install diffusers transformers accelerate scipy safetensors\\', \\'code\\': \\'from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\\\nimport torch\\\\nmodel_id = stabilityai/stable-diffusion-2-1-base\\\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\\\\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\\\\npipe = pipe.to(cuda)\\\\nprompt = a photo of an astronaut riding a horse on mars\\\\nimage = pipe(prompt).images[0]\\\\nimage.save(astronaut_rides_horse.png)\\'}, \\'performance\\': {\\'dataset\\': \\'COCO2017 validation set\\', \\'accuracy\\': \\'Not optimized for FID scores\\'}, \\'description\\': \\'Stable Diffusion v2-1-base is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H). It is intended for research purposes only and can be used in areas such as safe deployment of models, understanding limitations and biases of generative models, generation of artworks, and research on generative models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 790, "text": " We need to integrate the personal assistant we're building with the capability of having conversations with people, sometimes answering general knowledge questions.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Conversational\\', \\'api_name\\': \\'microsoft/GODEL-v1_1-large-seq2seq\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'microsoft/GODEL-v1_1-large-seq2seq\\')\", \\'api_arguments\\': {\\'instruction\\': \\'Instruction: given a dialog context, you need to response empathically.\\', \\'knowledge\\': \\'\\', \\'dialog\\': [\\'Does money buy happiness?\\', \\'It is a question. Money buys you a lot of things, but not enough to buy happiness.\\', \\'What is the best way to buy happiness ?\\']}, \\'python_environment_requirements\\': {\\'transformers\\': \\'AutoTokenizer, AutoModelForSeq2SeqLM\\'}, \\'example_code\\': \"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained(microsoft/GODEL-v1_1-large-seq2seq)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(microsoft/GODEL-v1_1-large-seq2seq)\\\\ndef generate(instruction, knowledge, dialog):\\\\n if knowledge != \\'\\':\\\\n knowledge = \\'[KNOWLEDGE] \\' + knowledge\\\\n dialog = \\' EOS \\'.join(dialog)\\\\n query = f{instruction} [CONTEXT] {dialog} {knowledge}\\\\n input_ids = tokenizer(f{query}, return_tensors=pt).input_ids\\\\n outputs = model.generate(input_ids, max_length=128, min_length=8, top_p=0.9, do_sample=True)\\\\n output = tokenizer.decode(outputs[0], skip_special_tokens=True)\\\\n return output\\\\n\\\\ninstruction = f\\'Instruction: given a dialog context, you need to response empathically.\\'\\\\nknowledge = \\'\\'\\\\ndialog = [\\\\n \\'Does money buy happiness?\\',\\\\n \\'It is a question. Money buys you a lot of things, but not enough to buy happiness.\\',\\\\n \\'What is the best way to buy happiness ?\\'\\\\n]\\\\nresponse = generate(instruction, knowledge, dialog)\\\\nprint(response)\", \\'performance\\': {\\'dataset\\': \\'Reddit discussion thread, instruction and knowledge grounded dialogs\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'GODEL is a large-scale pre-trained model for goal-directed dialogs. It is parameterized with a Transformer-based encoder-decoder model and trained for response generation grounded in external text, which allows more effective fine-tuning on dialog tasks that require conditioning the response on information that is external to the current conversation (e.g., a retrieved document). The pre-trained model can be efficiently fine-tuned and adapted to accomplish a new dialog task with a handful of task-specific dialogs. The v1.1 model is trained on 551M multi-turn dialogs from Reddit discussion thread, and 5M instruction and knowledge grounded dialogs.\\'}', metadata={})]", "category": "generic"}
{"question_id": 791, "text": " I want to create a chatbot for discussing gardening topics and tips, which can refer to external information while generating responses.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Conversational\\', \\'api_name\\': \\'microsoft/GODEL-v1_1-base-seq2seq\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'microsoft/GODEL-v1_1-base-seq2seq\\')\", \\'api_arguments\\': [\\'instruction\\', \\'knowledge\\', \\'dialog\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained(microsoft/GODEL-v1_1-base-seq2seq)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(microsoft/GODEL-v1_1-base-seq2seq)\\\\ndef generate(instruction, knowledge, dialog):\\\\n if knowledge != \\'\\':\\\\n knowledge = \\'[KNOWLEDGE] \\' + knowledge\\\\n dialog = \\' EOS \\'.join(dialog)\\\\n query = f{instruction} [CONTEXT] {dialog} {knowledge}\\\\n input_ids = tokenizer(f{query}, return_tensors=pt).input_ids\\\\n outputs = model.generate(input_ids, max_length=128, min_length=8, top_p=0.9, do_sample=True)\\\\n output = tokenizer.decode(outputs[0], skip_special_tokens=True)\\\\n return output\", \\'performance\\': {\\'dataset\\': \\'Reddit discussion thread, instruction and knowledge grounded dialogs\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'GODEL is a large-scale pre-trained model for goal-directed dialogs. It is parameterized with a Transformer-based encoder-decoder model and trained for response generation grounded in external text, which allows more effective fine-tuning on dialog tasks that require conditioning the response on information that is external to the current conversation (e.g., a retrieved document). The pre-trained model can be efficiently fine-tuned and adapted to accomplish a new dialog task with a handful of task-specific dialogs. The v1.1 model is trained on 551M multi-turn dialogs from Reddit discussion thread, and 5M instruction and knowledge grounded dialogs.\\'}', metadata={})]", "category": "generic"}
{"question_id": 792, "text": " We're trying to help out a friend who's developing an application for composing text. He is trying to create a bot that comes up with creative ideas for your paragraph.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 793, "text": " We are creating an AI newsletter application that generates summaries of news articles. We need the AI to generate a brief summary for a given article.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 794, "text": " I want a code completion tool to assist with finishing my incomplete Python code.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 795, "text": " We are making an AI copywriter for marketing content. Help me to provide content for a product relating to eco-friendly kitchenware.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 796, "text": " I am an astronaut in space, writing my diary every day. I need a summary of my diary before sharing it with my family.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 797, "text": " I have some paragraphs that I want to use as a source for generating questions, based on the information present in them.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \"PegasusForConditionalGeneration.from_pretrained(\\'tuner007/pegasus_summarizer\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 798, "text": " I am a novel writer. I plan to write some paragraphs, but I cannot find the exact word placeholder or missing word. Can you help me make a tool to complete the given text below?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'distilbart-cnn-12-6-samsum\\', \\'api_call\\': \"pipeline(\\'summarization\\', model=\\'philschmid/distilbart-cnn-12-6-samsum\\')\", \\'api_arguments\\': {\\'model\\': \\'philschmid/distilbart-cnn-12-6-samsum\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 41.09, \\'ROUGE-2\\': 20.746, \\'ROUGE-L\\': 31.595, \\'ROUGE-LSUM\\': 38.339}}, \\'description\\': \\'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 799, "text": " I am working in a dating chatapp development team. We want to generate sentences to make it more interactive.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 801, "text": " I am building a recommendation engine to group news articles. I need a way to determine the similarity between two sentences.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 802, "text": " The company needs a tool to analyze customers' reviews about their products. We need to find out which ones are positive, neutral, or negative.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'csebuetnlp/mT5_multilingual_XLSum\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'csebuetnlp/mT5_multilingual_XLSum\\')\", \\'api_arguments\\': [\\'model_name\\'], \\'python_environment_requirements\\': [\\'transformers==4.11.0.dev0\\'], \\'example_code\\': \"import re\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nWHITESPACE_HANDLER = lambda k: re.sub(\\'\\\\\\\\s+\\', \\' \\', re.sub(\\'\\\\\\\\n+\\', \\' \\', k.strip()))\\\\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people\\'s scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We\\'re expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\\\\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\\\ninput_ids = tokenizer(\\\\n [WHITESPACE_HANDLER(article_text)],\\\\n return_tensors=pt,\\\\n padding=max_length,\\\\n truncation=True,\\\\n max_length=512\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n max_length=84,\\\\n no_repeat_ngram_size=2,\\\\n num_beams=4\\\\n)[0]\\\\nsummary = tokenizer.decode(\\\\n output_ids,\\\\n skip_special_tokens=True,\\\\n clean_up_tokenization_spaces=False\\\\n)\\\\nprint(summary)\", \\'performance\\': {\\'dataset\\': \\'xsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.5, \\'ROUGE-2\\': 13.934, \\'ROUGE-L\\': 28.988, \\'ROUGE-LSUM\\': 28.996, \\'loss\\': 2.067, \\'gen_len\\': 26.973}}, \\'description\\': \\'This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 803, "text": " I am working on a project that ranks answers to questions based on their relevance. Can you help me find the most relevant answer to a specific question by using sentence similarity?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Summarization\\', \\'api_name\\': \\'distilbart-cnn-12-6-samsum\\', \\'api_call\\': \"pipeline(\\'summarization\\', model=\\'philschmid/distilbart-cnn-12-6-samsum\\')\", \\'api_arguments\\': {\\'model\\': \\'philschmid/distilbart-cnn-12-6-samsum\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/distilbart-cnn-12-6-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 41.09, \\'ROUGE-2\\': 20.746, \\'ROUGE-L\\': 31.595, \\'ROUGE-LSUM\\': 38.339}}, \\'description\\': \\'This model is a DistilBART-based text summarization model trained on the SAMsum dataset. It can be used to generate summaries of conversational text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 804, "text": " Assist a client producing an audiobook in Japanese. They need a solution to convert their text script into spoken Japanese.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Text-to-Speech\\', \\'framework\\': \\'ESPnet\\', \\'functionality\\': \\'Text-to-Speech\\', \\'api_name\\': \\'mio/Artoria\\', \\'api_call\\': \"pipeline(\\'text-to-speech\\', model=\\'mio/Artoria\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline; tts = pipeline(\\'text-to-speech\\', model=\\'mio/Artoria\\'); tts(\\'s\\')\", \\'performance\\': {\\'dataset\\': \\'fate\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'This model was trained by mio using fate recipe in espnet. It is a text-to-speech model that can convert text input into speech output.\\'}', metadata={})]", "category": "generic"}
{"question_id": 805, "text": " We have a product for the German-speaking audience, and we need to generate some sample voiceovers for the promotional videos.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \"PegasusForConditionalGeneration.from_pretrained(\\'tuner007/pegasus_summarizer\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 806, "text": " I work as a transcriptionist, and I have an hour-long recording of a meeting. I need to identify and segment the speech of various speakers in the recording.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentiment Analysis\\', \\'api_name\\': \\'michellejieli/NSFW_text_classifier\\', \\'api_call\\': \"pipeline(\\'sentiment-analysis\\', model=\\'michellejieli/NSFW_text_classification\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'classifier(I see you\u2019ve set aside this special time to humiliate yourself in public.)\\', \\'performance\\': {\\'dataset\\': \\'Reddit posts\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'DistilBERT is a transformer model that performs sentiment analysis. I fine-tuned the model on Reddit posts with the purpose of classifying not safe for work (NSFW) content, specifically text that is considered inappropriate and unprofessional. The model predicts 2 classes, which are NSFW or safe for work (SFW). The model is a fine-tuned version of DistilBERT. It was fine-tuned on 14317 Reddit posts pulled from the Reddit API.\\'}', metadata={})]", "category": "generic"}
{"question_id": 807, "text": " We are a company focusing on transcription services for Arabic speakers. We need to convert their speech to text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 808, "text": " An audio file is recorded in a conference and we need the text version of the conversation for record-keeping purposes.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \"PegasusForConditionalGeneration.from_pretrained(\\'tuner007/pegasus_summarizer\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 809, "text": " Provide a way to translate spoken English to spoken Hokkien for an audio file.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Automatic Speech Recognition\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'microsoft/wavlm-large\\', \\'api_call\\': \"Wav2Vec2Model.from_pretrained(\\'microsoft/wavlm-large\\')\", \\'api_arguments\\': \\'speech input\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'To fine-tune the model for speech recognition, see the official speech recognition example. To fine-tune the model for speech classification, see the official audio classification example.\\', \\'performance\\': {\\'dataset\\': \\'SUPERB benchmark\\', \\'accuracy\\': \\'state-of-the-art performance\\'}, \\'description\\': \\'WavLM-Large is a large model pretrained on 16kHz sampled speech audio. It is built based on the HuBERT framework, with an emphasis on both spoken content modeling and speaker identity preservation. WavLM is pretrained on 60,000 hours of Libri-Light, 10,000 hours of GigaSpeech, and 24,000 hours of VoxPopuli. It achieves state-of-the-art performance on the SUPERB benchmark and brings significant improvements for various speech processing tasks on their representative benchmarks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 810, "text": " We want to separate the speaker sources from the original audio file to filter the noise.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Automatic Speech Recognition\\', \\'framework\\': \\'pyannote.audio\\', \\'functionality\\': \\'Speaker Diarization\\', \\'api_name\\': \\'pyannote/speaker-diarization\\', \\'api_call\\': \"Pipeline.from_pretrained(\\'pyannote/speaker-diarization@2.1\\', use_auth_token=\\'ACCESS_TOKEN_GOES_HERE\\')\", \\'api_arguments\\': {\\'num_speakers\\': \\'int (optional)\\', \\'min_speakers\\': \\'int (optional)\\', \\'max_speakers\\': \\'int (optional)\\'}, \\'python_environment_requirements\\': \\'pyannote.audio 2.1.1\\', \\'example_code\\': [\\'from pyannote.audio import Pipeline\\', \\'pipeline = Pipeline.from_pretrained(pyannote/speaker-diarization@2.1, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\', \\'diarization = pipeline(audio.wav)\\', \\'with open(audio.rttm, w) as rttm:\\', \\'  diarization.write_rttm(rttm)\\'], \\'performance\\': {\\'dataset\\': \\'ami\\', \\'accuracy\\': {\\'DER%\\': \\'18.91\\', \\'FA%\\': \\'4.48\\', \\'Miss%\\': \\'9.51\\', \\'Conf%\\': \\'4.91\\'}}, \\'description\\': \\'This API provides an automatic speaker diarization pipeline using the pyannote.audio framework. It can process audio files and output speaker diarization results in RTTM format. The pipeline can also handle cases where the number of speakers is known in advance or when providing lower and/or upper bounds on the number of speakers.\\'}', metadata={})]", "category": "generic"}
{"question_id": 811, "text": " Our goal is to analyze the emotions expressed in a user's recorded message.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'michellejieli/emotion_text_classifier\\', \\'api_call\\': \"pipeline(\\'sentiment-analysis\\', model=\\'michellejieli/emotion_text_classifier\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'classifier(I love this!)\\', \\'performance\\': {\\'dataset\\': [\\'Crowdflower (2016)\\', \\'Emotion Dataset, Elvis et al. (2018)\\', \\'GoEmotions, Demszky et al. (2020)\\', \\'ISEAR, Vikash (2018)\\', \\'MELD, Poria et al. (2019)\\', \\'SemEval-2018, EI-reg, Mohammad et al. (2018)\\', \\'Emotion Lines (Friends)\\'], \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'DistilRoBERTa-base is a transformer model that performs sentiment analysis. I fine-tuned the model on transcripts from the Friends show with the goal of classifying emotions from text data, specifically dialogue from Netflix shows or movies. The model predicts 6 Ekman emotions and a neutral class. These emotions include anger, disgust, fear, joy, neutrality, sadness, and surprise.\\'}', metadata={})]", "category": "generic"}
{"question_id": 812, "text": " I want to make a conference call app which differentiates between the times when the user is speaking and when there is no voice activity. \\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'promptcap-coco-vqa\\', \\'api_call\\': \"PromptCap(\\'vqascore/promptcap-coco-vqa\\')\", \\'api_arguments\\': {\\'prompt\\': \\'string\\', \\'image\\': \\'string\\'}, \\'python_environment_requirements\\': \\'pip install promptcap\\', \\'example_code\\': [\\'import torch\\', \\'from promptcap import PromptCap\\', \\'model = PromptCap(vqascore/promptcap-coco-vqa)\\', \\'if torch.cuda.is_available():\\', \\'  model.cuda()\\', \\'prompt = please describe this image according to the given question: what piece of clothing is this boy putting on?\\', \\'image = glove_boy.jpeg\\', \\'print(model.caption(prompt, image))\\'], \\'performance\\': {\\'dataset\\': {\\'coco\\': {\\'accuracy\\': \\'150 CIDEr\\'}, \\'OK-VQA\\': {\\'accuracy\\': \\'60.4%\\'}, \\'A-OKVQA\\': {\\'accuracy\\': \\'59.6%\\'}}}, \\'description\\': \\'PromptCap is a captioning model that can be controlled by natural language instruction. The instruction may contain a question that the user is interested in. It achieves SOTA performance on COCO captioning (150 CIDEr) and knowledge-based VQA tasks when paired with GPT-3 (60.4% on OK-VQA and 59.6% on A-OKVQA).\\'}', metadata={})]", "category": "generic"}
{"question_id": 813, "text": " We want to analyze a conference call recording to identify the speakers and the segments of the conversation they participated in.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 814, "text": " Our company wants to predict the potential negative impact on the environment based on certain factors.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Zero-Shot Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Zero-Shot Image Classification\\', \\'api_name\\': \\'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k\\', \\'api_call\\': \"pipeline(\\'image-classification\\', model=\\'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k\\')\", \\'api_arguments\\': [\\'image\\', \\'possible_class_names\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline; classifier = pipeline(\\'image-classification\\', model=\\'laion/CLIP-ViT-bigG-14-laion2B-39B-b160k\\'); classifier(image, possible_class_names=[\\'cat\\', \\'dog\\'])\", \\'performance\\': {\\'dataset\\': \\'ImageNet-1k\\', \\'accuracy\\': \\'80.1\\'}, \\'description\\': \\'A CLIP ViT-bigG/14 model trained with the LAION-2B English subset of LAION-5B using OpenCLIP. The model is intended for research purposes and enables researchers to better understand and explore zero-shot, arbitrary image classification. It can be used for interdisciplinary studies of the potential impact of such models. The model achieves a 80.1 zero-shot top-1 accuracy on ImageNet-1k.\\'}', metadata={})]", "category": "generic"}
{"question_id": 815, "text": " I have a dataset with CO2 emissions in a CSV file, and I want to classify which sources have high or low emissions.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Text-to-Text Generation\\', \\'api_name\\': \\'optimum/t5-small\\', \\'api_call\\': \"ORTModelForSeq2SeqLM.from_pretrained(\\'optimum/t5-small\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\', \\'optimum.onnxruntime\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, pipeline\\\\nfrom optimum.onnxruntime import ORTModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained(optimum/t5-small)\\\\nmodel = ORTModelForSeq2SeqLM.from_pretrained(optimum/t5-small)\\\\ntranslator = pipeline(translation_en_to_fr, model=model, tokenizer=tokenizer)\\\\nresults = translator(My name is Eustache and I have a pet raccoon)\\\\nprint(results)\\', \\'performance\\': {\\'dataset\\': \\'c4\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'T5 is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks and for which each task is converted into a text-to-text format. It can be used for translation, text-to-text generation, and summarization.\\'}', metadata={})]", "category": "generic"}
{"question_id": 816, "text": " I am building a recommendation engine to recommend linkbuilding strategy to clients. I have data in csv files that needs to be classified.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 817, "text": " Create a model to estimate the carbon emissions of a specific device by using features like idle power, standby power, and active power.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'sshleifer/distilbart-cnn-12-6\\', \\'api_call\\': \"BartForConditionalGeneration.from_pretrained(\\'sshleifer/distilbart-cnn-12-6\\')\", \\'api_arguments\\': \\'\\', \\'python_environment_requirements\\': \\'huggingface/transformers\\', \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'Rouge 2\\': \\'22.12\\', \\'Rouge-L\\': \\'36.99\\'}}]}, \\'description\\': \"DistilBART is a distilled version of BART, a model for text summarization. This specific checkpoint, \\'sshleifer/distilbart-cnn-12-6\\', is trained on the cnn_dailymail dataset and provides a fast and effective way to generate summaries of text. The model can be loaded using the Hugging Face Transformers library.\"}', metadata={})]", "category": "generic"}
{"question_id": 818, "text": " We have a virtual reality investment in soccer simulations. We need an intelligent learning-based soccer player to make it more engaging and challenging.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 819, "text": " I want to build an AI that identifies the best marketing strategies for my website by trying different combinations of headlines and images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 820, "text": " A team is working on a video game where the player needs to land the spaceship on the lunar surface without crashing. They want to implement an AI module that can play the game and test it.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'csarron/bert-base-uncased-squad-v1\\', \\'api_call\\': \"pipeline(\\'question-answering\\', model=\\'csarron/bert-base-uncased-squad-v1\\', tokenizer=\\'csarron/bert-base-uncased-squad-v1\\')\", \\'api_arguments\\': {\\'model\\': \\'csarron/bert-base-uncased-squad-v1\\', \\'tokenizer\\': \\'csarron/bert-base-uncased-squad-v1\\'}, \\'python_environment_requirements\\': \\'Python 3.7.5\\', \\'example_code\\': \"from transformers import pipeline\\\\nqa_pipeline = pipeline(\\\\n question-answering,\\\\n model=csarron/bert-base-uncased-squad-v1,\\\\n tokenizer=csarron/bert-base-uncased-squad-v1\\\\n)\\\\npredictions = qa_pipeline({\\\\n \\'context\\': The game was played on February 7, 2016 at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California.,\\\\n \\'question\\': What day was the game played on?\\\\n})\\\\nprint(predictions)\", \\'performance\\': {\\'dataset\\': \\'SQuAD1.1\\', \\'accuracy\\': {\\'EM\\': 80.9, \\'F1\\': 88.2}}, \\'description\\': \\'BERT-base uncased model fine-tuned on SQuAD v1. This model is case-insensitive and does not make a difference between english and English.\\'}', metadata={})]", "category": "generic"}
{"question_id": 821, "text": " Develop an AI character that can play the SoccerTwos game with advanced strategies.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Reinforcement Learning\\', \\'framework\\': \\'Unity ML-Agents\\', \\'functionality\\': \\'Train and play SoccerTwos\\', \\'api_name\\': \\'Raiden-1001/poca-Soccerv7.1\\', \\'api_call\\': \"mlagents-load-from-hf --repo-id=\\'Raiden-1001/poca-Soccerv7.1\\' --local-dir=\\'./downloads\\'\", \\'api_arguments\\': [\\'your_configuration_file_path.yaml\\', \\'run_id\\'], \\'python_environment_requirements\\': [\\'ml-agents\\'], \\'example_code\\': \\'mlagents-learn <your_configuration_file_path.yaml> --run-id=<run_id> --resume\\', \\'performance\\': {\\'dataset\\': \\'SoccerTwos\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'A trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library.\\'}', metadata={})]", "category": "generic"}
{"question_id": 822, "text": " Our gaming company is looking for a reinforcement learning solution to implement an artificial agent that can play SoccerTwos proficiently.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Reinforcement Learning\\', \\'framework\\': \\'ML-Agents\\', \\'functionality\\': \\'SoccerTwos\\', \\'api_name\\': \\'Raiden-1001/poca-Soccerv7\\', \\'api_call\\': \"mlagents-load-from-hf --repo-id=\\'Raiden-1001/poca-Soccerv7.1\\' --local-dir=\\'./downloads\\'\", \\'api_arguments\\': [\\'your_configuration_file_path.yaml\\', \\'run_id\\'], \\'python_environment_requirements\\': [\\'unity-ml-agents\\', \\'deep-reinforcement-learning\\', \\'ML-Agents-SoccerTwos\\'], \\'example_code\\': \\'Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\\\\nStep 2: Select your .nn /.onnx file\\\\nClick on Watch the agent play \ud83d\udc40\\', \\'performance\\': {\\'dataset\\': \\'SoccerTwos\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library.\\'}', metadata={})]", "category": "generic"}
{"question_id": 823, "text": " Develop a language understanding feature for a chatbot that can help tourists with information on visiting Indonesia.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Conversational\\', \\'api_name\\': \\'facebook/blenderbot-400M-distill\\', \\'api_call\\': \"BlenderbotForConditionalGeneration.from_pretrained(\\'facebook/blenderbot-400M-distill\\')\", \\'api_arguments\\': \"[\\'message\\']\", \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': \\'Input a message to start chatting with facebook/blenderbot-400M-distill.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'BlenderBot-400M-distill is a distilled version of the BlenderBot model, trained on the Blended Skill Talk dataset. It is designed for open-domain chatbot tasks and can generate text-to-text responses in a conversational manner. The model is based on the Transformers library and can be used with PyTorch, TensorFlow, and JAX.\\'}', metadata={})]", "category": "generic"}
{"question_id": 824, "text": " I need to analyze images in real-time feeds from different locations for object recognition.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Reinforcement Learning Robotics\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'EmbodiedAI tasks, such as object manipulation and indoor navigation\\', \\'api_name\\': \\'facebook/vc1-large\\', \\'api_call\\': \"model_utils.load_model(\\'model_utils.VC1_BASE_NAME\\')\", \\'api_arguments\\': \\'img\\', \\'python_environment_requirements\\': \\'from vc_models.models.vit import model_utils\\', \\'example_code\\': \\'model,embd_size,model_transforms,model_info = model_utils.load_model(model_utils.VC1_BASE_NAME)\\\\nimg = your_function_here ...\\\\ntransformed_img = model_transforms(img)\\\\nembedding = model(transformed_img)\\', \\'performance\\': {\\'dataset\\': \\'CortexBench\\', \\'accuracy\\': \\'68.7 (Mean Success)\\'}, \\'description\\': \\'The VC-1 model is a vision transformer (ViT) pre-trained on over 4,000 hours of egocentric videos from 7 different sources, together with ImageNet. The model is trained using Masked Auto-Encoding (MAE) and is available in two sizes: ViT-B and ViT-L. The model is intended for use for EmbodiedAI tasks, such as object manipulation and indoor navigation.\\'}', metadata={})]", "category": "generic"}
{"question_id": 825, "text": " Our travel agency needs to build a chatbot that helps tourists find the best attractions in various destinations. The chatbot should answer questions related to tourist attractions.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'facebook/blenderbot-1B-distill\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'facebook/blenderbot-1B-distill\\')\", \\'api_arguments\\': {\\'input_message\\': \\'string\\'}, \\'python_environment_requirements\\': {\\'library\\': \\'transformers\\', \\'version\\': \\'latest\\'}, \\'example_code\\': \"inputs = tokenizer(\\'Hello, how are you?\\', return_tensors=\\'pt\\'); outputs = model.generate(inputs[\\'input_ids\\']); decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\", \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not mentioned\\'}, \\'description\\': \\'BlenderBot-1B is a large-scale open-domain chatbot model that can engage in conversations, ask and answer questions, and display knowledge, empathy, and personality. This distilled version is smaller and faster than the original 9.4B parameter model, making it more accessible for use.\\'}', metadata={})]", "category": "generic"}
{"question_id": 826, "text": " We want to develop a chatbot that can engage with multilingual users. Please help us create a model to encode sentences and understand user input in various languages.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text-generation\\', \\'api_name\\': \\'pygmalion-1.3b\\', \\'api_call\\': \"pipeline(\\'text-generation\\', \\'PygmalionAI/pygmalion-1.3b\\')\", \\'api_arguments\\': \\'input_prompt\\', \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"[CHARACTER]\\'s Persona: [A few sentences about the character you want the model to play]\\\\n[DIALOGUE HISTORY]\\\\nYou: [Your input message here]\\\\n[CHARACTER]:\", \\'performance\\': {\\'dataset\\': \\'56MB of dialogue data\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \"Pygmalion 1.3B is a proof-of-concept dialogue model based on EleutherAI\\'s pythia-1.3b-deduped. It is designed for generating conversational responses and can be used with a specific input format that includes character persona, dialogue history, and user input message.\"}', metadata={})]", "category": "generic"}
{"question_id": 827, "text": " We need to create a demo for a 3D printing company by generating images of some examples that the business will print.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 828, "text": " I work at an art school and our professor wants to create an AI chatbot that can study an image of a painting and answer questions about it.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 829, "text": " We are working on a project to develop an AI-based method to provide answers to questions about charts and diagrams. Please provide the necessary code for using the Pix2Struct model.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 830, "text": " Create a personalized online ad campaign for our new clothing line. The ad should generate a video showing a man wearing a stylish suit while walking in the city.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 831, "text": " We need to generate a short video showing Spider-Man water skiing in redshift style based on a textual prompt.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Video\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Video Synthesis\\', \\'api_name\\': \\'modelscope-damo-text-to-video-synthesis\\', \\'api_call\\': \"pipeline(\\'text-to-video-synthesis\\', model_dir.as_posix())\", \\'api_arguments\\': {\\'text\\': \\'A short text description in English\\'}, \\'python_environment_requirements\\': [\\'modelscope==1.4.2\\', \\'open_clip_torch\\', \\'pytorch-lightning\\'], \\'example_code\\': \"from huggingface_hub import snapshot_download\\\\nfrom modelscope.pipelines import pipeline\\\\nfrom modelscope.outputs import OutputKeys\\\\nimport pathlib\\\\n\\\\nmodel_dir = pathlib.Path(\\'weights\\')\\\\nsnapshot_download(\\'damo-vilab/modelscope-damo-text-to-video-synthesis\\',\\\\n repo_type=\\'model\\', local_dir=model_dir)\\\\n\\\\npipe = pipeline(\\'text-to-video-synthesis\\', model_dir.as_posix())\\\\n\\\\ntest_text = {\\\\n \\'text\\': \\'A panda eating bamboo on a rock.\\',\\\\n}\\\\n\\\\noutput_video_path = pipe(test_text,)[OutputKeys.OUTPUT_VIDEO]\\\\nprint(\\'output_video_path:\\', output_video_path)\", \\'performance\\': {\\'dataset\\': \\'Webvid, ImageNet, LAION5B\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported.\\'}', metadata={})]", "category": "generic"}
{"question_id": 832, "text": " Our company develops travel guides in Polish language. We are now planning to use artificial intelligence for quick captioning of Polish images.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Visual Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'azwierzc/vilt-b32-finetuned-vqa-pl\\', \\'api_call\\': \"pipeline(\\'visual-question-answering\\', model=\\'azwierzc/vilt-b32-finetuned-vqa-pl\\')\", \\'api_arguments\\': {\\'image\\': \\'path_to_image\\', \\'question\\': \\'question_text\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A Visual Question Answering model fine-tuned on the Polish language.\\'}', metadata={})]", "category": "generic"}
{"question_id": 833, "text": " A user has difficulty in visualizing images and needs help answering questions about an image. We need a model to answer questions based on the image provided.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'PyTorch Transformers\\', \\'functionality\\': \\'Table Question Answering\\', \\'api_name\\': \\'table-question-answering-tapas\\', \\'api_call\\': \"pipeline(\\'table-question-answering\\', model=\\'Meena/table-question-answering-tapas\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'This model can be loaded on the Inference API on-demand.\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'SQA (Sequential Question Answering by Microsoft)\\', \\'accuracy\\': None}, {\\'name\\': \\'WTQ (Wiki Table Questions by Stanford University)\\', \\'accuracy\\': None}, {\\'name\\': \\'WikiSQL (by Salesforce)\\', \\'accuracy\\': None}]}, \\'description\\': \\'TAPAS, the model learns an inner representation of the English language used in tables and associated texts, which can then be used to extract features useful for downstream tasks such as answering questions about a table, or determining whether a sentence is entailed or refuted by the contents of a table. It is a BERT-based model specifically designed (and pre-trained) for answering questions about tabular data. TAPAS uses relative position embeddings and has 7 token types that encode tabular structure. It is pre-trained on the masked language modeling (MLM) objective on a large dataset comprising millions of tables from English Wikipedia and corresponding texts.\\'}', metadata={})]", "category": "generic"}
{"question_id": 834, "text": " A real estate company wants to extract information from a scan of a property listing. They need a solution to recognize and retrieve critical information from the OCR of the scan.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Conversational\\', \\'api_name\\': \\'microsoft/GODEL-v1_1-large-seq2seq\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'microsoft/GODEL-v1_1-large-seq2seq\\')\", \\'api_arguments\\': {\\'instruction\\': \\'Instruction: given a dialog context, you need to response empathically.\\', \\'knowledge\\': \\'\\', \\'dialog\\': [\\'Does money buy happiness?\\', \\'It is a question. Money buys you a lot of things, but not enough to buy happiness.\\', \\'What is the best way to buy happiness ?\\']}, \\'python_environment_requirements\\': {\\'transformers\\': \\'AutoTokenizer, AutoModelForSeq2SeqLM\\'}, \\'example_code\\': \"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained(microsoft/GODEL-v1_1-large-seq2seq)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(microsoft/GODEL-v1_1-large-seq2seq)\\\\ndef generate(instruction, knowledge, dialog):\\\\n if knowledge != \\'\\':\\\\n knowledge = \\'[KNOWLEDGE] \\' + knowledge\\\\n dialog = \\' EOS \\'.join(dialog)\\\\n query = f{instruction} [CONTEXT] {dialog} {knowledge}\\\\n input_ids = tokenizer(f{query}, return_tensors=pt).input_ids\\\\n outputs = model.generate(input_ids, max_length=128, min_length=8, top_p=0.9, do_sample=True)\\\\n output = tokenizer.decode(outputs[0], skip_special_tokens=True)\\\\n return output\\\\n\\\\ninstruction = f\\'Instruction: given a dialog context, you need to response empathically.\\'\\\\nknowledge = \\'\\'\\\\ndialog = [\\\\n \\'Does money buy happiness?\\',\\\\n \\'It is a question. Money buys you a lot of things, but not enough to buy happiness.\\',\\\\n \\'What is the best way to buy happiness ?\\'\\\\n]\\\\nresponse = generate(instruction, knowledge, dialog)\\\\nprint(response)\", \\'performance\\': {\\'dataset\\': \\'Reddit discussion thread, instruction and knowledge grounded dialogs\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'GODEL is a large-scale pre-trained model for goal-directed dialogs. It is parameterized with a Transformer-based encoder-decoder model and trained for response generation grounded in external text, which allows more effective fine-tuning on dialog tasks that require conditioning the response on information that is external to the current conversation (e.g., a retrieved document). The pre-trained model can be efficiently fine-tuned and adapted to accomplish a new dialog task with a handful of task-specific dialogs. The v1.1 model is trained on 551M multi-turn dialogs from Reddit discussion thread, and 5M instruction and knowledge grounded dialogs.\\'}', metadata={})]", "category": "generic"}
{"question_id": 835, "text": " We want to utilize the machine learning model for predicting molecular properties in the drug discovery domain.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 836, "text": " We need to estimate the depth of a scene in an image using a pretrained model. Can you please suggest a way?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 837, "text": " Our company is working on an autonomous robot and needs a solution to estimate the depth of objects in its environment.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'csebuetnlp/mT5_multilingual_XLSum\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'csebuetnlp/mT5_multilingual_XLSum\\')\", \\'api_arguments\\': [\\'model_name\\'], \\'python_environment_requirements\\': [\\'transformers==4.11.0.dev0\\'], \\'example_code\\': \"import re\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nWHITESPACE_HANDLER = lambda k: re.sub(\\'\\\\\\\\s+\\', \\' \\', re.sub(\\'\\\\\\\\n+\\', \\' \\', k.strip()))\\\\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people\\'s scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We\\'re expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\\\\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\\\ninput_ids = tokenizer(\\\\n [WHITESPACE_HANDLER(article_text)],\\\\n return_tensors=pt,\\\\n padding=max_length,\\\\n truncation=True,\\\\n max_length=512\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n max_length=84,\\\\n no_repeat_ngram_size=2,\\\\n num_beams=4\\\\n)[0]\\\\nsummary = tokenizer.decode(\\\\n output_ids,\\\\n skip_special_tokens=True,\\\\n clean_up_tokenization_spaces=False\\\\n)\\\\nprint(summary)\", \\'performance\\': {\\'dataset\\': \\'xsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.5, \\'ROUGE-2\\': 13.934, \\'ROUGE-L\\': 28.988, \\'ROUGE-LSUM\\': 28.996, \\'loss\\': 2.067, \\'gen_len\\': 26.973}}, \\'description\\': \\'This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 838, "text": " I want to create a video streaming app for plants, in which plants are categorized by species. I need to find the species of plants in an image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text-to-Text Generation\\', \\'api_name\\': \\'philschmid/bart-large-cnn-samsum\\', \\'api_call\\': \"pipeline(\\'summarization\\', model=\\'philschmid/bart-large-cnn-samsum\\')\", \\'api_arguments\\': {\\'model\\': \\'philschmid/bart-large-cnn-samsum\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'latest\\'}, \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/bart-large-cnn-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'eval_rouge1\\': 42.621, \\'eval_rouge2\\': 21.9825, \\'eval_rougeL\\': 33.034, \\'eval_rougeLsum\\': 39.6783, \\'test_rouge1\\': 41.3174, \\'test_rouge2\\': 20.8716, \\'test_rougeL\\': 32.1337, \\'test_rougeLsum\\': 38.4149}}, \\'description\\': \\'philschmid/bart-large-cnn-samsum is a BART-based model trained for text summarization on the SAMSum dataset. It can be used to generate abstractive summaries of conversations.\\'}', metadata={})]", "category": "generic"}
{"question_id": 839, "text": " Develop an image classifier for distinguishing images of cats and dogs.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Zero-Shot Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Zero-Shot Image Classification\\', \\'api_name\\': \\'laion/CLIP-ViT-B-32-laion2B-s34B-b79K\\', \\'api_call\\': \"pipeline(\\'zero-shot-classification\\', model=\\'laion/CLIP-ViT-B-32-laion2B-s34B-b79K\\')\", \\'api_arguments\\': {\\'image\\': \\'path/to/image\\', \\'class_names\\': [\\'class1\\', \\'class2\\', \\'class3\\']}, \\'python_environment_requirements\\': {\\'transformers\\': \\'>=4.0.0\\'}, \\'example_code\\': \"from transformers import pipeline; classifier = pipeline(\\'zero-shot-classification\\', model=\\'laion/CLIP-ViT-B-32-laion2B-s34B-b79K\\'); classifier(image=\\'path/to/image\\', class_names=[\\'class1\\', \\'class2\\', \\'class3\\'])\", \\'performance\\': {\\'dataset\\': \\'ImageNet-1k\\', \\'accuracy\\': 66.6}, \\'description\\': \\'A CLIP ViT-B/32 model trained with the LAION-2B English subset of LAION-5B using OpenCLIP. It enables researchers to better understand and explore zero-shot, arbitrary image classification. The model can be used for zero-shot image classification, image and text retrieval, among others.\\'}', metadata={})]", "category": "generic"}
{"question_id": 840, "text": " We are building AI glasses that should tell us about things that we are seeing with classifications. We want to use a visual transformer architecture.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 841, "text": " A marketing firm has asked us to build an application to classify social media images into various categories.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Zero-Shot Image Classification\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Zero-Shot Image Classification\\', \\'api_name\\': \\'laion/CLIP-ViT-g-14-laion2B-s34B-b88K\\', \\'api_call\\': \"pipeline(\\'zero-shot-image-classification\\', model=\\'laion/CLIP-ViT-g-14-laion2B-s34B-b88K\\')\", \\'api_arguments\\': {\\'image\\': \\'path/to/image/file\\', \\'class_names\\': \\'list_of_class_names\\'}, \\'python_environment_requirements\\': {\\'huggingface_hub\\': \\'0.0.17\\', \\'transformers\\': \\'4.11.3\\', \\'torch\\': \\'1.9.0\\', \\'torchvision\\': \\'0.10.0\\'}, \\'example_code\\': None, \\'performance\\': {\\'dataset\\': None, \\'accuracy\\': None}, \\'description\\': \\'A zero-shot image classification model based on OpenCLIP, which can classify images into various categories without requiring any training data for those categories.\\'}', metadata={})]", "category": "generic"}
{"question_id": 842, "text": " John is a car enthusiast and he wants to build an application that can recognize the car brand in a picture he takes.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \"PegasusForConditionalGeneration.from_pretrained(\\'tuner007/pegasus_summarizer\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 843, "text": " We are building an app to help Counter-Strike: Global Offensive players improve their gameplay. We need to detect players in a live game of CS:GO.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Object Detection\\', \\'api_name\\': \\'keremberke/yolov8n-csgo-player-detection\\', \\'api_call\\': \"YOLO(\\'keremberke/yolov8n-csgo-player-detection\\')\", \\'api_arguments\\': {\\'image\\': \\'URL or local path to image\\'}, \\'python_environment_requirements\\': \\'pip install ultralyticsplus==0.0.23 ultralytics==8.0.21\\', \\'example_code\\': \"from ultralyticsplus import YOLO, render_result\\\\n\\\\nmodel = YOLO(\\'keremberke/yolov8n-csgo-player-detection\\')\\\\n\\\\nmodel.overrides[\\'conf\\'] = 0.25\\\\nmodel.overrides[\\'iou\\'] = 0.45\\\\nmodel.overrides[\\'agnostic_nms\\'] = False\\\\nmodel.overrides[\\'max_det\\'] = 1000\\\\n\\\\nimage = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\\\\n\\\\nresults = model.predict(image)\\\\n\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\", \\'performance\\': {\\'dataset\\': \\'csgo-object-detection\\', \\'accuracy\\': 0.844}, \\'description\\': \"A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players with supported labels: [\\'ct\\', \\'cthead\\', \\'t\\', \\'thead\\'].\"}', metadata={})]", "category": "generic"}
{"question_id": 844, "text": " The security department needs assistance to detect suspicious objects and people using a zero-shot text-conditioned object detection system.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'zero-shot-object-detection\\', \\'api_name\\': \\'google/owlvit-base-patch32\\', \\'api_call\\': \"OwlViTForObjectDetection.from_pretrained(\\'google/owlvit-base-patch32\\')\", \\'api_arguments\\': {\\'texts\\': \\'List of text queries\\', \\'images\\': \\'Image to be processed\\'}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'import requests\\\\nfrom PIL import Image\\\\nimport torch\\\\nfrom transformers import OwlViTProcessor, OwlViTForObjectDetection\\\\nprocessor = OwlViTProcessor.from_pretrained(google/owlvit-base-patch32)\\\\nmodel = OwlViTForObjectDetection.from_pretrained(google/owlvit-base-patch32)\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ntexts = [[a photo of a cat, a photo of a dog]]\\\\ninputs = processor(text=texts, images=image, return_tensors=pt)\\\\noutputs = model(**inputs)\\\\ntarget_sizes = torch.Tensor([image.size[::-1]])\\\\nresults = processor.post_process(outputs=outputs, target_sizes=target_sizes)\\', \\'performance\\': {\\'dataset\\': \\'COCO and OpenImages\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'OWL-ViT is a zero-shot text-conditioned object detection model that uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. The model can be used to query an image with one or multiple text queries.\\'}', metadata={})]", "category": "generic"}
{"question_id": 845, "text": " As a specialist in computer vision, we need to use the OwlViT model to identify objects in an image described by specific text phrases like \\\"a photo of a cat\\\" and \\\"a photo of a dog.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Zero-Shot Image Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Zero-Shot Image Classification\\', \\'api_name\\': \\'openai/clip-vit-large-patch14\\', \\'api_call\\': \"CLIPModel.from_pretrained(\\'openai/clip-vit-large-patch14\\')\", \\'api_arguments\\': {\\'text\\': [\\'a photo of a cat\\', \\'a photo of a dog\\'], \\'images\\': \\'image\\', \\'return_tensors\\': \\'pt\\', \\'padding\\': \\'True\\'}, \\'python_environment_requirements\\': {\\'packages\\': [\\'PIL\\', \\'requests\\', \\'transformers\\']}, \\'example_code\\': \\'from PIL import Image\\\\nimport requests\\\\nfrom transformers import CLIPProcessor, CLIPModel\\\\nmodel = CLIPModel.from_pretrained(openai/clip-vit-large-patch14)\\\\nprocessor = CLIPProcessor.from_pretrained(openai/clip-vit-large-patch14)\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = processor(text=[a photo of a cat, a photo of a dog], images=image, return_tensors=pt, padding=True)\\\\noutputs = model(**inputs)\\\\nlogits_per_image = outputs.logits_per_image\\\\nprobs = logits_per_image.softmax(dim=1)\\', \\'performance\\': {\\'dataset\\': [\\'Food101\\', \\'CIFAR10\\', \\'CIFAR100\\', \\'Birdsnap\\', \\'SUN397\\', \\'Stanford Cars\\', \\'FGVC Aircraft\\', \\'VOC2007\\', \\'DTD\\', \\'Oxford-IIIT Pet dataset\\', \\'Caltech101\\', \\'Flowers102\\', \\'MNIST\\', \\'SVHN\\', \\'IIIT5K\\', \\'Hateful Memes\\', \\'SST-2\\', \\'UCF101\\', \\'Kinetics700\\', \\'Country211\\', \\'CLEVR Counting\\', \\'KITTI Distance\\', \\'STL-10\\', \\'RareAct\\', \\'Flickr30\\', \\'MSCOCO\\', \\'ImageNet\\', \\'ImageNet-A\\', \\'ImageNet-R\\', \\'ImageNet Sketch\\', \\'ObjectNet (ImageNet Overlap)\\', \\'Youtube-BB\\', \\'ImageNet-Vid\\'], \\'accuracy\\': \\'varies depending on the dataset\\'}, \\'description\\': \\'The CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner.\\'}', metadata={})]", "category": "generic"}
{"question_id": 846, "text": " A medical research team requests an automated procedure for detecting blood cells in microscopic images of blood samples. Develop a solution to address this need.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Blood Cell Detection\\', \\'api_name\\': \\'keremberke/yolov8n-blood-cell-detection\\', \\'api_call\\': \"YOLO(\\'keremberke/yolov8n-blood-cell-detection\\')\", \\'api_arguments\\': {\\'conf\\': 0.25, \\'iou\\': 0.45, \\'agnostic_nms\\': False, \\'max_det\\': 1000}, \\'python_environment_requirements\\': \\'ultralyticsplus==0.0.23 ultralytics==8.0.21\\', \\'example_code\\': \"from ultralyticsplus import YOLO, render_result\\\\nmodel = YOLO(\\'keremberke/yolov8n-blood-cell-detection\\')\\\\nmodel.overrides[\\'conf\\'] = 0.25\\\\nmodel.overrides[\\'iou\\'] = 0.45\\\\nmodel.overrides[\\'agnostic_nms\\'] = False\\\\nmodel.overrides[\\'max_det\\'] = 1000\\\\nimage = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\\\\nresults = model.predict(image)\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\", \\'performance\\': {\\'dataset\\': \\'blood-cell-object-detection\\', \\'accuracy\\': 0.893}, \\'description\\': \\'This model detects blood cells in images, specifically Platelets, RBC, and WBC. It is based on the YOLOv8 architecture and trained on the blood-cell-object-detection dataset.\\'}', metadata={})]", "category": "generic"}
{"question_id": 847, "text": " Our client has a traffic camera system and wants to detect vehicles in the images captured. Implement a solution.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Classification\\', \\'framework\\': \\'Keras\\', \\'functionality\\': \\'Binary Classification\\', \\'api_name\\': \\'TF_Decision_Trees\\', \\'api_call\\': \\'TF_Decision_Trees(input_features, target)\\', \\'api_arguments\\': [\\'input_features\\', \\'target\\'], \\'python_environment_requirements\\': [\\'tensorflow >= 7.0\\'], \\'example_code\\': \\'https://github.com/tdubon/TF-GB-Forest/blob/c0cf4c7e3e29d819b996cfe4eecc1f2728115e52/TFDecisionTrees_Final.ipynb\\', \\'performance\\': {\\'dataset\\': \\'Census-Income Data Set\\', \\'accuracy\\': 96.57}, \\'description\\': \"Use TensorFlow\\'s Gradient Boosted Trees model in binary classification of structured data. Build a decision forests model by specifying the input feature usage. Implement a custom Binary Target encoder as a Keras Preprocessing layer to encode the categorical features with respect to their target value co-occurrences, and then use the encoded features to build a decision forests model. The model is trained on the US Census Income Dataset containing approximately 300k instances with 41 numerical and categorical variables. The task is to determine whether a person makes over 50k a year.\"}', metadata={})]", "category": "generic"}
{"question_id": 848, "text": " Detect the location of players in an image from a Counter-Strike: Global Offensive (CS:GO) game.\\n###Input: image_path = \\\"path/to/your/csgo_image.jpg\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Object Detection\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Object Detection\\', \\'api_name\\': \\'keremberke/yolov8n-csgo-player-detection\\', \\'api_call\\': \"YOLO(\\'keremberke/yolov8n-csgo-player-detection\\')\", \\'api_arguments\\': {\\'image\\': \\'URL or local path to image\\'}, \\'python_environment_requirements\\': \\'pip install ultralyticsplus==0.0.23 ultralytics==8.0.21\\', \\'example_code\\': \"from ultralyticsplus import YOLO, render_result\\\\n\\\\nmodel = YOLO(\\'keremberke/yolov8n-csgo-player-detection\\')\\\\n\\\\nmodel.overrides[\\'conf\\'] = 0.25\\\\nmodel.overrides[\\'iou\\'] = 0.45\\\\nmodel.overrides[\\'agnostic_nms\\'] = False\\\\nmodel.overrides[\\'max_det\\'] = 1000\\\\n\\\\nimage = \\'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg\\'\\\\n\\\\nresults = model.predict(image)\\\\n\\\\nprint(results[0].boxes)\\\\nrender = render_result(model=model, image=image, result=results[0])\\\\nrender.show()\", \\'performance\\': {\\'dataset\\': \\'csgo-object-detection\\', \\'accuracy\\': 0.844}, \\'description\\': \"A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players with supported labels: [\\'ct\\', \\'cthead\\', \\'t\\', \\'thead\\'].\"}', metadata={})]", "category": "generic"}
{"question_id": 849, "text": " Develop a fashion app that segments and identifies clothing items in an uploaded image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Image-to-Text\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'promptcap-coco-vqa\\', \\'api_call\\': \"PromptCap(\\'vqascore/promptcap-coco-vqa\\')\", \\'api_arguments\\': {\\'prompt\\': \\'string\\', \\'image\\': \\'string\\'}, \\'python_environment_requirements\\': \\'pip install promptcap\\', \\'example_code\\': [\\'import torch\\', \\'from promptcap import PromptCap\\', \\'model = PromptCap(vqascore/promptcap-coco-vqa)\\', \\'if torch.cuda.is_available():\\', \\'  model.cuda()\\', \\'prompt = please describe this image according to the given question: what piece of clothing is this boy putting on?\\', \\'image = glove_boy.jpeg\\', \\'print(model.caption(prompt, image))\\'], \\'performance\\': {\\'dataset\\': {\\'coco\\': {\\'accuracy\\': \\'150 CIDEr\\'}, \\'OK-VQA\\': {\\'accuracy\\': \\'60.4%\\'}, \\'A-OKVQA\\': {\\'accuracy\\': \\'59.6%\\'}}}, \\'description\\': \\'PromptCap is a captioning model that can be controlled by natural language instruction. The instruction may contain a question that the user is interested in. It achieves SOTA performance on COCO captioning (150 CIDEr) and knowledge-based VQA tasks when paired with GPT-3 (60.4% on OK-VQA and 59.6% on A-OKVQA).\\'}', metadata={})]", "category": "generic"}
{"question_id": 850, "text": " Our team is working on a project to develop autonomous vehicles. We need a model to identify different segments of an image captured from the vehicle's camera.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Multilingual Question Answering\\', \\'api_name\\': \\'mrm8488/bert-multi-cased-finetuned-xquadv1\\', \\'api_call\\': \"pipeline(\\'question-answering\\', model=\\'mrm8488/bert-multi-cased-finetuned-xquadv1\\', tokenizer=\\'mrm8488/bert-multi-cased-finetuned-xquadv1\\')\", \\'api_arguments\\': {\\'context\\': \\'string\\', \\'question\\': \\'string\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"qa_pipeline({\\\\n \\'context\\': Manuel Romero has been working hardly in the repository hugginface/transformers lately,\\\\n \\'question\\': Who has been working hard for hugginface/transformers lately?\\\\n})\", \\'performance\\': {\\'dataset\\': \\'XQuAD\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'This model is a BERT (base-multilingual-cased) fine-tuned for multilingual Question Answering on 11 different languages using the XQuAD dataset and additional data augmentation techniques.\\'}', metadata={})]", "category": "generic"}
{"question_id": 851, "text": " A real estate agency needs an application that can transform the floor plan images into simple straight line drawings, simplifying the visualization of the properties.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Classification\\', \\'framework\\': \\'Keras\\', \\'functionality\\': \\'Structured data learning with TabTransformer\\', \\'api_name\\': \\'keras-io/tab_transformer\\', \\'api_call\\': \\'TabTransformer.from_config()\\', \\'api_arguments\\': \\'N/A\\', \\'python_environment_requirements\\': \\'Hugging Face\\', \\'example_code\\': \\'N/A\\', \\'performance\\': {\\'dataset\\': \\'United States Census Income Dataset\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \"This model uses self-attention based Transformers structure followed by multiple feed forward layers to serve supervised and semi-supervised learning tasks on tabular data. The model\\'s inputs can contain both numerical and categorical features. Categorical features are encoded into embedding vectors before being fed into a stack of Transformer blocks. The contextual embeddings of the categorical features after the final Transformer layer are concatenated with the input numerical features and fed into a final MLP block. A SoftMax function is applied at the end of the model.\"}', metadata={})]", "category": "generic"}
{"question_id": 852, "text": " I'm working on creating images of various scenes based on their textual descriptions. The models should also consider the actual positions and poses of the objects in the scenes.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Automatic Speech Recognition\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'microsoft/wavlm-large\\', \\'api_call\\': \"Wav2Vec2Model.from_pretrained(\\'microsoft/wavlm-large\\')\", \\'api_arguments\\': \\'speech input\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'To fine-tune the model for speech recognition, see the official speech recognition example. To fine-tune the model for speech classification, see the official audio classification example.\\', \\'performance\\': {\\'dataset\\': \\'SUPERB benchmark\\', \\'accuracy\\': \\'state-of-the-art performance\\'}, \\'description\\': \\'WavLM-Large is a large model pretrained on 16kHz sampled speech audio. It is built based on the HuBERT framework, with an emphasis on both spoken content modeling and speaker identity preservation. WavLM is pretrained on 60,000 hours of Libri-Light, 10,000 hours of GigaSpeech, and 24,000 hours of VoxPopuli. It achieves state-of-the-art performance on the SUPERB benchmark and brings significant improvements for various speech processing tasks on their representative benchmarks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 853, "text": " Develop a new product that uses machine learning to enhance the quality of low-resolution images by upscaling them to twice their size.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 854, "text": " An animal shelter needs an original cat image for a fundraising event poster. Generate the image.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Information Retrieval\\', \\'api_name\\': \\'cross-encoder/ms-marco-TinyBERT-L-2-v2\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'cross-encoder/ms-marco-TinyBERT-L-2-v2\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\"}, \\'python_environment_requirements\\': [\\'transformers\\', \\'torch\\'], \\'example_code\\': {\\'import\\': \\'from transformers import AutoTokenizer, AutoModelForSequenceClassification\\', \\'model\\': \"model = AutoModelForSequenceClassification.from_pretrained(\\'model_name\\')\", \\'tokenizer\\': \"tokenizer = AutoTokenizer.from_pretrained(\\'model_name\\')\", \\'features\\': \"features = tokenizer([\\'How many people live in Berlin?\\', \\'How many people live in Berlin?\\'], [\\'Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\\', \\'New York City is famous for the Metropolitan Museum of Art.\\'], padding=True, truncation=True, return_tensors=\\'pt\\')\", \\'scores\\': \\'with torch.no_grad():\\\\n    scores = model(**features).logits\\\\n    print(scores)\\'}, \\'performance\\': {\\'dataset\\': \\'TREC Deep Learning 2019\\', \\'accuracy\\': \\'69.84 (NDCG@10)\\'}, \\'description\\': \\'This model was trained on the MS Marco Passage Ranking task. It can be used for Information Retrieval: Given a query, encode the query with all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. The training code is available here: SBERT.net Training MS Marco.\\'}', metadata={})]", "category": "generic"}
{"question_id": 855, "text": " Develop a video content recommendation engine that can understand and generate multiple categories, such as sports, comedy, and news, based on the videos.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Video\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Video\\', \\'api_name\\': \\'camenduru/text2-video-zero\\', \\'api_call\\': \"pipeline(\\'text-to-video\\', model=\\'camenduru/text2-video-zero\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'This model is used for generating videos from text inputs. It is based on the Hugging Face framework and can be used with the transformers library. The model is trained on a variety of text and video datasets, and can be used for tasks such as video summarization, video generation from text prompts, and more.\\'}', metadata={})]", "category": "generic"}
{"question_id": 856, "text": " Our customer is a fitness platform. We need to analyze workout videos for offering customized workout plans.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 857, "text": " We are a security company and we need a video classification model to analyze CCTV footage for suspicious activities.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 858, "text": " Design a model to classify the following image: a city park with a playground and a lake, surrounded by trees and skyscrapers.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'microsoft/tapex-large-sql-execution\\', \\'api_call\\': \"BartForConditionalGeneration.from_pretrained(\\'microsoft/tapex-large-sql-execution\\')\", \\'api_arguments\\': {\\'table\\': \\'pd.DataFrame\\', \\'query\\': \\'str\\'}, \\'python_environment_requirements\\': [\\'transformers\\', \\'pandas\\'], \\'example_code\\': \\'from transformers import TapexTokenizer, BartForConditionalGeneration\\\\nimport pandas as pd\\\\ntokenizer = TapexTokenizer.from_pretrained(microsoft/tapex-large-sql-execution)\\\\nmodel = BartForConditionalGeneration.from_pretrained(microsoft/tapex-large-sql-execution)\\\\ndata = {\\\\n year: [1896, 1900, 1904, 2004, 2008, 2012],\\\\n city: [athens, paris, st. louis, athens, beijing, london]\\\\n}\\\\ntable = pd.DataFrame.from_dict(data)\\\\nquery = select year where city = beijing\\\\nencoding = tokenizer(table=table, query=query, return_tensors=pt)\\\\noutputs = model.generate(**encoding)\\\\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\\', \\'performance\\': {\\'dataset\\': \\'synthetic corpus\\', \\'accuracy\\': \\'not specified\\'}, \\'description\\': \\'TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries. TAPEX is based on the BART architecture, the transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder.\\'}', metadata={})]", "category": "generic"}
{"question_id": 859, "text": " We are integrating a chatbot into our system. We want the chatbot to first detect the language of user input before providing a response.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 860, "text": " We have a dataset with customer reviews of our financial service app, and we'd like to analyze their sentiment.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 861, "text": " Our organization sells movies. We need to collect reviews from various platforms to understand the popularity of a movie.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'results-yelp\\', \\'api_call\\': \"AutoTokenizer.from_pretrained(\\'bert-base-uncased\\')\", \\'api_arguments\\': {\\'tokenizer\\': \"AutoTokenizer.from_pretrained(\\'bert-base-uncased\\')\", \\'config\\': \"AutoConfig.from_pretrained(\\'potatobunny/results-yelp\\')\"}, \\'python_environment_requirements\\': {\\'Transformers\\': \\'4.18.0\\', \\'Pytorch\\': \\'1.10.0+cu111\\', \\'Datasets\\': \\'2.0.0\\', \\'Tokenizers\\': \\'0.12.1\\'}, \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'Yelp\\', \\'accuracy\\': 0.9302}, \\'description\\': \\'This model is a fine-tuned version of textattack/bert-base-uncased-yelp-polarity on a filtered and manually reviewed Yelp dataset containing restaurant reviews only. It is intended to perform text classification, specifically sentiment analysis, on text data obtained from restaurant reviews to determine if the particular review is positive or negative.\\'}', metadata={})]", "category": "generic"}
{"question_id": 862, "text": " We want to enhance our search function by improving the ranking of search results.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 863, "text": " We are developing an AI chatbot to interact with users. We need the bot to recognize the user's emotions based on their text input.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 864, "text": " Please help me extract the names of people, organizations, and locations mentioned in the given text.\\n###Input: Hello, my name is John Doe, and I work at Microsoft. Tomorrow, I'll be going to a conference in San Francisco.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Token Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Named Entity Recognition\\', \\'api_name\\': \\'dslim/bert-base-NER-uncased\\', \\'api_call\\': \"pipeline(\\'ner\\', model=\\'dslim/bert-base-NER-uncased\\')\", \\'api_arguments\\': {}, \\'python_environment_requirements\\': {\\'transformers\\': \\'>=4.0.0\\'}, \\'example_code\\': \"nlp(\\'My name is John and I live in New York.\\')\", \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'A pretrained BERT model for Named Entity Recognition (NER) on uncased text. It can be used to extract entities such as person names, locations, and organizations from text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 865, "text": " The school wants a tool to teach foreign students Chinese grammar. They want you to develop a part-of-speech tagging system to detect the words' grammatical roles.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Token Classification\\', \\'framework\\': \\'Flair\\', \\'functionality\\': \\'Part-of-Speech Tagging\\', \\'api_name\\': \\'flair/upos-english\\', \\'api_call\\': \"SequenceTagger.load(\\'flair/upos-english\\')\", \\'api_arguments\\': [\\'sentence\\'], \\'python_environment_requirements\\': \\'pip install flair\\', \\'example_code\\': \"from flair.data import Sentence\\\\nfrom flair.models import SequenceTagger\\\\ntagger = SequenceTagger.load(\\'flair/upos-english\\')\\\\nsentence = Sentence(\\'I love Berlin.\\')\\\\ntagger.predict(sentence)\\\\nprint(sentence)\\\\nfor entity in sentence.get_spans(\\'pos\\'):\\\\n    print(entity)\", \\'performance\\': {\\'dataset\\': \\'ontonotes\\', \\'accuracy\\': 98.6}, \\'description\\': \\'This is the standard universal part-of-speech tagging model for English that ships with Flair. It predicts universal POS tags such as ADJ, ADP, ADV, AUX, CCONJ, DET, INTJ, NOUN, NUM, PART, PRON, PROPN, PUNCT, SCONJ, SYM, VERB, and X. The model is based on Flair embeddings and LSTM-CRF.\\'}', metadata={})]", "category": "generic"}
{"question_id": 866, "text": " Our company collects data on the salesperson performance in different regions for each month. We want to use the most suitable API from our subscribed_huggingface.co to analyze that in specific table format and then based on provided question, answer accordingly.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text-generation\\', \\'api_name\\': \\'pygmalion-2.7b\\', \\'api_call\\': \"pipeline(\\'text-generation\\', model=\\'PygmalionAI/pygmalion-2.7b\\')\", \\'api_arguments\\': {\\'input_prompt\\': \\'string\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'latest\\'}, \\'example_code\\': \"model([CHARACTER]\\'s Persona: [A few sentences about the character you want the model to play]<START>[DIALOGUE HISTORY]You: [Your input message here][CHARACTER]:)\", \\'performance\\': {\\'dataset\\': \\'56MB of dialogue data\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \"Pygmalion 2.7B is a proof-of-concept dialogue model based on EleutherAI\\'s gpt-neo-2.7B. It is fine-tuned on 56MB of dialogue data gathered from multiple sources, including real and partially machine-generated conversations. The model is intended for use in generating conversational responses and can be used with a specific input format that includes character persona, dialogue history, and user input message.\"}', metadata={})]", "category": "generic"}
{"question_id": 867, "text": " Assist me in finding the accurate information in a table related to the Korean stock market.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Table Question Answering\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Table Question Answering\\', \\'api_name\\': \\'google/tapas-medium-finetuned-sqa\\', \\'api_call\\': \"pipeline(\\'table-question-answering\\', model=\\'google/tapas-medium-finetuned-sqa\\')\", \\'api_arguments\\': \\'table, query\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline\\\\n\\\\n# Initialize the pipeline\\\\ntable_qa_pipeline = pipeline(\\'table-question-answering\\', model=\\'google/tapas-medium-finetuned-sqa\\')\\\\n\\\\n# Provide the table and query\\\\nresult = table_qa_pipeline(table=table, query=\\'What is the total revenue?\\')\", \\'performance\\': {\\'dataset\\': \\'msr_sqa\\', \\'accuracy\\': 0.6561}, \\'description\\': \\'TAPAS medium model fine-tuned on Sequential Question Answering (SQA). This model is pretrained on a large corpus of English data from Wikipedia and uses relative position embeddings. It can be used for answering questions related to a table in a conversational set-up.\\'}', metadata={})]", "category": "generic"}
{"question_id": 868, "text": " My company has a large data table of employees, containing their names, titles, departments, and hire dates. We need a tool that can find all employees with the title of \\\"Software Engineer\\\" hired in 2020.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 869, "text": " We received a business document in French. We need to extract some specific information from it. \\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 870, "text": " We have a coffee shop with different types of coffee on the menu. Determine the caffeine levels in each cup of coffee.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'Kirili4ik/mbart_ruDialogSum\\', \\'api_call\\': \"MBartForConditionalGeneration.from_pretrained(\\'Kirili4ik/mbart_ruDialogSum\\')\", \\'api_arguments\\': {\\'model_name\\': \\'Kirili4ik/mbart_ruDialogSum\\'}, \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import MBartTokenizer, MBartForConditionalGeneration\\\\nmodel_name = Kirili4ik/mbart_ruDialogSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = MBartForConditionalGeneration.from_pretrained(model_name)\\\\nmodel.eval()\\\\narticle_text = ...\\\\ninput_ids = tokenizer(\\\\n [article_text],\\\\n max_length=600,\\\\n padding=max_length,\\\\n truncation=True,\\\\n return_tensors=pt,\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n top_k=0,\\\\n num_beams=3,\\\\n no_repeat_ngram_size=3\\\\n)[0]\\\\nsummary = tokenizer.decode(output_ids, skip_special_tokens=True)\\\\nprint(summary)\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'SAMSum Corpus (translated to Russian)\\', \\'accuracy\\': {\\'Validation ROGUE-1\\': 34.5, \\'Validation ROGUE-L\\': 33, \\'Test ROGUE-1\\': 31, \\'Test ROGUE-L\\': 28}}]}, \\'description\\': \\'MBart for Russian summarization fine-tuned for dialogues summarization. This model was firstly fine-tuned by Ilya Gusev on Gazeta dataset. We have fine tuned that model on SamSum dataset translated to Russian using GoogleTranslateAPI. Moreover! We have implemented a ! telegram bot @summarization_bot ! with the inference of this model. Add it to the chat and get summaries instead of dozens spam messages!\\'}', metadata={})]", "category": "generic"}
{"question_id": 871, "text": " You are the head of the QA department and you want to create an application that extracts answers from large product manuals. Explain how you can use this API for the project.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 872, "text": " I want to have a personal assistant app that can answer questions from a given text.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Question Answering\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Question Answering\\', \\'api_name\\': \\'deepset/roberta-large-squad2\\', \\'api_call\\': \"pipeline(\\'question-answering\\', model=\\'deepset/roberta-large-squad2\\')\", \\'api_arguments\\': [\\'question\\', \\'context\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline; nlp = pipeline(\\'question-answering\\', model=\\'deepset/roberta-large-squad2\\'); nlp({\\'question\\': \\'What is the capital of Germany?\\', \\'context\\': \\'Berlin is the capital of Germany.\\'})\", \\'performance\\': {\\'dataset\\': \\'squad_v2\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'A pre-trained RoBERTa model for question answering tasks, specifically trained on the SQuAD v2 dataset. It can be used to answer questions based on a given context.\\'}', metadata={})]", "category": "generic"}
{"question_id": 873, "text": " I want to build a tool that helps me answer questions about specific information in a Korean newspaper article.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text-generation\\', \\'api_name\\': \\'pygmalion-1.3b\\', \\'api_call\\': \"pipeline(\\'text-generation\\', \\'PygmalionAI/pygmalion-1.3b\\')\", \\'api_arguments\\': \\'input_prompt\\', \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"[CHARACTER]\\'s Persona: [A few sentences about the character you want the model to play]\\\\n[DIALOGUE HISTORY]\\\\nYou: [Your input message here]\\\\n[CHARACTER]:\", \\'performance\\': {\\'dataset\\': \\'56MB of dialogue data\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \"Pygmalion 1.3B is a proof-of-concept dialogue model based on EleutherAI\\'s pythia-1.3b-deduped. It is designed for generating conversational responses and can be used with a specific input format that includes character persona, dialogue history, and user input message.\"}', metadata={})]", "category": "generic"}
{"question_id": 874, "text": " Develop a tool to help our team members find answers to essential questions from a long document.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'google/bigbird-pegasus-large-bigpatent\\', \\'api_call\\': \"BigBirdPegasusForConditionalGeneration.from_pretrained(\\'google/bigbird-pegasus-large-bigpatent\\')\", \\'api_arguments\\': {\\'attention_type\\': \\'original_full\\', \\'block_size\\': 16, \\'num_random_blocks\\': 2}, \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import BigBirdPegasusForConditionalGeneration, AutoTokenizer\\\\ntokenizer = AutoTokenizer.from_pretrained(google/bigbird-pegasus-large-bigpatent)\\\\nmodel = BigBirdPegasusForConditionalGeneration.from_pretrained(google/bigbird-pegasus-large-bigpatent)\\\\ntext = Replace me by any text you\\'d like.\\\\ninputs = tokenizer(text, return_tensors=\\'pt\\')\\\\nprediction = model.generate(**inputs)\\\\nprediction = tokenizer.batch_decode(prediction)\", \\'performance\\': {\\'dataset\\': \\'big_patent\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'BigBird, a sparse-attention based transformer, extends Transformer-based models like BERT to much longer sequences. It can handle sequences up to a length of 4096 at a much lower compute cost compared to BERT. BigBird has achieved state-of-the-art results on various tasks involving very long sequences such as long documents summarization and question-answering with long contexts.\\'}', metadata={})]", "category": "generic"}
{"question_id": 875, "text": " There is a news article stating, \\\"Angela Merkel ist eine Politikerin in Deutschland und Vorsitzende der CDU.\\\" We need to determine which category this article should fall under.\\n###Input: Angela Merkel ist eine Politikerin in Deutschland und Vorsitzende der CDU.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Zero-Shot Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Zero-Shot Classification\\', \\'api_name\\': \\'MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'MoritzLaurer/DeBERTa-v3-xsmall-mnli-fever-anli-ling-binary\\')\", \\'api_arguments\\': {\\'sequence_to_classify\\': \\'Angela Merkel ist eine Politikerin in Deutschland und Vorsitzende der CDU\\', \\'candidate_labels\\': [\\'politics\\', \\'economy\\', \\'entertainment\\', \\'environment\\'], \\'multi_label\\': False}, \\'python_environment_requirements\\': [\\'transformers==4.13\\'], \\'example_code\\': \\'from transformers import pipeline\\\\nclassifier = pipeline(zero-shot-classification, model=MoritzLaurer/mDeBERTa-v3-base-mnli-xnli)\\\\nsequence_to_classify = Angela Merkel ist eine Politikerin in Deutschland und Vorsitzende der CDU\\\\ncandidate_labels = [politics, economy, entertainment, environment]\\\\noutput = classifier(sequence_to_classify, candidate_labels, multi_label=False)\\\\nprint(output)\\', \\'performance\\': {\\'dataset\\': [{\\'name\\': \\'MultiNLI-matched\\', \\'accuracy\\': 0.857}, {\\'name\\': \\'MultiNLI-mismatched\\', \\'accuracy\\': 0.856}, {\\'name\\': \\'ANLI-all\\', \\'accuracy\\': 0.537}, {\\'name\\': \\'ANLI-r3\\', \\'accuracy\\': 0.497}, {\\'name\\': \\'WANLI\\', \\'accuracy\\': 0.732}, {\\'name\\': \\'LingNLI\\', \\'accuracy\\': 0.788}, {\\'name\\': \\'fever-nli\\', \\'accuracy\\': 0.761}]}, \\'description\\': \\'This multilingual model can perform natural language inference (NLI) on 100 languages and is therefore also suitable for multilingual zero-shot classification. The underlying mDeBERTa-v3-base model was pre-trained by Microsoft on the CC100 multilingual dataset with 100 languages. The model was then fine-tuned on the XNLI dataset and on the multilingual-NLI-26lang-2mil7 dataset. Both datasets contain more than 2.7 million hypothesis-premise pairs in 27 languages spoken by more than 4 billion people.\\'}', metadata={})]", "category": "generic"}
{"question_id": 876, "text": " We have generated user reviews for movies. We need to check user opinions about the movie 'Inception' based on their reviews.\\n###Input: The movie 'Inception' is an exceptional piece of cinematic art. The storyline is thought-provoking and keeps the audience engaged till the end. The special effects are breathtaking and complement the plot perfectly.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Classification\\', \\'api_name\\': \\'lvwerra/distilbert-imdb\\', \\'api_call\\': \"pipeline(\\'sentiment-analysis\\', model=\\'lvwerra/distilbert-imdb\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\', \\'pytorch\\'], \\'example_code\\': \"classifier(\\'I love this movie!\\')\", \\'performance\\': {\\'dataset\\': \\'imdb\\', \\'accuracy\\': 0.928}, \\'description\\': \\'This model is a fine-tuned version of distilbert-base-uncased on the imdb dataset. It is used for sentiment analysis on movie reviews and achieves an accuracy of 0.928 on the evaluation set.\\'}', metadata={})]", "category": "generic"}
{"question_id": 877, "text": " In our new app, we are building a feature that recommends books in different languages. To do this, first, we need to translate the book title and details from English to French. Help us to decide the best translation model to use here.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'financial-sentiment-analysis\\', \\'api_name\\': \\'yiyanghkust/finbert-tone\\', \\'api_call\\': \"BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\", \\'api_arguments\\': [\\'sentences\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import BertTokenizer, BertForSequenceClassification\\\\nfrom transformers import pipeline\\\\nfinbert = BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\\\\ntokenizer = BertTokenizer.from_pretrained(\\'yiyanghkust/finbert-tone\\')\\\\nnlp = pipeline(sentiment-analysis, model=finbert, tokenizer=tokenizer)\\\\nsentences = [there is a shortage of capital, and we need extra financing,\\\\n growth is strong and we have plenty of liquidity,\\\\n there are doubts about our finances,\\\\n profits are flat]\\\\nresults = nlp(sentences)\\\\nprint(results)\", \\'performance\\': {\\'dataset\\': \\'10,000 manually annotated sentences from analyst reports\\', \\'accuracy\\': \\'superior performance on financial tone analysis task\\'}, \\'description\\': \\'FinBERT is a BERT model pre-trained on financial communication text. It is trained on the following three financial communication corpus: Corporate Reports 10-K & 10-Q, Earnings Call Transcripts, and Analyst Reports. This released finbert-tone model is the FinBERT model fine-tuned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports. This model achieves superior performance on financial tone analysis task.\\'}', metadata={})]", "category": "generic"}
{"question_id": 878, "text": " Our multinational company is dealing with a French client. Please help us communicate in French by translating an English sentence into French.\\n###Input: \\\"Hello, how are you?\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Audio-to-Audio\\', \\'framework\\': \\'Fairseq\\', \\'functionality\\': \\'speech-to-speech-translation\\', \\'api_name\\': \\'facebook/textless_sm_en_fr\\', \\'api_call\\': \"load_model_ensemble_and_task_from_hf_hub(\\'facebook/textless_sm_en_fr\\')\", \\'api_arguments\\': [\\'input_file\\'], \\'python_environment_requirements\\': [\\'huggingface_hub\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'This model is a speech-to-speech translation model trained by Facebook. It is designed for translating English speech to French speech.\\'}', metadata={})]", "category": "generic"}
{"question_id": 879, "text": " We want to communicate product information to online customers. Translate the information from English to French.\\n###Input: \\\"Introducing the new eco-friendly water bottle made of high-quality stainless steel with double-wall insulation to keep your drinks cool for 24 hours or hot for 12 hours.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Audio Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'wav2vec2-base-superb-sv\\', \\'api_call\\': \"AutoModelForAudioXVector.from_pretrained(\\'anton-l/wav2vec2-base-superb-sv\\')\", \\'api_arguments\\': \\'anton-l/wav2vec2-base-superb-sv\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'from transformers import AutoProcessor, AutoModelForAudioXVector\\\\nprocessor = AutoProcessor.from_pretrained(anton-l/wav2vec2-base-superb-sv)\\\\nmodel = AutoModelForAudioXVector.from_pretrained(anton-l/wav2vec2-base-superb-sv)\\', \\'performance\\': {\\'dataset\\': \\'superb\\', \\'accuracy\\': \\'More information needed\\'}, \\'description\\': \"This is a ported version of S3PRL\\'s Wav2Vec2 for the SUPERB Speaker Verification task. The base model is wav2vec2-large-lv60, which is pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz. For more information refer to SUPERB: Speech processing Universal PERformance Benchmark.\"}', metadata={})]", "category": "generic"}
{"question_id": 880, "text": " Our Customer is a Swedish travel magazine with an English version. Translate this Swedish text to English for them: \\\"Stockholm \\u00e4r Sveriges huvudstad och st\\u00f6rsta stad. Den har en rik historia och erbjuder m\\u00e5nga kulturella och historiska sev\\u00e4rdheter.\\\"\\n###Input: \\\"Stockholm \\u00e4r Sveriges huvudstad och st\\u00f6rsta stad. Den har en rik historia och erbjuder m\\u00e5nga kulturella och historiska sev\\u00e4rdheter.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Translation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Translation\\', \\'api_name\\': \\'opus-mt-sv-en\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'Helsinki-NLP/opus-mt-sv-en\\').\", \\'api_arguments\\': [\\'inputs\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'Tatoeba.sv.en\\', \\'accuracy\\': \\'BLEU: 64.5, chr-F: 0.763\\'}, \\'description\\': \\'A Swedish to English translation model trained on the OPUS dataset using the transformer-align architecture. The model is pre-processed with normalization and SentencePiece.\\'}', metadata={})]", "category": "generic"}
{"question_id": 881, "text": " I have written a small story in English about the adventures of a superhero who saves the day from evil villains. I would like to translate it into French so that my friends in France can read it too.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'tuner007/pegasus_summarizer\\', \\'api_call\\': \"PegasusForConditionalGeneration.from_pretrained(\\'tuner007/pegasus_summarizer\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'pip install sentencepiece\\'], \\'example_code\\': \"context = \\\\nIndia wicket-keeper batsman Rishabh Pant has said someone from the crowd threw a ball on pacer Mohammed Siraj while he was fielding in the ongoing third Test against England on Wednesday. Pant revealed the incident made India skipper Virat Kohli upset. I think, somebody threw a ball inside, at Siraj, so he [Kohli] was upset, said Pant in a virtual press conference after the close of the first day\\'s play.You can say whatever you want to chant, but don\\'t throw things at the fielders and all those things. It is not good for cricket, I guess, he added.In the third session of the opening day of the third Test, a section of spectators seemed to have asked Siraj the score of the match to tease the pacer. The India pacer however came with a brilliant reply as he gestured 1-0 (India leading the Test series) towards the crowd.Earlier this month, during the second Test match, there was some bad crowd behaviour on a show as some unruly fans threw champagne corks at India batsman KL Rahul.Kohli also intervened and he was seen gesturing towards the opening batsman to know more about the incident. An over later, the TV visuals showed that many champagne corks were thrown inside the playing field, and the Indian players were visibly left frustrated.Coming back to the game, after bundling out India for 78, openers Rory Burns and Haseeb Hameed ensured that England took the honours on the opening day of the ongoing third Test.At stumps, England\\'s score reads 120/0 and the hosts have extended their lead to 42 runs. For the Three Lions, Burns (52) and Hameed (60) are currently unbeaten at the crease.Talking about the pitch on opening day, Pant said, They took the heavy roller, the wicket was much more settled down, and they batted nicely also, he said. But when we batted, the wicket was slightly soft, and they bowled in good areas, but we could have applied [ourselves] much better.Both England batsmen managed to see off the final session and the hosts concluded the opening day with all ten wickets intact, extending the lead to 42.(ANI)\\\\n\\\\nget_response(context)\", \\'performance\\': {\\'dataset\\': \\'cnn_dailymail\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.604, \\'ROUGE-2\\': 14.64, \\'ROUGE-L\\': 23.884, \\'ROUGE-LSUM\\': 32.902, \\'loss\\': 2.576, \\'gen_len\\': 76.398}}, \\'description\\': \\'PEGASUS fine-tuned for summarization\\'}', metadata={})]", "category": "generic"}
{"question_id": 882, "text": " Create an application that reads news articles and provides a brief summary of the article contents.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Feature Extraction\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Feature Extraction\\', \\'api_name\\': \\'DeepPavlov/rubert-base-cased\\', \\'api_call\\': \"AutoModel.from_pretrained(\\'DeepPavlov/rubert-base-cased\\')\", \\'api_arguments\\': [], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'\\', \\'performance\\': {\\'dataset\\': \\'Russian part of Wikipedia and news data\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'RuBERT (Russian, cased, 12\u2011layer, 768\u2011hidden, 12\u2011heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERT\u2011base as an initialization for RuBERT[1].\\'}', metadata={})]", "category": "generic"}
{"question_id": 883, "text": " We are building a platform for developers and want to provide automatic code documentation generation for Python functions.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 884, "text": " In order to engage our users and keep them interested in our platform, we require a conversational chatbot that discusses a wide range of topics.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 885, "text": " Use this API to get a suggestion on how to respond to a customer's complaint about the late delivery of their package.\\n###Input: {\\\"instruction\\\": \\\"How can I respond to a customer complaint about late delivery?\\\", \\\"knowledge\\\": \\\"The courier had external delays due to bad winter weather.\\\", \\\"dialog\\\": [\\\"Customer: My package is late. What's going on?\\\", \\\"Support: I apologize for the inconvenience. I'll check what's happening with the package and get back to you.\\\"]}\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Multimodal Text-to-Image\\', \\'framework\\': \\'Hugging Face\\', \\'functionality\\': \\'Text-to-Image\\', \\'api_name\\': \\'wavymulder/Analog-Diffusion\\', \\'api_call\\': \"pipeline(\\'text-to-image\\', model=\\'wavymulder/Analog-Diffusion\\')\", \\'api_arguments\\': [\\'prompt\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"text_to_image(\\'analog style landscape\\')\", \\'performance\\': {\\'dataset\\': \\'analog photographs\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \"Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token \\'analog style\\' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library.\"}', metadata={})]", "category": "generic"}
{"question_id": 886, "text": " A game studio is now creating a story for their new action game, they need a hint for creating the setting of the game.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'financial-summarization-pegasus\\', \\'api_call\\': \"PegasusForConditionalGeneration.from_pretrained(\\'human-centered-summarization/financial-summarization-pegasus\\')\", \\'api_arguments\\': [\\'model_name\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import PegasusTokenizer, PegasusForConditionalGeneration, TFPegasusForConditionalGeneration\\\\nmodel_name = human-centered-summarization/financial-summarization-pegasus\\\\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\\\\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name)\\\\ntext_to_summarize = National Commercial Bank (NCB), Saudi Arabia\u2019s largest lender by assets, agreed to buy rival Samba Financial Group for $15 billion in the biggest banking takeover this year.NCB will pay 28.45 riyals ($7.58) for each Samba share, according to a statement on Sunday, valuing it at about 55.7 billion riyals. NCB will offer 0.739 new shares for each Samba share, at the lower end of the 0.736-0.787 ratio the banks set when they signed an initial framework agreement in June.The offer is a 3.5% premium to Samba\u2019s Oct. 8 closing price of 27.50 riyals and about 24% higher than the level the shares traded at before the talks were made public. Bloomberg News first reported the merger discussions.The new bank will have total assets of more than $220 billion, creating the Gulf region\u2019s third-largest lender. The entity\u2019s $46 billion market capitalization nearly matches that of Qatar National Bank QPSC, which is still the Middle East\u2019s biggest lender with about $268 billion of assets.\\\\ninput_ids = tokenizer(text_to_summarize, return_tensors=pt).input_ids\\\\noutput = model.generate(input_ids, max_length=32, num_beams=5, early_stopping=True)\\\\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\\', \\'performance\\': {\\'dataset\\': \\'xsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 35.206, \\'ROUGE-2\\': 16.569, \\'ROUGE-L\\': 30.128, \\'ROUGE-LSUM\\': 30.171}}, \\'description\\': \\'This model was fine-tuned on a novel financial news dataset, which consists of 2K articles from Bloomberg, on topics such as stock, markets, currencies, rate and cryptocurrencies. It is based on the PEGASUS model and in particular PEGASUS fine-tuned on the Extreme Summarization (XSum) dataset: google/pegasus-xsum model. PEGASUS was originally proposed by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu in PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization.\\'}', metadata={})]", "category": "generic"}
{"question_id": 887, "text": " We are working on an AI chatbot for customer support and we need our chatbot to generate human-like responses to customers' questions.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text Generation\\', \\'api_name\\': \\'mywateriswet/ShuanBot\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'mywateriswet/ShuanBot\\')\", \\'api_arguments\\': \\'message\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"response = chatbot(\\'What is your name?\\')\", \\'performance\\': {\\'dataset\\': \\'N/A\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \\'ShuanBot is a conversational chatbot model based on the GPT-2 architecture. It can be used for generating human-like responses in a chat context.\\'}', metadata={})]", "category": "generic"}
{"question_id": 888, "text": " Translate the following English sentence to German: \\\"I have a doctor's appointment tomorrow morning.\\\"\\n###Input: \\\"I have a doctor's appointment tomorrow morning.\\\"\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 889, "text": " I am struggling with grammar while writing. Thus, I want to create a grammar correction tool for myself.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Grammar Synthesis\\', \\'api_name\\': \\'pszemraj/flan-t5-large-grammar-synthesis\\', \\'api_call\\': \"pipeline(\\'text2text-generation\\', \\'pszemraj/flan-t5-large-grammar-synthesis\\')\", \\'api_arguments\\': [\\'raw_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import pipeline\\\\n\\\\ncorrector = pipeline(\\\\n \\'text2text-generation\\',\\\\n \\'pszemraj/flan-t5-large-grammar-synthesis\\',\\\\n )\\\\n\\\\nraw_text = \\'i can has cheezburger\\'\\\\nresults = corrector(raw_text)\\\\nprint(results)\", \\'performance\\': {\\'dataset\\': \\'jfleg\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'A fine-tuned version of google/flan-t5-large for grammar correction on an expanded version of the JFLEG dataset.\\'}', metadata={})]", "category": "generic"}
{"question_id": 890, "text": " Our company is cooperating with a German partner. We have some materials in English, but need to translate them into German.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'text2text-generation\\', \\'api_name\\': \\'csebuetnlp/mT5_multilingual_XLSum\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'csebuetnlp/mT5_multilingual_XLSum\\')\", \\'api_arguments\\': [\\'model_name\\'], \\'python_environment_requirements\\': [\\'transformers==4.11.0.dev0\\'], \\'example_code\\': \"import re\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nWHITESPACE_HANDLER = lambda k: re.sub(\\'\\\\\\\\s+\\', \\' \\', re.sub(\\'\\\\\\\\n+\\', \\' \\', k.strip()))\\\\narticle_text = Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said. The policy includes the termination of accounts of anti-vaccine influencers. Tech giants have been criticised for not doing more to counter false health information on their sites. In July, US President Joe Biden said social media platforms were largely responsible for people\\'s scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue. YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines. In a blog post, the company said it had seen false claims about Covid jabs spill over into misinformation about vaccines in general. The new policy covers long-approved vaccines, such as those against measles or hepatitis B. We\\'re expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO, the post said, referring to the World Health Organization.\\\\nmodel_name = csebuetnlp/mT5_multilingual_XLSum\\\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\\\ninput_ids = tokenizer(\\\\n [WHITESPACE_HANDLER(article_text)],\\\\n return_tensors=pt,\\\\n padding=max_length,\\\\n truncation=True,\\\\n max_length=512\\\\n)[input_ids]\\\\noutput_ids = model.generate(\\\\n input_ids=input_ids,\\\\n max_length=84,\\\\n no_repeat_ngram_size=2,\\\\n num_beams=4\\\\n)[0]\\\\nsummary = tokenizer.decode(\\\\n output_ids,\\\\n skip_special_tokens=True,\\\\n clean_up_tokenization_spaces=False\\\\n)\\\\nprint(summary)\", \\'performance\\': {\\'dataset\\': \\'xsum\\', \\'accuracy\\': {\\'ROUGE-1\\': 36.5, \\'ROUGE-2\\': 13.934, \\'ROUGE-L\\': 28.988, \\'ROUGE-LSUM\\': 28.996, \\'loss\\': 2.067, \\'gen_len\\': 26.973}}, \\'description\\': \\'This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. It is a multilingual abstractive summarization model that supports text-to-text generation for 43 languages.\\'}', metadata={})]", "category": "generic"}
{"question_id": 891, "text": " The publisher has sent us a draft of an article, but some of the words have been masked. We need to identify the masked words.\\n###Input: \\\"<mask> are large, slow-moving reptiles native to the southeastern United States. They are well-adapted to life in <mask>, and they are a common sight in swamps, rivers, and lakes.\\\" \\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 892, "text": " Our company is exploring the Chinese market and we need to communicate effectively with our clients. Help me create a generic Chinese response.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'financial-sentiment-analysis\\', \\'api_name\\': \\'yiyanghkust/finbert-tone\\', \\'api_call\\': \"BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\", \\'api_arguments\\': [\\'sentences\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \"from transformers import BertTokenizer, BertForSequenceClassification\\\\nfrom transformers import pipeline\\\\nfinbert = BertForSequenceClassification.from_pretrained(\\'yiyanghkust/finbert-tone\\',num_labels=3)\\\\ntokenizer = BertTokenizer.from_pretrained(\\'yiyanghkust/finbert-tone\\')\\\\nnlp = pipeline(sentiment-analysis, model=finbert, tokenizer=tokenizer)\\\\nsentences = [there is a shortage of capital, and we need extra financing,\\\\n growth is strong and we have plenty of liquidity,\\\\n there are doubts about our finances,\\\\n profits are flat]\\\\nresults = nlp(sentences)\\\\nprint(results)\", \\'performance\\': {\\'dataset\\': \\'10,000 manually annotated sentences from analyst reports\\', \\'accuracy\\': \\'superior performance on financial tone analysis task\\'}, \\'description\\': \\'FinBERT is a BERT model pre-trained on financial communication text. It is trained on the following three financial communication corpus: Corporate Reports 10-K & 10-Q, Earnings Call Transcripts, and Analyst Reports. This released finbert-tone model is the FinBERT model fine-tuned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports. This model achieves superior performance on financial tone analysis task.\\'}', metadata={})]", "category": "generic"}
{"question_id": 893, "text": " We are an educational platform improving student writing skills. We need a program that can fill in the blanks in sentences.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 894, "text": " I am building a plagiarism-detection tool. I need to evaluate the similarity between two sentences.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentence Correction\\', \\'api_name\\': \\'flexudy/t5-base-multi-sentence-doctor\\', \\'api_call\\': \"AutoModelWithLMHead.from_pretrained(\\'flexudy/t5-base-multi-sentence-doctor\\')\", \\'api_arguments\\': [\\'input_text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelWithLMHead\\\\ntokenizer = AutoTokenizer.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\nmodel = AutoModelWithLMHead.from_pretrained(flexudy/t5-base-multi-sentence-doctor)\\\\ninput_text = repair_sentence: m a medical doct context: {That is my job I a}{or I save lives} </s>\\\\ninput_ids = tokenizer.encode(input_text, return_tensors=pt)\\\\noutputs = model.generate(input_ids, max_length=32, num_beams=1)\\\\nsentence = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\\\\nassert sentence == I am a medical doctor.\\', \\'performance\\': {\\'dataset\\': \\'tatoeba\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'Sentence doctor is a T5 model that attempts to correct the errors or mistakes found in sentences. Model works on English, German and French text.\\'}', metadata={})]", "category": "generic"}
{"question_id": 895, "text": " We are working on creating an audiobook. Convert this text: \\\"The sun was shining brightly, and the birds were singing sweetly\\\" into speech.\\n###Input: The sun was shining brightly, and the birds were singing sweetly.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Image Segmentation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'facebook/mask2former-swin-large-coco-panoptic\\', \\'api_call\\': \"Mask2FormerForUniversalSegmentation.from_pretrained(\\'facebook/mask2former-swin-large-coco-panoptic\\')\", \\'api_arguments\\': [\\'image\\'], \\'python_environment_requirements\\': [\\'requests\\', \\'torch\\', \\'PIL\\', \\'transformers\\'], \\'example_code\\': \\'processor = AutoImageProcessor.from_pretrained(facebook/mask2former-swin-large-coco-panoptic)\\\\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained(facebook/mask2former-swin-large-coco-panoptic)\\\\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\\\\nimage = Image.open(requests.get(url, stream=True).raw)\\\\ninputs = processor(images=image, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n    outputs = model(**inputs)\\\\nclass_queries_logits = outputs.class_queries_logits\\\\nmasks_queries_logits = outputs.masks_queries_logits\\\\nresult = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\\\\npredicted_panoptic_map = result[segmentation]\\', \\'performance\\': {\\'dataset\\': \\'COCO\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Mask2Former model trained on COCO panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.\\'}', metadata={})]", "category": "generic"}
{"question_id": 896, "text": " A researcher needs information about how to use the Whisper ASR model to transcribe and analyze the sentiment of an audio file.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'financial-sentiment-analysis\\', \\'api_name\\': \\'ProsusAI/finbert\\', \\'api_call\\': \"AutoModelForSequenceClassification.from_pretrained(\\'ProsusAI/finbert\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \"from transformers import pipeline; classifier = pipeline(\\'sentiment-analysis\\', model=\\'ProsusAI/finbert\\'); classifier(\\'your_text_here\\')\", \\'performance\\': {\\'dataset\\': \\'Financial PhraseBank\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'FinBERT is a pre-trained NLP model to analyze sentiment of financial text. It is built by further training the BERT language model in the finance domain, using a large financial corpus and thereby fine-tuning it for financial sentiment classification. Financial PhraseBank by Malo et al. (2014) is used for fine-tuning.\\'}', metadata={})]", "category": "generic"}
{"question_id": 897, "text": " I recently interviewed a person in Japanese. I need to transcribe the interview in order to find relevant quotes for my article.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text-to-Text Generation\\', \\'api_name\\': \\'philschmid/bart-large-cnn-samsum\\', \\'api_call\\': \"pipeline(\\'summarization\\', model=\\'philschmid/bart-large-cnn-samsum\\')\", \\'api_arguments\\': {\\'model\\': \\'philschmid/bart-large-cnn-samsum\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'latest\\'}, \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/bart-large-cnn-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'eval_rouge1\\': 42.621, \\'eval_rouge2\\': 21.9825, \\'eval_rougeL\\': 33.034, \\'eval_rougeLsum\\': 39.6783, \\'test_rouge1\\': 41.3174, \\'test_rouge2\\': 20.8716, \\'test_rougeL\\': 32.1337, \\'test_rougeLsum\\': 38.4149}}, \\'description\\': \\'philschmid/bart-large-cnn-samsum is a BART-based model trained for text summarization on the SAMSum dataset. It can be used to generate abstractive summaries of conversations.\\'}', metadata={})]", "category": "generic"}
{"question_id": 898, "text": " Our startup produces a podcast editing software. We need to convert speech to text automatically for transcription purposes.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Automatic Speech Recognition\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transcription and Translation\\', \\'api_name\\': \\'openai/whisper-tiny\\', \\'api_call\\': \"WhisperForConditionalGeneration.from_pretrained(\\'openai/whisper-tiny\\')\", \\'api_arguments\\': {\\'forced_decoder_ids\\': \"WhisperProcessor.get_decoder_prompt_ids(language=\\'english\\', task=\\'transcribe\\')\"}, \\'python_environment_requirements\\': [\\'transformers\\', \\'datasets\\'], \\'example_code\\': [\\'from transformers import WhisperProcessor, WhisperForConditionalGeneration\\', \\'from datasets import load_dataset\\', \"processor = WhisperProcessor.from_pretrained(\\'openai/whisper-tiny\\')\", \"model = WhisperForConditionalGeneration.from_pretrained(\\'openai/whisper-tiny\\')\", \\'model.config.forced_decoder_ids = None\\', \"ds = load_dataset(\\'hf-internal-testing/librispeech_asr_dummy\\', \\'clean\\', split=\\'validation\\')\", \"sample = ds[0][\\'audio\\']\", \"input_features = processor(sample[\\'array\\'], sampling_rate=sample[\\'sampling_rate\\'], return_tensors=\\'pt\\').input_features\", \\'predicted_ids = model.generate(input_features)\\', \\'transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\\'], \\'performance\\': {\\'dataset\\': \\'LibriSpeech (clean)\\', \\'accuracy\\': 7.54}, \\'description\\': \\'Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need for fine-tuning. It is a Transformer-based encoder-decoder model that can be used for transcription and translation tasks.\\'}', metadata={})]", "category": "generic"}
{"question_id": 899, "text": " Lately, the quality of our audio has degraded due to background noise, and our robot needs to process speech to understand voice commands. Please help.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 900, "text": " Create a recommendation engine for a podcast platform that enhances the audio quality of low-quality recordings before recommending it to users seeking high-quality content.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 901, "text": " Create a system that translates and synthesizes speech from one language to another using the given model.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Program Synthesis\\', \\'api_name\\': \\'Salesforce/codegen-350M-multi\\', \\'api_call\\': \"AutoTokenizer.from_pretrained(\\'Salesforce/codegen-350M-multi\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'from transformers import AutoTokenizer, AutoModelForCausalLM\\\\ntokenizer = AutoTokenizer.from_pretrained(Salesforce/codegen-350M-multi)\\\\nmodel = AutoModelForCausalLM.from_pretrained(Salesforce/codegen-350M-multi)\\\\ntext = def hello_world():\\\\ninput_ids = tokenizer(text, return_tensors=pt).input_ids\\\\ngenerated_ids = model.generate(input_ids, max_length=128)\\\\nprint(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\\', \\'performance\\': {\\'dataset\\': \\'HumanEval and MTPB\\', \\'accuracy\\': \\'Refer to the paper for accuracy details\\'}, \\'description\\': \\'CodeGen is a family of autoregressive language models for program synthesis. The checkpoint included in this repository is denoted as CodeGen-Multi 350M, where Multi means the model is initialized with CodeGen-NL 350M and further pre-trained on a dataset of multiple programming languages, and 350M refers to the number of trainable parameters. The model is capable of extracting features from given natural language and programming language texts, and calculating the likelihood of them. It is best at program synthesis, generating executable code given English prompts, and can complete partially-generated code as well.\\'}', metadata={})]", "category": "generic"}
{"question_id": 902, "text": " I have a single channel audio recording containing the voices of two speakers very close, and I would like to separate the voices in this audio recording.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Audio Automatic Speech Recognition\\', \\'framework\\': \\'pyannote.audio\\', \\'functionality\\': \\'overlapped-speech-detection\\', \\'api_name\\': \\'pyannote/overlapped-speech-detection\\', \\'api_call\\': \"pipeline.from_pretrained(\\'pyannote/overlapped-speech-detection\\', use_auth_token=\\'ACCESS_TOKEN_GOES_HERE\\')\", \\'api_arguments\\': [\\'audio.wav\\'], \\'python_environment_requirements\\': [\\'pyannote.audio 2.1\\'], \\'example_code\\': \\'from pyannote.audio import Pipeline\\\\npipeline = Pipeline.from_pretrained(pyannote/overlapped-speech-detection, use_auth_token=ACCESS_TOKEN_GOES_HERE)\\\\noutput = pipeline(audio.wav)\\\\nfor speech in output.get_timeline().support():\\\\n  # two or more speakers are active between speech.start and speech.end\\\\n  ...\\', \\'performance\\': {\\'dataset\\': \\'ami\\', \\'accuracy\\': None}, \\'description\\': \\'Automatic overlapped speech detection using pyannote.audio framework. The model detects when two or more speakers are active in an audio file.\\'}', metadata={})]", "category": "generic"}
{"question_id": 903, "text": " We are building an app to improve people's public speaking skills. We want to analyze their emotional speech as feedback.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 904, "text": " A psychology company is building a revolutionary means to detect emotions of its clients. Help them create a system to understand emotions from spoken words.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'michellejieli/emotion_text_classifier\\', \\'api_call\\': \"pipeline(\\'sentiment-analysis\\', model=\\'michellejieli/emotion_text_classifier\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'classifier(I love this!)\\', \\'performance\\': {\\'dataset\\': [\\'Crowdflower (2016)\\', \\'Emotion Dataset, Elvis et al. (2018)\\', \\'GoEmotions, Demszky et al. (2020)\\', \\'ISEAR, Vikash (2018)\\', \\'MELD, Poria et al. (2019)\\', \\'SemEval-2018, EI-reg, Mohammad et al. (2018)\\', \\'Emotion Lines (Friends)\\'], \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'DistilRoBERTa-base is a transformer model that performs sentiment analysis. I fine-tuned the model on transcripts from the Friends show with the goal of classifying emotions from text data, specifically dialogue from Netflix shows or movies. The model predicts 6 Ekman emotions and a neutral class. These emotions include anger, disgust, fear, joy, neutrality, sadness, and surprise.\\'}', metadata={})]", "category": "generic"}
{"question_id": 905, "text": " We are a company manufacturing AI-based toothbrushes for children. We want to analyze the emotion of children while they brush their teeth.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'hyunwoongko/blenderbot-9B\\', \\'api_call\\': \"pipeline(\\'conversational\\', model=\\'hyunwoongko/blenderbot-9B\\')\", \\'api_arguments\\': \\'text\\', \\'python_environment_requirements\\': \\'transformers\\', \\'example_code\\': \\'Input a message to start chatting with hyunwoongko/blenderbot-9B.\\', \\'performance\\': {\\'dataset\\': \\'blended_skill_talk\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\\'}', metadata={})]", "category": "generic"}
{"question_id": 906, "text": " I am a hearing impaired individual who relies on technology to lead a qualitative life. Develop an application that measures noise levels in the environment to help me decide if it's suitable for communication.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Computer Vision Video Classification\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Video Classification\\', \\'api_name\\': \\'videomae-small-finetuned-ssv2\\', \\'api_call\\': \"VideoMAEForVideoClassification.from_pretrained(\\'MCG-NJU/videomae-small-finetuned-ssv2\\')\", \\'api_arguments\\': {\\'model_name\\': \\'MCG-NJU/videomae-small-finetuned-ssv2\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\\', \\'numpy\\': \\'import numpy as np\\', \\'torch\\': \\'import torch\\'}, \\'example_code\\': \\'video = list(np.random.randn(16, 3, 224, 224))\\\\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\nmodel = VideoMAEForVideoClassification.from_pretrained(MCG-NJU/videomae-small-finetuned-ssv2)\\\\ninputs = feature_extractor(video, return_tensors=pt)\\\\nwith torch.no_grad():\\\\n  outputs = model(**inputs)\\\\n  logits = outputs.logits\\\\npredicted_class_idx = logits.argmax(-1).item()\\\\nprint(Predicted class:, model.config.id2label[predicted_class_idx])\\', \\'performance\\': {\\'dataset\\': \\'Something-Something V2\\', \\'accuracy\\': {\\'top-1\\': 66.8, \\'top-5\\': 90.3}}, \\'description\\': \\'VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.\\'}', metadata={})]", "category": "generic"}
{"question_id": 907, "text": " Can you classify the audio clip to determine whether it is silent or contains speech?\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Conversational\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Transformers\\', \\'api_name\\': \\'ToddGoldfarb/Cadet-Tiny\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'ToddGoldfarb/Cadet-Tiny\\', low_cpu_mem_usage=True)\", \\'api_arguments\\': {\\'pretrained_model\\': \\'t5-small\\', \\'model_max_length\\': 512}, \\'python_environment_requirements\\': {\\'torch\\': \\'\\', \\'transformers\\': \\'\\', \\'colorful\\': \\'\\'}, \\'example_code\\': \"import torch\\\\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\nimport colorful as cf\\\\ncf.use_true_colors()\\\\ncf.use_style(\\'monokai\\')\\\\nclass CadetTinyAgent:\\\\n def <strong>init</strong>(self):\\\\n  print(cf.bold | cf.purple(Waking up Cadet-Tiny...))\\\\n  self.device = torch.device(cuda if torch.cuda.is_available() else cpu)\\\\n  self.tokenizer = AutoTokenizer.from_pretrained(t5-small, model_max_length=512)\\\\n  self.model = AutoModelForSeq2SeqLM.from_pretrained(ToddGoldfarb/Cadet-Tiny, low_cpu_mem_usage=True).to(self.device)\\\\n  self.conversation_history = \\\\ndef observe(self, observation):\\\\n self.conversation_history = self.conversation_history + observation\\\\n # The number 400 below is just a truncation safety net. It leaves room for 112 input tokens.\\\\n if len(self.conversation_history) &gt; 400:\\\\n self.conversation_history = self.conversation_history[112:]\\\\ndef set_input(self, situation_narrative=, role_instruction=):\\\\n input_text = dialogue: \\\\n if situation_narrative != :\\\\n input_text = input_text + situation_narrative\\\\n if role_instruction != :\\\\n input_text = input_text +  &lt;SEP&gt;  + role_instruction\\\\n input_text = input_text +  &lt;TURN&gt;  + self.conversation_history\\\\n # Uncomment the line below to see what is fed to the model.\\\\n # print(input_text)\\\\n return input_text\\\\ndef generate(self, situation_narrative, role_instruction, user_response):\\\\n user_response = user_response +  &lt;TURN&gt; \\\\n self.observe(user_response)\\\\n input_text = self.set_input(situation_narrative, role_instruction)\\\\n inputs = self.tokenizer([input_text], return_tensors=pt).to(self.device)\\\\n # I encourage you to change the hyperparameters of the model! Start by trying to modify the temperature.\\\\n outputs = self.model.generate(inputs[input_ids], max_new_tokens=512, temperature=1, top_p=.95,\\\\n do_sample=True)\\\\n cadet_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\\\\n added_turn = cadet_response +  &lt;TURN&gt; \\\\n self.observe(added_turn)\\\\n return cadet_response\\\\ndef reset_history(self):\\\\n self.conversation_history = []\\\\ndef run(self):\\\\n def get_valid_input(prompt, default):\\\\n  while True:\\\\n   user_input = input(prompt)\\\\n   if user_input in [Y, N, y, n]:\\\\n    return user_input\\\\n   if user_input == :\\\\n    return default\\\\n  while True:\\\\n   continue_chat = \\\\n   # MODIFY THESE STRINGS TO YOUR LIKING :)\\\\n   situation_narrative = Imagine you are Cadet-Tiny talking to ???.\\\\n   role_instruction = You are Cadet-Tiny, and you are talking to ???.\\\\n   self.chat(situation_narrative, role_instruction)\\\\n   continue_chat = get_valid_input(cf.purple(Start a new conversation with new setup? [Y/N]:), Y)\\\\n   if continue_chat in [N, n]:\\\\n    break\\\\n   print(cf.blue(CT: See you!))\\\\ndef chat(self, situation_narrative, role_instruction):\\\\n print(cf.green(\\\\n  Cadet-Tiny is running! Input [RESET] to reset the conversation history and [END] to end the conversation.))\\\\n while True:\\\\n  user_input = input(You: )\\\\n  if user_input == [RESET]:\\\\n   self.reset_history()\\\\n   print(cf.green([Conversation history cleared. Chat with Cadet-Tiny!]))\\\\n   continue\\\\n  if user_input == [END]:\\\\n   break\\\\n  response = self.generate(situation_narrative, role_instruction, user_input)\\\\n  print(cf.blue(CT:  + response))\\\\ndef main():\\\\n print(cf.bold | cf.blue(LOADING MODEL))\\\\nCadetTiny = CadetTinyAgent()\\\\nCadetTiny.run()\\\\nif <strong>name</strong> == \\'<strong>main</strong>\\':\\\\n main()\", \\'performance\\': {\\'dataset\\': \\'allenai/soda\\', \\'accuracy\\': \\'\\'}, \\'description\\': \\'Cadet-Tiny is a very small conversational model trained off of the SODA dataset. Cadet-Tiny is intended for inference at the edge (on something as small as a 2GB RAM Raspberry Pi). Cadet-Tiny is trained off of the t5-small pretrained model from Google, and is, as a result, is about 2% of the size of the Cosmo-3B model.\\'}', metadata={})]", "category": "generic"}
{"question_id": 908, "text": " I need to predict digit categories based on some tabular data inputs. I need a pretrained model to use for classification.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Summarization\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Text-to-Text Generation\\', \\'api_name\\': \\'philschmid/bart-large-cnn-samsum\\', \\'api_call\\': \"pipeline(\\'summarization\\', model=\\'philschmid/bart-large-cnn-samsum\\')\", \\'api_arguments\\': {\\'model\\': \\'philschmid/bart-large-cnn-samsum\\'}, \\'python_environment_requirements\\': {\\'transformers\\': \\'latest\\'}, \\'example_code\\': \"from transformers import pipeline\\\\nsummarizer = pipeline(summarization, model=philschmid/bart-large-cnn-samsum)\\\\nconversation = \\'\\'\\'Jeff: Can I train a \ud83e\udd17 Transformers model on Amazon SageMaker? \\\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\\\nJeff: ok.\\\\nJeff: and how can I get started? \\\\nJeff: where can I find documentation? \\\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face <br />\\\\n\\'\\'\\'\\\\nsummarizer(conversation)\", \\'performance\\': {\\'dataset\\': \\'samsum\\', \\'accuracy\\': {\\'eval_rouge1\\': 42.621, \\'eval_rouge2\\': 21.9825, \\'eval_rougeL\\': 33.034, \\'eval_rougeLsum\\': 39.6783, \\'test_rouge1\\': 41.3174, \\'test_rouge2\\': 20.8716, \\'test_rougeL\\': 32.1337, \\'test_rougeLsum\\': 38.4149}}, \\'description\\': \\'philschmid/bart-large-cnn-samsum is a BART-based model trained for text summarization on the SAMSum dataset. It can be used to generate abstractive summaries of conversations.\\'}', metadata={})]", "category": "generic"}
{"question_id": 909, "text": " In surveillance operations, transcripts are used to turn the audio feed into chunks such that after processing transcripts there are no speakers talking over each other in the output.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Tabular Tabular Classification\\', \\'framework\\': \\'Keras\\', \\'functionality\\': \\'Structured data learning with TabTransformer\\', \\'api_name\\': \\'keras-io/tab_transformer\\', \\'api_call\\': \\'TabTransformer.from_config()\\', \\'api_arguments\\': \\'N/A\\', \\'python_environment_requirements\\': \\'Hugging Face\\', \\'example_code\\': \\'N/A\\', \\'performance\\': {\\'dataset\\': \\'United States Census Income Dataset\\', \\'accuracy\\': \\'N/A\\'}, \\'description\\': \"This model uses self-attention based Transformers structure followed by multiple feed forward layers to serve supervised and semi-supervised learning tasks on tabular data. The model\\'s inputs can contain both numerical and categorical features. Categorical features are encoded into embedding vectors before being fed into a stack of Transformer blocks. The contextual embeddings of the categorical features after the final Transformer layer are concatenated with the input numerical features and fed into a final MLP block. A SoftMax function is applied at the end of the model.\"}', metadata={})]", "category": "generic"}
{"question_id": 910, "text": " I work at GreenTech, a startup that provides eco-friendly solutions, and need to determine if a given set of input data will result in high carbon emissions or not.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text Classification\\', \\'framework\\': \\'Transformers\\', \\'functionality\\': \\'Sentiment Analysis\\', \\'api_name\\': \\'michellejieli/NSFW_text_classifier\\', \\'api_call\\': \"pipeline(\\'sentiment-analysis\\', model=\\'michellejieli/NSFW_text_classification\\')\", \\'api_arguments\\': [\\'text\\'], \\'python_environment_requirements\\': [\\'transformers\\'], \\'example_code\\': \\'classifier(I see you\u2019ve set aside this special time to humiliate yourself in public.)\\', \\'performance\\': {\\'dataset\\': \\'Reddit posts\\', \\'accuracy\\': \\'Not specified\\'}, \\'description\\': \\'DistilBERT is a transformer model that performs sentiment analysis. I fine-tuned the model on Reddit posts with the purpose of classifying not safe for work (NSFW) content, specifically text that is considered inappropriate and unprofessional. The model predicts 2 classes, which are NSFW or safe for work (SFW). The model is a fine-tuned version of DistilBERT. It was fine-tuned on 14317 Reddit posts pulled from the Reddit API.\\'}', metadata={})]", "category": "generic"}
{"question_id": 911, "text": " Can you figure a way to predict electricity consumption? We need to better allocate resources.\\n \n Use this API documentation for reference: [Document(page_content='{\\'domain\\': \\'Natural Language Processing Text2Text Generation\\', \\'framework\\': \\'Hugging Face Transformers\\', \\'functionality\\': \\'Conversational\\', \\'api_name\\': \\'microsoft/GODEL-v1_1-large-seq2seq\\', \\'api_call\\': \"AutoModelForSeq2SeqLM.from_pretrained(\\'microsoft/GODEL-v1_1-large-seq2seq\\')\", \\'api_arguments\\': {\\'instruction\\': \\'Instruction: given a dialog context, you need to response empathically.\\', \\'knowledge\\': \\'\\', \\'dialog\\': [\\'Does money buy happiness?\\', \\'It is a question. Money buys you a lot of things, but not enough to buy happiness.\\', \\'What is the best way to buy happiness ?\\']}, \\'python_environment_requirements\\': {\\'transformers\\': \\'AutoTokenizer, AutoModelForSeq2SeqLM\\'}, \\'example_code\\': \"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\\\ntokenizer = AutoTokenizer.from_pretrained(microsoft/GODEL-v1_1-large-seq2seq)\\\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(microsoft/GODEL-v1_1-large-seq2seq)\\\\ndef generate(instruction, knowledge, dialog):\\\\n if knowledge != \\'\\':\\\\n knowledge = \\'[KNOWLEDGE] \\' + knowledge\\\\n dialog = \\' EOS \\'.join(dialog)\\\\n query = f{instruction} [CONTEXT] {dialog} {knowledge}\\\\n input_ids = tokenizer(f{query}, return_tensors=pt).input_ids\\\\n outputs = model.generate(input_ids, max_length=128, min_length=8, top_p=0.9, do_sample=True)\\\\n output = tokenizer.decode(outputs[0], skip_special_tokens=True)\\\\n return output\\\\n\\\\ninstruction = f\\'Instruction: given a dialog context, you need to response empathically.\\'\\\\nknowledge = \\'\\'\\\\ndialog = [\\\\n \\'Does money buy happiness?\\',\\\\n \\'It is a question. Money buys you a lot of things, but not enough to buy happiness.\\',\\\\n \\'What is the best way to buy happiness ?\\'\\\\n]\\\\nresponse = generate(instruction, knowledge, dialog)\\\\nprint(response)\", \\'performance\\': {\\'dataset\\': \\'Reddit discussion thread, instruction and knowledge grounded dialogs\\', \\'accuracy\\': \\'Not provided\\'}, \\'description\\': \\'GODEL is a large-scale pre-trained model for goal-directed dialogs. It is parameterized with a Transformer-based encoder-decoder model and trained for response generation grounded in external text, which allows more effective fine-tuning on dialog tasks that require conditioning the response on information that is external to the current conversation (e.g., a retrieved document). The pre-trained model can be efficiently fine-tuned and adapted to accomplish a new dialog task with a handful of task-specific dialogs. The v1.1 model is trained on 551M multi-turn dialogs from Reddit discussion thread, and 5M instruction and knowledge grounded dialogs.\\'}', metadata={})]", "category": "generic"}
