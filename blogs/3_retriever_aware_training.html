<!DOCTYPE html>
<html>
<head>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-NRZJLJCSH6"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-NRZJLJCSH6');
    </script>


    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, shrink-to-fit=no">
    <title>RAT (Retrieval Aware Training)</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.5.0/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro">
    <link rel="stylesheet" href="../assets/css/Highlight-Clean.css">
    <link rel="stylesheet" href="../assets/css/styles.css">
    <link rel="stylesheet" href="../assets/css/Team-Clean.css">

</head>

<body>
    <!-- Navigation Bar -->
    <div class="navbar" style="position: absolute; top: 0; right: 20px; padding: 10px; z-index: 100;; font-size: 18px;">
        <a href="/index.html">Home</a>
        <a href="/blog.html">Blogs</a>
        <a href="/leaderboard.html">BFCL Leaderboard</a>
        <!-- <a href="/apizoo/">API Zoo Index</a> -->
    </div>
    <div class="highlight-clean-blog" style="padding-bottom: 10px;">
        <h1 class="text-center" style="padding-bottom: 10px;"> ü¶ç Gorilla: Large Language Model Connected with Massive APIs</h1>

        <div class="box-index">
            <h3>Blog 3: Retrieval Aware Training (RAT)</h3>
            <ul>
                <ul>
                        <li><a href="3_retreiver_aware_training.html#traditional-drawbacks">Drawbacks of Traditional Tuning</a></li>
                        <li><a href="3_retreiver_aware_training.html#retriever-aware">Embracing Retriever-Aware Training</a></li>
                        <li><a href="3_retreiver_aware_training.html#free-lunch">A free lunch for RAT?</a></li>
                        <li><a href="3_retreiver_aware_training.html#evolving-api">Keeping up with frequently evolving APIs</a></li>
                        <li><a href="3_retreiver_aware_training.html#llm-and-retrievers">LLMs and Retrievers</a></li>
                        <li><a href="3_retreiver_aware_training.html#hallucinations">Hallucinations</a></li>
                        <li class="more-blogs">
                            <a href="javascript:void(0);" onclick="toggleMoreBlogs()">More Blogs <span class="caret">&#9654;</span></a>
                            <ul class="sub-menu">
                                <li><a href="1_gorilla_intro.html">Introduction to Gorilla LLM</a></li>
                                <li><a href="2_hallucination.html">How to Measure Hallucination?</a></li>
                                <!-- Add more blog entries as needed -->
                            </ul>
                        </li>  
                </ul>
                <!-- Add more entries as needed -->
            </ul>
        </div>

        <div class="blog-container">
            <div class="blog-post">
                <h2 class="blog-title">Retriever-Aware Training (RAT): Are LLMs memorizing or understanding?</h2>
                <div class="author-date">
                    <p class="author"> <a href="https://hqjenny.com/"> Qijing Huang </a>  </p>
                    <p class="date">Oct 4, 2023</p>
                </div>
                <!-- Image here -->
                <div class="preview">
                    <p>
                        Pretrained Language Models (LLMs) need instruction tuning to better align with human incentives. These methods both improve the model‚Äôs behavior and accuracy when they are trying to answer questions for a specific domain. However, traditional instruction tuning has limitations regarding adaptability, dependency on the in-context examples, and the potential to hallucinate. We introduce "retriever-aware training," a new methodology that holds the promise of addressing some of these challenges. Let's dive into the details of that.
                    </p>  
                    <h4 id="traditional-drawbacks">The Drawbacks of Traditional Instruction Tuning</h4>
                    <p>
                        At its core, instruction tuning allows LLMs to generate responses based on specific instructions embedded within the prompts. However, this approach has its limitations:
                        <ul>
                            <li>Limited Adaptability: Traditional LLMs may struggle when faced with real-time changes or updates in the information. This can be particularly concerning when using evolving data sources, such as API documentation.
                            </li>
                            <li>Dependence on In-Context Learning: Instruction tuning relies heavily on in-context learning, which can be restrictive. Without the ability to pivot based on fresh or updated data, the model might produce outdated or inaccurate outputs.
                            </li>
                            <li>Potential for Hallucination: One of the significant challenges with LLMs is their tendency to "hallucinate" or generate incorrect or unrelated information. Traditional tuning methods have been known to exacerbate this issue.
                            </li>
                        </ul>
                    </p>

                    <h4 id="retriever-aware">Embracing Retriever-Aware Training</h4>
                    <p>
                        With the above challenges in mind, retriever-aware training was introduced. 
                        The principle behind this method is to append additional supporting documentation to the user's prompt, 
                        such as "Use this API documentation for reference: <retrieved_API_doc_JSON>." 
                        The goal is to teach the LLM to utilize the supporting documents provided 
                        (the reference to API documentation) to generate a more informed answer to the first half.

                        The advantages of this approach are manifold:

                        <ul>
                            <li>Adaptability: The model becomes more adaptable to changes in API documentation at test-time. This means that even if an API undergoes changes, the model can reference the latest documentation to provide accurate responses.
                            </li>
                            <li>Improved In-Context Learning: By referencing external documentation, the LLM can go beyond its inherent knowledge and pull from real-time data, leading to more accurate and updated responses.
                            </li>
                            <li>Reduction in Hallucination: A key benefit noted was the reduction in hallucination errors. By grounding responses in real-time data, the chances of the LLM generating false or unrelated information diminish.
                            </li>
                        </ul>
                    </p>
                </div>

                <div class="body">
                    <h4 id="free-lunch">A free lunch for RAT?</h4>
                    <p>
                        However, the promise of retriever-aware training is not fully utilized yet in reality. 
                        This is mainly because the accuracy of the retriever is not good enough. In other words, 
                        the recall for the retriever has become a bottleneck for the final performance of the LLM. 
                        Imagine the model would easily get confused if it got the question to 
                        ‚Äúlook up whether in Berkeley‚Äù but with the supporting documents of ‚Äúbiography of 
                        Albert Einstein‚Äù. Thus, balancing between the recall of the retriever and the frequency 
                        of updating LLMs is a choice to make.
                    </p>

                    <p>
                        Wrapping Up

                        As with all innovations, retriever-aware training comes with its set of pros and cons. 
                        But its introduction marks an exciting shift towards creating LLMs that are more adaptable, 
                        accurate, and less prone to errors. As we continue to refine this methodology, there's 
                        no doubt that the future of LLM training is brimming with potential.

                    </p>

                </div>

                <!-- APIs change frequently -->
                <p style="text-align: center; margin-bottom: 0">
                    <img src="../assets/img/blog_post_1_apichange.png" alt="APIs change frequently" width="95%">
                    <br>
                </p>
                <p width="80%" style="text-align:center; margin-left:10%; margin-right:10%; padding-bottom: -10px">
                    <i style="font-size: 0.9em;">
                        APIs evolve frequently! For example, there were 31 API modifications for AWS APIs just yesterday.
                    </i>
                </p>


                <h4 id="evolving-api"> Keeping up with frequently evolving APIs</h4>
                <div class="body">
                    <p>
                        APIs are known to evolve frequently - more frequently than it is possible to re-train LLMs. So, how can LLMs keep up with this, and not serve the user out-lawed APIs? To handle this, Gorilla, can be used for inference in two modes: <em>zero-shot</em> and <em>with retrieval</em>. In zero-shot, during inference, user provides the prompt in natural language. This can be for a simple task (e.g, "I would like to identify the objects in an image"), or you can specify a vague goal, (e.g, "I am going to the zoo, and would like to track animals"). This prompt (with NO further prompt tuning) is fed to the Gorilla LLM model which then returns the API call that will help in accomplishing the task and/or goal. 
                        In retrieval mode, the retriever first retrieves the most up-to-date API documentation stored in APIZoo, an API Database for LLMs.
                        Before being sent to Gorilla, the API documentation is concatenated to the user prompt along with the message "Use this API documentation for reference:"  The output of Gorilla is an API to be invoked. The retriever aware inference mode, enables Gorilla to be robust to frequent changes in APIs! We have open-sourced our APIZoo, and welcome contributions from the community!   
                    </p>

                </div>

                <!-- Retrievers and LLMs  -->
                <p style="text-align: center; margin-bottom: 0">
                    <img src="../assets/img/blog_post_1_inference.jpg" alt="Gorilla can be used in zero-shot and with retrievers" width="95%">
                    <br>
                </p>
                <p width="80%" style="text-align:center; margin-left:10%; margin-right:10%; padding-bottom: -10px">
                    <i style="font-size: 0.9em;">
                        Gorilla, can be used for inference in two modes: <em>zero-shot</em> and <em>with retrieval</em>. In zero-shot, the prompt is directly fed to the Gorilla LLM model. 
                        In retrieval mode, the retriever first retrieves the most up-to-date API documentation stored in APIZoo. 
                    </i>
                </p>

                <h4 id="llm-and-retrievers">Love at First Query: The Untold Bond of LLMs and Retrievers</h4>
                <div class="body">
                    <p>
                        If you are deploying LLMs in production today, you might be
                        augmenting your model with retrievers such as in Retriever Augmented Generation (RAG) paradigms. Given, most LLMs today are used with retrievers, shouldn't the training recipe for the LLM consider this!! In Gorilla, we consider retrievers to be first class citizens, and train our models to be <em>retriever aware</em>. If you are thinking about integrating LLMs with llamaindex, vector databases such as Weviate, etc check out our blog post on <a class="continue-link" href="blogs/blog_post_2_rat.html"> Retrieval Aware Training (RAT)</a> where we teach LLMs to "work-together" with retrievers! 
                    </p>
                </div>

                <!-- Gorilla side by side comparison -->
                <p style="text-align: center; margin-bottom: 0">
                    <img src="../assets/img/blog_post_1_result.png" alt="How well does Gorilla perform" width="65%">
                    <br>
                </p>
                <p width="80%" style="text-align:center; margin-left:10%; margin-right:10%; padding-bottom: -10px">
                    <i style="font-size: 0.9em;">
                        Examples of API calls. In this example, for the given prompt GPT-4 presents a model that doesn't exist, and Claude picks an incorrect library. In contrast, our Gorilla model can identify the task correctly and suggest a fully-qualified API call.
                    </i>
                </p>

                <h4 id="hallucinations">Reality Bytes: When LLMs See Things That Aren't There</h4>
                <div class="body">
                    <p>
                        Hallucination is the center of discussions for all things LLMs. In the context of API generation, hallucination can be defined as the model generating API calls that do not exist. An LLM generation can be in-accurate or it could be hallucinated. One does not mean the other. For example, if the user asks for a classifier for medical images, if the model generates a Stripe API call for a image classifier - it is hallucination, since it doesn't exist! On the other hand, if the model recommends to use the Stripe API for checking your balance, it is an incorrect usage of the API, but at least not made up (noh-hallucinated). In our blog <a href=""></a> we describe Gorilla's innovative approach of using Abstract Syntax Trees (ASTs) to measure hallucination of the generated API calls. 
                        Though not generalizable to all tasks, to the best of our knowledge, Gorilla is the first to measure and quantify hallucination for LLM generations! 

                    </p>
                </div>
            </div>
        </div>
    </div>

    <style>
        body {
            font-family: 'Source Sans Pro', sans-serif;
            margin: 0;
            padding: 0;
            background: white;
            justify-content: center;
            align-items: center;
        }
        .blog-container {
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
        }
        .blog-post {
            margin: 20px;
            padding: 20px;
            max-width: 1000px; 
            justify-content: center;
        }
        .blog-post img {
            display: block;
            margin: 0 auto;
        }
        .blog-title{
            color: #055ada;
            text-align: center;
        }

        .author-date {
                display: flex;
                margin-bottom: 0px;
                justify-content: center; 
        }
        .author {
                font-size: 16px;
                color: #1E90FF;
                margin-right: 20px;
        }

        .date {
            font-size: 16px;
            color: #7e8790;
        }

        .preview {
            text-align: justify; 
            text-justify: inter-word; 
        }

        .highlight-clean-blog {
            color: #313437;
            background-color: #fff;
            padding: 50px 0;
        }

        .box-index {
        position: fixed;
        top: 50%; 
        left: 0px; 
        transform: translateY(-50%);
        background-color: #f9f9f9;
        padding: 20px;
        border-radius: 8px;
        box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
        max-width: 150px;
        }

        .box-index h3 {
        font-size: 1.2em;
        margin-bottom: 10px;
        }

        .box-index ul {
        list-style-type: disc;
        padding: 0;
        }

        .box-index ul li {
        margin-bottom: 10px;
        }

        .box-index ul li a {
        text-decoration: none;
        color: #333;
        }

        .box-index ul li a:hover {
        color: #1E90FF;
        }

        
        .more-blogs .sub-menu {
            display: none;
        }

        .more-blogs .sub-menu.expanded {
            display: block;
            max-height: 200px; /* Adjust the max height as needed */
            overflow-y: auto;
        }

        .more-blogs .sub-menu li {
            padding: 10px;
            border-bottom: 1px solid #ccc;
        }

        .more-blogs .sub-menu li:last-child {
            border-bottom: none;
        }

        .more-blogs .caret {
            transition: transform 0.3s ease-in-out;
            display: inline-block;
            transform: rotate(0deg);
            font-size: 12px; /* Adjust the font size to change the caret size */
        }

        .more-blogs.expanded .caret {
            transform: rotate(90deg);
        }

        @media screen and (max-width: 768px) {
        .blog-post {
            padding: 10px; /* Adjust spacing for smaller screens */
        }
        .blog-post img {
            max-width: 80%; /* Reduce image size for smaller screens */
        }
        .box-index {
        display: none; /* Hide the index on smaller screens */
        }
    }

    </style>
</body>
</html>

<script>
    function toggleMoreBlogs() {
        var subMenu = document.querySelector('.more-blogs .sub-menu');
        var parentItem = document.querySelector('.more-blogs');
        subMenu.classList.toggle('expanded');
        parentItem.classList.toggle('expanded');
    }
</script>