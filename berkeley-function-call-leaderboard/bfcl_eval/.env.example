# Required for web search categories (see README.md)
SERPAPI_API_KEY=

# Provide the API key for the model(s) you intend to use
OPENAI_API_KEY=sk-XXXXXX
OPENAI_DEFAULT_HEADERS=
OPENAI_BASE_URL=

ANTHROPIC_API_KEY=
# We use Google AI Studio to inference Google Gemini models
GOOGLE_API_KEY=
NVIDIA_API_KEY=nvapi-XXXXXX
GROK_API_KEY=xai-XXXXXX
COHERE_API_KEY=
DEEPSEEK_API_KEY=sk-XXXXXX
# We use Alibaba Cloud (aliyun.com) to inference Qwen models
QWEN_API_KEY=sk-XXXXXX
GLM_API_KEY=sk-XXXXXX
KIMI_API_KEY=sk-XXXXXX
MISTRAL_API_KEY=
FIREWORKS_API_KEY=
WRITER_API_KEY=
GOGOAGENT_API_KEY=

NBG_API_KEY=sk-XXXXXX

MINING_BASE_URL=XXXXXX
MINING_API_KEY=sk-XXXXXX

DMCITO_BASE_URL=XXXXXX
DMCITO_API_KEY=sk-XXXXXX

# For Nova models, 
AWS_SSO_PROFILE_NAME=

# We use the API Key from Alipay to inference Bailing (Ling) models (see https://zxb.alipay.com/llm/landing)
LING_API_KEY=sk-XXXXXX

# [OPTIONAL] For inference via Novita AI endpoint
NOVITA_API_KEY=sk-XXXXXX

# [OPTIONAL] For local vllm/sglang server configuration
# Defaults to localhost port 1053 if not provided
LOCAL_SERVER_ENDPOINT=localhost
LOCAL_SERVER_PORT=1053

# [OPTIONAL] For custom local/remote OpenAI-compatible server configuration (e.g., vLLM deployments)
# These allow custom base URL and API key for OpenAI-compatible endpoints
# REMOTE_OPENAI_BASE_URL=https://your-vllm-server.com/v1
# REMOTE_OPENAI_API_KEY=your-api-key-here
# REMOTE_OPENAI_TOKENIZER_PATH=/path/to/local/tokenizer  # Optional: specify local tokenizer for local/remote endpoints

# [OPTIONAL] For WandB to log the generated .csv in the format 'entity:project
WANDB_BFCL_PROJECT=ENTITY:PROJECT
